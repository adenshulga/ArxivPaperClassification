{
  "best_global_step": null,
  "best_metric": 0.6472577015372045,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 12300,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00024390243902439024,
      "grad_norm": 0.8162382245063782,
      "learning_rate": 5e-05,
      "loss": 0.6843,
      "step": 1
    },
    {
      "epoch": 0.0004878048780487805,
      "grad_norm": 0.8356656432151794,
      "learning_rate": 4.999999918441327e-05,
      "loss": 0.6868,
      "step": 2
    },
    {
      "epoch": 0.0007317073170731707,
      "grad_norm": 0.8204846382141113,
      "learning_rate": 4.999999673765312e-05,
      "loss": 0.6504,
      "step": 3
    },
    {
      "epoch": 0.000975609756097561,
      "grad_norm": 0.7479256391525269,
      "learning_rate": 4.9999992659719716e-05,
      "loss": 0.6183,
      "step": 4
    },
    {
      "epoch": 0.0012195121951219512,
      "grad_norm": 0.7137321829795837,
      "learning_rate": 4.9999986950613324e-05,
      "loss": 0.592,
      "step": 5
    },
    {
      "epoch": 0.0014634146341463415,
      "grad_norm": 0.6979261040687561,
      "learning_rate": 4.999997961033432e-05,
      "loss": 0.5679,
      "step": 6
    },
    {
      "epoch": 0.0017073170731707317,
      "grad_norm": 0.7409339547157288,
      "learning_rate": 4.999997063888317e-05,
      "loss": 0.548,
      "step": 7
    },
    {
      "epoch": 0.001951219512195122,
      "grad_norm": 0.666620135307312,
      "learning_rate": 4.999996003626047e-05,
      "loss": 0.5285,
      "step": 8
    },
    {
      "epoch": 0.0021951219512195124,
      "grad_norm": 0.7006980180740356,
      "learning_rate": 4.9999947802466916e-05,
      "loss": 0.5165,
      "step": 9
    },
    {
      "epoch": 0.0024390243902439024,
      "grad_norm": 0.6598583459854126,
      "learning_rate": 4.9999933937503295e-05,
      "loss": 0.4987,
      "step": 10
    },
    {
      "epoch": 0.002682926829268293,
      "grad_norm": 0.6341462135314941,
      "learning_rate": 4.999991844137052e-05,
      "loss": 0.4806,
      "step": 11
    },
    {
      "epoch": 0.002926829268292683,
      "grad_norm": 0.6478189826011658,
      "learning_rate": 4.9999901314069596e-05,
      "loss": 0.4689,
      "step": 12
    },
    {
      "epoch": 0.0031707317073170734,
      "grad_norm": 0.6084445118904114,
      "learning_rate": 4.9999882555601646e-05,
      "loss": 0.4538,
      "step": 13
    },
    {
      "epoch": 0.0034146341463414634,
      "grad_norm": 0.6007123589515686,
      "learning_rate": 4.999986216596789e-05,
      "loss": 0.4455,
      "step": 14
    },
    {
      "epoch": 0.003658536585365854,
      "grad_norm": 0.5895159244537354,
      "learning_rate": 4.999984014516966e-05,
      "loss": 0.4298,
      "step": 15
    },
    {
      "epoch": 0.003902439024390244,
      "grad_norm": 0.5783035755157471,
      "learning_rate": 4.99998164932084e-05,
      "loss": 0.4206,
      "step": 16
    },
    {
      "epoch": 0.004146341463414634,
      "grad_norm": 0.5762036442756653,
      "learning_rate": 4.999979121008563e-05,
      "loss": 0.4072,
      "step": 17
    },
    {
      "epoch": 0.004390243902439025,
      "grad_norm": 0.567038357257843,
      "learning_rate": 4.999976429580302e-05,
      "loss": 0.3951,
      "step": 18
    },
    {
      "epoch": 0.004634146341463414,
      "grad_norm": 0.5512452125549316,
      "learning_rate": 4.999973575036233e-05,
      "loss": 0.3845,
      "step": 19
    },
    {
      "epoch": 0.004878048780487805,
      "grad_norm": 0.544802725315094,
      "learning_rate": 4.99997055737654e-05,
      "loss": 0.3731,
      "step": 20
    },
    {
      "epoch": 0.005121951219512195,
      "grad_norm": 0.5288815498352051,
      "learning_rate": 4.999967376601422e-05,
      "loss": 0.3656,
      "step": 21
    },
    {
      "epoch": 0.005365853658536586,
      "grad_norm": 0.5171306729316711,
      "learning_rate": 4.999964032711086e-05,
      "loss": 0.3572,
      "step": 22
    },
    {
      "epoch": 0.005609756097560975,
      "grad_norm": 0.5162968039512634,
      "learning_rate": 4.99996052570575e-05,
      "loss": 0.3429,
      "step": 23
    },
    {
      "epoch": 0.005853658536585366,
      "grad_norm": 0.5010064840316772,
      "learning_rate": 4.999956855585642e-05,
      "loss": 0.3363,
      "step": 24
    },
    {
      "epoch": 0.006097560975609756,
      "grad_norm": 0.5042251348495483,
      "learning_rate": 4.9999530223510036e-05,
      "loss": 0.322,
      "step": 25
    },
    {
      "epoch": 0.006341463414634147,
      "grad_norm": 0.487880140542984,
      "learning_rate": 4.999949026002083e-05,
      "loss": 0.32,
      "step": 26
    },
    {
      "epoch": 0.006585365853658536,
      "grad_norm": 0.47147971391677856,
      "learning_rate": 4.999944866539141e-05,
      "loss": 0.3082,
      "step": 27
    },
    {
      "epoch": 0.006829268292682927,
      "grad_norm": 0.4619201123714447,
      "learning_rate": 4.99994054396245e-05,
      "loss": 0.302,
      "step": 28
    },
    {
      "epoch": 0.007073170731707317,
      "grad_norm": 0.45354557037353516,
      "learning_rate": 4.999936058272292e-05,
      "loss": 0.2919,
      "step": 29
    },
    {
      "epoch": 0.007317073170731708,
      "grad_norm": 0.4422709047794342,
      "learning_rate": 4.999931409468959e-05,
      "loss": 0.2862,
      "step": 30
    },
    {
      "epoch": 0.007560975609756097,
      "grad_norm": 0.4344114661216736,
      "learning_rate": 4.9999265975527546e-05,
      "loss": 0.2791,
      "step": 31
    },
    {
      "epoch": 0.007804878048780488,
      "grad_norm": 0.4220210015773773,
      "learning_rate": 4.999921622523992e-05,
      "loss": 0.2683,
      "step": 32
    },
    {
      "epoch": 0.008048780487804878,
      "grad_norm": 0.4192688465118408,
      "learning_rate": 4.999916484382997e-05,
      "loss": 0.2652,
      "step": 33
    },
    {
      "epoch": 0.008292682926829269,
      "grad_norm": 0.4078800678253174,
      "learning_rate": 4.9999111831301035e-05,
      "loss": 0.2569,
      "step": 34
    },
    {
      "epoch": 0.00853658536585366,
      "grad_norm": 0.3983708322048187,
      "learning_rate": 4.9999057187656595e-05,
      "loss": 0.2494,
      "step": 35
    },
    {
      "epoch": 0.00878048780487805,
      "grad_norm": 0.4019240736961365,
      "learning_rate": 4.99990009129002e-05,
      "loss": 0.244,
      "step": 36
    },
    {
      "epoch": 0.009024390243902438,
      "grad_norm": 0.3845493793487549,
      "learning_rate": 4.999894300703553e-05,
      "loss": 0.2359,
      "step": 37
    },
    {
      "epoch": 0.009268292682926829,
      "grad_norm": 0.37132951617240906,
      "learning_rate": 4.999888347006635e-05,
      "loss": 0.2398,
      "step": 38
    },
    {
      "epoch": 0.00951219512195122,
      "grad_norm": 0.37000781297683716,
      "learning_rate": 4.999882230199655e-05,
      "loss": 0.2301,
      "step": 39
    },
    {
      "epoch": 0.00975609756097561,
      "grad_norm": 0.35786905884742737,
      "learning_rate": 4.9998759502830126e-05,
      "loss": 0.2215,
      "step": 40
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.35365962982177734,
      "learning_rate": 4.999869507257118e-05,
      "loss": 0.2136,
      "step": 41
    },
    {
      "epoch": 0.01024390243902439,
      "grad_norm": 0.3445499539375305,
      "learning_rate": 4.99986290112239e-05,
      "loss": 0.2088,
      "step": 42
    },
    {
      "epoch": 0.010487804878048781,
      "grad_norm": 0.33237871527671814,
      "learning_rate": 4.9998561318792606e-05,
      "loss": 0.2057,
      "step": 43
    },
    {
      "epoch": 0.010731707317073172,
      "grad_norm": 0.32796260714530945,
      "learning_rate": 4.999849199528172e-05,
      "loss": 0.2002,
      "step": 44
    },
    {
      "epoch": 0.01097560975609756,
      "grad_norm": 0.32720044255256653,
      "learning_rate": 4.9998421040695755e-05,
      "loss": 0.1915,
      "step": 45
    },
    {
      "epoch": 0.01121951219512195,
      "grad_norm": 0.3180735111236572,
      "learning_rate": 4.9998348455039343e-05,
      "loss": 0.1861,
      "step": 46
    },
    {
      "epoch": 0.011463414634146341,
      "grad_norm": 0.3109114170074463,
      "learning_rate": 4.999827423831723e-05,
      "loss": 0.1822,
      "step": 47
    },
    {
      "epoch": 0.011707317073170732,
      "grad_norm": 0.3017640709877014,
      "learning_rate": 4.999819839053424e-05,
      "loss": 0.1777,
      "step": 48
    },
    {
      "epoch": 0.011951219512195122,
      "grad_norm": 0.2928464710712433,
      "learning_rate": 4.9998120911695334e-05,
      "loss": 0.1779,
      "step": 49
    },
    {
      "epoch": 0.012195121951219513,
      "grad_norm": 0.2866354286670685,
      "learning_rate": 4.9998041801805566e-05,
      "loss": 0.1683,
      "step": 50
    },
    {
      "epoch": 0.012439024390243903,
      "grad_norm": 0.2880299389362335,
      "learning_rate": 4.99979610608701e-05,
      "loss": 0.1737,
      "step": 51
    },
    {
      "epoch": 0.012682926829268294,
      "grad_norm": 0.28353604674339294,
      "learning_rate": 4.99978786888942e-05,
      "loss": 0.1668,
      "step": 52
    },
    {
      "epoch": 0.012926829268292682,
      "grad_norm": 0.2689051330089569,
      "learning_rate": 4.9997794685883236e-05,
      "loss": 0.1675,
      "step": 53
    },
    {
      "epoch": 0.013170731707317073,
      "grad_norm": 0.27119073271751404,
      "learning_rate": 4.99977090518427e-05,
      "loss": 0.158,
      "step": 54
    },
    {
      "epoch": 0.013414634146341463,
      "grad_norm": 0.25477972626686096,
      "learning_rate": 4.999762178677817e-05,
      "loss": 0.1583,
      "step": 55
    },
    {
      "epoch": 0.013658536585365854,
      "grad_norm": 0.25583311915397644,
      "learning_rate": 4.9997532890695345e-05,
      "loss": 0.1548,
      "step": 56
    },
    {
      "epoch": 0.013902439024390244,
      "grad_norm": 0.2551009953022003,
      "learning_rate": 4.9997442363600024e-05,
      "loss": 0.1522,
      "step": 57
    },
    {
      "epoch": 0.014146341463414635,
      "grad_norm": 0.24554991722106934,
      "learning_rate": 4.9997350205498114e-05,
      "loss": 0.1426,
      "step": 58
    },
    {
      "epoch": 0.014390243902439025,
      "grad_norm": 0.24975089728832245,
      "learning_rate": 4.9997256416395626e-05,
      "loss": 0.1377,
      "step": 59
    },
    {
      "epoch": 0.014634146341463415,
      "grad_norm": 0.23286683857440948,
      "learning_rate": 4.9997160996298686e-05,
      "loss": 0.1423,
      "step": 60
    },
    {
      "epoch": 0.014878048780487804,
      "grad_norm": 0.2315455675125122,
      "learning_rate": 4.999706394521351e-05,
      "loss": 0.1324,
      "step": 61
    },
    {
      "epoch": 0.015121951219512195,
      "grad_norm": 0.22574013471603394,
      "learning_rate": 4.999696526314643e-05,
      "loss": 0.1342,
      "step": 62
    },
    {
      "epoch": 0.015365853658536585,
      "grad_norm": 0.22121594846248627,
      "learning_rate": 4.999686495010389e-05,
      "loss": 0.1282,
      "step": 63
    },
    {
      "epoch": 0.015609756097560976,
      "grad_norm": 0.21481889486312866,
      "learning_rate": 4.9996763006092445e-05,
      "loss": 0.1361,
      "step": 64
    },
    {
      "epoch": 0.015853658536585366,
      "grad_norm": 0.21478629112243652,
      "learning_rate": 4.999665943111873e-05,
      "loss": 0.1284,
      "step": 65
    },
    {
      "epoch": 0.016097560975609757,
      "grad_norm": 0.20680248737335205,
      "learning_rate": 4.999655422518951e-05,
      "loss": 0.1274,
      "step": 66
    },
    {
      "epoch": 0.016341463414634147,
      "grad_norm": 0.20503416657447815,
      "learning_rate": 4.999644738831165e-05,
      "loss": 0.118,
      "step": 67
    },
    {
      "epoch": 0.016585365853658537,
      "grad_norm": 0.20599256455898285,
      "learning_rate": 4.9996338920492115e-05,
      "loss": 0.1184,
      "step": 68
    },
    {
      "epoch": 0.016829268292682928,
      "grad_norm": 0.2037608027458191,
      "learning_rate": 4.9996228821737986e-05,
      "loss": 0.1269,
      "step": 69
    },
    {
      "epoch": 0.01707317073170732,
      "grad_norm": 0.19644920527935028,
      "learning_rate": 4.999611709205646e-05,
      "loss": 0.1156,
      "step": 70
    },
    {
      "epoch": 0.01731707317073171,
      "grad_norm": 0.19201423227787018,
      "learning_rate": 4.99960037314548e-05,
      "loss": 0.1137,
      "step": 71
    },
    {
      "epoch": 0.0175609756097561,
      "grad_norm": 0.19339336454868317,
      "learning_rate": 4.9995888739940424e-05,
      "loss": 0.106,
      "step": 72
    },
    {
      "epoch": 0.017804878048780486,
      "grad_norm": 0.18436072766780853,
      "learning_rate": 4.9995772117520824e-05,
      "loss": 0.1065,
      "step": 73
    },
    {
      "epoch": 0.018048780487804877,
      "grad_norm": 0.19224579632282257,
      "learning_rate": 4.999565386420361e-05,
      "loss": 0.1072,
      "step": 74
    },
    {
      "epoch": 0.018292682926829267,
      "grad_norm": 0.18621958792209625,
      "learning_rate": 4.99955339799965e-05,
      "loss": 0.1125,
      "step": 75
    },
    {
      "epoch": 0.018536585365853658,
      "grad_norm": 0.17569711804389954,
      "learning_rate": 4.9995412464907333e-05,
      "loss": 0.1117,
      "step": 76
    },
    {
      "epoch": 0.018780487804878048,
      "grad_norm": 0.17434340715408325,
      "learning_rate": 4.9995289318944004e-05,
      "loss": 0.0999,
      "step": 77
    },
    {
      "epoch": 0.01902439024390244,
      "grad_norm": 0.17377236485481262,
      "learning_rate": 4.999516454211457e-05,
      "loss": 0.1036,
      "step": 78
    },
    {
      "epoch": 0.01926829268292683,
      "grad_norm": 0.16665424406528473,
      "learning_rate": 4.999503813442717e-05,
      "loss": 0.1072,
      "step": 79
    },
    {
      "epoch": 0.01951219512195122,
      "grad_norm": 0.174089714884758,
      "learning_rate": 4.9994910095890044e-05,
      "loss": 0.0977,
      "step": 80
    },
    {
      "epoch": 0.01975609756097561,
      "grad_norm": 0.1635374128818512,
      "learning_rate": 4.9994780426511545e-05,
      "loss": 0.0921,
      "step": 81
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.161473348736763,
      "learning_rate": 4.999464912630015e-05,
      "loss": 0.0925,
      "step": 82
    },
    {
      "epoch": 0.02024390243902439,
      "grad_norm": 0.1551707684993744,
      "learning_rate": 4.999451619526441e-05,
      "loss": 0.0952,
      "step": 83
    },
    {
      "epoch": 0.02048780487804878,
      "grad_norm": 0.20948150753974915,
      "learning_rate": 4.999438163341301e-05,
      "loss": 0.1072,
      "step": 84
    },
    {
      "epoch": 0.020731707317073172,
      "grad_norm": 0.1526367962360382,
      "learning_rate": 4.999424544075472e-05,
      "loss": 0.0883,
      "step": 85
    },
    {
      "epoch": 0.020975609756097562,
      "grad_norm": 0.15483978390693665,
      "learning_rate": 4.9994107617298424e-05,
      "loss": 0.0865,
      "step": 86
    },
    {
      "epoch": 0.021219512195121953,
      "grad_norm": 0.14776328206062317,
      "learning_rate": 4.9993968163053126e-05,
      "loss": 0.0914,
      "step": 87
    },
    {
      "epoch": 0.021463414634146343,
      "grad_norm": 0.16925889253616333,
      "learning_rate": 4.999382707802792e-05,
      "loss": 0.0914,
      "step": 88
    },
    {
      "epoch": 0.02170731707317073,
      "grad_norm": 0.1592935025691986,
      "learning_rate": 4.999368436223201e-05,
      "loss": 0.0958,
      "step": 89
    },
    {
      "epoch": 0.02195121951219512,
      "grad_norm": 0.14404325187206268,
      "learning_rate": 4.999354001567471e-05,
      "loss": 0.09,
      "step": 90
    },
    {
      "epoch": 0.02219512195121951,
      "grad_norm": 0.14482314884662628,
      "learning_rate": 4.999339403836544e-05,
      "loss": 0.088,
      "step": 91
    },
    {
      "epoch": 0.0224390243902439,
      "grad_norm": 0.13674089312553406,
      "learning_rate": 4.999324643031371e-05,
      "loss": 0.0905,
      "step": 92
    },
    {
      "epoch": 0.022682926829268292,
      "grad_norm": 0.135550394654274,
      "learning_rate": 4.999309719152917e-05,
      "loss": 0.0815,
      "step": 93
    },
    {
      "epoch": 0.022926829268292682,
      "grad_norm": 0.1297236829996109,
      "learning_rate": 4.999294632202154e-05,
      "loss": 0.0802,
      "step": 94
    },
    {
      "epoch": 0.023170731707317073,
      "grad_norm": 0.13795582950115204,
      "learning_rate": 4.999279382180069e-05,
      "loss": 0.0867,
      "step": 95
    },
    {
      "epoch": 0.023414634146341463,
      "grad_norm": 0.1287507265806198,
      "learning_rate": 4.999263969087654e-05,
      "loss": 0.0783,
      "step": 96
    },
    {
      "epoch": 0.023658536585365854,
      "grad_norm": 0.1360154002904892,
      "learning_rate": 4.999248392925916e-05,
      "loss": 0.0812,
      "step": 97
    },
    {
      "epoch": 0.023902439024390244,
      "grad_norm": 0.13922615349292755,
      "learning_rate": 4.9992326536958726e-05,
      "loss": 0.0757,
      "step": 98
    },
    {
      "epoch": 0.024146341463414635,
      "grad_norm": 0.12829086184501648,
      "learning_rate": 4.9992167513985476e-05,
      "loss": 0.078,
      "step": 99
    },
    {
      "epoch": 0.024390243902439025,
      "grad_norm": 0.13952413201332092,
      "learning_rate": 4.999200686034982e-05,
      "loss": 0.0856,
      "step": 100
    },
    {
      "epoch": 0.024634146341463416,
      "grad_norm": 0.11905709654092789,
      "learning_rate": 4.999184457606221e-05,
      "loss": 0.0781,
      "step": 101
    },
    {
      "epoch": 0.024878048780487806,
      "grad_norm": 0.1244615763425827,
      "learning_rate": 4.999168066113326e-05,
      "loss": 0.0783,
      "step": 102
    },
    {
      "epoch": 0.025121951219512197,
      "grad_norm": 0.13242554664611816,
      "learning_rate": 4.999151511557365e-05,
      "loss": 0.0784,
      "step": 103
    },
    {
      "epoch": 0.025365853658536587,
      "grad_norm": 0.13561740517616272,
      "learning_rate": 4.9991347939394184e-05,
      "loss": 0.0824,
      "step": 104
    },
    {
      "epoch": 0.025609756097560974,
      "grad_norm": 0.1340102255344391,
      "learning_rate": 4.999117913260577e-05,
      "loss": 0.0801,
      "step": 105
    },
    {
      "epoch": 0.025853658536585365,
      "grad_norm": 0.1307283639907837,
      "learning_rate": 4.999100869521943e-05,
      "loss": 0.0715,
      "step": 106
    },
    {
      "epoch": 0.026097560975609755,
      "grad_norm": 0.13272124528884888,
      "learning_rate": 4.9990836627246276e-05,
      "loss": 0.0779,
      "step": 107
    },
    {
      "epoch": 0.026341463414634145,
      "grad_norm": 0.11460193991661072,
      "learning_rate": 4.9990662928697527e-05,
      "loss": 0.0725,
      "step": 108
    },
    {
      "epoch": 0.026585365853658536,
      "grad_norm": 0.1143057569861412,
      "learning_rate": 4.9990487599584536e-05,
      "loss": 0.0776,
      "step": 109
    },
    {
      "epoch": 0.026829268292682926,
      "grad_norm": 0.11855210363864899,
      "learning_rate": 4.999031063991872e-05,
      "loss": 0.0751,
      "step": 110
    },
    {
      "epoch": 0.027073170731707317,
      "grad_norm": 0.10984103381633759,
      "learning_rate": 4.999013204971165e-05,
      "loss": 0.0773,
      "step": 111
    },
    {
      "epoch": 0.027317073170731707,
      "grad_norm": 0.12715788185596466,
      "learning_rate": 4.998995182897496e-05,
      "loss": 0.0714,
      "step": 112
    },
    {
      "epoch": 0.027560975609756098,
      "grad_norm": 0.12483478337526321,
      "learning_rate": 4.9989769977720416e-05,
      "loss": 0.0687,
      "step": 113
    },
    {
      "epoch": 0.027804878048780488,
      "grad_norm": 0.11675537377595901,
      "learning_rate": 4.9989586495959876e-05,
      "loss": 0.0765,
      "step": 114
    },
    {
      "epoch": 0.02804878048780488,
      "grad_norm": 0.1035124734044075,
      "learning_rate": 4.9989401383705324e-05,
      "loss": 0.0644,
      "step": 115
    },
    {
      "epoch": 0.02829268292682927,
      "grad_norm": 0.1105053573846817,
      "learning_rate": 4.9989214640968836e-05,
      "loss": 0.0718,
      "step": 116
    },
    {
      "epoch": 0.02853658536585366,
      "grad_norm": 0.11921506375074387,
      "learning_rate": 4.998902626776258e-05,
      "loss": 0.0693,
      "step": 117
    },
    {
      "epoch": 0.02878048780487805,
      "grad_norm": 0.11750134080648422,
      "learning_rate": 4.998883626409887e-05,
      "loss": 0.0691,
      "step": 118
    },
    {
      "epoch": 0.02902439024390244,
      "grad_norm": 0.10648559778928757,
      "learning_rate": 4.998864462999009e-05,
      "loss": 0.0731,
      "step": 119
    },
    {
      "epoch": 0.02926829268292683,
      "grad_norm": 0.10805104672908783,
      "learning_rate": 4.998845136544874e-05,
      "loss": 0.0748,
      "step": 120
    },
    {
      "epoch": 0.029512195121951218,
      "grad_norm": 0.12401188164949417,
      "learning_rate": 4.9988256470487436e-05,
      "loss": 0.0714,
      "step": 121
    },
    {
      "epoch": 0.02975609756097561,
      "grad_norm": 0.11451946943998337,
      "learning_rate": 4.99880599451189e-05,
      "loss": 0.074,
      "step": 122
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.10557282716035843,
      "learning_rate": 4.998786178935594e-05,
      "loss": 0.0672,
      "step": 123
    },
    {
      "epoch": 0.03024390243902439,
      "grad_norm": 0.09758678823709488,
      "learning_rate": 4.99876620032115e-05,
      "loss": 0.0669,
      "step": 124
    },
    {
      "epoch": 0.03048780487804878,
      "grad_norm": 0.10502471774816513,
      "learning_rate": 4.998746058669861e-05,
      "loss": 0.0701,
      "step": 125
    },
    {
      "epoch": 0.03073170731707317,
      "grad_norm": 0.10099507123231888,
      "learning_rate": 4.9987257539830405e-05,
      "loss": 0.0581,
      "step": 126
    },
    {
      "epoch": 0.03097560975609756,
      "grad_norm": 0.0944276675581932,
      "learning_rate": 4.9987052862620146e-05,
      "loss": 0.0617,
      "step": 127
    },
    {
      "epoch": 0.03121951219512195,
      "grad_norm": 0.10042259097099304,
      "learning_rate": 4.998684655508117e-05,
      "loss": 0.0641,
      "step": 128
    },
    {
      "epoch": 0.03146341463414634,
      "grad_norm": 0.13542066514492035,
      "learning_rate": 4.9986638617226956e-05,
      "loss": 0.0676,
      "step": 129
    },
    {
      "epoch": 0.03170731707317073,
      "grad_norm": 0.10063723474740982,
      "learning_rate": 4.998642904907106e-05,
      "loss": 0.0608,
      "step": 130
    },
    {
      "epoch": 0.03195121951219512,
      "grad_norm": 0.12233365327119827,
      "learning_rate": 4.998621785062716e-05,
      "loss": 0.0642,
      "step": 131
    },
    {
      "epoch": 0.03219512195121951,
      "grad_norm": 0.11625036597251892,
      "learning_rate": 4.9986005021909035e-05,
      "loss": 0.0636,
      "step": 132
    },
    {
      "epoch": 0.032439024390243903,
      "grad_norm": 0.11100215464830399,
      "learning_rate": 4.998579056293058e-05,
      "loss": 0.072,
      "step": 133
    },
    {
      "epoch": 0.032682926829268294,
      "grad_norm": 0.09016652405261993,
      "learning_rate": 4.998557447370577e-05,
      "loss": 0.0668,
      "step": 134
    },
    {
      "epoch": 0.032926829268292684,
      "grad_norm": 0.08489326387643814,
      "learning_rate": 4.998535675424871e-05,
      "loss": 0.0542,
      "step": 135
    },
    {
      "epoch": 0.033170731707317075,
      "grad_norm": 0.11919001489877701,
      "learning_rate": 4.998513740457362e-05,
      "loss": 0.0608,
      "step": 136
    },
    {
      "epoch": 0.033414634146341465,
      "grad_norm": 0.09006265550851822,
      "learning_rate": 4.99849164246948e-05,
      "loss": 0.0701,
      "step": 137
    },
    {
      "epoch": 0.033658536585365856,
      "grad_norm": 0.0965549498796463,
      "learning_rate": 4.998469381462667e-05,
      "loss": 0.0622,
      "step": 138
    },
    {
      "epoch": 0.033902439024390246,
      "grad_norm": 0.0997922271490097,
      "learning_rate": 4.9984469574383745e-05,
      "loss": 0.0548,
      "step": 139
    },
    {
      "epoch": 0.03414634146341464,
      "grad_norm": 0.08282643556594849,
      "learning_rate": 4.998424370398067e-05,
      "loss": 0.061,
      "step": 140
    },
    {
      "epoch": 0.03439024390243903,
      "grad_norm": 0.10452789813280106,
      "learning_rate": 4.998401620343218e-05,
      "loss": 0.0518,
      "step": 141
    },
    {
      "epoch": 0.03463414634146342,
      "grad_norm": 0.11039166897535324,
      "learning_rate": 4.998378707275312e-05,
      "loss": 0.0694,
      "step": 142
    },
    {
      "epoch": 0.03487804878048781,
      "grad_norm": 0.11123636364936829,
      "learning_rate": 4.9983556311958426e-05,
      "loss": 0.0619,
      "step": 143
    },
    {
      "epoch": 0.0351219512195122,
      "grad_norm": 0.08304435759782791,
      "learning_rate": 4.998332392106317e-05,
      "loss": 0.0564,
      "step": 144
    },
    {
      "epoch": 0.03536585365853658,
      "grad_norm": 0.10824733227491379,
      "learning_rate": 4.998308990008251e-05,
      "loss": 0.0552,
      "step": 145
    },
    {
      "epoch": 0.03560975609756097,
      "grad_norm": 0.09689163416624069,
      "learning_rate": 4.9982854249031704e-05,
      "loss": 0.0639,
      "step": 146
    },
    {
      "epoch": 0.03585365853658536,
      "grad_norm": 0.10987116396427155,
      "learning_rate": 4.9982616967926154e-05,
      "loss": 0.0608,
      "step": 147
    },
    {
      "epoch": 0.03609756097560975,
      "grad_norm": 0.09147200733423233,
      "learning_rate": 4.998237805678132e-05,
      "loss": 0.0555,
      "step": 148
    },
    {
      "epoch": 0.036341463414634144,
      "grad_norm": 0.10645897686481476,
      "learning_rate": 4.998213751561279e-05,
      "loss": 0.0647,
      "step": 149
    },
    {
      "epoch": 0.036585365853658534,
      "grad_norm": 0.07950553297996521,
      "learning_rate": 4.998189534443628e-05,
      "loss": 0.056,
      "step": 150
    },
    {
      "epoch": 0.036829268292682925,
      "grad_norm": 0.08474428206682205,
      "learning_rate": 4.998165154326756e-05,
      "loss": 0.0516,
      "step": 151
    },
    {
      "epoch": 0.037073170731707315,
      "grad_norm": 0.09479404985904694,
      "learning_rate": 4.998140611212256e-05,
      "loss": 0.0557,
      "step": 152
    },
    {
      "epoch": 0.037317073170731706,
      "grad_norm": 0.08592883497476578,
      "learning_rate": 4.998115905101729e-05,
      "loss": 0.0591,
      "step": 153
    },
    {
      "epoch": 0.037560975609756096,
      "grad_norm": 0.07747631520032883,
      "learning_rate": 4.9980910359967856e-05,
      "loss": 0.0471,
      "step": 154
    },
    {
      "epoch": 0.03780487804878049,
      "grad_norm": 0.07829805463552475,
      "learning_rate": 4.99806600389905e-05,
      "loss": 0.0494,
      "step": 155
    },
    {
      "epoch": 0.03804878048780488,
      "grad_norm": 0.11727973818778992,
      "learning_rate": 4.998040808810155e-05,
      "loss": 0.0733,
      "step": 156
    },
    {
      "epoch": 0.03829268292682927,
      "grad_norm": 0.08836472779512405,
      "learning_rate": 4.998015450731745e-05,
      "loss": 0.0576,
      "step": 157
    },
    {
      "epoch": 0.03853658536585366,
      "grad_norm": 0.09776850789785385,
      "learning_rate": 4.997989929665474e-05,
      "loss": 0.0557,
      "step": 158
    },
    {
      "epoch": 0.03878048780487805,
      "grad_norm": 0.10705459862947464,
      "learning_rate": 4.997964245613006e-05,
      "loss": 0.0555,
      "step": 159
    },
    {
      "epoch": 0.03902439024390244,
      "grad_norm": 0.09119880944490433,
      "learning_rate": 4.9979383985760184e-05,
      "loss": 0.058,
      "step": 160
    },
    {
      "epoch": 0.03926829268292683,
      "grad_norm": 0.0834663063287735,
      "learning_rate": 4.997912388556198e-05,
      "loss": 0.0578,
      "step": 161
    },
    {
      "epoch": 0.03951219512195122,
      "grad_norm": 0.08872430771589279,
      "learning_rate": 4.997886215555241e-05,
      "loss": 0.058,
      "step": 162
    },
    {
      "epoch": 0.03975609756097561,
      "grad_norm": 0.10315282642841339,
      "learning_rate": 4.997859879574854e-05,
      "loss": 0.0526,
      "step": 163
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.08064472675323486,
      "learning_rate": 4.997833380616758e-05,
      "loss": 0.0545,
      "step": 164
    },
    {
      "epoch": 0.04024390243902439,
      "grad_norm": 0.08773837983608246,
      "learning_rate": 4.9978067186826796e-05,
      "loss": 0.0544,
      "step": 165
    },
    {
      "epoch": 0.04048780487804878,
      "grad_norm": 0.07622752338647842,
      "learning_rate": 4.997779893774359e-05,
      "loss": 0.0577,
      "step": 166
    },
    {
      "epoch": 0.04073170731707317,
      "grad_norm": 0.07418394833803177,
      "learning_rate": 4.997752905893548e-05,
      "loss": 0.0503,
      "step": 167
    },
    {
      "epoch": 0.04097560975609756,
      "grad_norm": 0.07993630319833755,
      "learning_rate": 4.997725755042006e-05,
      "loss": 0.0513,
      "step": 168
    },
    {
      "epoch": 0.04121951219512195,
      "grad_norm": 0.09172355383634567,
      "learning_rate": 4.997698441221503e-05,
      "loss": 0.0478,
      "step": 169
    },
    {
      "epoch": 0.041463414634146344,
      "grad_norm": 0.07529868930578232,
      "learning_rate": 4.997670964433825e-05,
      "loss": 0.0537,
      "step": 170
    },
    {
      "epoch": 0.041707317073170734,
      "grad_norm": 0.08943500369787216,
      "learning_rate": 4.997643324680762e-05,
      "loss": 0.0589,
      "step": 171
    },
    {
      "epoch": 0.041951219512195125,
      "grad_norm": 0.07738761603832245,
      "learning_rate": 4.997615521964118e-05,
      "loss": 0.047,
      "step": 172
    },
    {
      "epoch": 0.042195121951219515,
      "grad_norm": 0.09168421477079391,
      "learning_rate": 4.997587556285707e-05,
      "loss": 0.0509,
      "step": 173
    },
    {
      "epoch": 0.042439024390243905,
      "grad_norm": 0.08063642680644989,
      "learning_rate": 4.997559427647354e-05,
      "loss": 0.0593,
      "step": 174
    },
    {
      "epoch": 0.042682926829268296,
      "grad_norm": 0.09042294323444366,
      "learning_rate": 4.997531136050894e-05,
      "loss": 0.0526,
      "step": 175
    },
    {
      "epoch": 0.042926829268292686,
      "grad_norm": 0.12391197681427002,
      "learning_rate": 4.997502681498173e-05,
      "loss": 0.0423,
      "step": 176
    },
    {
      "epoch": 0.04317073170731707,
      "grad_norm": 0.07195476442575455,
      "learning_rate": 4.997474063991048e-05,
      "loss": 0.0469,
      "step": 177
    },
    {
      "epoch": 0.04341463414634146,
      "grad_norm": 0.10614971071481705,
      "learning_rate": 4.9974452835313856e-05,
      "loss": 0.0464,
      "step": 178
    },
    {
      "epoch": 0.04365853658536585,
      "grad_norm": 0.0744171217083931,
      "learning_rate": 4.997416340121063e-05,
      "loss": 0.0494,
      "step": 179
    },
    {
      "epoch": 0.04390243902439024,
      "grad_norm": 0.13617472350597382,
      "learning_rate": 4.997387233761971e-05,
      "loss": 0.0535,
      "step": 180
    },
    {
      "epoch": 0.04414634146341463,
      "grad_norm": 0.11546336114406586,
      "learning_rate": 4.997357964456006e-05,
      "loss": 0.0668,
      "step": 181
    },
    {
      "epoch": 0.04439024390243902,
      "grad_norm": 0.07599358260631561,
      "learning_rate": 4.99732853220508e-05,
      "loss": 0.0485,
      "step": 182
    },
    {
      "epoch": 0.04463414634146341,
      "grad_norm": 0.08437281847000122,
      "learning_rate": 4.9972989370111115e-05,
      "loss": 0.0477,
      "step": 183
    },
    {
      "epoch": 0.0448780487804878,
      "grad_norm": 0.0765012800693512,
      "learning_rate": 4.997269178876033e-05,
      "loss": 0.0643,
      "step": 184
    },
    {
      "epoch": 0.045121951219512194,
      "grad_norm": 0.08986972272396088,
      "learning_rate": 4.9972392578017846e-05,
      "loss": 0.0442,
      "step": 185
    },
    {
      "epoch": 0.045365853658536584,
      "grad_norm": 0.09716224670410156,
      "learning_rate": 4.9972091737903205e-05,
      "loss": 0.0581,
      "step": 186
    },
    {
      "epoch": 0.045609756097560974,
      "grad_norm": 0.08512292057275772,
      "learning_rate": 4.997178926843602e-05,
      "loss": 0.0465,
      "step": 187
    },
    {
      "epoch": 0.045853658536585365,
      "grad_norm": 0.08770023286342621,
      "learning_rate": 4.997148516963603e-05,
      "loss": 0.0576,
      "step": 188
    },
    {
      "epoch": 0.046097560975609755,
      "grad_norm": 0.07400573045015335,
      "learning_rate": 4.9971179441523076e-05,
      "loss": 0.0495,
      "step": 189
    },
    {
      "epoch": 0.046341463414634146,
      "grad_norm": 0.0729842483997345,
      "learning_rate": 4.997087208411711e-05,
      "loss": 0.051,
      "step": 190
    },
    {
      "epoch": 0.046585365853658536,
      "grad_norm": 0.07699857652187347,
      "learning_rate": 4.997056309743818e-05,
      "loss": 0.0472,
      "step": 191
    },
    {
      "epoch": 0.04682926829268293,
      "grad_norm": 0.07954328507184982,
      "learning_rate": 4.997025248150646e-05,
      "loss": 0.0411,
      "step": 192
    },
    {
      "epoch": 0.04707317073170732,
      "grad_norm": 0.09148465842008591,
      "learning_rate": 4.9969940236342196e-05,
      "loss": 0.0515,
      "step": 193
    },
    {
      "epoch": 0.04731707317073171,
      "grad_norm": 0.08016788959503174,
      "learning_rate": 4.996962636196578e-05,
      "loss": 0.0473,
      "step": 194
    },
    {
      "epoch": 0.0475609756097561,
      "grad_norm": 0.07576373964548111,
      "learning_rate": 4.996931085839768e-05,
      "loss": 0.0502,
      "step": 195
    },
    {
      "epoch": 0.04780487804878049,
      "grad_norm": 0.0735267847776413,
      "learning_rate": 4.996899372565849e-05,
      "loss": 0.0422,
      "step": 196
    },
    {
      "epoch": 0.04804878048780488,
      "grad_norm": 0.12461987882852554,
      "learning_rate": 4.9968674963768884e-05,
      "loss": 0.0512,
      "step": 197
    },
    {
      "epoch": 0.04829268292682927,
      "grad_norm": 0.09332498162984848,
      "learning_rate": 4.996835457274968e-05,
      "loss": 0.0426,
      "step": 198
    },
    {
      "epoch": 0.04853658536585366,
      "grad_norm": 0.10256443917751312,
      "learning_rate": 4.996803255262178e-05,
      "loss": 0.0547,
      "step": 199
    },
    {
      "epoch": 0.04878048780487805,
      "grad_norm": 0.0941568911075592,
      "learning_rate": 4.996770890340618e-05,
      "loss": 0.0379,
      "step": 200
    },
    {
      "epoch": 0.04902439024390244,
      "grad_norm": 0.07811491936445236,
      "learning_rate": 4.996738362512402e-05,
      "loss": 0.0512,
      "step": 201
    },
    {
      "epoch": 0.04926829268292683,
      "grad_norm": 0.0741586908698082,
      "learning_rate": 4.99670567177965e-05,
      "loss": 0.0584,
      "step": 202
    },
    {
      "epoch": 0.04951219512195122,
      "grad_norm": 0.08831410109996796,
      "learning_rate": 4.996672818144497e-05,
      "loss": 0.0493,
      "step": 203
    },
    {
      "epoch": 0.04975609756097561,
      "grad_norm": 0.08576727658510208,
      "learning_rate": 4.996639801609085e-05,
      "loss": 0.0436,
      "step": 204
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.08258454501628876,
      "learning_rate": 4.9966066221755695e-05,
      "loss": 0.0624,
      "step": 205
    },
    {
      "epoch": 0.05024390243902439,
      "grad_norm": 0.05719994008541107,
      "learning_rate": 4.996573279846114e-05,
      "loss": 0.0464,
      "step": 206
    },
    {
      "epoch": 0.050487804878048784,
      "grad_norm": 0.06544406712055206,
      "learning_rate": 4.9965397746228945e-05,
      "loss": 0.0481,
      "step": 207
    },
    {
      "epoch": 0.050731707317073174,
      "grad_norm": 0.08025000244379044,
      "learning_rate": 4.9965061065080984e-05,
      "loss": 0.0521,
      "step": 208
    },
    {
      "epoch": 0.05097560975609756,
      "grad_norm": 0.09934333711862564,
      "learning_rate": 4.996472275503921e-05,
      "loss": 0.0576,
      "step": 209
    },
    {
      "epoch": 0.05121951219512195,
      "grad_norm": 0.08714867383241653,
      "learning_rate": 4.99643828161257e-05,
      "loss": 0.0455,
      "step": 210
    },
    {
      "epoch": 0.05146341463414634,
      "grad_norm": 0.06876804679632187,
      "learning_rate": 4.9964041248362634e-05,
      "loss": 0.0462,
      "step": 211
    },
    {
      "epoch": 0.05170731707317073,
      "grad_norm": 0.08346009254455566,
      "learning_rate": 4.99636980517723e-05,
      "loss": 0.0539,
      "step": 212
    },
    {
      "epoch": 0.05195121951219512,
      "grad_norm": 0.07124125212430954,
      "learning_rate": 4.996335322637709e-05,
      "loss": 0.0511,
      "step": 213
    },
    {
      "epoch": 0.05219512195121951,
      "grad_norm": 0.07301579415798187,
      "learning_rate": 4.99630067721995e-05,
      "loss": 0.0466,
      "step": 214
    },
    {
      "epoch": 0.0524390243902439,
      "grad_norm": 0.06870941072702408,
      "learning_rate": 4.9962658689262145e-05,
      "loss": 0.0396,
      "step": 215
    },
    {
      "epoch": 0.05268292682926829,
      "grad_norm": 0.077603280544281,
      "learning_rate": 4.996230897758771e-05,
      "loss": 0.0505,
      "step": 216
    },
    {
      "epoch": 0.05292682926829268,
      "grad_norm": 0.07419969886541367,
      "learning_rate": 4.996195763719905e-05,
      "loss": 0.0506,
      "step": 217
    },
    {
      "epoch": 0.05317073170731707,
      "grad_norm": 0.07245089113712311,
      "learning_rate": 4.996160466811906e-05,
      "loss": 0.0457,
      "step": 218
    },
    {
      "epoch": 0.05341463414634146,
      "grad_norm": 0.08162379264831543,
      "learning_rate": 4.996125007037079e-05,
      "loss": 0.0532,
      "step": 219
    },
    {
      "epoch": 0.05365853658536585,
      "grad_norm": 0.07174333930015564,
      "learning_rate": 4.996089384397735e-05,
      "loss": 0.0422,
      "step": 220
    },
    {
      "epoch": 0.05390243902439024,
      "grad_norm": 0.06265819072723389,
      "learning_rate": 4.996053598896201e-05,
      "loss": 0.0426,
      "step": 221
    },
    {
      "epoch": 0.054146341463414634,
      "grad_norm": 0.07081557810306549,
      "learning_rate": 4.9960176505348114e-05,
      "loss": 0.0445,
      "step": 222
    },
    {
      "epoch": 0.054390243902439024,
      "grad_norm": 0.08957091718912125,
      "learning_rate": 4.99598153931591e-05,
      "loss": 0.0517,
      "step": 223
    },
    {
      "epoch": 0.054634146341463415,
      "grad_norm": 0.07297199219465256,
      "learning_rate": 4.9959452652418546e-05,
      "loss": 0.047,
      "step": 224
    },
    {
      "epoch": 0.054878048780487805,
      "grad_norm": 0.08237328380346298,
      "learning_rate": 4.995908828315011e-05,
      "loss": 0.0539,
      "step": 225
    },
    {
      "epoch": 0.055121951219512196,
      "grad_norm": 0.0846092626452446,
      "learning_rate": 4.995872228537758e-05,
      "loss": 0.0424,
      "step": 226
    },
    {
      "epoch": 0.055365853658536586,
      "grad_norm": 0.10656621307134628,
      "learning_rate": 4.995835465912482e-05,
      "loss": 0.0489,
      "step": 227
    },
    {
      "epoch": 0.055609756097560976,
      "grad_norm": 0.07653927057981491,
      "learning_rate": 4.995798540441583e-05,
      "loss": 0.0377,
      "step": 228
    },
    {
      "epoch": 0.05585365853658537,
      "grad_norm": 0.053855251520872116,
      "learning_rate": 4.995761452127469e-05,
      "loss": 0.0365,
      "step": 229
    },
    {
      "epoch": 0.05609756097560976,
      "grad_norm": 0.10275077074766159,
      "learning_rate": 4.9957242009725603e-05,
      "loss": 0.0485,
      "step": 230
    },
    {
      "epoch": 0.05634146341463415,
      "grad_norm": 0.07644540816545486,
      "learning_rate": 4.995686786979288e-05,
      "loss": 0.0444,
      "step": 231
    },
    {
      "epoch": 0.05658536585365854,
      "grad_norm": 0.10218966007232666,
      "learning_rate": 4.995649210150093e-05,
      "loss": 0.0473,
      "step": 232
    },
    {
      "epoch": 0.05682926829268293,
      "grad_norm": 0.08514029532670975,
      "learning_rate": 4.995611470487427e-05,
      "loss": 0.0403,
      "step": 233
    },
    {
      "epoch": 0.05707317073170732,
      "grad_norm": 0.0895385891199112,
      "learning_rate": 4.995573567993752e-05,
      "loss": 0.051,
      "step": 234
    },
    {
      "epoch": 0.05731707317073171,
      "grad_norm": 0.06912557035684586,
      "learning_rate": 4.995535502671541e-05,
      "loss": 0.0452,
      "step": 235
    },
    {
      "epoch": 0.0575609756097561,
      "grad_norm": 0.06415659934282303,
      "learning_rate": 4.995497274523279e-05,
      "loss": 0.0442,
      "step": 236
    },
    {
      "epoch": 0.05780487804878049,
      "grad_norm": 0.11375962942838669,
      "learning_rate": 4.9954588835514585e-05,
      "loss": 0.0351,
      "step": 237
    },
    {
      "epoch": 0.05804878048780488,
      "grad_norm": 0.08041547238826752,
      "learning_rate": 4.9954203297585854e-05,
      "loss": 0.0456,
      "step": 238
    },
    {
      "epoch": 0.05829268292682927,
      "grad_norm": 0.07130778580904007,
      "learning_rate": 4.995381613147175e-05,
      "loss": 0.0368,
      "step": 239
    },
    {
      "epoch": 0.05853658536585366,
      "grad_norm": 0.07726563513278961,
      "learning_rate": 4.995342733719754e-05,
      "loss": 0.0386,
      "step": 240
    },
    {
      "epoch": 0.058780487804878045,
      "grad_norm": 0.07615230232477188,
      "learning_rate": 4.995303691478858e-05,
      "loss": 0.0414,
      "step": 241
    },
    {
      "epoch": 0.059024390243902436,
      "grad_norm": 0.05937838554382324,
      "learning_rate": 4.9952644864270346e-05,
      "loss": 0.0448,
      "step": 242
    },
    {
      "epoch": 0.059268292682926826,
      "grad_norm": 0.09581944346427917,
      "learning_rate": 4.995225118566843e-05,
      "loss": 0.0579,
      "step": 243
    },
    {
      "epoch": 0.05951219512195122,
      "grad_norm": 0.07707315683364868,
      "learning_rate": 4.995185587900851e-05,
      "loss": 0.0443,
      "step": 244
    },
    {
      "epoch": 0.05975609756097561,
      "grad_norm": 0.059835951775312424,
      "learning_rate": 4.995145894431638e-05,
      "loss": 0.0387,
      "step": 245
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.09017907083034515,
      "learning_rate": 4.995106038161793e-05,
      "loss": 0.0424,
      "step": 246
    },
    {
      "epoch": 0.06024390243902439,
      "grad_norm": 0.10288659483194351,
      "learning_rate": 4.995066019093918e-05,
      "loss": 0.0479,
      "step": 247
    },
    {
      "epoch": 0.06048780487804878,
      "grad_norm": 0.0747348964214325,
      "learning_rate": 4.9950258372306224e-05,
      "loss": 0.0451,
      "step": 248
    },
    {
      "epoch": 0.06073170731707317,
      "grad_norm": 0.07895244657993317,
      "learning_rate": 4.99498549257453e-05,
      "loss": 0.0348,
      "step": 249
    },
    {
      "epoch": 0.06097560975609756,
      "grad_norm": 0.0693446546792984,
      "learning_rate": 4.9949449851282715e-05,
      "loss": 0.0435,
      "step": 250
    },
    {
      "epoch": 0.06121951219512195,
      "grad_norm": 0.06869431585073471,
      "learning_rate": 4.9949043148944906e-05,
      "loss": 0.0467,
      "step": 251
    },
    {
      "epoch": 0.06146341463414634,
      "grad_norm": 0.08013976365327835,
      "learning_rate": 4.994863481875841e-05,
      "loss": 0.0561,
      "step": 252
    },
    {
      "epoch": 0.06170731707317073,
      "grad_norm": 0.070297971367836,
      "learning_rate": 4.994822486074987e-05,
      "loss": 0.0483,
      "step": 253
    },
    {
      "epoch": 0.06195121951219512,
      "grad_norm": 0.09795775264501572,
      "learning_rate": 4.994781327494602e-05,
      "loss": 0.047,
      "step": 254
    },
    {
      "epoch": 0.06219512195121951,
      "grad_norm": 0.06838614493608475,
      "learning_rate": 4.994740006137374e-05,
      "loss": 0.0439,
      "step": 255
    },
    {
      "epoch": 0.0624390243902439,
      "grad_norm": 0.07764148712158203,
      "learning_rate": 4.994698522005997e-05,
      "loss": 0.0353,
      "step": 256
    },
    {
      "epoch": 0.06268292682926829,
      "grad_norm": 0.09895584732294083,
      "learning_rate": 4.9946568751031784e-05,
      "loss": 0.0558,
      "step": 257
    },
    {
      "epoch": 0.06292682926829268,
      "grad_norm": 0.07160746306180954,
      "learning_rate": 4.994615065431636e-05,
      "loss": 0.0487,
      "step": 258
    },
    {
      "epoch": 0.06317073170731707,
      "grad_norm": 0.11107323318719864,
      "learning_rate": 4.994573092994097e-05,
      "loss": 0.0532,
      "step": 259
    },
    {
      "epoch": 0.06341463414634146,
      "grad_norm": 0.055280622094869614,
      "learning_rate": 4.9945309577932995e-05,
      "loss": 0.0434,
      "step": 260
    },
    {
      "epoch": 0.06365853658536585,
      "grad_norm": 0.08755797892808914,
      "learning_rate": 4.994488659831995e-05,
      "loss": 0.0503,
      "step": 261
    },
    {
      "epoch": 0.06390243902439025,
      "grad_norm": 0.08292202651500702,
      "learning_rate": 4.994446199112941e-05,
      "loss": 0.0387,
      "step": 262
    },
    {
      "epoch": 0.06414634146341464,
      "grad_norm": 0.08072113245725632,
      "learning_rate": 4.994403575638909e-05,
      "loss": 0.0443,
      "step": 263
    },
    {
      "epoch": 0.06439024390243903,
      "grad_norm": 0.0979185551404953,
      "learning_rate": 4.99436078941268e-05,
      "loss": 0.053,
      "step": 264
    },
    {
      "epoch": 0.06463414634146342,
      "grad_norm": 0.06259148567914963,
      "learning_rate": 4.994317840437045e-05,
      "loss": 0.0362,
      "step": 265
    },
    {
      "epoch": 0.06487804878048781,
      "grad_norm": 0.053485285490751266,
      "learning_rate": 4.9942747287148066e-05,
      "loss": 0.0305,
      "step": 266
    },
    {
      "epoch": 0.0651219512195122,
      "grad_norm": 0.08508146554231644,
      "learning_rate": 4.9942314542487783e-05,
      "loss": 0.0431,
      "step": 267
    },
    {
      "epoch": 0.06536585365853659,
      "grad_norm": 0.06985442340373993,
      "learning_rate": 4.9941880170417834e-05,
      "loss": 0.0357,
      "step": 268
    },
    {
      "epoch": 0.06560975609756098,
      "grad_norm": 0.07490574568510056,
      "learning_rate": 4.994144417096655e-05,
      "loss": 0.0453,
      "step": 269
    },
    {
      "epoch": 0.06585365853658537,
      "grad_norm": 0.09531798958778381,
      "learning_rate": 4.99410065441624e-05,
      "loss": 0.0316,
      "step": 270
    },
    {
      "epoch": 0.06609756097560976,
      "grad_norm": 0.05542031675577164,
      "learning_rate": 4.994056729003391e-05,
      "loss": 0.0405,
      "step": 271
    },
    {
      "epoch": 0.06634146341463415,
      "grad_norm": 0.08044534921646118,
      "learning_rate": 4.994012640860976e-05,
      "loss": 0.0418,
      "step": 272
    },
    {
      "epoch": 0.06658536585365854,
      "grad_norm": 0.10248406231403351,
      "learning_rate": 4.993968389991871e-05,
      "loss": 0.0353,
      "step": 273
    },
    {
      "epoch": 0.06682926829268293,
      "grad_norm": 0.06468606740236282,
      "learning_rate": 4.993923976398964e-05,
      "loss": 0.0422,
      "step": 274
    },
    {
      "epoch": 0.06707317073170732,
      "grad_norm": 0.06780336797237396,
      "learning_rate": 4.993879400085151e-05,
      "loss": 0.0323,
      "step": 275
    },
    {
      "epoch": 0.06731707317073171,
      "grad_norm": 0.09643200039863586,
      "learning_rate": 4.993834661053343e-05,
      "loss": 0.0555,
      "step": 276
    },
    {
      "epoch": 0.0675609756097561,
      "grad_norm": 0.08896750956773758,
      "learning_rate": 4.9937897593064565e-05,
      "loss": 0.0409,
      "step": 277
    },
    {
      "epoch": 0.06780487804878049,
      "grad_norm": 0.07416211068630219,
      "learning_rate": 4.993744694847423e-05,
      "loss": 0.0394,
      "step": 278
    },
    {
      "epoch": 0.06804878048780488,
      "grad_norm": 0.08325258642435074,
      "learning_rate": 4.9936994676791824e-05,
      "loss": 0.0567,
      "step": 279
    },
    {
      "epoch": 0.06829268292682927,
      "grad_norm": 0.08840042352676392,
      "learning_rate": 4.993654077804686e-05,
      "loss": 0.0509,
      "step": 280
    },
    {
      "epoch": 0.06853658536585366,
      "grad_norm": 0.06921442598104477,
      "learning_rate": 4.993608525226894e-05,
      "loss": 0.0424,
      "step": 281
    },
    {
      "epoch": 0.06878048780487805,
      "grad_norm": 0.06634430587291718,
      "learning_rate": 4.9935628099487794e-05,
      "loss": 0.0529,
      "step": 282
    },
    {
      "epoch": 0.06902439024390244,
      "grad_norm": 0.08542648702859879,
      "learning_rate": 4.993516931973325e-05,
      "loss": 0.0402,
      "step": 283
    },
    {
      "epoch": 0.06926829268292684,
      "grad_norm": 0.07219628989696503,
      "learning_rate": 4.993470891303524e-05,
      "loss": 0.0506,
      "step": 284
    },
    {
      "epoch": 0.06951219512195123,
      "grad_norm": 0.08506769686937332,
      "learning_rate": 4.9934246879423805e-05,
      "loss": 0.0343,
      "step": 285
    },
    {
      "epoch": 0.06975609756097562,
      "grad_norm": 0.08805152773857117,
      "learning_rate": 4.99337832189291e-05,
      "loss": 0.0442,
      "step": 286
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.1146385446190834,
      "learning_rate": 4.993331793158137e-05,
      "loss": 0.071,
      "step": 287
    },
    {
      "epoch": 0.0702439024390244,
      "grad_norm": 0.06870894134044647,
      "learning_rate": 4.993285101741096e-05,
      "loss": 0.0393,
      "step": 288
    },
    {
      "epoch": 0.07048780487804877,
      "grad_norm": 0.056544095277786255,
      "learning_rate": 4.993238247644836e-05,
      "loss": 0.0366,
      "step": 289
    },
    {
      "epoch": 0.07073170731707316,
      "grad_norm": 0.06329814344644547,
      "learning_rate": 4.9931912308724124e-05,
      "loss": 0.0429,
      "step": 290
    },
    {
      "epoch": 0.07097560975609755,
      "grad_norm": 0.15325142443180084,
      "learning_rate": 4.993144051426894e-05,
      "loss": 0.0471,
      "step": 291
    },
    {
      "epoch": 0.07121951219512195,
      "grad_norm": 0.05633948743343353,
      "learning_rate": 4.9930967093113576e-05,
      "loss": 0.041,
      "step": 292
    },
    {
      "epoch": 0.07146341463414634,
      "grad_norm": 0.10375712811946869,
      "learning_rate": 4.993049204528894e-05,
      "loss": 0.0477,
      "step": 293
    },
    {
      "epoch": 0.07170731707317073,
      "grad_norm": 0.07056776434183121,
      "learning_rate": 4.993001537082601e-05,
      "loss": 0.041,
      "step": 294
    },
    {
      "epoch": 0.07195121951219512,
      "grad_norm": 0.10716236382722855,
      "learning_rate": 4.99295370697559e-05,
      "loss": 0.0462,
      "step": 295
    },
    {
      "epoch": 0.0721951219512195,
      "grad_norm": 0.08298556506633759,
      "learning_rate": 4.9929057142109806e-05,
      "loss": 0.0475,
      "step": 296
    },
    {
      "epoch": 0.0724390243902439,
      "grad_norm": 0.06340586394071579,
      "learning_rate": 4.992857558791906e-05,
      "loss": 0.032,
      "step": 297
    },
    {
      "epoch": 0.07268292682926829,
      "grad_norm": 0.10614215582609177,
      "learning_rate": 4.992809240721505e-05,
      "loss": 0.0519,
      "step": 298
    },
    {
      "epoch": 0.07292682926829268,
      "grad_norm": 0.07987971603870392,
      "learning_rate": 4.992760760002935e-05,
      "loss": 0.038,
      "step": 299
    },
    {
      "epoch": 0.07317073170731707,
      "grad_norm": 0.07624509930610657,
      "learning_rate": 4.992712116639354e-05,
      "loss": 0.0441,
      "step": 300
    },
    {
      "epoch": 0.07341463414634146,
      "grad_norm": 0.08948488533496857,
      "learning_rate": 4.99266331063394e-05,
      "loss": 0.0422,
      "step": 301
    },
    {
      "epoch": 0.07365853658536585,
      "grad_norm": 0.06482638418674469,
      "learning_rate": 4.992614341989875e-05,
      "loss": 0.033,
      "step": 302
    },
    {
      "epoch": 0.07390243902439024,
      "grad_norm": 0.09625472128391266,
      "learning_rate": 4.9925652107103546e-05,
      "loss": 0.0493,
      "step": 303
    },
    {
      "epoch": 0.07414634146341463,
      "grad_norm": 0.1037510558962822,
      "learning_rate": 4.992515916798586e-05,
      "loss": 0.0581,
      "step": 304
    },
    {
      "epoch": 0.07439024390243902,
      "grad_norm": 0.06611954420804977,
      "learning_rate": 4.9924664602577824e-05,
      "loss": 0.0325,
      "step": 305
    },
    {
      "epoch": 0.07463414634146341,
      "grad_norm": 0.05906921252608299,
      "learning_rate": 4.9924168410911734e-05,
      "loss": 0.0373,
      "step": 306
    },
    {
      "epoch": 0.0748780487804878,
      "grad_norm": 0.08546323329210281,
      "learning_rate": 4.992367059301995e-05,
      "loss": 0.0683,
      "step": 307
    },
    {
      "epoch": 0.07512195121951219,
      "grad_norm": 0.07259199768304825,
      "learning_rate": 4.9923171148934966e-05,
      "loss": 0.0285,
      "step": 308
    },
    {
      "epoch": 0.07536585365853658,
      "grad_norm": 0.08799991756677628,
      "learning_rate": 4.992267007868936e-05,
      "loss": 0.0521,
      "step": 309
    },
    {
      "epoch": 0.07560975609756097,
      "grad_norm": 0.06178553029894829,
      "learning_rate": 4.992216738231582e-05,
      "loss": 0.0423,
      "step": 310
    },
    {
      "epoch": 0.07585365853658536,
      "grad_norm": 0.07787668704986572,
      "learning_rate": 4.992166305984716e-05,
      "loss": 0.0384,
      "step": 311
    },
    {
      "epoch": 0.07609756097560975,
      "grad_norm": 0.07461468875408173,
      "learning_rate": 4.992115711131627e-05,
      "loss": 0.0418,
      "step": 312
    },
    {
      "epoch": 0.07634146341463414,
      "grad_norm": 0.049602195620536804,
      "learning_rate": 4.992064953675618e-05,
      "loss": 0.0404,
      "step": 313
    },
    {
      "epoch": 0.07658536585365854,
      "grad_norm": 0.08315172791481018,
      "learning_rate": 4.992014033619999e-05,
      "loss": 0.0438,
      "step": 314
    },
    {
      "epoch": 0.07682926829268293,
      "grad_norm": 0.07161284238100052,
      "learning_rate": 4.991962950968093e-05,
      "loss": 0.0397,
      "step": 315
    },
    {
      "epoch": 0.07707317073170732,
      "grad_norm": 0.080326609313488,
      "learning_rate": 4.991911705723234e-05,
      "loss": 0.0487,
      "step": 316
    },
    {
      "epoch": 0.0773170731707317,
      "grad_norm": 0.08004774898290634,
      "learning_rate": 4.991860297888764e-05,
      "loss": 0.0365,
      "step": 317
    },
    {
      "epoch": 0.0775609756097561,
      "grad_norm": 0.06961455941200256,
      "learning_rate": 4.9918087274680374e-05,
      "loss": 0.0315,
      "step": 318
    },
    {
      "epoch": 0.07780487804878049,
      "grad_norm": 0.09858182072639465,
      "learning_rate": 4.99175699446442e-05,
      "loss": 0.0474,
      "step": 319
    },
    {
      "epoch": 0.07804878048780488,
      "grad_norm": 0.06703609228134155,
      "learning_rate": 4.9917050988812866e-05,
      "loss": 0.0365,
      "step": 320
    },
    {
      "epoch": 0.07829268292682927,
      "grad_norm": 0.07779239118099213,
      "learning_rate": 4.9916530407220235e-05,
      "loss": 0.0399,
      "step": 321
    },
    {
      "epoch": 0.07853658536585366,
      "grad_norm": 0.0824090912938118,
      "learning_rate": 4.991600819990027e-05,
      "loss": 0.0419,
      "step": 322
    },
    {
      "epoch": 0.07878048780487805,
      "grad_norm": 0.08254439383745193,
      "learning_rate": 4.9915484366887045e-05,
      "loss": 0.0494,
      "step": 323
    },
    {
      "epoch": 0.07902439024390244,
      "grad_norm": 0.12300045788288116,
      "learning_rate": 4.9914958908214744e-05,
      "loss": 0.0414,
      "step": 324
    },
    {
      "epoch": 0.07926829268292683,
      "grad_norm": 0.07726722955703735,
      "learning_rate": 4.991443182391764e-05,
      "loss": 0.0571,
      "step": 325
    },
    {
      "epoch": 0.07951219512195122,
      "grad_norm": 0.04554399475455284,
      "learning_rate": 4.9913903114030135e-05,
      "loss": 0.0311,
      "step": 326
    },
    {
      "epoch": 0.07975609756097561,
      "grad_norm": 0.07971569150686264,
      "learning_rate": 4.9913372778586714e-05,
      "loss": 0.0313,
      "step": 327
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.05573339760303497,
      "learning_rate": 4.9912840817621995e-05,
      "loss": 0.0379,
      "step": 328
    },
    {
      "epoch": 0.08024390243902439,
      "grad_norm": 0.0928625762462616,
      "learning_rate": 4.9912307231170666e-05,
      "loss": 0.0327,
      "step": 329
    },
    {
      "epoch": 0.08048780487804878,
      "grad_norm": 0.07704301923513412,
      "learning_rate": 4.991177201926757e-05,
      "loss": 0.0551,
      "step": 330
    },
    {
      "epoch": 0.08073170731707317,
      "grad_norm": 0.11031448841094971,
      "learning_rate": 4.9911235181947597e-05,
      "loss": 0.0327,
      "step": 331
    },
    {
      "epoch": 0.08097560975609756,
      "grad_norm": 0.0693671777844429,
      "learning_rate": 4.9910696719245795e-05,
      "loss": 0.0349,
      "step": 332
    },
    {
      "epoch": 0.08121951219512195,
      "grad_norm": 0.08347968012094498,
      "learning_rate": 4.991015663119729e-05,
      "loss": 0.0398,
      "step": 333
    },
    {
      "epoch": 0.08146341463414634,
      "grad_norm": 0.09019340574741364,
      "learning_rate": 4.9909614917837325e-05,
      "loss": 0.0454,
      "step": 334
    },
    {
      "epoch": 0.08170731707317073,
      "grad_norm": 0.07273375988006592,
      "learning_rate": 4.990907157920124e-05,
      "loss": 0.054,
      "step": 335
    },
    {
      "epoch": 0.08195121951219513,
      "grad_norm": 0.0851714164018631,
      "learning_rate": 4.990852661532449e-05,
      "loss": 0.0355,
      "step": 336
    },
    {
      "epoch": 0.08219512195121952,
      "grad_norm": 0.04823907092213631,
      "learning_rate": 4.990798002624263e-05,
      "loss": 0.0287,
      "step": 337
    },
    {
      "epoch": 0.0824390243902439,
      "grad_norm": 0.10069452226161957,
      "learning_rate": 4.9907431811991324e-05,
      "loss": 0.051,
      "step": 338
    },
    {
      "epoch": 0.0826829268292683,
      "grad_norm": 0.0777144804596901,
      "learning_rate": 4.990688197260634e-05,
      "loss": 0.0412,
      "step": 339
    },
    {
      "epoch": 0.08292682926829269,
      "grad_norm": 0.08384962379932404,
      "learning_rate": 4.9906330508123555e-05,
      "loss": 0.0456,
      "step": 340
    },
    {
      "epoch": 0.08317073170731708,
      "grad_norm": 0.09797388315200806,
      "learning_rate": 4.9905777418578945e-05,
      "loss": 0.0469,
      "step": 341
    },
    {
      "epoch": 0.08341463414634147,
      "grad_norm": 0.07643482834100723,
      "learning_rate": 4.990522270400861e-05,
      "loss": 0.0503,
      "step": 342
    },
    {
      "epoch": 0.08365853658536586,
      "grad_norm": 0.07662272453308105,
      "learning_rate": 4.9904666364448737e-05,
      "loss": 0.0271,
      "step": 343
    },
    {
      "epoch": 0.08390243902439025,
      "grad_norm": 0.0793616846203804,
      "learning_rate": 4.990410839993562e-05,
      "loss": 0.0353,
      "step": 344
    },
    {
      "epoch": 0.08414634146341464,
      "grad_norm": 0.07996112108230591,
      "learning_rate": 4.990354881050567e-05,
      "loss": 0.0314,
      "step": 345
    },
    {
      "epoch": 0.08439024390243903,
      "grad_norm": 0.10060110688209534,
      "learning_rate": 4.990298759619539e-05,
      "loss": 0.0321,
      "step": 346
    },
    {
      "epoch": 0.08463414634146342,
      "grad_norm": 0.08543054014444351,
      "learning_rate": 4.990242475704141e-05,
      "loss": 0.0408,
      "step": 347
    },
    {
      "epoch": 0.08487804878048781,
      "grad_norm": 0.0844823569059372,
      "learning_rate": 4.990186029308046e-05,
      "loss": 0.0426,
      "step": 348
    },
    {
      "epoch": 0.0851219512195122,
      "grad_norm": 0.08453942090272903,
      "learning_rate": 4.990129420434935e-05,
      "loss": 0.0377,
      "step": 349
    },
    {
      "epoch": 0.08536585365853659,
      "grad_norm": 0.07680591195821762,
      "learning_rate": 4.9900726490885015e-05,
      "loss": 0.0281,
      "step": 350
    },
    {
      "epoch": 0.08560975609756098,
      "grad_norm": 0.07827933877706528,
      "learning_rate": 4.990015715272451e-05,
      "loss": 0.0351,
      "step": 351
    },
    {
      "epoch": 0.08585365853658537,
      "grad_norm": 0.07510348409414291,
      "learning_rate": 4.989958618990498e-05,
      "loss": 0.0336,
      "step": 352
    },
    {
      "epoch": 0.08609756097560975,
      "grad_norm": 0.08638378232717514,
      "learning_rate": 4.9899013602463676e-05,
      "loss": 0.0441,
      "step": 353
    },
    {
      "epoch": 0.08634146341463414,
      "grad_norm": 0.06957294791936874,
      "learning_rate": 4.989843939043795e-05,
      "loss": 0.0328,
      "step": 354
    },
    {
      "epoch": 0.08658536585365853,
      "grad_norm": 0.09115305542945862,
      "learning_rate": 4.9897863553865284e-05,
      "loss": 0.0447,
      "step": 355
    },
    {
      "epoch": 0.08682926829268292,
      "grad_norm": 0.08302447199821472,
      "learning_rate": 4.989728609278323e-05,
      "loss": 0.0422,
      "step": 356
    },
    {
      "epoch": 0.08707317073170731,
      "grad_norm": 0.10036738961935043,
      "learning_rate": 4.989670700722949e-05,
      "loss": 0.0449,
      "step": 357
    },
    {
      "epoch": 0.0873170731707317,
      "grad_norm": 0.06653150916099548,
      "learning_rate": 4.989612629724183e-05,
      "loss": 0.0447,
      "step": 358
    },
    {
      "epoch": 0.08756097560975609,
      "grad_norm": 0.08157205581665039,
      "learning_rate": 4.989554396285814e-05,
      "loss": 0.0454,
      "step": 359
    },
    {
      "epoch": 0.08780487804878048,
      "grad_norm": 0.0846332311630249,
      "learning_rate": 4.989496000411642e-05,
      "loss": 0.0397,
      "step": 360
    },
    {
      "epoch": 0.08804878048780487,
      "grad_norm": 0.08026594668626785,
      "learning_rate": 4.989437442105477e-05,
      "loss": 0.0437,
      "step": 361
    },
    {
      "epoch": 0.08829268292682926,
      "grad_norm": 0.08770766109228134,
      "learning_rate": 4.98937872137114e-05,
      "loss": 0.0519,
      "step": 362
    },
    {
      "epoch": 0.08853658536585365,
      "grad_norm": 0.08128009736537933,
      "learning_rate": 4.9893198382124616e-05,
      "loss": 0.0516,
      "step": 363
    },
    {
      "epoch": 0.08878048780487804,
      "grad_norm": 0.04922855272889137,
      "learning_rate": 4.989260792633285e-05,
      "loss": 0.0315,
      "step": 364
    },
    {
      "epoch": 0.08902439024390243,
      "grad_norm": 0.07379432022571564,
      "learning_rate": 4.989201584637462e-05,
      "loss": 0.0407,
      "step": 365
    },
    {
      "epoch": 0.08926829268292683,
      "grad_norm": 0.09308405965566635,
      "learning_rate": 4.989142214228856e-05,
      "loss": 0.0402,
      "step": 366
    },
    {
      "epoch": 0.08951219512195122,
      "grad_norm": 0.0863848477602005,
      "learning_rate": 4.9890826814113396e-05,
      "loss": 0.0341,
      "step": 367
    },
    {
      "epoch": 0.0897560975609756,
      "grad_norm": 0.05745711922645569,
      "learning_rate": 4.9890229861887986e-05,
      "loss": 0.0428,
      "step": 368
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.058056753128767014,
      "learning_rate": 4.988963128565127e-05,
      "loss": 0.037,
      "step": 369
    },
    {
      "epoch": 0.09024390243902439,
      "grad_norm": 0.08317216485738754,
      "learning_rate": 4.988903108544231e-05,
      "loss": 0.0288,
      "step": 370
    },
    {
      "epoch": 0.09048780487804878,
      "grad_norm": 0.07717035710811615,
      "learning_rate": 4.988842926130026e-05,
      "loss": 0.0396,
      "step": 371
    },
    {
      "epoch": 0.09073170731707317,
      "grad_norm": 0.05919531732797623,
      "learning_rate": 4.98878258132644e-05,
      "loss": 0.028,
      "step": 372
    },
    {
      "epoch": 0.09097560975609756,
      "grad_norm": 0.1162307858467102,
      "learning_rate": 4.98872207413741e-05,
      "loss": 0.0507,
      "step": 373
    },
    {
      "epoch": 0.09121951219512195,
      "grad_norm": 0.07266657799482346,
      "learning_rate": 4.9886614045668814e-05,
      "loss": 0.0301,
      "step": 374
    },
    {
      "epoch": 0.09146341463414634,
      "grad_norm": 0.08334612101316452,
      "learning_rate": 4.9886005726188156e-05,
      "loss": 0.0315,
      "step": 375
    },
    {
      "epoch": 0.09170731707317073,
      "grad_norm": 0.12018655985593796,
      "learning_rate": 4.988539578297181e-05,
      "loss": 0.0696,
      "step": 376
    },
    {
      "epoch": 0.09195121951219512,
      "grad_norm": 0.09638550877571106,
      "learning_rate": 4.988478421605957e-05,
      "loss": 0.0586,
      "step": 377
    },
    {
      "epoch": 0.09219512195121951,
      "grad_norm": 0.0789848268032074,
      "learning_rate": 4.988417102549134e-05,
      "loss": 0.031,
      "step": 378
    },
    {
      "epoch": 0.0924390243902439,
      "grad_norm": 0.08189394325017929,
      "learning_rate": 4.9883556211307125e-05,
      "loss": 0.0406,
      "step": 379
    },
    {
      "epoch": 0.09268292682926829,
      "grad_norm": 0.0737152248620987,
      "learning_rate": 4.988293977354705e-05,
      "loss": 0.0324,
      "step": 380
    },
    {
      "epoch": 0.09292682926829268,
      "grad_norm": 0.11315245181322098,
      "learning_rate": 4.988232171225132e-05,
      "loss": 0.031,
      "step": 381
    },
    {
      "epoch": 0.09317073170731707,
      "grad_norm": 0.09534507244825363,
      "learning_rate": 4.988170202746027e-05,
      "loss": 0.0379,
      "step": 382
    },
    {
      "epoch": 0.09341463414634146,
      "grad_norm": 0.12852559983730316,
      "learning_rate": 4.9881080719214345e-05,
      "loss": 0.0623,
      "step": 383
    },
    {
      "epoch": 0.09365853658536585,
      "grad_norm": 0.09239910542964935,
      "learning_rate": 4.9880457787554064e-05,
      "loss": 0.0358,
      "step": 384
    },
    {
      "epoch": 0.09390243902439024,
      "grad_norm": 0.08300764858722687,
      "learning_rate": 4.987983323252008e-05,
      "loss": 0.0479,
      "step": 385
    },
    {
      "epoch": 0.09414634146341463,
      "grad_norm": 0.06705333292484283,
      "learning_rate": 4.987920705415314e-05,
      "loss": 0.0389,
      "step": 386
    },
    {
      "epoch": 0.09439024390243902,
      "grad_norm": 0.07196890562772751,
      "learning_rate": 4.987857925249411e-05,
      "loss": 0.0395,
      "step": 387
    },
    {
      "epoch": 0.09463414634146342,
      "grad_norm": 0.09887418150901794,
      "learning_rate": 4.987794982758393e-05,
      "loss": 0.0415,
      "step": 388
    },
    {
      "epoch": 0.0948780487804878,
      "grad_norm": 0.0889248475432396,
      "learning_rate": 4.9877318779463686e-05,
      "loss": 0.0321,
      "step": 389
    },
    {
      "epoch": 0.0951219512195122,
      "grad_norm": 0.09153355658054352,
      "learning_rate": 4.9876686108174555e-05,
      "loss": 0.0405,
      "step": 390
    },
    {
      "epoch": 0.09536585365853659,
      "grad_norm": 0.11597126722335815,
      "learning_rate": 4.987605181375781e-05,
      "loss": 0.0421,
      "step": 391
    },
    {
      "epoch": 0.09560975609756098,
      "grad_norm": 0.057889025658369064,
      "learning_rate": 4.9875415896254826e-05,
      "loss": 0.0393,
      "step": 392
    },
    {
      "epoch": 0.09585365853658537,
      "grad_norm": 0.07080725580453873,
      "learning_rate": 4.987477835570712e-05,
      "loss": 0.0486,
      "step": 393
    },
    {
      "epoch": 0.09609756097560976,
      "grad_norm": 0.12135536223649979,
      "learning_rate": 4.987413919215627e-05,
      "loss": 0.0336,
      "step": 394
    },
    {
      "epoch": 0.09634146341463415,
      "grad_norm": 0.05792437493801117,
      "learning_rate": 4.987349840564398e-05,
      "loss": 0.0341,
      "step": 395
    },
    {
      "epoch": 0.09658536585365854,
      "grad_norm": 0.09046273678541183,
      "learning_rate": 4.9872855996212077e-05,
      "loss": 0.039,
      "step": 396
    },
    {
      "epoch": 0.09682926829268293,
      "grad_norm": 0.09144458174705505,
      "learning_rate": 4.987221196390245e-05,
      "loss": 0.0386,
      "step": 397
    },
    {
      "epoch": 0.09707317073170732,
      "grad_norm": 0.05682693421840668,
      "learning_rate": 4.987156630875714e-05,
      "loss": 0.0424,
      "step": 398
    },
    {
      "epoch": 0.09731707317073171,
      "grad_norm": 0.07906843721866608,
      "learning_rate": 4.9870919030818266e-05,
      "loss": 0.04,
      "step": 399
    },
    {
      "epoch": 0.0975609756097561,
      "grad_norm": 0.05779951810836792,
      "learning_rate": 4.987027013012806e-05,
      "loss": 0.0405,
      "step": 400
    },
    {
      "epoch": 0.09780487804878049,
      "grad_norm": 0.06736312806606293,
      "learning_rate": 4.9869619606728866e-05,
      "loss": 0.0387,
      "step": 401
    },
    {
      "epoch": 0.09804878048780488,
      "grad_norm": 0.06501059234142303,
      "learning_rate": 4.986896746066313e-05,
      "loss": 0.0444,
      "step": 402
    },
    {
      "epoch": 0.09829268292682927,
      "grad_norm": 0.073690265417099,
      "learning_rate": 4.986831369197339e-05,
      "loss": 0.0308,
      "step": 403
    },
    {
      "epoch": 0.09853658536585366,
      "grad_norm": 0.06934519112110138,
      "learning_rate": 4.9867658300702316e-05,
      "loss": 0.0399,
      "step": 404
    },
    {
      "epoch": 0.09878048780487805,
      "grad_norm": 0.04997027665376663,
      "learning_rate": 4.986700128689266e-05,
      "loss": 0.033,
      "step": 405
    },
    {
      "epoch": 0.09902439024390244,
      "grad_norm": 0.06797844916582108,
      "learning_rate": 4.9866342650587306e-05,
      "loss": 0.0426,
      "step": 406
    },
    {
      "epoch": 0.09926829268292683,
      "grad_norm": 0.1015583947300911,
      "learning_rate": 4.9865682391829205e-05,
      "loss": 0.0345,
      "step": 407
    },
    {
      "epoch": 0.09951219512195122,
      "grad_norm": 0.0993579626083374,
      "learning_rate": 4.986502051066145e-05,
      "loss": 0.0439,
      "step": 408
    },
    {
      "epoch": 0.09975609756097562,
      "grad_norm": 0.06102630868554115,
      "learning_rate": 4.986435700712724e-05,
      "loss": 0.0426,
      "step": 409
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.07991509884595871,
      "learning_rate": 4.986369188126983e-05,
      "loss": 0.0387,
      "step": 410
    },
    {
      "epoch": 0.1002439024390244,
      "grad_norm": 0.09553935378789902,
      "learning_rate": 4.9863025133132656e-05,
      "loss": 0.0273,
      "step": 411
    },
    {
      "epoch": 0.10048780487804879,
      "grad_norm": 0.07021316140890121,
      "learning_rate": 4.9862356762759205e-05,
      "loss": 0.0361,
      "step": 412
    },
    {
      "epoch": 0.10073170731707318,
      "grad_norm": 0.05293157696723938,
      "learning_rate": 4.9861686770193085e-05,
      "loss": 0.029,
      "step": 413
    },
    {
      "epoch": 0.10097560975609757,
      "grad_norm": 0.048954788595438004,
      "learning_rate": 4.986101515547801e-05,
      "loss": 0.0331,
      "step": 414
    },
    {
      "epoch": 0.10121951219512196,
      "grad_norm": 0.06804194301366806,
      "learning_rate": 4.98603419186578e-05,
      "loss": 0.0322,
      "step": 415
    },
    {
      "epoch": 0.10146341463414635,
      "grad_norm": 0.06180751696228981,
      "learning_rate": 4.9859667059776385e-05,
      "loss": 0.0292,
      "step": 416
    },
    {
      "epoch": 0.10170731707317072,
      "grad_norm": 0.09951695054769516,
      "learning_rate": 4.9858990578877804e-05,
      "loss": 0.0453,
      "step": 417
    },
    {
      "epoch": 0.10195121951219512,
      "grad_norm": 0.09653589129447937,
      "learning_rate": 4.985831247600618e-05,
      "loss": 0.0396,
      "step": 418
    },
    {
      "epoch": 0.1021951219512195,
      "grad_norm": 0.06840618699789047,
      "learning_rate": 4.9857632751205775e-05,
      "loss": 0.0297,
      "step": 419
    },
    {
      "epoch": 0.1024390243902439,
      "grad_norm": 0.10304320603609085,
      "learning_rate": 4.985695140452092e-05,
      "loss": 0.032,
      "step": 420
    },
    {
      "epoch": 0.10268292682926829,
      "grad_norm": 0.06574958562850952,
      "learning_rate": 4.9856268435996086e-05,
      "loss": 0.0344,
      "step": 421
    },
    {
      "epoch": 0.10292682926829268,
      "grad_norm": 0.0685170590877533,
      "learning_rate": 4.985558384567582e-05,
      "loss": 0.0321,
      "step": 422
    },
    {
      "epoch": 0.10317073170731707,
      "grad_norm": 0.08770409971475601,
      "learning_rate": 4.985489763360481e-05,
      "loss": 0.0303,
      "step": 423
    },
    {
      "epoch": 0.10341463414634146,
      "grad_norm": 0.08263324201107025,
      "learning_rate": 4.9854209799827814e-05,
      "loss": 0.0412,
      "step": 424
    },
    {
      "epoch": 0.10365853658536585,
      "grad_norm": 0.06666331738233566,
      "learning_rate": 4.985352034438971e-05,
      "loss": 0.0477,
      "step": 425
    },
    {
      "epoch": 0.10390243902439024,
      "grad_norm": 0.09586383402347565,
      "learning_rate": 4.9852829267335495e-05,
      "loss": 0.0408,
      "step": 426
    },
    {
      "epoch": 0.10414634146341463,
      "grad_norm": 0.0673103928565979,
      "learning_rate": 4.985213656871024e-05,
      "loss": 0.0339,
      "step": 427
    },
    {
      "epoch": 0.10439024390243902,
      "grad_norm": 0.07337801158428192,
      "learning_rate": 4.9851442248559165e-05,
      "loss": 0.0482,
      "step": 428
    },
    {
      "epoch": 0.10463414634146341,
      "grad_norm": 0.08426070213317871,
      "learning_rate": 4.985074630692756e-05,
      "loss": 0.0436,
      "step": 429
    },
    {
      "epoch": 0.1048780487804878,
      "grad_norm": 0.06920512765645981,
      "learning_rate": 4.9850048743860834e-05,
      "loss": 0.034,
      "step": 430
    },
    {
      "epoch": 0.10512195121951219,
      "grad_norm": 0.13212040066719055,
      "learning_rate": 4.98493495594045e-05,
      "loss": 0.0446,
      "step": 431
    },
    {
      "epoch": 0.10536585365853658,
      "grad_norm": 0.09313099086284637,
      "learning_rate": 4.984864875360417e-05,
      "loss": 0.032,
      "step": 432
    },
    {
      "epoch": 0.10560975609756097,
      "grad_norm": 0.09071575105190277,
      "learning_rate": 4.984794632650559e-05,
      "loss": 0.053,
      "step": 433
    },
    {
      "epoch": 0.10585365853658536,
      "grad_norm": 0.06451049447059631,
      "learning_rate": 4.984724227815457e-05,
      "loss": 0.0386,
      "step": 434
    },
    {
      "epoch": 0.10609756097560975,
      "grad_norm": 0.09286642074584961,
      "learning_rate": 4.984653660859706e-05,
      "loss": 0.0429,
      "step": 435
    },
    {
      "epoch": 0.10634146341463414,
      "grad_norm": 0.06505332887172699,
      "learning_rate": 4.98458293178791e-05,
      "loss": 0.0337,
      "step": 436
    },
    {
      "epoch": 0.10658536585365853,
      "grad_norm": 0.10696045309305191,
      "learning_rate": 4.984512040604683e-05,
      "loss": 0.0335,
      "step": 437
    },
    {
      "epoch": 0.10682926829268292,
      "grad_norm": 0.06915085017681122,
      "learning_rate": 4.984440987314652e-05,
      "loss": 0.035,
      "step": 438
    },
    {
      "epoch": 0.10707317073170732,
      "grad_norm": 0.09241428971290588,
      "learning_rate": 4.984369771922452e-05,
      "loss": 0.0394,
      "step": 439
    },
    {
      "epoch": 0.1073170731707317,
      "grad_norm": 0.07930555939674377,
      "learning_rate": 4.9842983944327296e-05,
      "loss": 0.039,
      "step": 440
    },
    {
      "epoch": 0.1075609756097561,
      "grad_norm": 0.07909055054187775,
      "learning_rate": 4.9842268548501424e-05,
      "loss": 0.0311,
      "step": 441
    },
    {
      "epoch": 0.10780487804878049,
      "grad_norm": 0.07793484628200531,
      "learning_rate": 4.9841551531793576e-05,
      "loss": 0.0373,
      "step": 442
    },
    {
      "epoch": 0.10804878048780488,
      "grad_norm": 0.040790293365716934,
      "learning_rate": 4.984083289425054e-05,
      "loss": 0.0272,
      "step": 443
    },
    {
      "epoch": 0.10829268292682927,
      "grad_norm": 0.05808541178703308,
      "learning_rate": 4.98401126359192e-05,
      "loss": 0.0408,
      "step": 444
    },
    {
      "epoch": 0.10853658536585366,
      "grad_norm": 0.09540708363056183,
      "learning_rate": 4.983939075684655e-05,
      "loss": 0.0428,
      "step": 445
    },
    {
      "epoch": 0.10878048780487805,
      "grad_norm": 0.0748303011059761,
      "learning_rate": 4.98386672570797e-05,
      "loss": 0.0421,
      "step": 446
    },
    {
      "epoch": 0.10902439024390244,
      "grad_norm": 0.05937952920794487,
      "learning_rate": 4.9837942136665846e-05,
      "loss": 0.0291,
      "step": 447
    },
    {
      "epoch": 0.10926829268292683,
      "grad_norm": 0.07492125034332275,
      "learning_rate": 4.9837215395652314e-05,
      "loss": 0.0436,
      "step": 448
    },
    {
      "epoch": 0.10951219512195122,
      "grad_norm": 0.09373125433921814,
      "learning_rate": 4.98364870340865e-05,
      "loss": 0.0355,
      "step": 449
    },
    {
      "epoch": 0.10975609756097561,
      "grad_norm": 0.09487105906009674,
      "learning_rate": 4.983575705201594e-05,
      "loss": 0.0504,
      "step": 450
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.10683869570493698,
      "learning_rate": 4.9835025449488274e-05,
      "loss": 0.039,
      "step": 451
    },
    {
      "epoch": 0.11024390243902439,
      "grad_norm": 0.08433617651462555,
      "learning_rate": 4.983429222655121e-05,
      "loss": 0.0398,
      "step": 452
    },
    {
      "epoch": 0.11048780487804878,
      "grad_norm": 0.07613435387611389,
      "learning_rate": 4.9833557383252606e-05,
      "loss": 0.0377,
      "step": 453
    },
    {
      "epoch": 0.11073170731707317,
      "grad_norm": 0.061195988208055496,
      "learning_rate": 4.983282091964041e-05,
      "loss": 0.0308,
      "step": 454
    },
    {
      "epoch": 0.11097560975609756,
      "grad_norm": 0.08292952179908752,
      "learning_rate": 4.983208283576267e-05,
      "loss": 0.0473,
      "step": 455
    },
    {
      "epoch": 0.11121951219512195,
      "grad_norm": 0.10104907304048538,
      "learning_rate": 4.983134313166754e-05,
      "loss": 0.0401,
      "step": 456
    },
    {
      "epoch": 0.11146341463414634,
      "grad_norm": 0.09622244536876678,
      "learning_rate": 4.983060180740329e-05,
      "loss": 0.0317,
      "step": 457
    },
    {
      "epoch": 0.11170731707317073,
      "grad_norm": 0.060477931052446365,
      "learning_rate": 4.982985886301828e-05,
      "loss": 0.0281,
      "step": 458
    },
    {
      "epoch": 0.11195121951219512,
      "grad_norm": 0.05398865044116974,
      "learning_rate": 4.9829114298561e-05,
      "loss": 0.0329,
      "step": 459
    },
    {
      "epoch": 0.11219512195121951,
      "grad_norm": 0.07416335493326187,
      "learning_rate": 4.982836811408001e-05,
      "loss": 0.0419,
      "step": 460
    },
    {
      "epoch": 0.1124390243902439,
      "grad_norm": 0.08571415394544601,
      "learning_rate": 4.9827620309624015e-05,
      "loss": 0.0461,
      "step": 461
    },
    {
      "epoch": 0.1126829268292683,
      "grad_norm": 0.07692678272724152,
      "learning_rate": 4.98268708852418e-05,
      "loss": 0.0342,
      "step": 462
    },
    {
      "epoch": 0.11292682926829269,
      "grad_norm": 0.05784251168370247,
      "learning_rate": 4.982611984098226e-05,
      "loss": 0.0243,
      "step": 463
    },
    {
      "epoch": 0.11317073170731708,
      "grad_norm": 0.05782776698470116,
      "learning_rate": 4.98253671768944e-05,
      "loss": 0.0416,
      "step": 464
    },
    {
      "epoch": 0.11341463414634147,
      "grad_norm": 0.06932869553565979,
      "learning_rate": 4.982461289302732e-05,
      "loss": 0.0403,
      "step": 465
    },
    {
      "epoch": 0.11365853658536586,
      "grad_norm": 0.071245938539505,
      "learning_rate": 4.9823856989430253e-05,
      "loss": 0.0484,
      "step": 466
    },
    {
      "epoch": 0.11390243902439025,
      "grad_norm": 0.09383168071508408,
      "learning_rate": 4.982309946615251e-05,
      "loss": 0.045,
      "step": 467
    },
    {
      "epoch": 0.11414634146341464,
      "grad_norm": 0.0748765766620636,
      "learning_rate": 4.982234032324352e-05,
      "loss": 0.0437,
      "step": 468
    },
    {
      "epoch": 0.11439024390243903,
      "grad_norm": 0.060360148549079895,
      "learning_rate": 4.982157956075282e-05,
      "loss": 0.0318,
      "step": 469
    },
    {
      "epoch": 0.11463414634146342,
      "grad_norm": 0.07918936759233475,
      "learning_rate": 4.9820817178730026e-05,
      "loss": 0.0351,
      "step": 470
    },
    {
      "epoch": 0.11487804878048781,
      "grad_norm": 0.07400240749120712,
      "learning_rate": 4.98200531772249e-05,
      "loss": 0.0288,
      "step": 471
    },
    {
      "epoch": 0.1151219512195122,
      "grad_norm": 0.06657431274652481,
      "learning_rate": 4.9819287556287286e-05,
      "loss": 0.0429,
      "step": 472
    },
    {
      "epoch": 0.11536585365853659,
      "grad_norm": 0.09011514484882355,
      "learning_rate": 4.9818520315967134e-05,
      "loss": 0.0403,
      "step": 473
    },
    {
      "epoch": 0.11560975609756098,
      "grad_norm": 0.08752687275409698,
      "learning_rate": 4.981775145631451e-05,
      "loss": 0.0337,
      "step": 474
    },
    {
      "epoch": 0.11585365853658537,
      "grad_norm": 0.059543073177337646,
      "learning_rate": 4.981698097737958e-05,
      "loss": 0.0298,
      "step": 475
    },
    {
      "epoch": 0.11609756097560976,
      "grad_norm": 0.09018857777118683,
      "learning_rate": 4.981620887921261e-05,
      "loss": 0.0412,
      "step": 476
    },
    {
      "epoch": 0.11634146341463415,
      "grad_norm": 0.0796033963561058,
      "learning_rate": 4.981543516186399e-05,
      "loss": 0.0311,
      "step": 477
    },
    {
      "epoch": 0.11658536585365854,
      "grad_norm": 0.06762673705816269,
      "learning_rate": 4.9814659825384185e-05,
      "loss": 0.0275,
      "step": 478
    },
    {
      "epoch": 0.11682926829268293,
      "grad_norm": 0.06810323894023895,
      "learning_rate": 4.981388286982379e-05,
      "loss": 0.041,
      "step": 479
    },
    {
      "epoch": 0.11707317073170732,
      "grad_norm": 0.06444866210222244,
      "learning_rate": 4.981310429523351e-05,
      "loss": 0.0306,
      "step": 480
    },
    {
      "epoch": 0.1173170731707317,
      "grad_norm": 0.10154884308576584,
      "learning_rate": 4.981232410166412e-05,
      "loss": 0.0417,
      "step": 481
    },
    {
      "epoch": 0.11756097560975609,
      "grad_norm": 0.0837535485625267,
      "learning_rate": 4.981154228916655e-05,
      "loss": 0.0394,
      "step": 482
    },
    {
      "epoch": 0.11780487804878048,
      "grad_norm": 0.07776030898094177,
      "learning_rate": 4.98107588577918e-05,
      "loss": 0.0317,
      "step": 483
    },
    {
      "epoch": 0.11804878048780487,
      "grad_norm": 0.06149902567267418,
      "learning_rate": 4.9809973807590985e-05,
      "loss": 0.0397,
      "step": 484
    },
    {
      "epoch": 0.11829268292682926,
      "grad_norm": 0.12366362661123276,
      "learning_rate": 4.980918713861533e-05,
      "loss": 0.0468,
      "step": 485
    },
    {
      "epoch": 0.11853658536585365,
      "grad_norm": 0.07756291329860687,
      "learning_rate": 4.980839885091616e-05,
      "loss": 0.0324,
      "step": 486
    },
    {
      "epoch": 0.11878048780487804,
      "grad_norm": 0.07730048149824142,
      "learning_rate": 4.980760894454492e-05,
      "loss": 0.0341,
      "step": 487
    },
    {
      "epoch": 0.11902439024390243,
      "grad_norm": 0.06551382690668106,
      "learning_rate": 4.980681741955313e-05,
      "loss": 0.0328,
      "step": 488
    },
    {
      "epoch": 0.11926829268292682,
      "grad_norm": 0.1567613035440445,
      "learning_rate": 4.9806024275992455e-05,
      "loss": 0.0497,
      "step": 489
    },
    {
      "epoch": 0.11951219512195121,
      "grad_norm": 0.05363524332642555,
      "learning_rate": 4.980522951391462e-05,
      "loss": 0.0315,
      "step": 490
    },
    {
      "epoch": 0.1197560975609756,
      "grad_norm": 0.058235764503479004,
      "learning_rate": 4.980443313337151e-05,
      "loss": 0.0435,
      "step": 491
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.0797344371676445,
      "learning_rate": 4.980363513441506e-05,
      "loss": 0.0391,
      "step": 492
    },
    {
      "epoch": 0.12024390243902439,
      "grad_norm": 0.06820965558290482,
      "learning_rate": 4.980283551709736e-05,
      "loss": 0.0328,
      "step": 493
    },
    {
      "epoch": 0.12048780487804878,
      "grad_norm": 0.07400291413068771,
      "learning_rate": 4.980203428147057e-05,
      "loss": 0.0407,
      "step": 494
    },
    {
      "epoch": 0.12073170731707317,
      "grad_norm": 0.062063831835985184,
      "learning_rate": 4.980123142758696e-05,
      "loss": 0.0281,
      "step": 495
    },
    {
      "epoch": 0.12097560975609756,
      "grad_norm": 0.08335894346237183,
      "learning_rate": 4.980042695549893e-05,
      "loss": 0.0496,
      "step": 496
    },
    {
      "epoch": 0.12121951219512195,
      "grad_norm": 0.05429130792617798,
      "learning_rate": 4.979962086525895e-05,
      "loss": 0.0263,
      "step": 497
    },
    {
      "epoch": 0.12146341463414634,
      "grad_norm": 0.08241809159517288,
      "learning_rate": 4.979881315691964e-05,
      "loss": 0.0259,
      "step": 498
    },
    {
      "epoch": 0.12170731707317073,
      "grad_norm": 0.1176743432879448,
      "learning_rate": 4.979800383053369e-05,
      "loss": 0.0408,
      "step": 499
    },
    {
      "epoch": 0.12195121951219512,
      "grad_norm": 0.13998502492904663,
      "learning_rate": 4.97971928861539e-05,
      "loss": 0.0348,
      "step": 500
    },
    {
      "epoch": 0.12219512195121951,
      "grad_norm": 0.09161805361509323,
      "learning_rate": 4.979638032383318e-05,
      "loss": 0.0291,
      "step": 501
    },
    {
      "epoch": 0.1224390243902439,
      "grad_norm": 0.062462352216243744,
      "learning_rate": 4.979556614362456e-05,
      "loss": 0.0424,
      "step": 502
    },
    {
      "epoch": 0.12268292682926829,
      "grad_norm": 0.07598356902599335,
      "learning_rate": 4.979475034558115e-05,
      "loss": 0.0341,
      "step": 503
    },
    {
      "epoch": 0.12292682926829268,
      "grad_norm": 0.07321566343307495,
      "learning_rate": 4.9793932929756196e-05,
      "loss": 0.0424,
      "step": 504
    },
    {
      "epoch": 0.12317073170731707,
      "grad_norm": 0.08746282011270523,
      "learning_rate": 4.979311389620301e-05,
      "loss": 0.0385,
      "step": 505
    },
    {
      "epoch": 0.12341463414634146,
      "grad_norm": 0.09696151316165924,
      "learning_rate": 4.979229324497504e-05,
      "loss": 0.0493,
      "step": 506
    },
    {
      "epoch": 0.12365853658536585,
      "grad_norm": 0.07515886425971985,
      "learning_rate": 4.9791470976125834e-05,
      "loss": 0.0386,
      "step": 507
    },
    {
      "epoch": 0.12390243902439024,
      "grad_norm": 0.06603161245584488,
      "learning_rate": 4.979064708970904e-05,
      "loss": 0.035,
      "step": 508
    },
    {
      "epoch": 0.12414634146341463,
      "grad_norm": 0.07358340173959732,
      "learning_rate": 4.978982158577842e-05,
      "loss": 0.0288,
      "step": 509
    },
    {
      "epoch": 0.12439024390243902,
      "grad_norm": 0.09236006438732147,
      "learning_rate": 4.978899446438783e-05,
      "loss": 0.0354,
      "step": 510
    },
    {
      "epoch": 0.12463414634146341,
      "grad_norm": 0.062358904629945755,
      "learning_rate": 4.978816572559123e-05,
      "loss": 0.0365,
      "step": 511
    },
    {
      "epoch": 0.1248780487804878,
      "grad_norm": 0.06912057101726532,
      "learning_rate": 4.978733536944271e-05,
      "loss": 0.0325,
      "step": 512
    },
    {
      "epoch": 0.1251219512195122,
      "grad_norm": 0.09705591946840286,
      "learning_rate": 4.978650339599643e-05,
      "loss": 0.0368,
      "step": 513
    },
    {
      "epoch": 0.12536585365853659,
      "grad_norm": 0.06161901354789734,
      "learning_rate": 4.978566980530669e-05,
      "loss": 0.0311,
      "step": 514
    },
    {
      "epoch": 0.12560975609756098,
      "grad_norm": 0.056798942387104034,
      "learning_rate": 4.978483459742787e-05,
      "loss": 0.0335,
      "step": 515
    },
    {
      "epoch": 0.12585365853658537,
      "grad_norm": 0.0889318436384201,
      "learning_rate": 4.978399777241446e-05,
      "loss": 0.0292,
      "step": 516
    },
    {
      "epoch": 0.12609756097560976,
      "grad_norm": 0.047896187752485275,
      "learning_rate": 4.978315933032107e-05,
      "loss": 0.0283,
      "step": 517
    },
    {
      "epoch": 0.12634146341463415,
      "grad_norm": 0.10688997805118561,
      "learning_rate": 4.97823192712024e-05,
      "loss": 0.0377,
      "step": 518
    },
    {
      "epoch": 0.12658536585365854,
      "grad_norm": 0.06301195174455643,
      "learning_rate": 4.978147759511327e-05,
      "loss": 0.0396,
      "step": 519
    },
    {
      "epoch": 0.12682926829268293,
      "grad_norm": 0.07844711095094681,
      "learning_rate": 4.978063430210858e-05,
      "loss": 0.0413,
      "step": 520
    },
    {
      "epoch": 0.12707317073170732,
      "grad_norm": 0.10147813707590103,
      "learning_rate": 4.977978939224337e-05,
      "loss": 0.0383,
      "step": 521
    },
    {
      "epoch": 0.1273170731707317,
      "grad_norm": 0.06231248751282692,
      "learning_rate": 4.977894286557276e-05,
      "loss": 0.0406,
      "step": 522
    },
    {
      "epoch": 0.1275609756097561,
      "grad_norm": 0.05618850886821747,
      "learning_rate": 4.977809472215198e-05,
      "loss": 0.0398,
      "step": 523
    },
    {
      "epoch": 0.1278048780487805,
      "grad_norm": 0.05522388964891434,
      "learning_rate": 4.9777244962036374e-05,
      "loss": 0.0333,
      "step": 524
    },
    {
      "epoch": 0.12804878048780488,
      "grad_norm": 0.07078279554843903,
      "learning_rate": 4.977639358528139e-05,
      "loss": 0.0423,
      "step": 525
    },
    {
      "epoch": 0.12829268292682927,
      "grad_norm": 0.05249302089214325,
      "learning_rate": 4.9775540591942566e-05,
      "loss": 0.0298,
      "step": 526
    },
    {
      "epoch": 0.12853658536585366,
      "grad_norm": 0.10484017431735992,
      "learning_rate": 4.977468598207556e-05,
      "loss": 0.0486,
      "step": 527
    },
    {
      "epoch": 0.12878048780487805,
      "grad_norm": 0.10666751116514206,
      "learning_rate": 4.9773829755736145e-05,
      "loss": 0.0498,
      "step": 528
    },
    {
      "epoch": 0.12902439024390244,
      "grad_norm": 0.07524485141038895,
      "learning_rate": 4.9772971912980175e-05,
      "loss": 0.037,
      "step": 529
    },
    {
      "epoch": 0.12926829268292683,
      "grad_norm": 0.060896631330251694,
      "learning_rate": 4.977211245386363e-05,
      "loss": 0.0397,
      "step": 530
    },
    {
      "epoch": 0.12951219512195122,
      "grad_norm": 0.09244644641876221,
      "learning_rate": 4.977125137844257e-05,
      "loss": 0.0426,
      "step": 531
    },
    {
      "epoch": 0.12975609756097561,
      "grad_norm": 0.062212806195020676,
      "learning_rate": 4.97703886867732e-05,
      "loss": 0.0265,
      "step": 532
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.06279252469539642,
      "learning_rate": 4.976952437891179e-05,
      "loss": 0.0337,
      "step": 533
    },
    {
      "epoch": 0.1302439024390244,
      "grad_norm": 0.10845047980546951,
      "learning_rate": 4.9768658454914754e-05,
      "loss": 0.0376,
      "step": 534
    },
    {
      "epoch": 0.13048780487804879,
      "grad_norm": 0.11953095346689224,
      "learning_rate": 4.9767790914838566e-05,
      "loss": 0.0427,
      "step": 535
    },
    {
      "epoch": 0.13073170731707318,
      "grad_norm": 0.09160062670707703,
      "learning_rate": 4.976692175873985e-05,
      "loss": 0.0429,
      "step": 536
    },
    {
      "epoch": 0.13097560975609757,
      "grad_norm": 0.07949305325746536,
      "learning_rate": 4.97660509866753e-05,
      "loss": 0.0328,
      "step": 537
    },
    {
      "epoch": 0.13121951219512196,
      "grad_norm": 0.08398471027612686,
      "learning_rate": 4.976517859870175e-05,
      "loss": 0.0378,
      "step": 538
    },
    {
      "epoch": 0.13146341463414635,
      "grad_norm": 0.055667225271463394,
      "learning_rate": 4.9764304594876096e-05,
      "loss": 0.0349,
      "step": 539
    },
    {
      "epoch": 0.13170731707317074,
      "grad_norm": 0.09638989716768265,
      "learning_rate": 4.976342897525539e-05,
      "loss": 0.0336,
      "step": 540
    },
    {
      "epoch": 0.13195121951219513,
      "grad_norm": 0.07982378453016281,
      "learning_rate": 4.976255173989674e-05,
      "loss": 0.0369,
      "step": 541
    },
    {
      "epoch": 0.13219512195121952,
      "grad_norm": 0.08373619616031647,
      "learning_rate": 4.97616728888574e-05,
      "loss": 0.0366,
      "step": 542
    },
    {
      "epoch": 0.1324390243902439,
      "grad_norm": 0.0647912323474884,
      "learning_rate": 4.9760792422194714e-05,
      "loss": 0.0309,
      "step": 543
    },
    {
      "epoch": 0.1326829268292683,
      "grad_norm": 0.07229425013065338,
      "learning_rate": 4.975991033996611e-05,
      "loss": 0.0443,
      "step": 544
    },
    {
      "epoch": 0.1329268292682927,
      "grad_norm": 0.06695978343486786,
      "learning_rate": 4.975902664222916e-05,
      "loss": 0.0342,
      "step": 545
    },
    {
      "epoch": 0.13317073170731708,
      "grad_norm": 0.06310383975505829,
      "learning_rate": 4.975814132904152e-05,
      "loss": 0.0236,
      "step": 546
    },
    {
      "epoch": 0.13341463414634147,
      "grad_norm": 0.058290161192417145,
      "learning_rate": 4.975725440046095e-05,
      "loss": 0.0407,
      "step": 547
    },
    {
      "epoch": 0.13365853658536586,
      "grad_norm": 0.084477499127388,
      "learning_rate": 4.975636585654531e-05,
      "loss": 0.0355,
      "step": 548
    },
    {
      "epoch": 0.13390243902439025,
      "grad_norm": 0.08815830945968628,
      "learning_rate": 4.9755475697352596e-05,
      "loss": 0.0362,
      "step": 549
    },
    {
      "epoch": 0.13414634146341464,
      "grad_norm": 0.11065312474966049,
      "learning_rate": 4.975458392294087e-05,
      "loss": 0.047,
      "step": 550
    },
    {
      "epoch": 0.13439024390243903,
      "grad_norm": 0.06334343552589417,
      "learning_rate": 4.975369053336832e-05,
      "loss": 0.0278,
      "step": 551
    },
    {
      "epoch": 0.13463414634146342,
      "grad_norm": 0.10576045513153076,
      "learning_rate": 4.975279552869325e-05,
      "loss": 0.0426,
      "step": 552
    },
    {
      "epoch": 0.1348780487804878,
      "grad_norm": 0.07654095441102982,
      "learning_rate": 4.9751898908974036e-05,
      "loss": 0.0445,
      "step": 553
    },
    {
      "epoch": 0.1351219512195122,
      "grad_norm": 0.07822796702384949,
      "learning_rate": 4.97510006742692e-05,
      "loss": 0.0329,
      "step": 554
    },
    {
      "epoch": 0.1353658536585366,
      "grad_norm": 0.08753190189599991,
      "learning_rate": 4.975010082463734e-05,
      "loss": 0.0303,
      "step": 555
    },
    {
      "epoch": 0.13560975609756099,
      "grad_norm": 0.07667145133018494,
      "learning_rate": 4.9749199360137164e-05,
      "loss": 0.0316,
      "step": 556
    },
    {
      "epoch": 0.13585365853658538,
      "grad_norm": 0.06360435485839844,
      "learning_rate": 4.97482962808275e-05,
      "loss": 0.0309,
      "step": 557
    },
    {
      "epoch": 0.13609756097560977,
      "grad_norm": 0.06867347657680511,
      "learning_rate": 4.974739158676727e-05,
      "loss": 0.0285,
      "step": 558
    },
    {
      "epoch": 0.13634146341463416,
      "grad_norm": 0.06708958745002747,
      "learning_rate": 4.9746485278015486e-05,
      "loss": 0.035,
      "step": 559
    },
    {
      "epoch": 0.13658536585365855,
      "grad_norm": 0.07459678500890732,
      "learning_rate": 4.97455773546313e-05,
      "loss": 0.031,
      "step": 560
    },
    {
      "epoch": 0.13682926829268294,
      "grad_norm": 0.06319309771060944,
      "learning_rate": 4.974466781667394e-05,
      "loss": 0.0327,
      "step": 561
    },
    {
      "epoch": 0.13707317073170733,
      "grad_norm": 0.06001976877450943,
      "learning_rate": 4.974375666420276e-05,
      "loss": 0.0207,
      "step": 562
    },
    {
      "epoch": 0.13731707317073172,
      "grad_norm": 0.10197357833385468,
      "learning_rate": 4.9742843897277205e-05,
      "loss": 0.0407,
      "step": 563
    },
    {
      "epoch": 0.1375609756097561,
      "grad_norm": 0.0807543396949768,
      "learning_rate": 4.974192951595683e-05,
      "loss": 0.0435,
      "step": 564
    },
    {
      "epoch": 0.1378048780487805,
      "grad_norm": 0.06901230663061142,
      "learning_rate": 4.97410135203013e-05,
      "loss": 0.0322,
      "step": 565
    },
    {
      "epoch": 0.1380487804878049,
      "grad_norm": 0.06177770718932152,
      "learning_rate": 4.974009591037037e-05,
      "loss": 0.0274,
      "step": 566
    },
    {
      "epoch": 0.13829268292682928,
      "grad_norm": 0.06246274709701538,
      "learning_rate": 4.9739176686223926e-05,
      "loss": 0.0329,
      "step": 567
    },
    {
      "epoch": 0.13853658536585367,
      "grad_norm": 0.09409689903259277,
      "learning_rate": 4.9738255847921933e-05,
      "loss": 0.034,
      "step": 568
    },
    {
      "epoch": 0.13878048780487806,
      "grad_norm": 0.1011076346039772,
      "learning_rate": 4.973733339552448e-05,
      "loss": 0.0372,
      "step": 569
    },
    {
      "epoch": 0.13902439024390245,
      "grad_norm": 0.07156527787446976,
      "learning_rate": 4.9736409329091746e-05,
      "loss": 0.0347,
      "step": 570
    },
    {
      "epoch": 0.13926829268292684,
      "grad_norm": 0.08399983495473862,
      "learning_rate": 4.9735483648684033e-05,
      "loss": 0.0346,
      "step": 571
    },
    {
      "epoch": 0.13951219512195123,
      "grad_norm": 0.07017457485198975,
      "learning_rate": 4.973455635436173e-05,
      "loss": 0.0282,
      "step": 572
    },
    {
      "epoch": 0.13975609756097562,
      "grad_norm": 0.058187633752822876,
      "learning_rate": 4.973362744618535e-05,
      "loss": 0.0311,
      "step": 573
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.05250558629631996,
      "learning_rate": 4.973269692421549e-05,
      "loss": 0.0256,
      "step": 574
    },
    {
      "epoch": 0.1402439024390244,
      "grad_norm": 0.086885467171669,
      "learning_rate": 4.973176478851288e-05,
      "loss": 0.0386,
      "step": 575
    },
    {
      "epoch": 0.1404878048780488,
      "grad_norm": 0.043865758925676346,
      "learning_rate": 4.973083103913832e-05,
      "loss": 0.0237,
      "step": 576
    },
    {
      "epoch": 0.14073170731707316,
      "grad_norm": 0.06794356554746628,
      "learning_rate": 4.972989567615275e-05,
      "loss": 0.0356,
      "step": 577
    },
    {
      "epoch": 0.14097560975609755,
      "grad_norm": 0.06138835474848747,
      "learning_rate": 4.972895869961719e-05,
      "loss": 0.0319,
      "step": 578
    },
    {
      "epoch": 0.14121951219512194,
      "grad_norm": 0.06967312097549438,
      "learning_rate": 4.972802010959277e-05,
      "loss": 0.0325,
      "step": 579
    },
    {
      "epoch": 0.14146341463414633,
      "grad_norm": 0.09768449515104294,
      "learning_rate": 4.9727079906140746e-05,
      "loss": 0.036,
      "step": 580
    },
    {
      "epoch": 0.14170731707317072,
      "grad_norm": 0.16003106534481049,
      "learning_rate": 4.972613808932245e-05,
      "loss": 0.048,
      "step": 581
    },
    {
      "epoch": 0.1419512195121951,
      "grad_norm": 0.07429355382919312,
      "learning_rate": 4.9725194659199346e-05,
      "loss": 0.0362,
      "step": 582
    },
    {
      "epoch": 0.1421951219512195,
      "grad_norm": 0.10203506052494049,
      "learning_rate": 4.9724249615832975e-05,
      "loss": 0.0527,
      "step": 583
    },
    {
      "epoch": 0.1424390243902439,
      "grad_norm": 0.06857502460479736,
      "learning_rate": 4.9723302959285004e-05,
      "loss": 0.0253,
      "step": 584
    },
    {
      "epoch": 0.14268292682926828,
      "grad_norm": 0.06407959759235382,
      "learning_rate": 4.97223546896172e-05,
      "loss": 0.0296,
      "step": 585
    },
    {
      "epoch": 0.14292682926829267,
      "grad_norm": 0.08743616938591003,
      "learning_rate": 4.972140480689144e-05,
      "loss": 0.0294,
      "step": 586
    },
    {
      "epoch": 0.14317073170731706,
      "grad_norm": 0.0777868777513504,
      "learning_rate": 4.972045331116969e-05,
      "loss": 0.0277,
      "step": 587
    },
    {
      "epoch": 0.14341463414634145,
      "grad_norm": 0.05848979577422142,
      "learning_rate": 4.9719500202514035e-05,
      "loss": 0.0314,
      "step": 588
    },
    {
      "epoch": 0.14365853658536584,
      "grad_norm": 0.09346693009138107,
      "learning_rate": 4.9718545480986675e-05,
      "loss": 0.0312,
      "step": 589
    },
    {
      "epoch": 0.14390243902439023,
      "grad_norm": 0.10490643233060837,
      "learning_rate": 4.971758914664989e-05,
      "loss": 0.028,
      "step": 590
    },
    {
      "epoch": 0.14414634146341462,
      "grad_norm": 0.06594213098287582,
      "learning_rate": 4.9716631199566074e-05,
      "loss": 0.0305,
      "step": 591
    },
    {
      "epoch": 0.144390243902439,
      "grad_norm": 0.10826906561851501,
      "learning_rate": 4.971567163979775e-05,
      "loss": 0.0333,
      "step": 592
    },
    {
      "epoch": 0.1446341463414634,
      "grad_norm": 0.06660110503435135,
      "learning_rate": 4.97147104674075e-05,
      "loss": 0.0305,
      "step": 593
    },
    {
      "epoch": 0.1448780487804878,
      "grad_norm": 0.06734812259674072,
      "learning_rate": 4.971374768245806e-05,
      "loss": 0.029,
      "step": 594
    },
    {
      "epoch": 0.14512195121951219,
      "grad_norm": 0.06754320859909058,
      "learning_rate": 4.971278328501223e-05,
      "loss": 0.0405,
      "step": 595
    },
    {
      "epoch": 0.14536585365853658,
      "grad_norm": 0.07597320526838303,
      "learning_rate": 4.971181727513296e-05,
      "loss": 0.0379,
      "step": 596
    },
    {
      "epoch": 0.14560975609756097,
      "grad_norm": 0.0591716505587101,
      "learning_rate": 4.971084965288325e-05,
      "loss": 0.0295,
      "step": 597
    },
    {
      "epoch": 0.14585365853658536,
      "grad_norm": 0.0870213657617569,
      "learning_rate": 4.970988041832625e-05,
      "loss": 0.0316,
      "step": 598
    },
    {
      "epoch": 0.14609756097560975,
      "grad_norm": 0.10909275710582733,
      "learning_rate": 4.97089095715252e-05,
      "loss": 0.053,
      "step": 599
    },
    {
      "epoch": 0.14634146341463414,
      "grad_norm": 0.08340934664011002,
      "learning_rate": 4.970793711254343e-05,
      "loss": 0.0339,
      "step": 600
    },
    {
      "epoch": 0.14658536585365853,
      "grad_norm": 0.11266573518514633,
      "learning_rate": 4.970696304144442e-05,
      "loss": 0.0399,
      "step": 601
    },
    {
      "epoch": 0.14682926829268292,
      "grad_norm": 0.09051866084337234,
      "learning_rate": 4.97059873582917e-05,
      "loss": 0.0487,
      "step": 602
    },
    {
      "epoch": 0.1470731707317073,
      "grad_norm": 0.05642912536859512,
      "learning_rate": 4.9705010063148935e-05,
      "loss": 0.0304,
      "step": 603
    },
    {
      "epoch": 0.1473170731707317,
      "grad_norm": 0.10788349062204361,
      "learning_rate": 4.970403115607989e-05,
      "loss": 0.0443,
      "step": 604
    },
    {
      "epoch": 0.1475609756097561,
      "grad_norm": 0.0997973158955574,
      "learning_rate": 4.970305063714844e-05,
      "loss": 0.0319,
      "step": 605
    },
    {
      "epoch": 0.14780487804878048,
      "grad_norm": 0.11040803790092468,
      "learning_rate": 4.970206850641856e-05,
      "loss": 0.033,
      "step": 606
    },
    {
      "epoch": 0.14804878048780487,
      "grad_norm": 0.10455699265003204,
      "learning_rate": 4.970108476395433e-05,
      "loss": 0.0378,
      "step": 607
    },
    {
      "epoch": 0.14829268292682926,
      "grad_norm": 0.08389648795127869,
      "learning_rate": 4.970009940981994e-05,
      "loss": 0.0492,
      "step": 608
    },
    {
      "epoch": 0.14853658536585365,
      "grad_norm": 0.0773332417011261,
      "learning_rate": 4.9699112444079675e-05,
      "loss": 0.0273,
      "step": 609
    },
    {
      "epoch": 0.14878048780487804,
      "grad_norm": 0.08576358109712601,
      "learning_rate": 4.969812386679793e-05,
      "loss": 0.0323,
      "step": 610
    },
    {
      "epoch": 0.14902439024390243,
      "grad_norm": 0.1516755074262619,
      "learning_rate": 4.969713367803922e-05,
      "loss": 0.0542,
      "step": 611
    },
    {
      "epoch": 0.14926829268292682,
      "grad_norm": 0.13427290320396423,
      "learning_rate": 4.9696141877868144e-05,
      "loss": 0.0423,
      "step": 612
    },
    {
      "epoch": 0.1495121951219512,
      "grad_norm": 0.07813792675733566,
      "learning_rate": 4.96951484663494e-05,
      "loss": 0.0373,
      "step": 613
    },
    {
      "epoch": 0.1497560975609756,
      "grad_norm": 0.05792416259646416,
      "learning_rate": 4.969415344354782e-05,
      "loss": 0.0333,
      "step": 614
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.0616840124130249,
      "learning_rate": 4.969315680952833e-05,
      "loss": 0.0286,
      "step": 615
    },
    {
      "epoch": 0.15024390243902438,
      "grad_norm": 0.07546527683734894,
      "learning_rate": 4.969215856435595e-05,
      "loss": 0.0318,
      "step": 616
    },
    {
      "epoch": 0.15048780487804878,
      "grad_norm": 0.07678277045488358,
      "learning_rate": 4.9691158708095815e-05,
      "loss": 0.0452,
      "step": 617
    },
    {
      "epoch": 0.15073170731707317,
      "grad_norm": 0.07369130849838257,
      "learning_rate": 4.969015724081316e-05,
      "loss": 0.0385,
      "step": 618
    },
    {
      "epoch": 0.15097560975609756,
      "grad_norm": 0.07164681702852249,
      "learning_rate": 4.968915416257332e-05,
      "loss": 0.0385,
      "step": 619
    },
    {
      "epoch": 0.15121951219512195,
      "grad_norm": 0.07866069674491882,
      "learning_rate": 4.9688149473441765e-05,
      "loss": 0.04,
      "step": 620
    },
    {
      "epoch": 0.15146341463414634,
      "grad_norm": 0.217635378241539,
      "learning_rate": 4.968714317348403e-05,
      "loss": 0.0369,
      "step": 621
    },
    {
      "epoch": 0.15170731707317073,
      "grad_norm": 0.07607681304216385,
      "learning_rate": 4.9686135262765776e-05,
      "loss": 0.0351,
      "step": 622
    },
    {
      "epoch": 0.15195121951219512,
      "grad_norm": 0.05614585801959038,
      "learning_rate": 4.9685125741352764e-05,
      "loss": 0.0349,
      "step": 623
    },
    {
      "epoch": 0.1521951219512195,
      "grad_norm": 0.14062349498271942,
      "learning_rate": 4.968411460931087e-05,
      "loss": 0.0354,
      "step": 624
    },
    {
      "epoch": 0.1524390243902439,
      "grad_norm": 0.09730290621519089,
      "learning_rate": 4.968310186670607e-05,
      "loss": 0.0566,
      "step": 625
    },
    {
      "epoch": 0.1526829268292683,
      "grad_norm": 0.15262730419635773,
      "learning_rate": 4.9682087513604426e-05,
      "loss": 0.0532,
      "step": 626
    },
    {
      "epoch": 0.15292682926829268,
      "grad_norm": 0.08087661117315292,
      "learning_rate": 4.9681071550072136e-05,
      "loss": 0.043,
      "step": 627
    },
    {
      "epoch": 0.15317073170731707,
      "grad_norm": 0.06488432735204697,
      "learning_rate": 4.968005397617548e-05,
      "loss": 0.0341,
      "step": 628
    },
    {
      "epoch": 0.15341463414634146,
      "grad_norm": 0.05870409682393074,
      "learning_rate": 4.967903479198085e-05,
      "loss": 0.0271,
      "step": 629
    },
    {
      "epoch": 0.15365853658536585,
      "grad_norm": 0.11686813086271286,
      "learning_rate": 4.967801399755476e-05,
      "loss": 0.0415,
      "step": 630
    },
    {
      "epoch": 0.15390243902439024,
      "grad_norm": 0.07132479548454285,
      "learning_rate": 4.9676991592963796e-05,
      "loss": 0.0415,
      "step": 631
    },
    {
      "epoch": 0.15414634146341463,
      "grad_norm": 0.07087322324514389,
      "learning_rate": 4.967596757827467e-05,
      "loss": 0.0263,
      "step": 632
    },
    {
      "epoch": 0.15439024390243902,
      "grad_norm": 0.06122620776295662,
      "learning_rate": 4.967494195355421e-05,
      "loss": 0.0274,
      "step": 633
    },
    {
      "epoch": 0.1546341463414634,
      "grad_norm": 0.11363587528467178,
      "learning_rate": 4.967391471886932e-05,
      "loss": 0.0397,
      "step": 634
    },
    {
      "epoch": 0.1548780487804878,
      "grad_norm": 0.10411570966243744,
      "learning_rate": 4.9672885874287034e-05,
      "loss": 0.0496,
      "step": 635
    },
    {
      "epoch": 0.1551219512195122,
      "grad_norm": 0.2018328607082367,
      "learning_rate": 4.967185541987447e-05,
      "loss": 0.0387,
      "step": 636
    },
    {
      "epoch": 0.15536585365853658,
      "grad_norm": 0.1299741268157959,
      "learning_rate": 4.967082335569887e-05,
      "loss": 0.0328,
      "step": 637
    },
    {
      "epoch": 0.15560975609756098,
      "grad_norm": 0.13516101241111755,
      "learning_rate": 4.9669789681827573e-05,
      "loss": 0.052,
      "step": 638
    },
    {
      "epoch": 0.15585365853658537,
      "grad_norm": 0.08425730466842651,
      "learning_rate": 4.966875439832801e-05,
      "loss": 0.0264,
      "step": 639
    },
    {
      "epoch": 0.15609756097560976,
      "grad_norm": 0.07497410476207733,
      "learning_rate": 4.9667717505267756e-05,
      "loss": 0.033,
      "step": 640
    },
    {
      "epoch": 0.15634146341463415,
      "grad_norm": 0.12662003934383392,
      "learning_rate": 4.966667900271444e-05,
      "loss": 0.0372,
      "step": 641
    },
    {
      "epoch": 0.15658536585365854,
      "grad_norm": 0.06951525062322617,
      "learning_rate": 4.9665638890735834e-05,
      "loss": 0.0313,
      "step": 642
    },
    {
      "epoch": 0.15682926829268293,
      "grad_norm": 0.07839283347129822,
      "learning_rate": 4.96645971693998e-05,
      "loss": 0.0403,
      "step": 643
    },
    {
      "epoch": 0.15707317073170732,
      "grad_norm": 0.07647930085659027,
      "learning_rate": 4.96635538387743e-05,
      "loss": 0.0332,
      "step": 644
    },
    {
      "epoch": 0.1573170731707317,
      "grad_norm": 0.0873926654458046,
      "learning_rate": 4.966250889892742e-05,
      "loss": 0.0388,
      "step": 645
    },
    {
      "epoch": 0.1575609756097561,
      "grad_norm": 0.06278476119041443,
      "learning_rate": 4.966146234992734e-05,
      "loss": 0.0349,
      "step": 646
    },
    {
      "epoch": 0.1578048780487805,
      "grad_norm": 0.09258034825325012,
      "learning_rate": 4.966041419184232e-05,
      "loss": 0.0303,
      "step": 647
    },
    {
      "epoch": 0.15804878048780488,
      "grad_norm": 0.16195765137672424,
      "learning_rate": 4.9659364424740784e-05,
      "loss": 0.0323,
      "step": 648
    },
    {
      "epoch": 0.15829268292682927,
      "grad_norm": 0.10167545080184937,
      "learning_rate": 4.9658313048691195e-05,
      "loss": 0.0384,
      "step": 649
    },
    {
      "epoch": 0.15853658536585366,
      "grad_norm": 0.0962439626455307,
      "learning_rate": 4.965726006376218e-05,
      "loss": 0.028,
      "step": 650
    },
    {
      "epoch": 0.15878048780487805,
      "grad_norm": 0.0661652460694313,
      "learning_rate": 4.965620547002242e-05,
      "loss": 0.0306,
      "step": 651
    },
    {
      "epoch": 0.15902439024390244,
      "grad_norm": 0.11683884263038635,
      "learning_rate": 4.9655149267540736e-05,
      "loss": 0.0406,
      "step": 652
    },
    {
      "epoch": 0.15926829268292683,
      "grad_norm": 0.14360485970973969,
      "learning_rate": 4.965409145638603e-05,
      "loss": 0.0641,
      "step": 653
    },
    {
      "epoch": 0.15951219512195122,
      "grad_norm": 0.06827496737241745,
      "learning_rate": 4.9653032036627346e-05,
      "loss": 0.0297,
      "step": 654
    },
    {
      "epoch": 0.1597560975609756,
      "grad_norm": 0.09780948609113693,
      "learning_rate": 4.965197100833378e-05,
      "loss": 0.0344,
      "step": 655
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.05697529762983322,
      "learning_rate": 4.965090837157458e-05,
      "loss": 0.0259,
      "step": 656
    },
    {
      "epoch": 0.1602439024390244,
      "grad_norm": 0.04785560071468353,
      "learning_rate": 4.9649844126419065e-05,
      "loss": 0.0182,
      "step": 657
    },
    {
      "epoch": 0.16048780487804878,
      "grad_norm": 0.07357104122638702,
      "learning_rate": 4.964877827293668e-05,
      "loss": 0.0331,
      "step": 658
    },
    {
      "epoch": 0.16073170731707317,
      "grad_norm": 0.17037072777748108,
      "learning_rate": 4.964771081119698e-05,
      "loss": 0.0373,
      "step": 659
    },
    {
      "epoch": 0.16097560975609757,
      "grad_norm": 0.10437629371881485,
      "learning_rate": 4.964664174126959e-05,
      "loss": 0.0216,
      "step": 660
    },
    {
      "epoch": 0.16121951219512196,
      "grad_norm": 0.09035224467515945,
      "learning_rate": 4.964557106322429e-05,
      "loss": 0.0328,
      "step": 661
    },
    {
      "epoch": 0.16146341463414635,
      "grad_norm": 0.08970461785793304,
      "learning_rate": 4.964449877713092e-05,
      "loss": 0.0384,
      "step": 662
    },
    {
      "epoch": 0.16170731707317074,
      "grad_norm": 0.11150462180376053,
      "learning_rate": 4.964342488305945e-05,
      "loss": 0.0416,
      "step": 663
    },
    {
      "epoch": 0.16195121951219513,
      "grad_norm": 0.07432714104652405,
      "learning_rate": 4.9642349381079945e-05,
      "loss": 0.0235,
      "step": 664
    },
    {
      "epoch": 0.16219512195121952,
      "grad_norm": 0.24005566537380219,
      "learning_rate": 4.9641272271262585e-05,
      "loss": 0.02,
      "step": 665
    },
    {
      "epoch": 0.1624390243902439,
      "grad_norm": 0.09214232861995697,
      "learning_rate": 4.964019355367764e-05,
      "loss": 0.0301,
      "step": 666
    },
    {
      "epoch": 0.1626829268292683,
      "grad_norm": 0.04404454678297043,
      "learning_rate": 4.9639113228395495e-05,
      "loss": 0.021,
      "step": 667
    },
    {
      "epoch": 0.1629268292682927,
      "grad_norm": 0.06740673631429672,
      "learning_rate": 4.9638031295486644e-05,
      "loss": 0.0313,
      "step": 668
    },
    {
      "epoch": 0.16317073170731708,
      "grad_norm": 0.08854702860116959,
      "learning_rate": 4.963694775502167e-05,
      "loss": 0.0337,
      "step": 669
    },
    {
      "epoch": 0.16341463414634147,
      "grad_norm": 0.0701928585767746,
      "learning_rate": 4.9635862607071284e-05,
      "loss": 0.0386,
      "step": 670
    },
    {
      "epoch": 0.16365853658536586,
      "grad_norm": 0.06596647948026657,
      "learning_rate": 4.963477585170627e-05,
      "loss": 0.0314,
      "step": 671
    },
    {
      "epoch": 0.16390243902439025,
      "grad_norm": 0.03179054334759712,
      "learning_rate": 4.963368748899755e-05,
      "loss": 0.0172,
      "step": 672
    },
    {
      "epoch": 0.16414634146341464,
      "grad_norm": 0.0667635127902031,
      "learning_rate": 4.963259751901613e-05,
      "loss": 0.0425,
      "step": 673
    },
    {
      "epoch": 0.16439024390243903,
      "grad_norm": 0.09842860698699951,
      "learning_rate": 4.963150594183313e-05,
      "loss": 0.041,
      "step": 674
    },
    {
      "epoch": 0.16463414634146342,
      "grad_norm": 0.09106063097715378,
      "learning_rate": 4.963041275751978e-05,
      "loss": 0.0318,
      "step": 675
    },
    {
      "epoch": 0.1648780487804878,
      "grad_norm": 0.06438722461462021,
      "learning_rate": 4.962931796614739e-05,
      "loss": 0.0265,
      "step": 676
    },
    {
      "epoch": 0.1651219512195122,
      "grad_norm": 0.10432157665491104,
      "learning_rate": 4.9628221567787406e-05,
      "loss": 0.0297,
      "step": 677
    },
    {
      "epoch": 0.1653658536585366,
      "grad_norm": 0.050158221274614334,
      "learning_rate": 4.9627123562511356e-05,
      "loss": 0.025,
      "step": 678
    },
    {
      "epoch": 0.16560975609756098,
      "grad_norm": 0.09085889905691147,
      "learning_rate": 4.962602395039088e-05,
      "loss": 0.0307,
      "step": 679
    },
    {
      "epoch": 0.16585365853658537,
      "grad_norm": 0.10583008825778961,
      "learning_rate": 4.9624922731497735e-05,
      "loss": 0.0386,
      "step": 680
    },
    {
      "epoch": 0.16609756097560976,
      "grad_norm": 0.06621906161308289,
      "learning_rate": 4.962381990590376e-05,
      "loss": 0.0452,
      "step": 681
    },
    {
      "epoch": 0.16634146341463416,
      "grad_norm": 0.08877942711114883,
      "learning_rate": 4.9622715473680925e-05,
      "loss": 0.0369,
      "step": 682
    },
    {
      "epoch": 0.16658536585365855,
      "grad_norm": 0.07724756747484207,
      "learning_rate": 4.962160943490128e-05,
      "loss": 0.0338,
      "step": 683
    },
    {
      "epoch": 0.16682926829268294,
      "grad_norm": 0.08261293917894363,
      "learning_rate": 4.9620501789636985e-05,
      "loss": 0.0411,
      "step": 684
    },
    {
      "epoch": 0.16707317073170733,
      "grad_norm": 0.08292520046234131,
      "learning_rate": 4.961939253796033e-05,
      "loss": 0.0439,
      "step": 685
    },
    {
      "epoch": 0.16731707317073172,
      "grad_norm": 0.09595081210136414,
      "learning_rate": 4.961828167994367e-05,
      "loss": 0.0295,
      "step": 686
    },
    {
      "epoch": 0.1675609756097561,
      "grad_norm": 0.07773590087890625,
      "learning_rate": 4.9617169215659506e-05,
      "loss": 0.0384,
      "step": 687
    },
    {
      "epoch": 0.1678048780487805,
      "grad_norm": 0.06779234111309052,
      "learning_rate": 4.96160551451804e-05,
      "loss": 0.0288,
      "step": 688
    },
    {
      "epoch": 0.1680487804878049,
      "grad_norm": 0.1639934927225113,
      "learning_rate": 4.961493946857906e-05,
      "loss": 0.0398,
      "step": 689
    },
    {
      "epoch": 0.16829268292682928,
      "grad_norm": 0.0968051329255104,
      "learning_rate": 4.9613822185928275e-05,
      "loss": 0.0287,
      "step": 690
    },
    {
      "epoch": 0.16853658536585367,
      "grad_norm": 0.07251600921154022,
      "learning_rate": 4.9612703297300936e-05,
      "loss": 0.0232,
      "step": 691
    },
    {
      "epoch": 0.16878048780487806,
      "grad_norm": 0.09801840782165527,
      "learning_rate": 4.961158280277005e-05,
      "loss": 0.0552,
      "step": 692
    },
    {
      "epoch": 0.16902439024390245,
      "grad_norm": 0.08333013206720352,
      "learning_rate": 4.961046070240873e-05,
      "loss": 0.0233,
      "step": 693
    },
    {
      "epoch": 0.16926829268292684,
      "grad_norm": 0.0684996172785759,
      "learning_rate": 4.960933699629019e-05,
      "loss": 0.0346,
      "step": 694
    },
    {
      "epoch": 0.16951219512195123,
      "grad_norm": 0.06074545904994011,
      "learning_rate": 4.960821168448776e-05,
      "loss": 0.0263,
      "step": 695
    },
    {
      "epoch": 0.16975609756097562,
      "grad_norm": 0.08832333236932755,
      "learning_rate": 4.9607084767074844e-05,
      "loss": 0.029,
      "step": 696
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.1713009476661682,
      "learning_rate": 4.960595624412497e-05,
      "loss": 0.0239,
      "step": 697
    },
    {
      "epoch": 0.1702439024390244,
      "grad_norm": 0.05847255513072014,
      "learning_rate": 4.9604826115711776e-05,
      "loss": 0.0299,
      "step": 698
    },
    {
      "epoch": 0.1704878048780488,
      "grad_norm": 0.10568533837795258,
      "learning_rate": 4.960369438190901e-05,
      "loss": 0.0358,
      "step": 699
    },
    {
      "epoch": 0.17073170731707318,
      "grad_norm": 0.06911711394786835,
      "learning_rate": 4.96025610427905e-05,
      "loss": 0.0277,
      "step": 700
    },
    {
      "epoch": 0.17097560975609757,
      "grad_norm": 0.08033335208892822,
      "learning_rate": 4.9601426098430196e-05,
      "loss": 0.0454,
      "step": 701
    },
    {
      "epoch": 0.17121951219512196,
      "grad_norm": 0.10263925045728683,
      "learning_rate": 4.9600289548902154e-05,
      "loss": 0.0322,
      "step": 702
    },
    {
      "epoch": 0.17146341463414635,
      "grad_norm": 0.09742754697799683,
      "learning_rate": 4.959915139428053e-05,
      "loss": 0.0413,
      "step": 703
    },
    {
      "epoch": 0.17170731707317075,
      "grad_norm": 0.08727596700191498,
      "learning_rate": 4.9598011634639577e-05,
      "loss": 0.0339,
      "step": 704
    },
    {
      "epoch": 0.1719512195121951,
      "grad_norm": 0.07460064440965652,
      "learning_rate": 4.959687027005367e-05,
      "loss": 0.0365,
      "step": 705
    },
    {
      "epoch": 0.1721951219512195,
      "grad_norm": 0.19736959040164948,
      "learning_rate": 4.959572730059727e-05,
      "loss": 0.0293,
      "step": 706
    },
    {
      "epoch": 0.1724390243902439,
      "grad_norm": 0.12436877936124802,
      "learning_rate": 4.959458272634496e-05,
      "loss": 0.0384,
      "step": 707
    },
    {
      "epoch": 0.17268292682926828,
      "grad_norm": 0.07558172941207886,
      "learning_rate": 4.959343654737143e-05,
      "loss": 0.0315,
      "step": 708
    },
    {
      "epoch": 0.17292682926829267,
      "grad_norm": 0.08946293592453003,
      "learning_rate": 4.9592288763751445e-05,
      "loss": 0.0264,
      "step": 709
    },
    {
      "epoch": 0.17317073170731706,
      "grad_norm": 0.0695483386516571,
      "learning_rate": 4.95911393755599e-05,
      "loss": 0.0319,
      "step": 710
    },
    {
      "epoch": 0.17341463414634145,
      "grad_norm": 0.062007565051317215,
      "learning_rate": 4.958998838287179e-05,
      "loss": 0.0241,
      "step": 711
    },
    {
      "epoch": 0.17365853658536584,
      "grad_norm": 0.0878899097442627,
      "learning_rate": 4.958883578576222e-05,
      "loss": 0.043,
      "step": 712
    },
    {
      "epoch": 0.17390243902439023,
      "grad_norm": 0.04369523003697395,
      "learning_rate": 4.958768158430639e-05,
      "loss": 0.0241,
      "step": 713
    },
    {
      "epoch": 0.17414634146341462,
      "grad_norm": 0.05956278368830681,
      "learning_rate": 4.9586525778579606e-05,
      "loss": 0.0238,
      "step": 714
    },
    {
      "epoch": 0.174390243902439,
      "grad_norm": 0.07296774536371231,
      "learning_rate": 4.958536836865728e-05,
      "loss": 0.0388,
      "step": 715
    },
    {
      "epoch": 0.1746341463414634,
      "grad_norm": 0.08113735169172287,
      "learning_rate": 4.958420935461493e-05,
      "loss": 0.0227,
      "step": 716
    },
    {
      "epoch": 0.1748780487804878,
      "grad_norm": 0.051197391003370285,
      "learning_rate": 4.958304873652818e-05,
      "loss": 0.0205,
      "step": 717
    },
    {
      "epoch": 0.17512195121951218,
      "grad_norm": 0.11224817484617233,
      "learning_rate": 4.958188651447275e-05,
      "loss": 0.0318,
      "step": 718
    },
    {
      "epoch": 0.17536585365853657,
      "grad_norm": 0.10840094089508057,
      "learning_rate": 4.958072268852449e-05,
      "loss": 0.0386,
      "step": 719
    },
    {
      "epoch": 0.17560975609756097,
      "grad_norm": 0.07630587369203568,
      "learning_rate": 4.9579557258759314e-05,
      "loss": 0.0392,
      "step": 720
    },
    {
      "epoch": 0.17585365853658536,
      "grad_norm": 0.1897008866071701,
      "learning_rate": 4.957839022525328e-05,
      "loss": 0.0395,
      "step": 721
    },
    {
      "epoch": 0.17609756097560975,
      "grad_norm": 0.2735914885997772,
      "learning_rate": 4.9577221588082515e-05,
      "loss": 0.0335,
      "step": 722
    },
    {
      "epoch": 0.17634146341463414,
      "grad_norm": 0.04507003352046013,
      "learning_rate": 4.9576051347323286e-05,
      "loss": 0.0239,
      "step": 723
    },
    {
      "epoch": 0.17658536585365853,
      "grad_norm": 0.056297410279512405,
      "learning_rate": 4.957487950305194e-05,
      "loss": 0.0257,
      "step": 724
    },
    {
      "epoch": 0.17682926829268292,
      "grad_norm": 0.09068816155195236,
      "learning_rate": 4.9573706055344935e-05,
      "loss": 0.0287,
      "step": 725
    },
    {
      "epoch": 0.1770731707317073,
      "grad_norm": 0.202632337808609,
      "learning_rate": 4.957253100427884e-05,
      "loss": 0.045,
      "step": 726
    },
    {
      "epoch": 0.1773170731707317,
      "grad_norm": 0.10691766440868378,
      "learning_rate": 4.957135434993032e-05,
      "loss": 0.0284,
      "step": 727
    },
    {
      "epoch": 0.1775609756097561,
      "grad_norm": 0.10143505781888962,
      "learning_rate": 4.957017609237615e-05,
      "loss": 0.0362,
      "step": 728
    },
    {
      "epoch": 0.17780487804878048,
      "grad_norm": 0.06341586261987686,
      "learning_rate": 4.9568996231693206e-05,
      "loss": 0.0238,
      "step": 729
    },
    {
      "epoch": 0.17804878048780487,
      "grad_norm": 0.1433306485414505,
      "learning_rate": 4.9567814767958466e-05,
      "loss": 0.0352,
      "step": 730
    },
    {
      "epoch": 0.17829268292682926,
      "grad_norm": 0.08149771392345428,
      "learning_rate": 4.956663170124903e-05,
      "loss": 0.0243,
      "step": 731
    },
    {
      "epoch": 0.17853658536585365,
      "grad_norm": 0.06714072823524475,
      "learning_rate": 4.9565447031642075e-05,
      "loss": 0.0351,
      "step": 732
    },
    {
      "epoch": 0.17878048780487804,
      "grad_norm": 0.1110684871673584,
      "learning_rate": 4.95642607592149e-05,
      "loss": 0.0374,
      "step": 733
    },
    {
      "epoch": 0.17902439024390243,
      "grad_norm": 0.06522451341152191,
      "learning_rate": 4.9563072884044916e-05,
      "loss": 0.0268,
      "step": 734
    },
    {
      "epoch": 0.17926829268292682,
      "grad_norm": 0.05689842998981476,
      "learning_rate": 4.9561883406209616e-05,
      "loss": 0.0283,
      "step": 735
    },
    {
      "epoch": 0.1795121951219512,
      "grad_norm": 0.07526244968175888,
      "learning_rate": 4.9560692325786625e-05,
      "loss": 0.0387,
      "step": 736
    },
    {
      "epoch": 0.1797560975609756,
      "grad_norm": 0.06944555044174194,
      "learning_rate": 4.955949964285363e-05,
      "loss": 0.0245,
      "step": 737
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.07390360534191132,
      "learning_rate": 4.955830535748849e-05,
      "loss": 0.0297,
      "step": 738
    },
    {
      "epoch": 0.18024390243902438,
      "grad_norm": 0.06773689389228821,
      "learning_rate": 4.955710946976909e-05,
      "loss": 0.0257,
      "step": 739
    },
    {
      "epoch": 0.18048780487804877,
      "grad_norm": 0.05877859145402908,
      "learning_rate": 4.9555911979773474e-05,
      "loss": 0.0258,
      "step": 740
    },
    {
      "epoch": 0.18073170731707316,
      "grad_norm": 0.06523562967777252,
      "learning_rate": 4.955471288757978e-05,
      "loss": 0.025,
      "step": 741
    },
    {
      "epoch": 0.18097560975609756,
      "grad_norm": 0.05620620399713516,
      "learning_rate": 4.955351219326623e-05,
      "loss": 0.0186,
      "step": 742
    },
    {
      "epoch": 0.18121951219512195,
      "grad_norm": 0.13312660157680511,
      "learning_rate": 4.955230989691119e-05,
      "loss": 0.0421,
      "step": 743
    },
    {
      "epoch": 0.18146341463414634,
      "grad_norm": 0.13322828710079193,
      "learning_rate": 4.9551105998593075e-05,
      "loss": 0.0384,
      "step": 744
    },
    {
      "epoch": 0.18170731707317073,
      "grad_norm": 0.10398146510124207,
      "learning_rate": 4.9549900498390464e-05,
      "loss": 0.0502,
      "step": 745
    },
    {
      "epoch": 0.18195121951219512,
      "grad_norm": 0.1062420904636383,
      "learning_rate": 4.954869339638199e-05,
      "loss": 0.0319,
      "step": 746
    },
    {
      "epoch": 0.1821951219512195,
      "grad_norm": 0.09082012623548508,
      "learning_rate": 4.954748469264643e-05,
      "loss": 0.0443,
      "step": 747
    },
    {
      "epoch": 0.1824390243902439,
      "grad_norm": 0.08990223705768585,
      "learning_rate": 4.9546274387262636e-05,
      "loss": 0.0399,
      "step": 748
    },
    {
      "epoch": 0.1826829268292683,
      "grad_norm": 0.06588660180568695,
      "learning_rate": 4.954506248030958e-05,
      "loss": 0.0268,
      "step": 749
    },
    {
      "epoch": 0.18292682926829268,
      "grad_norm": 0.08109401166439056,
      "learning_rate": 4.954384897186635e-05,
      "loss": 0.0379,
      "step": 750
    },
    {
      "epoch": 0.18317073170731707,
      "grad_norm": 0.06655150651931763,
      "learning_rate": 4.9542633862012094e-05,
      "loss": 0.0268,
      "step": 751
    },
    {
      "epoch": 0.18341463414634146,
      "grad_norm": 0.06899825483560562,
      "learning_rate": 4.954141715082612e-05,
      "loss": 0.0308,
      "step": 752
    },
    {
      "epoch": 0.18365853658536585,
      "grad_norm": 0.07190217822790146,
      "learning_rate": 4.95401988383878e-05,
      "loss": 0.0423,
      "step": 753
    },
    {
      "epoch": 0.18390243902439024,
      "grad_norm": 0.06276983767747879,
      "learning_rate": 4.9538978924776634e-05,
      "loss": 0.0246,
      "step": 754
    },
    {
      "epoch": 0.18414634146341463,
      "grad_norm": 0.07155448198318481,
      "learning_rate": 4.953775741007222e-05,
      "loss": 0.0329,
      "step": 755
    },
    {
      "epoch": 0.18439024390243902,
      "grad_norm": 0.06944435089826584,
      "learning_rate": 4.9536534294354244e-05,
      "loss": 0.0345,
      "step": 756
    },
    {
      "epoch": 0.1846341463414634,
      "grad_norm": 0.05729801580309868,
      "learning_rate": 4.9535309577702516e-05,
      "loss": 0.0275,
      "step": 757
    },
    {
      "epoch": 0.1848780487804878,
      "grad_norm": 0.11206862330436707,
      "learning_rate": 4.953408326019696e-05,
      "loss": 0.0378,
      "step": 758
    },
    {
      "epoch": 0.1851219512195122,
      "grad_norm": 0.0548175573348999,
      "learning_rate": 4.9532855341917566e-05,
      "loss": 0.0269,
      "step": 759
    },
    {
      "epoch": 0.18536585365853658,
      "grad_norm": 0.0916713997721672,
      "learning_rate": 4.953162582294447e-05,
      "loss": 0.05,
      "step": 760
    },
    {
      "epoch": 0.18560975609756097,
      "grad_norm": 0.07204168289899826,
      "learning_rate": 4.953039470335788e-05,
      "loss": 0.029,
      "step": 761
    },
    {
      "epoch": 0.18585365853658536,
      "grad_norm": 0.10413066297769547,
      "learning_rate": 4.9529161983238136e-05,
      "loss": 0.0298,
      "step": 762
    },
    {
      "epoch": 0.18609756097560975,
      "grad_norm": 0.07009361684322357,
      "learning_rate": 4.952792766266566e-05,
      "loss": 0.029,
      "step": 763
    },
    {
      "epoch": 0.18634146341463415,
      "grad_norm": 0.0622423030436039,
      "learning_rate": 4.9526691741721e-05,
      "loss": 0.0192,
      "step": 764
    },
    {
      "epoch": 0.18658536585365854,
      "grad_norm": 0.09083066135644913,
      "learning_rate": 4.9525454220484776e-05,
      "loss": 0.0346,
      "step": 765
    },
    {
      "epoch": 0.18682926829268293,
      "grad_norm": 0.11332841962575912,
      "learning_rate": 4.952421509903775e-05,
      "loss": 0.0315,
      "step": 766
    },
    {
      "epoch": 0.18707317073170732,
      "grad_norm": 0.18810589611530304,
      "learning_rate": 4.952297437746077e-05,
      "loss": 0.0351,
      "step": 767
    },
    {
      "epoch": 0.1873170731707317,
      "grad_norm": 0.06404107064008713,
      "learning_rate": 4.952173205583478e-05,
      "loss": 0.0371,
      "step": 768
    },
    {
      "epoch": 0.1875609756097561,
      "grad_norm": 0.08835729211568832,
      "learning_rate": 4.952048813424083e-05,
      "loss": 0.037,
      "step": 769
    },
    {
      "epoch": 0.1878048780487805,
      "grad_norm": 0.15060055255889893,
      "learning_rate": 4.951924261276011e-05,
      "loss": 0.0343,
      "step": 770
    },
    {
      "epoch": 0.18804878048780488,
      "grad_norm": 0.0662631094455719,
      "learning_rate": 4.9517995491473855e-05,
      "loss": 0.0377,
      "step": 771
    },
    {
      "epoch": 0.18829268292682927,
      "grad_norm": 0.07007113099098206,
      "learning_rate": 4.9516746770463465e-05,
      "loss": 0.026,
      "step": 772
    },
    {
      "epoch": 0.18853658536585366,
      "grad_norm": 0.19597989320755005,
      "learning_rate": 4.9515496449810395e-05,
      "loss": 0.0366,
      "step": 773
    },
    {
      "epoch": 0.18878048780487805,
      "grad_norm": 0.1804131716489792,
      "learning_rate": 4.951424452959623e-05,
      "loss": 0.0259,
      "step": 774
    },
    {
      "epoch": 0.18902439024390244,
      "grad_norm": 0.07524548470973969,
      "learning_rate": 4.9512991009902656e-05,
      "loss": 0.0335,
      "step": 775
    },
    {
      "epoch": 0.18926829268292683,
      "grad_norm": 0.06405765563249588,
      "learning_rate": 4.9511735890811464e-05,
      "loss": 0.0266,
      "step": 776
    },
    {
      "epoch": 0.18951219512195122,
      "grad_norm": 0.10177547484636307,
      "learning_rate": 4.951047917240453e-05,
      "loss": 0.0384,
      "step": 777
    },
    {
      "epoch": 0.1897560975609756,
      "grad_norm": 0.1547328233718872,
      "learning_rate": 4.950922085476387e-05,
      "loss": 0.0378,
      "step": 778
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.08773195743560791,
      "learning_rate": 4.950796093797159e-05,
      "loss": 0.0261,
      "step": 779
    },
    {
      "epoch": 0.1902439024390244,
      "grad_norm": 0.07701733708381653,
      "learning_rate": 4.950669942210987e-05,
      "loss": 0.0373,
      "step": 780
    },
    {
      "epoch": 0.19048780487804878,
      "grad_norm": 0.08040511608123779,
      "learning_rate": 4.9505436307261046e-05,
      "loss": 0.024,
      "step": 781
    },
    {
      "epoch": 0.19073170731707317,
      "grad_norm": 0.07821732014417648,
      "learning_rate": 4.9504171593507516e-05,
      "loss": 0.0199,
      "step": 782
    },
    {
      "epoch": 0.19097560975609756,
      "grad_norm": 0.07168872654438019,
      "learning_rate": 4.95029052809318e-05,
      "loss": 0.0207,
      "step": 783
    },
    {
      "epoch": 0.19121951219512195,
      "grad_norm": 0.06560639292001724,
      "learning_rate": 4.950163736961653e-05,
      "loss": 0.0354,
      "step": 784
    },
    {
      "epoch": 0.19146341463414634,
      "grad_norm": 0.08143145591020584,
      "learning_rate": 4.950036785964443e-05,
      "loss": 0.0334,
      "step": 785
    },
    {
      "epoch": 0.19170731707317074,
      "grad_norm": 0.09169294685125351,
      "learning_rate": 4.9499096751098325e-05,
      "loss": 0.0456,
      "step": 786
    },
    {
      "epoch": 0.19195121951219513,
      "grad_norm": 0.09524056315422058,
      "learning_rate": 4.949782404406116e-05,
      "loss": 0.029,
      "step": 787
    },
    {
      "epoch": 0.19219512195121952,
      "grad_norm": 0.08912605047225952,
      "learning_rate": 4.949654973861597e-05,
      "loss": 0.0446,
      "step": 788
    },
    {
      "epoch": 0.1924390243902439,
      "grad_norm": 0.07971392571926117,
      "learning_rate": 4.94952738348459e-05,
      "loss": 0.0261,
      "step": 789
    },
    {
      "epoch": 0.1926829268292683,
      "grad_norm": 0.12501934170722961,
      "learning_rate": 4.9493996332834204e-05,
      "loss": 0.037,
      "step": 790
    },
    {
      "epoch": 0.1929268292682927,
      "grad_norm": 0.0877971276640892,
      "learning_rate": 4.9492717232664224e-05,
      "loss": 0.0436,
      "step": 791
    },
    {
      "epoch": 0.19317073170731708,
      "grad_norm": 0.11353614926338196,
      "learning_rate": 4.949143653441943e-05,
      "loss": 0.0409,
      "step": 792
    },
    {
      "epoch": 0.19341463414634147,
      "grad_norm": 0.1149793490767479,
      "learning_rate": 4.949015423818337e-05,
      "loss": 0.0364,
      "step": 793
    },
    {
      "epoch": 0.19365853658536586,
      "grad_norm": 0.06694412976503372,
      "learning_rate": 4.948887034403972e-05,
      "loss": 0.026,
      "step": 794
    },
    {
      "epoch": 0.19390243902439025,
      "grad_norm": 0.08134017139673233,
      "learning_rate": 4.948758485207226e-05,
      "loss": 0.0266,
      "step": 795
    },
    {
      "epoch": 0.19414634146341464,
      "grad_norm": 0.18644435703754425,
      "learning_rate": 4.948629776236483e-05,
      "loss": 0.0313,
      "step": 796
    },
    {
      "epoch": 0.19439024390243903,
      "grad_norm": 0.10705268383026123,
      "learning_rate": 4.948500907500144e-05,
      "loss": 0.0237,
      "step": 797
    },
    {
      "epoch": 0.19463414634146342,
      "grad_norm": 0.09089742600917816,
      "learning_rate": 4.948371879006617e-05,
      "loss": 0.0283,
      "step": 798
    },
    {
      "epoch": 0.1948780487804878,
      "grad_norm": 0.06532591581344604,
      "learning_rate": 4.94824269076432e-05,
      "loss": 0.0258,
      "step": 799
    },
    {
      "epoch": 0.1951219512195122,
      "grad_norm": 0.09866181761026382,
      "learning_rate": 4.948113342781682e-05,
      "loss": 0.0407,
      "step": 800
    },
    {
      "epoch": 0.1953658536585366,
      "grad_norm": 0.23418289422988892,
      "learning_rate": 4.947983835067143e-05,
      "loss": 0.0308,
      "step": 801
    },
    {
      "epoch": 0.19560975609756098,
      "grad_norm": 0.08102785795927048,
      "learning_rate": 4.947854167629152e-05,
      "loss": 0.0276,
      "step": 802
    },
    {
      "epoch": 0.19585365853658537,
      "grad_norm": 0.05298662930727005,
      "learning_rate": 4.9477243404761706e-05,
      "loss": 0.0145,
      "step": 803
    },
    {
      "epoch": 0.19609756097560976,
      "grad_norm": 0.06406019628047943,
      "learning_rate": 4.947594353616669e-05,
      "loss": 0.031,
      "step": 804
    },
    {
      "epoch": 0.19634146341463415,
      "grad_norm": 0.08948918431997299,
      "learning_rate": 4.947464207059129e-05,
      "loss": 0.0323,
      "step": 805
    },
    {
      "epoch": 0.19658536585365854,
      "grad_norm": 0.17573459446430206,
      "learning_rate": 4.947333900812041e-05,
      "loss": 0.0252,
      "step": 806
    },
    {
      "epoch": 0.19682926829268294,
      "grad_norm": 0.11928561329841614,
      "learning_rate": 4.947203434883909e-05,
      "loss": 0.0263,
      "step": 807
    },
    {
      "epoch": 0.19707317073170733,
      "grad_norm": 0.1602768898010254,
      "learning_rate": 4.947072809283244e-05,
      "loss": 0.0265,
      "step": 808
    },
    {
      "epoch": 0.19731707317073172,
      "grad_norm": 0.0546894371509552,
      "learning_rate": 4.94694202401857e-05,
      "loss": 0.027,
      "step": 809
    },
    {
      "epoch": 0.1975609756097561,
      "grad_norm": 0.05745473876595497,
      "learning_rate": 4.946811079098419e-05,
      "loss": 0.0198,
      "step": 810
    },
    {
      "epoch": 0.1978048780487805,
      "grad_norm": 0.07169684022665024,
      "learning_rate": 4.9466799745313366e-05,
      "loss": 0.0356,
      "step": 811
    },
    {
      "epoch": 0.1980487804878049,
      "grad_norm": 0.08780734241008759,
      "learning_rate": 4.9465487103258744e-05,
      "loss": 0.0425,
      "step": 812
    },
    {
      "epoch": 0.19829268292682928,
      "grad_norm": 0.08889256417751312,
      "learning_rate": 4.946417286490599e-05,
      "loss": 0.0412,
      "step": 813
    },
    {
      "epoch": 0.19853658536585367,
      "grad_norm": 0.055509570986032486,
      "learning_rate": 4.9462857030340855e-05,
      "loss": 0.016,
      "step": 814
    },
    {
      "epoch": 0.19878048780487806,
      "grad_norm": 0.07712498307228088,
      "learning_rate": 4.9461539599649176e-05,
      "loss": 0.0224,
      "step": 815
    },
    {
      "epoch": 0.19902439024390245,
      "grad_norm": 0.11823718994855881,
      "learning_rate": 4.946022057291693e-05,
      "loss": 0.0246,
      "step": 816
    },
    {
      "epoch": 0.19926829268292684,
      "grad_norm": 0.05979372188448906,
      "learning_rate": 4.945889995023017e-05,
      "loss": 0.0255,
      "step": 817
    },
    {
      "epoch": 0.19951219512195123,
      "grad_norm": 0.07884048670530319,
      "learning_rate": 4.9457577731675065e-05,
      "loss": 0.0299,
      "step": 818
    },
    {
      "epoch": 0.19975609756097562,
      "grad_norm": 0.1793789565563202,
      "learning_rate": 4.945625391733788e-05,
      "loss": 0.042,
      "step": 819
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.06340628117322922,
      "learning_rate": 4.9454928507304996e-05,
      "loss": 0.0201,
      "step": 820
    },
    {
      "epoch": 0.2002439024390244,
      "grad_norm": 0.05868056043982506,
      "learning_rate": 4.945360150166289e-05,
      "loss": 0.0252,
      "step": 821
    },
    {
      "epoch": 0.2004878048780488,
      "grad_norm": 0.09450078755617142,
      "learning_rate": 4.945227290049815e-05,
      "loss": 0.0249,
      "step": 822
    },
    {
      "epoch": 0.20073170731707318,
      "grad_norm": 0.09284377098083496,
      "learning_rate": 4.9450942703897463e-05,
      "loss": 0.0347,
      "step": 823
    },
    {
      "epoch": 0.20097560975609757,
      "grad_norm": 0.06589097529649734,
      "learning_rate": 4.944961091194761e-05,
      "loss": 0.0213,
      "step": 824
    },
    {
      "epoch": 0.20121951219512196,
      "grad_norm": 0.06744570285081863,
      "learning_rate": 4.9448277524735484e-05,
      "loss": 0.0336,
      "step": 825
    },
    {
      "epoch": 0.20146341463414635,
      "grad_norm": 0.11548496037721634,
      "learning_rate": 4.9446942542348106e-05,
      "loss": 0.0261,
      "step": 826
    },
    {
      "epoch": 0.20170731707317074,
      "grad_norm": 0.07816456258296967,
      "learning_rate": 4.9445605964872564e-05,
      "loss": 0.0368,
      "step": 827
    },
    {
      "epoch": 0.20195121951219513,
      "grad_norm": 0.08335297554731369,
      "learning_rate": 4.944426779239606e-05,
      "loss": 0.0267,
      "step": 828
    },
    {
      "epoch": 0.20219512195121953,
      "grad_norm": 0.1769828051328659,
      "learning_rate": 4.944292802500593e-05,
      "loss": 0.0298,
      "step": 829
    },
    {
      "epoch": 0.20243902439024392,
      "grad_norm": 0.1061769425868988,
      "learning_rate": 4.944158666278956e-05,
      "loss": 0.0478,
      "step": 830
    },
    {
      "epoch": 0.2026829268292683,
      "grad_norm": 0.1490703970193863,
      "learning_rate": 4.9440243705834485e-05,
      "loss": 0.023,
      "step": 831
    },
    {
      "epoch": 0.2029268292682927,
      "grad_norm": 0.07293250411748886,
      "learning_rate": 4.9438899154228324e-05,
      "loss": 0.0352,
      "step": 832
    },
    {
      "epoch": 0.20317073170731706,
      "grad_norm": 0.05663909763097763,
      "learning_rate": 4.9437553008058814e-05,
      "loss": 0.0195,
      "step": 833
    },
    {
      "epoch": 0.20341463414634145,
      "grad_norm": 0.14905712008476257,
      "learning_rate": 4.943620526741378e-05,
      "loss": 0.0254,
      "step": 834
    },
    {
      "epoch": 0.20365853658536584,
      "grad_norm": 0.483499139547348,
      "learning_rate": 4.943485593238115e-05,
      "loss": 0.0225,
      "step": 835
    },
    {
      "epoch": 0.20390243902439023,
      "grad_norm": 0.13445915281772614,
      "learning_rate": 4.943350500304899e-05,
      "loss": 0.04,
      "step": 836
    },
    {
      "epoch": 0.20414634146341462,
      "grad_norm": 0.3502219021320343,
      "learning_rate": 4.943215247950541e-05,
      "loss": 0.0559,
      "step": 837
    },
    {
      "epoch": 0.204390243902439,
      "grad_norm": 0.047906823456287384,
      "learning_rate": 4.943079836183868e-05,
      "loss": 0.0223,
      "step": 838
    },
    {
      "epoch": 0.2046341463414634,
      "grad_norm": 0.06602425873279572,
      "learning_rate": 4.942944265013715e-05,
      "loss": 0.0268,
      "step": 839
    },
    {
      "epoch": 0.2048780487804878,
      "grad_norm": 0.0631512850522995,
      "learning_rate": 4.942808534448927e-05,
      "loss": 0.0341,
      "step": 840
    },
    {
      "epoch": 0.20512195121951218,
      "grad_norm": 0.14718887209892273,
      "learning_rate": 4.9426726444983606e-05,
      "loss": 0.0363,
      "step": 841
    },
    {
      "epoch": 0.20536585365853657,
      "grad_norm": 0.09142861515283585,
      "learning_rate": 4.942536595170881e-05,
      "loss": 0.0248,
      "step": 842
    },
    {
      "epoch": 0.20560975609756096,
      "grad_norm": 0.09067387133836746,
      "learning_rate": 4.942400386475367e-05,
      "loss": 0.0326,
      "step": 843
    },
    {
      "epoch": 0.20585365853658535,
      "grad_norm": 0.08752066642045975,
      "learning_rate": 4.9422640184207046e-05,
      "loss": 0.0422,
      "step": 844
    },
    {
      "epoch": 0.20609756097560974,
      "grad_norm": 0.07093856483697891,
      "learning_rate": 4.942127491015791e-05,
      "loss": 0.0361,
      "step": 845
    },
    {
      "epoch": 0.20634146341463414,
      "grad_norm": 0.0857565850019455,
      "learning_rate": 4.9419908042695346e-05,
      "loss": 0.0367,
      "step": 846
    },
    {
      "epoch": 0.20658536585365853,
      "grad_norm": 0.06350496411323547,
      "learning_rate": 4.941853958190854e-05,
      "loss": 0.0257,
      "step": 847
    },
    {
      "epoch": 0.20682926829268292,
      "grad_norm": 0.10149228572845459,
      "learning_rate": 4.941716952788678e-05,
      "loss": 0.0358,
      "step": 848
    },
    {
      "epoch": 0.2070731707317073,
      "grad_norm": 0.08847342431545258,
      "learning_rate": 4.9415797880719455e-05,
      "loss": 0.0309,
      "step": 849
    },
    {
      "epoch": 0.2073170731707317,
      "grad_norm": 0.10531892627477646,
      "learning_rate": 4.941442464049606e-05,
      "loss": 0.0451,
      "step": 850
    },
    {
      "epoch": 0.2075609756097561,
      "grad_norm": 0.09978164732456207,
      "learning_rate": 4.94130498073062e-05,
      "loss": 0.0234,
      "step": 851
    },
    {
      "epoch": 0.20780487804878048,
      "grad_norm": 0.09983131289482117,
      "learning_rate": 4.9411673381239575e-05,
      "loss": 0.0525,
      "step": 852
    },
    {
      "epoch": 0.20804878048780487,
      "grad_norm": 0.05023038387298584,
      "learning_rate": 4.9410295362385994e-05,
      "loss": 0.0223,
      "step": 853
    },
    {
      "epoch": 0.20829268292682926,
      "grad_norm": 0.0892750695347786,
      "learning_rate": 4.940891575083537e-05,
      "loss": 0.0253,
      "step": 854
    },
    {
      "epoch": 0.20853658536585365,
      "grad_norm": 0.13706189393997192,
      "learning_rate": 4.940753454667771e-05,
      "loss": 0.0379,
      "step": 855
    },
    {
      "epoch": 0.20878048780487804,
      "grad_norm": 0.0715261846780777,
      "learning_rate": 4.940615175000315e-05,
      "loss": 0.0347,
      "step": 856
    },
    {
      "epoch": 0.20902439024390243,
      "grad_norm": 0.0629124864935875,
      "learning_rate": 4.9404767360901895e-05,
      "loss": 0.0241,
      "step": 857
    },
    {
      "epoch": 0.20926829268292682,
      "grad_norm": 0.16544803977012634,
      "learning_rate": 4.940338137946427e-05,
      "loss": 0.0372,
      "step": 858
    },
    {
      "epoch": 0.2095121951219512,
      "grad_norm": 0.09166493266820908,
      "learning_rate": 4.9401993805780735e-05,
      "loss": 0.032,
      "step": 859
    },
    {
      "epoch": 0.2097560975609756,
      "grad_norm": 0.10039091855287552,
      "learning_rate": 4.9400604639941794e-05,
      "loss": 0.027,
      "step": 860
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.05703040957450867,
      "learning_rate": 4.93992138820381e-05,
      "loss": 0.0167,
      "step": 861
    },
    {
      "epoch": 0.21024390243902438,
      "grad_norm": 0.11474007368087769,
      "learning_rate": 4.9397821532160396e-05,
      "loss": 0.0228,
      "step": 862
    },
    {
      "epoch": 0.21048780487804877,
      "grad_norm": 0.08044242858886719,
      "learning_rate": 4.939642759039952e-05,
      "loss": 0.0254,
      "step": 863
    },
    {
      "epoch": 0.21073170731707316,
      "grad_norm": 0.0765567347407341,
      "learning_rate": 4.939503205684643e-05,
      "loss": 0.0328,
      "step": 864
    },
    {
      "epoch": 0.21097560975609755,
      "grad_norm": 0.0750783234834671,
      "learning_rate": 4.9393634931592186e-05,
      "loss": 0.0283,
      "step": 865
    },
    {
      "epoch": 0.21121951219512194,
      "grad_norm": 0.15467582643032074,
      "learning_rate": 4.939223621472794e-05,
      "loss": 0.0428,
      "step": 866
    },
    {
      "epoch": 0.21146341463414633,
      "grad_norm": 0.06156051903963089,
      "learning_rate": 4.939083590634494e-05,
      "loss": 0.0277,
      "step": 867
    },
    {
      "epoch": 0.21170731707317073,
      "grad_norm": 0.07788101583719254,
      "learning_rate": 4.938943400653457e-05,
      "loss": 0.0252,
      "step": 868
    },
    {
      "epoch": 0.21195121951219512,
      "grad_norm": 0.10770959407091141,
      "learning_rate": 4.9388030515388294e-05,
      "loss": 0.0237,
      "step": 869
    },
    {
      "epoch": 0.2121951219512195,
      "grad_norm": 0.06780559569597244,
      "learning_rate": 4.9386625432997694e-05,
      "loss": 0.0219,
      "step": 870
    },
    {
      "epoch": 0.2124390243902439,
      "grad_norm": 0.2167002558708191,
      "learning_rate": 4.9385218759454435e-05,
      "loss": 0.0399,
      "step": 871
    },
    {
      "epoch": 0.2126829268292683,
      "grad_norm": 0.13691923022270203,
      "learning_rate": 4.938381049485029e-05,
      "loss": 0.023,
      "step": 872
    },
    {
      "epoch": 0.21292682926829268,
      "grad_norm": 0.05733152851462364,
      "learning_rate": 4.938240063927717e-05,
      "loss": 0.0211,
      "step": 873
    },
    {
      "epoch": 0.21317073170731707,
      "grad_norm": 0.05875827372074127,
      "learning_rate": 4.938098919282704e-05,
      "loss": 0.0287,
      "step": 874
    },
    {
      "epoch": 0.21341463414634146,
      "grad_norm": 0.09504001587629318,
      "learning_rate": 4.937957615559201e-05,
      "loss": 0.0204,
      "step": 875
    },
    {
      "epoch": 0.21365853658536585,
      "grad_norm": 0.04692188277840614,
      "learning_rate": 4.937816152766427e-05,
      "loss": 0.0148,
      "step": 876
    },
    {
      "epoch": 0.21390243902439024,
      "grad_norm": 0.05534486472606659,
      "learning_rate": 4.937674530913612e-05,
      "loss": 0.0323,
      "step": 877
    },
    {
      "epoch": 0.21414634146341463,
      "grad_norm": 0.055905621498823166,
      "learning_rate": 4.937532750009996e-05,
      "loss": 0.0279,
      "step": 878
    },
    {
      "epoch": 0.21439024390243902,
      "grad_norm": 0.06063750758767128,
      "learning_rate": 4.93739081006483e-05,
      "loss": 0.0211,
      "step": 879
    },
    {
      "epoch": 0.2146341463414634,
      "grad_norm": 0.0911649763584137,
      "learning_rate": 4.937248711087375e-05,
      "loss": 0.034,
      "step": 880
    },
    {
      "epoch": 0.2148780487804878,
      "grad_norm": 0.1119367927312851,
      "learning_rate": 4.937106453086903e-05,
      "loss": 0.0246,
      "step": 881
    },
    {
      "epoch": 0.2151219512195122,
      "grad_norm": 0.09192270040512085,
      "learning_rate": 4.9369640360726957e-05,
      "loss": 0.0249,
      "step": 882
    },
    {
      "epoch": 0.21536585365853658,
      "grad_norm": 0.05266619101166725,
      "learning_rate": 4.9368214600540455e-05,
      "loss": 0.0316,
      "step": 883
    },
    {
      "epoch": 0.21560975609756097,
      "grad_norm": 0.07364341616630554,
      "learning_rate": 4.9366787250402544e-05,
      "loss": 0.026,
      "step": 884
    },
    {
      "epoch": 0.21585365853658536,
      "grad_norm": 0.06275514513254166,
      "learning_rate": 4.9365358310406355e-05,
      "loss": 0.0213,
      "step": 885
    },
    {
      "epoch": 0.21609756097560975,
      "grad_norm": 0.10286788642406464,
      "learning_rate": 4.936392778064514e-05,
      "loss": 0.04,
      "step": 886
    },
    {
      "epoch": 0.21634146341463414,
      "grad_norm": 0.09547793120145798,
      "learning_rate": 4.936249566121221e-05,
      "loss": 0.0222,
      "step": 887
    },
    {
      "epoch": 0.21658536585365853,
      "grad_norm": 0.06725577265024185,
      "learning_rate": 4.9361061952201016e-05,
      "loss": 0.0262,
      "step": 888
    },
    {
      "epoch": 0.21682926829268293,
      "grad_norm": 0.09220165014266968,
      "learning_rate": 4.9359626653705116e-05,
      "loss": 0.0406,
      "step": 889
    },
    {
      "epoch": 0.21707317073170732,
      "grad_norm": 0.06814863532781601,
      "learning_rate": 4.935818976581814e-05,
      "loss": 0.0313,
      "step": 890
    },
    {
      "epoch": 0.2173170731707317,
      "grad_norm": 0.20000998675823212,
      "learning_rate": 4.9356751288633864e-05,
      "loss": 0.039,
      "step": 891
    },
    {
      "epoch": 0.2175609756097561,
      "grad_norm": 0.29464420676231384,
      "learning_rate": 4.935531122224611e-05,
      "loss": 0.0264,
      "step": 892
    },
    {
      "epoch": 0.2178048780487805,
      "grad_norm": 0.06921868026256561,
      "learning_rate": 4.9353869566748875e-05,
      "loss": 0.032,
      "step": 893
    },
    {
      "epoch": 0.21804878048780488,
      "grad_norm": 0.09527876228094101,
      "learning_rate": 4.935242632223619e-05,
      "loss": 0.0487,
      "step": 894
    },
    {
      "epoch": 0.21829268292682927,
      "grad_norm": 0.11461234837770462,
      "learning_rate": 4.935098148880225e-05,
      "loss": 0.041,
      "step": 895
    },
    {
      "epoch": 0.21853658536585366,
      "grad_norm": 0.11396816372871399,
      "learning_rate": 4.934953506654131e-05,
      "loss": 0.0452,
      "step": 896
    },
    {
      "epoch": 0.21878048780487805,
      "grad_norm": 0.1169552430510521,
      "learning_rate": 4.9348087055547744e-05,
      "loss": 0.0396,
      "step": 897
    },
    {
      "epoch": 0.21902439024390244,
      "grad_norm": 0.05866263061761856,
      "learning_rate": 4.9346637455916033e-05,
      "loss": 0.0179,
      "step": 898
    },
    {
      "epoch": 0.21926829268292683,
      "grad_norm": 0.051196034997701645,
      "learning_rate": 4.934518626774077e-05,
      "loss": 0.0304,
      "step": 899
    },
    {
      "epoch": 0.21951219512195122,
      "grad_norm": 0.0698353722691536,
      "learning_rate": 4.9343733491116625e-05,
      "loss": 0.0447,
      "step": 900
    },
    {
      "epoch": 0.2197560975609756,
      "grad_norm": 0.11600593477487564,
      "learning_rate": 4.934227912613839e-05,
      "loss": 0.0562,
      "step": 901
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.06634566187858582,
      "learning_rate": 4.934082317290096e-05,
      "loss": 0.0263,
      "step": 902
    },
    {
      "epoch": 0.2202439024390244,
      "grad_norm": 2.4338364601135254,
      "learning_rate": 4.933936563149934e-05,
      "loss": 0.0235,
      "step": 903
    },
    {
      "epoch": 0.22048780487804878,
      "grad_norm": 0.0539020374417305,
      "learning_rate": 4.933790650202862e-05,
      "loss": 0.033,
      "step": 904
    },
    {
      "epoch": 0.22073170731707317,
      "grad_norm": 0.07617990672588348,
      "learning_rate": 4.933644578458401e-05,
      "loss": 0.0402,
      "step": 905
    },
    {
      "epoch": 0.22097560975609756,
      "grad_norm": 0.053120698779821396,
      "learning_rate": 4.93349834792608e-05,
      "loss": 0.0298,
      "step": 906
    },
    {
      "epoch": 0.22121951219512195,
      "grad_norm": 0.09760730713605881,
      "learning_rate": 4.9333519586154426e-05,
      "loss": 0.0253,
      "step": 907
    },
    {
      "epoch": 0.22146341463414634,
      "grad_norm": 0.039788469672203064,
      "learning_rate": 4.9332054105360384e-05,
      "loss": 0.0185,
      "step": 908
    },
    {
      "epoch": 0.22170731707317073,
      "grad_norm": 0.0949576124548912,
      "learning_rate": 4.9330587036974306e-05,
      "loss": 0.0449,
      "step": 909
    },
    {
      "epoch": 0.22195121951219512,
      "grad_norm": 0.07118648290634155,
      "learning_rate": 4.9329118381091904e-05,
      "loss": 0.024,
      "step": 910
    },
    {
      "epoch": 0.22219512195121952,
      "grad_norm": 0.07518494874238968,
      "learning_rate": 4.9327648137809004e-05,
      "loss": 0.0339,
      "step": 911
    },
    {
      "epoch": 0.2224390243902439,
      "grad_norm": 0.06576696038246155,
      "learning_rate": 4.9326176307221537e-05,
      "loss": 0.0235,
      "step": 912
    },
    {
      "epoch": 0.2226829268292683,
      "grad_norm": 0.07163596153259277,
      "learning_rate": 4.932470288942555e-05,
      "loss": 0.0351,
      "step": 913
    },
    {
      "epoch": 0.2229268292682927,
      "grad_norm": 0.24854864180088043,
      "learning_rate": 4.932322788451714e-05,
      "loss": 0.0286,
      "step": 914
    },
    {
      "epoch": 0.22317073170731708,
      "grad_norm": 0.08475394546985626,
      "learning_rate": 4.932175129259259e-05,
      "loss": 0.0368,
      "step": 915
    },
    {
      "epoch": 0.22341463414634147,
      "grad_norm": 0.07675407081842422,
      "learning_rate": 4.932027311374822e-05,
      "loss": 0.041,
      "step": 916
    },
    {
      "epoch": 0.22365853658536586,
      "grad_norm": 0.11257550865411758,
      "learning_rate": 4.931879334808047e-05,
      "loss": 0.0379,
      "step": 917
    },
    {
      "epoch": 0.22390243902439025,
      "grad_norm": 0.08153896778821945,
      "learning_rate": 4.931731199568591e-05,
      "loss": 0.0323,
      "step": 918
    },
    {
      "epoch": 0.22414634146341464,
      "grad_norm": 0.054410822689533234,
      "learning_rate": 4.931582905666119e-05,
      "loss": 0.0204,
      "step": 919
    },
    {
      "epoch": 0.22439024390243903,
      "grad_norm": 0.126595139503479,
      "learning_rate": 4.931434453110305e-05,
      "loss": 0.021,
      "step": 920
    },
    {
      "epoch": 0.22463414634146342,
      "grad_norm": 0.06880797445774078,
      "learning_rate": 4.931285841910838e-05,
      "loss": 0.025,
      "step": 921
    },
    {
      "epoch": 0.2248780487804878,
      "grad_norm": 0.06899800896644592,
      "learning_rate": 4.931137072077411e-05,
      "loss": 0.0229,
      "step": 922
    },
    {
      "epoch": 0.2251219512195122,
      "grad_norm": 0.12974832952022552,
      "learning_rate": 4.930988143619733e-05,
      "loss": 0.0202,
      "step": 923
    },
    {
      "epoch": 0.2253658536585366,
      "grad_norm": 0.07170449197292328,
      "learning_rate": 4.930839056547521e-05,
      "loss": 0.0384,
      "step": 924
    },
    {
      "epoch": 0.22560975609756098,
      "grad_norm": 0.0772334560751915,
      "learning_rate": 4.930689810870501e-05,
      "loss": 0.0206,
      "step": 925
    },
    {
      "epoch": 0.22585365853658537,
      "grad_norm": 0.1186433881521225,
      "learning_rate": 4.9305404065984126e-05,
      "loss": 0.0343,
      "step": 926
    },
    {
      "epoch": 0.22609756097560976,
      "grad_norm": 0.11806493997573853,
      "learning_rate": 4.930390843741003e-05,
      "loss": 0.0313,
      "step": 927
    },
    {
      "epoch": 0.22634146341463415,
      "grad_norm": 0.11216262727975845,
      "learning_rate": 4.930241122308032e-05,
      "loss": 0.0281,
      "step": 928
    },
    {
      "epoch": 0.22658536585365854,
      "grad_norm": 0.10583443194627762,
      "learning_rate": 4.9300912423092666e-05,
      "loss": 0.0208,
      "step": 929
    },
    {
      "epoch": 0.22682926829268293,
      "grad_norm": 0.14899133145809174,
      "learning_rate": 4.929941203754487e-05,
      "loss": 0.0231,
      "step": 930
    },
    {
      "epoch": 0.22707317073170732,
      "grad_norm": 0.0456693060696125,
      "learning_rate": 4.9297910066534826e-05,
      "loss": 0.0135,
      "step": 931
    },
    {
      "epoch": 0.22731707317073171,
      "grad_norm": 0.06335011124610901,
      "learning_rate": 4.929640651016053e-05,
      "loss": 0.0347,
      "step": 932
    },
    {
      "epoch": 0.2275609756097561,
      "grad_norm": 0.10360711812973022,
      "learning_rate": 4.92949013685201e-05,
      "loss": 0.0291,
      "step": 933
    },
    {
      "epoch": 0.2278048780487805,
      "grad_norm": 0.6311314702033997,
      "learning_rate": 4.929339464171171e-05,
      "loss": 0.0373,
      "step": 934
    },
    {
      "epoch": 0.2280487804878049,
      "grad_norm": 0.0943048968911171,
      "learning_rate": 4.9291886329833705e-05,
      "loss": 0.038,
      "step": 935
    },
    {
      "epoch": 0.22829268292682928,
      "grad_norm": 0.20636069774627686,
      "learning_rate": 4.929037643298448e-05,
      "loss": 0.0314,
      "step": 936
    },
    {
      "epoch": 0.22853658536585367,
      "grad_norm": 0.08673381805419922,
      "learning_rate": 4.9288864951262543e-05,
      "loss": 0.0271,
      "step": 937
    },
    {
      "epoch": 0.22878048780487806,
      "grad_norm": 0.11490296572446823,
      "learning_rate": 4.928735188476653e-05,
      "loss": 0.0328,
      "step": 938
    },
    {
      "epoch": 0.22902439024390245,
      "grad_norm": 0.07033479958772659,
      "learning_rate": 4.9285837233595156e-05,
      "loss": 0.0262,
      "step": 939
    },
    {
      "epoch": 0.22926829268292684,
      "grad_norm": 0.06358486413955688,
      "learning_rate": 4.9284320997847245e-05,
      "loss": 0.0266,
      "step": 940
    },
    {
      "epoch": 0.22951219512195123,
      "grad_norm": 0.1034051850438118,
      "learning_rate": 4.928280317762174e-05,
      "loss": 0.0322,
      "step": 941
    },
    {
      "epoch": 0.22975609756097562,
      "grad_norm": 0.052750058472156525,
      "learning_rate": 4.928128377301765e-05,
      "loss": 0.0274,
      "step": 942
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.08062025904655457,
      "learning_rate": 4.9279762784134135e-05,
      "loss": 0.034,
      "step": 943
    },
    {
      "epoch": 0.2302439024390244,
      "grad_norm": 0.059477731585502625,
      "learning_rate": 4.927824021107043e-05,
      "loss": 0.03,
      "step": 944
    },
    {
      "epoch": 0.2304878048780488,
      "grad_norm": 0.05883043259382248,
      "learning_rate": 4.9276716053925864e-05,
      "loss": 0.0237,
      "step": 945
    },
    {
      "epoch": 0.23073170731707318,
      "grad_norm": 0.06548727303743362,
      "learning_rate": 4.92751903127999e-05,
      "loss": 0.0309,
      "step": 946
    },
    {
      "epoch": 0.23097560975609757,
      "grad_norm": 0.06436402350664139,
      "learning_rate": 4.927366298779208e-05,
      "loss": 0.0265,
      "step": 947
    },
    {
      "epoch": 0.23121951219512196,
      "grad_norm": 0.3247224688529968,
      "learning_rate": 4.9272134079002063e-05,
      "loss": 0.0218,
      "step": 948
    },
    {
      "epoch": 0.23146341463414635,
      "grad_norm": 0.21502746641635895,
      "learning_rate": 4.9270603586529594e-05,
      "loss": 0.0378,
      "step": 949
    },
    {
      "epoch": 0.23170731707317074,
      "grad_norm": 0.14146491885185242,
      "learning_rate": 4.926907151047455e-05,
      "loss": 0.0273,
      "step": 950
    },
    {
      "epoch": 0.23195121951219513,
      "grad_norm": 0.17887894809246063,
      "learning_rate": 4.926753785093687e-05,
      "loss": 0.0436,
      "step": 951
    },
    {
      "epoch": 0.23219512195121952,
      "grad_norm": 0.04933314397931099,
      "learning_rate": 4.9266002608016646e-05,
      "loss": 0.0206,
      "step": 952
    },
    {
      "epoch": 0.23243902439024391,
      "grad_norm": 0.14055337011814117,
      "learning_rate": 4.926446578181404e-05,
      "loss": 0.0285,
      "step": 953
    },
    {
      "epoch": 0.2326829268292683,
      "grad_norm": 0.8094354867935181,
      "learning_rate": 4.926292737242932e-05,
      "loss": 0.0214,
      "step": 954
    },
    {
      "epoch": 0.2329268292682927,
      "grad_norm": 0.07245200872421265,
      "learning_rate": 4.926138737996286e-05,
      "loss": 0.024,
      "step": 955
    },
    {
      "epoch": 0.23317073170731709,
      "grad_norm": 0.07341962307691574,
      "learning_rate": 4.925984580451515e-05,
      "loss": 0.0268,
      "step": 956
    },
    {
      "epoch": 0.23341463414634148,
      "grad_norm": 0.15366525948047638,
      "learning_rate": 4.925830264618676e-05,
      "loss": 0.0239,
      "step": 957
    },
    {
      "epoch": 0.23365853658536587,
      "grad_norm": 0.09991130977869034,
      "learning_rate": 4.9256757905078395e-05,
      "loss": 0.0514,
      "step": 958
    },
    {
      "epoch": 0.23390243902439026,
      "grad_norm": 0.044214919209480286,
      "learning_rate": 4.9255211581290826e-05,
      "loss": 0.0167,
      "step": 959
    },
    {
      "epoch": 0.23414634146341465,
      "grad_norm": 0.080940380692482,
      "learning_rate": 4.925366367492496e-05,
      "loss": 0.0308,
      "step": 960
    },
    {
      "epoch": 0.234390243902439,
      "grad_norm": 0.05404907092452049,
      "learning_rate": 4.9252114186081775e-05,
      "loss": 0.0217,
      "step": 961
    },
    {
      "epoch": 0.2346341463414634,
      "grad_norm": 0.05815685912966728,
      "learning_rate": 4.925056311486239e-05,
      "loss": 0.0313,
      "step": 962
    },
    {
      "epoch": 0.2348780487804878,
      "grad_norm": 0.3015023171901703,
      "learning_rate": 4.9249010461368e-05,
      "loss": 0.0308,
      "step": 963
    },
    {
      "epoch": 0.23512195121951218,
      "grad_norm": 0.08223970234394073,
      "learning_rate": 4.924745622569991e-05,
      "loss": 0.0382,
      "step": 964
    },
    {
      "epoch": 0.23536585365853657,
      "grad_norm": 0.14659486711025238,
      "learning_rate": 4.924590040795953e-05,
      "loss": 0.0313,
      "step": 965
    },
    {
      "epoch": 0.23560975609756096,
      "grad_norm": 0.44728317856788635,
      "learning_rate": 4.924434300824837e-05,
      "loss": 0.0162,
      "step": 966
    },
    {
      "epoch": 0.23585365853658535,
      "grad_norm": 0.08079171180725098,
      "learning_rate": 4.924278402666805e-05,
      "loss": 0.024,
      "step": 967
    },
    {
      "epoch": 0.23609756097560974,
      "grad_norm": 0.04944095015525818,
      "learning_rate": 4.924122346332029e-05,
      "loss": 0.0192,
      "step": 968
    },
    {
      "epoch": 0.23634146341463413,
      "grad_norm": 0.07541578263044357,
      "learning_rate": 4.9239661318306906e-05,
      "loss": 0.0337,
      "step": 969
    },
    {
      "epoch": 0.23658536585365852,
      "grad_norm": 0.286834716796875,
      "learning_rate": 4.923809759172983e-05,
      "loss": 0.0343,
      "step": 970
    },
    {
      "epoch": 0.23682926829268292,
      "grad_norm": 0.45253950357437134,
      "learning_rate": 4.923653228369107e-05,
      "loss": 0.0289,
      "step": 971
    },
    {
      "epoch": 0.2370731707317073,
      "grad_norm": 0.06803951412439346,
      "learning_rate": 4.923496539429279e-05,
      "loss": 0.0305,
      "step": 972
    },
    {
      "epoch": 0.2373170731707317,
      "grad_norm": 0.06939142942428589,
      "learning_rate": 4.92333969236372e-05,
      "loss": 0.0296,
      "step": 973
    },
    {
      "epoch": 0.2375609756097561,
      "grad_norm": 0.08390097320079803,
      "learning_rate": 4.9231826871826655e-05,
      "loss": 0.026,
      "step": 974
    },
    {
      "epoch": 0.23780487804878048,
      "grad_norm": 0.2073962390422821,
      "learning_rate": 4.923025523896358e-05,
      "loss": 0.0436,
      "step": 975
    },
    {
      "epoch": 0.23804878048780487,
      "grad_norm": 0.08094615489244461,
      "learning_rate": 4.922868202515053e-05,
      "loss": 0.0274,
      "step": 976
    },
    {
      "epoch": 0.23829268292682926,
      "grad_norm": 0.057700175791978836,
      "learning_rate": 4.922710723049014e-05,
      "loss": 0.0166,
      "step": 977
    },
    {
      "epoch": 0.23853658536585365,
      "grad_norm": 0.2761833965778351,
      "learning_rate": 4.9225530855085175e-05,
      "loss": 0.0337,
      "step": 978
    },
    {
      "epoch": 0.23878048780487804,
      "grad_norm": 0.04316939041018486,
      "learning_rate": 4.92239528990385e-05,
      "loss": 0.019,
      "step": 979
    },
    {
      "epoch": 0.23902439024390243,
      "grad_norm": 0.16039158403873444,
      "learning_rate": 4.922237336245303e-05,
      "loss": 0.0257,
      "step": 980
    },
    {
      "epoch": 0.23926829268292682,
      "grad_norm": 0.19425374269485474,
      "learning_rate": 4.9220792245431855e-05,
      "loss": 0.021,
      "step": 981
    },
    {
      "epoch": 0.2395121951219512,
      "grad_norm": 0.14675430953502655,
      "learning_rate": 4.9219209548078146e-05,
      "loss": 0.0265,
      "step": 982
    },
    {
      "epoch": 0.2397560975609756,
      "grad_norm": 0.07035674899816513,
      "learning_rate": 4.921762527049514e-05,
      "loss": 0.0253,
      "step": 983
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.07107299566268921,
      "learning_rate": 4.921603941278624e-05,
      "loss": 0.034,
      "step": 984
    },
    {
      "epoch": 0.24024390243902438,
      "grad_norm": 0.057755544781684875,
      "learning_rate": 4.921445197505489e-05,
      "loss": 0.0219,
      "step": 985
    },
    {
      "epoch": 0.24048780487804877,
      "grad_norm": 0.13380345702171326,
      "learning_rate": 4.921286295740467e-05,
      "loss": 0.0312,
      "step": 986
    },
    {
      "epoch": 0.24073170731707316,
      "grad_norm": 0.06374750286340714,
      "learning_rate": 4.9211272359939276e-05,
      "loss": 0.0423,
      "step": 987
    },
    {
      "epoch": 0.24097560975609755,
      "grad_norm": 0.15020766854286194,
      "learning_rate": 4.920968018276247e-05,
      "loss": 0.0359,
      "step": 988
    },
    {
      "epoch": 0.24121951219512194,
      "grad_norm": 0.14648514986038208,
      "learning_rate": 4.920808642597815e-05,
      "loss": 0.0314,
      "step": 989
    },
    {
      "epoch": 0.24146341463414633,
      "grad_norm": 0.4673895239830017,
      "learning_rate": 4.92064910896903e-05,
      "loss": 0.0234,
      "step": 990
    },
    {
      "epoch": 0.24170731707317072,
      "grad_norm": 0.19759488105773926,
      "learning_rate": 4.920489417400301e-05,
      "loss": 0.0314,
      "step": 991
    },
    {
      "epoch": 0.24195121951219511,
      "grad_norm": 0.04969878867268562,
      "learning_rate": 4.920329567902047e-05,
      "loss": 0.0171,
      "step": 992
    },
    {
      "epoch": 0.2421951219512195,
      "grad_norm": 0.0795886367559433,
      "learning_rate": 4.9201695604846976e-05,
      "loss": 0.0256,
      "step": 993
    },
    {
      "epoch": 0.2424390243902439,
      "grad_norm": 0.08274530619382858,
      "learning_rate": 4.920009395158694e-05,
      "loss": 0.0297,
      "step": 994
    },
    {
      "epoch": 0.2426829268292683,
      "grad_norm": 0.06719102710485458,
      "learning_rate": 4.919849071934486e-05,
      "loss": 0.0238,
      "step": 995
    },
    {
      "epoch": 0.24292682926829268,
      "grad_norm": 0.06808063387870789,
      "learning_rate": 4.9196885908225334e-05,
      "loss": 0.0149,
      "step": 996
    },
    {
      "epoch": 0.24317073170731707,
      "grad_norm": 0.11686727404594421,
      "learning_rate": 4.919527951833307e-05,
      "loss": 0.0246,
      "step": 997
    },
    {
      "epoch": 0.24341463414634146,
      "grad_norm": 0.10282041132450104,
      "learning_rate": 4.919367154977289e-05,
      "loss": 0.0281,
      "step": 998
    },
    {
      "epoch": 0.24365853658536585,
      "grad_norm": 0.08765798062086105,
      "learning_rate": 4.919206200264971e-05,
      "loss": 0.043,
      "step": 999
    },
    {
      "epoch": 0.24390243902439024,
      "grad_norm": 0.07553093880414963,
      "learning_rate": 4.919045087706854e-05,
      "loss": 0.0292,
      "step": 1000
    },
    {
      "epoch": 0.24414634146341463,
      "grad_norm": 0.5015999674797058,
      "learning_rate": 4.918883817313451e-05,
      "loss": 0.0242,
      "step": 1001
    },
    {
      "epoch": 0.24439024390243902,
      "grad_norm": 0.0622265487909317,
      "learning_rate": 4.918722389095283e-05,
      "loss": 0.0326,
      "step": 1002
    },
    {
      "epoch": 0.2446341463414634,
      "grad_norm": 0.06058814749121666,
      "learning_rate": 4.918560803062884e-05,
      "loss": 0.0249,
      "step": 1003
    },
    {
      "epoch": 0.2448780487804878,
      "grad_norm": 0.06802552938461304,
      "learning_rate": 4.918399059226796e-05,
      "loss": 0.0378,
      "step": 1004
    },
    {
      "epoch": 0.2451219512195122,
      "grad_norm": 0.06148359924554825,
      "learning_rate": 4.9182371575975736e-05,
      "loss": 0.0327,
      "step": 1005
    },
    {
      "epoch": 0.24536585365853658,
      "grad_norm": 0.14669980108737946,
      "learning_rate": 4.9180750981857795e-05,
      "loss": 0.0293,
      "step": 1006
    },
    {
      "epoch": 0.24560975609756097,
      "grad_norm": 0.09751283377408981,
      "learning_rate": 4.917912881001987e-05,
      "loss": 0.0208,
      "step": 1007
    },
    {
      "epoch": 0.24585365853658536,
      "grad_norm": 0.2279900163412094,
      "learning_rate": 4.917750506056782e-05,
      "loss": 0.0266,
      "step": 1008
    },
    {
      "epoch": 0.24609756097560975,
      "grad_norm": 0.039119329303503036,
      "learning_rate": 4.917587973360757e-05,
      "loss": 0.0133,
      "step": 1009
    },
    {
      "epoch": 0.24634146341463414,
      "grad_norm": 0.9138460755348206,
      "learning_rate": 4.9174252829245184e-05,
      "loss": 0.0326,
      "step": 1010
    },
    {
      "epoch": 0.24658536585365853,
      "grad_norm": 0.14639760553836823,
      "learning_rate": 4.91726243475868e-05,
      "loss": 0.0413,
      "step": 1011
    },
    {
      "epoch": 0.24682926829268292,
      "grad_norm": 0.21031524240970612,
      "learning_rate": 4.9170994288738676e-05,
      "loss": 0.0386,
      "step": 1012
    },
    {
      "epoch": 0.24707317073170731,
      "grad_norm": 0.28892645239830017,
      "learning_rate": 4.9169362652807174e-05,
      "loss": 0.0229,
      "step": 1013
    },
    {
      "epoch": 0.2473170731707317,
      "grad_norm": 0.08169588446617126,
      "learning_rate": 4.916772943989875e-05,
      "loss": 0.0256,
      "step": 1014
    },
    {
      "epoch": 0.2475609756097561,
      "grad_norm": 0.14874912798404694,
      "learning_rate": 4.916609465011995e-05,
      "loss": 0.0293,
      "step": 1015
    },
    {
      "epoch": 0.24780487804878049,
      "grad_norm": 0.08570899069309235,
      "learning_rate": 4.916445828357747e-05,
      "loss": 0.0194,
      "step": 1016
    },
    {
      "epoch": 0.24804878048780488,
      "grad_norm": 0.10226251184940338,
      "learning_rate": 4.916282034037806e-05,
      "loss": 0.0326,
      "step": 1017
    },
    {
      "epoch": 0.24829268292682927,
      "grad_norm": 0.3366590440273285,
      "learning_rate": 4.9161180820628584e-05,
      "loss": 0.0293,
      "step": 1018
    },
    {
      "epoch": 0.24853658536585366,
      "grad_norm": 0.11398697644472122,
      "learning_rate": 4.9159539724436023e-05,
      "loss": 0.0525,
      "step": 1019
    },
    {
      "epoch": 0.24878048780487805,
      "grad_norm": 0.06307487189769745,
      "learning_rate": 4.9157897051907464e-05,
      "loss": 0.0351,
      "step": 1020
    },
    {
      "epoch": 0.24902439024390244,
      "grad_norm": 0.24670040607452393,
      "learning_rate": 4.915625280315007e-05,
      "loss": 0.0276,
      "step": 1021
    },
    {
      "epoch": 0.24926829268292683,
      "grad_norm": 0.074417345225811,
      "learning_rate": 4.9154606978271136e-05,
      "loss": 0.0356,
      "step": 1022
    },
    {
      "epoch": 0.24951219512195122,
      "grad_norm": 0.09662055969238281,
      "learning_rate": 4.915295957737803e-05,
      "loss": 0.0383,
      "step": 1023
    },
    {
      "epoch": 0.2497560975609756,
      "grad_norm": 0.10538893193006516,
      "learning_rate": 4.915131060057826e-05,
      "loss": 0.0412,
      "step": 1024
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.19649377465248108,
      "learning_rate": 4.914966004797941e-05,
      "loss": 0.0319,
      "step": 1025
    },
    {
      "epoch": 0.2502439024390244,
      "grad_norm": 0.958288848400116,
      "learning_rate": 4.914800791968917e-05,
      "loss": 0.0312,
      "step": 1026
    },
    {
      "epoch": 0.2504878048780488,
      "grad_norm": 0.06448857486248016,
      "learning_rate": 4.914635421581533e-05,
      "loss": 0.0245,
      "step": 1027
    },
    {
      "epoch": 0.25073170731707317,
      "grad_norm": 0.06435318291187286,
      "learning_rate": 4.9144698936465795e-05,
      "loss": 0.0202,
      "step": 1028
    },
    {
      "epoch": 0.25097560975609756,
      "grad_norm": 0.08634671568870544,
      "learning_rate": 4.9143042081748575e-05,
      "loss": 0.0301,
      "step": 1029
    },
    {
      "epoch": 0.25121951219512195,
      "grad_norm": 0.06232258304953575,
      "learning_rate": 4.9141383651771765e-05,
      "loss": 0.0277,
      "step": 1030
    },
    {
      "epoch": 0.25146341463414634,
      "grad_norm": 0.058809082955121994,
      "learning_rate": 4.9139723646643576e-05,
      "loss": 0.0235,
      "step": 1031
    },
    {
      "epoch": 0.25170731707317073,
      "grad_norm": 0.06078820675611496,
      "learning_rate": 4.9138062066472326e-05,
      "loss": 0.0237,
      "step": 1032
    },
    {
      "epoch": 0.2519512195121951,
      "grad_norm": 0.10485885292291641,
      "learning_rate": 4.913639891136641e-05,
      "loss": 0.03,
      "step": 1033
    },
    {
      "epoch": 0.2521951219512195,
      "grad_norm": 0.06386499851942062,
      "learning_rate": 4.913473418143436e-05,
      "loss": 0.0327,
      "step": 1034
    },
    {
      "epoch": 0.2524390243902439,
      "grad_norm": 0.09633460640907288,
      "learning_rate": 4.913306787678478e-05,
      "loss": 0.0309,
      "step": 1035
    },
    {
      "epoch": 0.2526829268292683,
      "grad_norm": 0.16835682094097137,
      "learning_rate": 4.913139999752641e-05,
      "loss": 0.0408,
      "step": 1036
    },
    {
      "epoch": 0.2529268292682927,
      "grad_norm": 0.19459159672260284,
      "learning_rate": 4.9129730543768054e-05,
      "loss": 0.0342,
      "step": 1037
    },
    {
      "epoch": 0.2531707317073171,
      "grad_norm": 0.06713783740997314,
      "learning_rate": 4.912805951561865e-05,
      "loss": 0.0209,
      "step": 1038
    },
    {
      "epoch": 0.25341463414634147,
      "grad_norm": 0.26153644919395447,
      "learning_rate": 4.912638691318723e-05,
      "loss": 0.0389,
      "step": 1039
    },
    {
      "epoch": 0.25365853658536586,
      "grad_norm": 0.09173247218132019,
      "learning_rate": 4.9124712736582914e-05,
      "loss": 0.0356,
      "step": 1040
    },
    {
      "epoch": 0.25390243902439025,
      "grad_norm": 0.10231786221265793,
      "learning_rate": 4.912303698591496e-05,
      "loss": 0.0244,
      "step": 1041
    },
    {
      "epoch": 0.25414634146341464,
      "grad_norm": 0.16338470578193665,
      "learning_rate": 4.912135966129268e-05,
      "loss": 0.0199,
      "step": 1042
    },
    {
      "epoch": 0.25439024390243903,
      "grad_norm": 0.2892345190048218,
      "learning_rate": 4.911968076282552e-05,
      "loss": 0.0181,
      "step": 1043
    },
    {
      "epoch": 0.2546341463414634,
      "grad_norm": 0.11082706600427628,
      "learning_rate": 4.9118000290623035e-05,
      "loss": 0.0342,
      "step": 1044
    },
    {
      "epoch": 0.2548780487804878,
      "grad_norm": 0.26316890120506287,
      "learning_rate": 4.9116318244794865e-05,
      "loss": 0.0419,
      "step": 1045
    },
    {
      "epoch": 0.2551219512195122,
      "grad_norm": 0.10845938324928284,
      "learning_rate": 4.911463462545075e-05,
      "loss": 0.0314,
      "step": 1046
    },
    {
      "epoch": 0.2553658536585366,
      "grad_norm": 0.25197747349739075,
      "learning_rate": 4.911294943270055e-05,
      "loss": 0.0343,
      "step": 1047
    },
    {
      "epoch": 0.255609756097561,
      "grad_norm": 0.06712667644023895,
      "learning_rate": 4.911126266665422e-05,
      "loss": 0.0229,
      "step": 1048
    },
    {
      "epoch": 0.25585365853658537,
      "grad_norm": 0.2686781585216522,
      "learning_rate": 4.91095743274218e-05,
      "loss": 0.0225,
      "step": 1049
    },
    {
      "epoch": 0.25609756097560976,
      "grad_norm": 0.20870989561080933,
      "learning_rate": 4.910788441511348e-05,
      "loss": 0.046,
      "step": 1050
    },
    {
      "epoch": 0.25634146341463415,
      "grad_norm": 0.12359245121479034,
      "learning_rate": 4.910619292983949e-05,
      "loss": 0.03,
      "step": 1051
    },
    {
      "epoch": 0.25658536585365854,
      "grad_norm": 0.05583261325955391,
      "learning_rate": 4.9104499871710217e-05,
      "loss": 0.0309,
      "step": 1052
    },
    {
      "epoch": 0.25682926829268293,
      "grad_norm": 0.2504222095012665,
      "learning_rate": 4.910280524083611e-05,
      "loss": 0.0213,
      "step": 1053
    },
    {
      "epoch": 0.2570731707317073,
      "grad_norm": 0.29292812943458557,
      "learning_rate": 4.9101109037327754e-05,
      "loss": 0.0375,
      "step": 1054
    },
    {
      "epoch": 0.2573170731707317,
      "grad_norm": 0.1014973521232605,
      "learning_rate": 4.909941126129581e-05,
      "loss": 0.0438,
      "step": 1055
    },
    {
      "epoch": 0.2575609756097561,
      "grad_norm": 0.08177562803030014,
      "learning_rate": 4.909771191285105e-05,
      "loss": 0.033,
      "step": 1056
    },
    {
      "epoch": 0.2578048780487805,
      "grad_norm": 0.06683386862277985,
      "learning_rate": 4.909601099210437e-05,
      "loss": 0.0288,
      "step": 1057
    },
    {
      "epoch": 0.2580487804878049,
      "grad_norm": 0.055281396955251694,
      "learning_rate": 4.909430849916673e-05,
      "loss": 0.0221,
      "step": 1058
    },
    {
      "epoch": 0.2582926829268293,
      "grad_norm": 0.09435325115919113,
      "learning_rate": 4.9092604434149227e-05,
      "loss": 0.0335,
      "step": 1059
    },
    {
      "epoch": 0.25853658536585367,
      "grad_norm": 0.11033895611763,
      "learning_rate": 4.909089879716303e-05,
      "loss": 0.0373,
      "step": 1060
    },
    {
      "epoch": 0.25878048780487806,
      "grad_norm": 0.20459342002868652,
      "learning_rate": 4.908919158831944e-05,
      "loss": 0.0274,
      "step": 1061
    },
    {
      "epoch": 0.25902439024390245,
      "grad_norm": 0.24495044350624084,
      "learning_rate": 4.9087482807729844e-05,
      "loss": 0.0569,
      "step": 1062
    },
    {
      "epoch": 0.25926829268292684,
      "grad_norm": 0.11475372314453125,
      "learning_rate": 4.908577245550573e-05,
      "loss": 0.0177,
      "step": 1063
    },
    {
      "epoch": 0.25951219512195123,
      "grad_norm": 0.07709438353776932,
      "learning_rate": 4.90840605317587e-05,
      "loss": 0.0397,
      "step": 1064
    },
    {
      "epoch": 0.2597560975609756,
      "grad_norm": 0.0688910111784935,
      "learning_rate": 4.9082347036600454e-05,
      "loss": 0.0402,
      "step": 1065
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.0764227882027626,
      "learning_rate": 4.908063197014278e-05,
      "loss": 0.0287,
      "step": 1066
    },
    {
      "epoch": 0.2602439024390244,
      "grad_norm": 0.07156849652528763,
      "learning_rate": 4.907891533249759e-05,
      "loss": 0.0283,
      "step": 1067
    },
    {
      "epoch": 0.2604878048780488,
      "grad_norm": 0.7351083159446716,
      "learning_rate": 4.907719712377689e-05,
      "loss": 0.0318,
      "step": 1068
    },
    {
      "epoch": 0.2607317073170732,
      "grad_norm": 0.08099155873060226,
      "learning_rate": 4.907547734409278e-05,
      "loss": 0.0441,
      "step": 1069
    },
    {
      "epoch": 0.26097560975609757,
      "grad_norm": 0.18230965733528137,
      "learning_rate": 4.907375599355747e-05,
      "loss": 0.0201,
      "step": 1070
    },
    {
      "epoch": 0.26121951219512196,
      "grad_norm": 0.08081421256065369,
      "learning_rate": 4.907203307228329e-05,
      "loss": 0.0194,
      "step": 1071
    },
    {
      "epoch": 0.26146341463414635,
      "grad_norm": 0.05479337275028229,
      "learning_rate": 4.907030858038264e-05,
      "loss": 0.0292,
      "step": 1072
    },
    {
      "epoch": 0.26170731707317074,
      "grad_norm": 0.08240111172199249,
      "learning_rate": 4.9068582517968044e-05,
      "loss": 0.0271,
      "step": 1073
    },
    {
      "epoch": 0.26195121951219513,
      "grad_norm": 0.04912304878234863,
      "learning_rate": 4.906685488515212e-05,
      "loss": 0.0212,
      "step": 1074
    },
    {
      "epoch": 0.2621951219512195,
      "grad_norm": 0.09666125476360321,
      "learning_rate": 4.906512568204757e-05,
      "loss": 0.0258,
      "step": 1075
    },
    {
      "epoch": 0.2624390243902439,
      "grad_norm": 0.06415297836065292,
      "learning_rate": 4.906339490876726e-05,
      "loss": 0.026,
      "step": 1076
    },
    {
      "epoch": 0.2626829268292683,
      "grad_norm": 0.07800507545471191,
      "learning_rate": 4.906166256542409e-05,
      "loss": 0.0437,
      "step": 1077
    },
    {
      "epoch": 0.2629268292682927,
      "grad_norm": 0.10423067212104797,
      "learning_rate": 4.905992865213111e-05,
      "loss": 0.0307,
      "step": 1078
    },
    {
      "epoch": 0.2631707317073171,
      "grad_norm": 0.2495875209569931,
      "learning_rate": 4.9058193169001435e-05,
      "loss": 0.0192,
      "step": 1079
    },
    {
      "epoch": 0.2634146341463415,
      "grad_norm": 0.09584416449069977,
      "learning_rate": 4.9056456116148295e-05,
      "loss": 0.0262,
      "step": 1080
    },
    {
      "epoch": 0.26365853658536587,
      "grad_norm": 0.08539561927318573,
      "learning_rate": 4.9054717493685045e-05,
      "loss": 0.0286,
      "step": 1081
    },
    {
      "epoch": 0.26390243902439026,
      "grad_norm": 0.12089929729700089,
      "learning_rate": 4.905297730172511e-05,
      "loss": 0.0252,
      "step": 1082
    },
    {
      "epoch": 0.26414634146341465,
      "grad_norm": 0.06661295890808105,
      "learning_rate": 4.905123554038205e-05,
      "loss": 0.0356,
      "step": 1083
    },
    {
      "epoch": 0.26439024390243904,
      "grad_norm": 0.9966459274291992,
      "learning_rate": 4.9049492209769495e-05,
      "loss": 0.0316,
      "step": 1084
    },
    {
      "epoch": 0.2646341463414634,
      "grad_norm": 0.14901325106620789,
      "learning_rate": 4.9047747310001194e-05,
      "loss": 0.0212,
      "step": 1085
    },
    {
      "epoch": 0.2648780487804878,
      "grad_norm": 0.07261848449707031,
      "learning_rate": 4.9046000841191e-05,
      "loss": 0.0198,
      "step": 1086
    },
    {
      "epoch": 0.2651219512195122,
      "grad_norm": 0.06329741328954697,
      "learning_rate": 4.9044252803452854e-05,
      "loss": 0.0302,
      "step": 1087
    },
    {
      "epoch": 0.2653658536585366,
      "grad_norm": 0.18797416985034943,
      "learning_rate": 4.904250319690083e-05,
      "loss": 0.0235,
      "step": 1088
    },
    {
      "epoch": 0.265609756097561,
      "grad_norm": 0.060121968388557434,
      "learning_rate": 4.904075202164907e-05,
      "loss": 0.0343,
      "step": 1089
    },
    {
      "epoch": 0.2658536585365854,
      "grad_norm": 0.10740512609481812,
      "learning_rate": 4.903899927781184e-05,
      "loss": 0.0375,
      "step": 1090
    },
    {
      "epoch": 0.26609756097560977,
      "grad_norm": 0.050732046365737915,
      "learning_rate": 4.9037244965503494e-05,
      "loss": 0.0205,
      "step": 1091
    },
    {
      "epoch": 0.26634146341463416,
      "grad_norm": 0.07532957196235657,
      "learning_rate": 4.903548908483849e-05,
      "loss": 0.0462,
      "step": 1092
    },
    {
      "epoch": 0.26658536585365855,
      "grad_norm": 0.24814431369304657,
      "learning_rate": 4.903373163593142e-05,
      "loss": 0.0338,
      "step": 1093
    },
    {
      "epoch": 0.26682926829268294,
      "grad_norm": 0.1613374650478363,
      "learning_rate": 4.9031972618896926e-05,
      "loss": 0.0236,
      "step": 1094
    },
    {
      "epoch": 0.26707317073170733,
      "grad_norm": 0.1341344267129898,
      "learning_rate": 4.9030212033849784e-05,
      "loss": 0.0321,
      "step": 1095
    },
    {
      "epoch": 0.2673170731707317,
      "grad_norm": 0.22473746538162231,
      "learning_rate": 4.902844988090488e-05,
      "loss": 0.0305,
      "step": 1096
    },
    {
      "epoch": 0.2675609756097561,
      "grad_norm": 0.28083378076553345,
      "learning_rate": 4.9026686160177174e-05,
      "loss": 0.0341,
      "step": 1097
    },
    {
      "epoch": 0.2678048780487805,
      "grad_norm": 0.11306283622980118,
      "learning_rate": 4.9024920871781746e-05,
      "loss": 0.0327,
      "step": 1098
    },
    {
      "epoch": 0.2680487804878049,
      "grad_norm": 0.12918493151664734,
      "learning_rate": 4.9023154015833786e-05,
      "loss": 0.0292,
      "step": 1099
    },
    {
      "epoch": 0.2682926829268293,
      "grad_norm": 0.6357728242874146,
      "learning_rate": 4.9021385592448566e-05,
      "loss": 0.0248,
      "step": 1100
    },
    {
      "epoch": 0.2685365853658537,
      "grad_norm": 0.08637627959251404,
      "learning_rate": 4.9019615601741464e-05,
      "loss": 0.0291,
      "step": 1101
    },
    {
      "epoch": 0.26878048780487807,
      "grad_norm": 0.13049620389938354,
      "learning_rate": 4.901784404382799e-05,
      "loss": 0.0268,
      "step": 1102
    },
    {
      "epoch": 0.26902439024390246,
      "grad_norm": 0.5754401683807373,
      "learning_rate": 4.9016070918823704e-05,
      "loss": 0.0293,
      "step": 1103
    },
    {
      "epoch": 0.26926829268292685,
      "grad_norm": 0.05163544416427612,
      "learning_rate": 4.901429622684431e-05,
      "loss": 0.0244,
      "step": 1104
    },
    {
      "epoch": 0.26951219512195124,
      "grad_norm": 0.043750882148742676,
      "learning_rate": 4.9012519968005614e-05,
      "loss": 0.0179,
      "step": 1105
    },
    {
      "epoch": 0.2697560975609756,
      "grad_norm": 0.09288996458053589,
      "learning_rate": 4.901074214242349e-05,
      "loss": 0.0312,
      "step": 1106
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.0935821607708931,
      "learning_rate": 4.900896275021396e-05,
      "loss": 0.0358,
      "step": 1107
    },
    {
      "epoch": 0.2702439024390244,
      "grad_norm": 2.4565863609313965,
      "learning_rate": 4.9007181791493094e-05,
      "loss": 0.0349,
      "step": 1108
    },
    {
      "epoch": 0.2704878048780488,
      "grad_norm": 0.2487224042415619,
      "learning_rate": 4.900539926637711e-05,
      "loss": 0.0182,
      "step": 1109
    },
    {
      "epoch": 0.2707317073170732,
      "grad_norm": 0.2932743728160858,
      "learning_rate": 4.9003615174982317e-05,
      "loss": 0.0348,
      "step": 1110
    },
    {
      "epoch": 0.2709756097560976,
      "grad_norm": 0.1065518781542778,
      "learning_rate": 4.9001829517425114e-05,
      "loss": 0.0251,
      "step": 1111
    },
    {
      "epoch": 0.27121951219512197,
      "grad_norm": 0.055567964911460876,
      "learning_rate": 4.9000042293822014e-05,
      "loss": 0.0157,
      "step": 1112
    },
    {
      "epoch": 0.27146341463414636,
      "grad_norm": 0.5598373413085938,
      "learning_rate": 4.8998253504289616e-05,
      "loss": 0.0408,
      "step": 1113
    },
    {
      "epoch": 0.27170731707317075,
      "grad_norm": 0.16735726594924927,
      "learning_rate": 4.899646314894465e-05,
      "loss": 0.0398,
      "step": 1114
    },
    {
      "epoch": 0.27195121951219514,
      "grad_norm": 0.0791829526424408,
      "learning_rate": 4.899467122790392e-05,
      "loss": 0.0239,
      "step": 1115
    },
    {
      "epoch": 0.27219512195121953,
      "grad_norm": 0.09365785121917725,
      "learning_rate": 4.8992877741284353e-05,
      "loss": 0.039,
      "step": 1116
    },
    {
      "epoch": 0.2724390243902439,
      "grad_norm": 0.06710319221019745,
      "learning_rate": 4.8991082689202964e-05,
      "loss": 0.0299,
      "step": 1117
    },
    {
      "epoch": 0.2726829268292683,
      "grad_norm": 0.11937021464109421,
      "learning_rate": 4.8989286071776864e-05,
      "loss": 0.0221,
      "step": 1118
    },
    {
      "epoch": 0.2729268292682927,
      "grad_norm": 0.2809200882911682,
      "learning_rate": 4.89874878891233e-05,
      "loss": 0.0375,
      "step": 1119
    },
    {
      "epoch": 0.2731707317073171,
      "grad_norm": 0.1286887377500534,
      "learning_rate": 4.898568814135956e-05,
      "loss": 0.0385,
      "step": 1120
    },
    {
      "epoch": 0.2734146341463415,
      "grad_norm": 0.08985550701618195,
      "learning_rate": 4.898388682860312e-05,
      "loss": 0.0323,
      "step": 1121
    },
    {
      "epoch": 0.2736585365853659,
      "grad_norm": 0.09698932617902756,
      "learning_rate": 4.898208395097148e-05,
      "loss": 0.0286,
      "step": 1122
    },
    {
      "epoch": 0.27390243902439027,
      "grad_norm": 0.05921337381005287,
      "learning_rate": 4.898027950858227e-05,
      "loss": 0.0293,
      "step": 1123
    },
    {
      "epoch": 0.27414634146341466,
      "grad_norm": 0.14813360571861267,
      "learning_rate": 4.897847350155324e-05,
      "loss": 0.0267,
      "step": 1124
    },
    {
      "epoch": 0.27439024390243905,
      "grad_norm": 0.6740430593490601,
      "learning_rate": 4.897666593000222e-05,
      "loss": 0.038,
      "step": 1125
    },
    {
      "epoch": 0.27463414634146344,
      "grad_norm": 0.07812906801700592,
      "learning_rate": 4.8974856794047144e-05,
      "loss": 0.0252,
      "step": 1126
    },
    {
      "epoch": 0.2748780487804878,
      "grad_norm": 0.24277576804161072,
      "learning_rate": 4.897304609380606e-05,
      "loss": 0.035,
      "step": 1127
    },
    {
      "epoch": 0.2751219512195122,
      "grad_norm": 0.07725600898265839,
      "learning_rate": 4.89712338293971e-05,
      "loss": 0.0223,
      "step": 1128
    },
    {
      "epoch": 0.2753658536585366,
      "grad_norm": 0.23503360152244568,
      "learning_rate": 4.8969420000938527e-05,
      "loss": 0.0412,
      "step": 1129
    },
    {
      "epoch": 0.275609756097561,
      "grad_norm": 0.19950391352176666,
      "learning_rate": 4.8967604608548666e-05,
      "loss": 0.0293,
      "step": 1130
    },
    {
      "epoch": 0.2758536585365854,
      "grad_norm": 0.10125235468149185,
      "learning_rate": 4.896578765234598e-05,
      "loss": 0.0339,
      "step": 1131
    },
    {
      "epoch": 0.2760975609756098,
      "grad_norm": 0.0524827279150486,
      "learning_rate": 4.896396913244902e-05,
      "loss": 0.0148,
      "step": 1132
    },
    {
      "epoch": 0.27634146341463417,
      "grad_norm": 0.17245036363601685,
      "learning_rate": 4.8962149048976425e-05,
      "loss": 0.0306,
      "step": 1133
    },
    {
      "epoch": 0.27658536585365856,
      "grad_norm": 0.10358766466379166,
      "learning_rate": 4.896032740204697e-05,
      "loss": 0.0314,
      "step": 1134
    },
    {
      "epoch": 0.27682926829268295,
      "grad_norm": 0.18555767834186554,
      "learning_rate": 4.8958504191779494e-05,
      "loss": 0.0307,
      "step": 1135
    },
    {
      "epoch": 0.27707317073170734,
      "grad_norm": 0.19211550056934357,
      "learning_rate": 4.895667941829296e-05,
      "loss": 0.0348,
      "step": 1136
    },
    {
      "epoch": 0.27731707317073173,
      "grad_norm": 0.10988181829452515,
      "learning_rate": 4.8954853081706444e-05,
      "loss": 0.0264,
      "step": 1137
    },
    {
      "epoch": 0.2775609756097561,
      "grad_norm": 0.09252457320690155,
      "learning_rate": 4.895302518213909e-05,
      "loss": 0.028,
      "step": 1138
    },
    {
      "epoch": 0.2778048780487805,
      "grad_norm": 0.2210942804813385,
      "learning_rate": 4.895119571971017e-05,
      "loss": 0.0366,
      "step": 1139
    },
    {
      "epoch": 0.2780487804878049,
      "grad_norm": 0.2556551396846771,
      "learning_rate": 4.8949364694539055e-05,
      "loss": 0.0282,
      "step": 1140
    },
    {
      "epoch": 0.2782926829268293,
      "grad_norm": 0.05388855189085007,
      "learning_rate": 4.8947532106745206e-05,
      "loss": 0.025,
      "step": 1141
    },
    {
      "epoch": 0.2785365853658537,
      "grad_norm": 0.10345684736967087,
      "learning_rate": 4.89456979564482e-05,
      "loss": 0.0408,
      "step": 1142
    },
    {
      "epoch": 0.2787804878048781,
      "grad_norm": 0.1000177189707756,
      "learning_rate": 4.894386224376771e-05,
      "loss": 0.0356,
      "step": 1143
    },
    {
      "epoch": 0.27902439024390246,
      "grad_norm": 0.07155945152044296,
      "learning_rate": 4.8942024968823504e-05,
      "loss": 0.0293,
      "step": 1144
    },
    {
      "epoch": 0.27926829268292686,
      "grad_norm": 0.08130437135696411,
      "learning_rate": 4.894018613173546e-05,
      "loss": 0.0178,
      "step": 1145
    },
    {
      "epoch": 0.27951219512195125,
      "grad_norm": 0.1068352535367012,
      "learning_rate": 4.893834573262356e-05,
      "loss": 0.0332,
      "step": 1146
    },
    {
      "epoch": 0.27975609756097564,
      "grad_norm": 0.07352364808320999,
      "learning_rate": 4.893650377160789e-05,
      "loss": 0.0164,
      "step": 1147
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.1735377460718155,
      "learning_rate": 4.893466024880862e-05,
      "loss": 0.0327,
      "step": 1148
    },
    {
      "epoch": 0.2802439024390244,
      "grad_norm": 0.07479347288608551,
      "learning_rate": 4.8932815164346036e-05,
      "loss": 0.0277,
      "step": 1149
    },
    {
      "epoch": 0.2804878048780488,
      "grad_norm": 0.1526334583759308,
      "learning_rate": 4.8930968518340533e-05,
      "loss": 0.032,
      "step": 1150
    },
    {
      "epoch": 0.2807317073170732,
      "grad_norm": 0.11689034849405289,
      "learning_rate": 4.892912031091259e-05,
      "loss": 0.032,
      "step": 1151
    },
    {
      "epoch": 0.2809756097560976,
      "grad_norm": 0.05930720269680023,
      "learning_rate": 4.892727054218281e-05,
      "loss": 0.034,
      "step": 1152
    },
    {
      "epoch": 0.281219512195122,
      "grad_norm": 0.13219501078128815,
      "learning_rate": 4.892541921227186e-05,
      "loss": 0.0367,
      "step": 1153
    },
    {
      "epoch": 0.2814634146341463,
      "grad_norm": 0.07675870507955551,
      "learning_rate": 4.892356632130056e-05,
      "loss": 0.0273,
      "step": 1154
    },
    {
      "epoch": 0.2817073170731707,
      "grad_norm": 0.0412566103041172,
      "learning_rate": 4.89217118693898e-05,
      "loss": 0.0148,
      "step": 1155
    },
    {
      "epoch": 0.2819512195121951,
      "grad_norm": 0.08108928799629211,
      "learning_rate": 4.891985585666056e-05,
      "loss": 0.0306,
      "step": 1156
    },
    {
      "epoch": 0.2821951219512195,
      "grad_norm": 0.16410808265209198,
      "learning_rate": 4.8917998283233956e-05,
      "loss": 0.0197,
      "step": 1157
    },
    {
      "epoch": 0.2824390243902439,
      "grad_norm": 0.08694402873516083,
      "learning_rate": 4.891613914923119e-05,
      "loss": 0.0366,
      "step": 1158
    },
    {
      "epoch": 0.28268292682926827,
      "grad_norm": 0.244531512260437,
      "learning_rate": 4.891427845477354e-05,
      "loss": 0.025,
      "step": 1159
    },
    {
      "epoch": 0.28292682926829266,
      "grad_norm": 0.1327894926071167,
      "learning_rate": 4.8912416199982446e-05,
      "loss": 0.0485,
      "step": 1160
    },
    {
      "epoch": 0.28317073170731705,
      "grad_norm": 0.06921012699604034,
      "learning_rate": 4.8910552384979396e-05,
      "loss": 0.023,
      "step": 1161
    },
    {
      "epoch": 0.28341463414634144,
      "grad_norm": 0.09666035324335098,
      "learning_rate": 4.8908687009886e-05,
      "loss": 0.0239,
      "step": 1162
    },
    {
      "epoch": 0.28365853658536583,
      "grad_norm": 0.20715218782424927,
      "learning_rate": 4.890682007482397e-05,
      "loss": 0.0409,
      "step": 1163
    },
    {
      "epoch": 0.2839024390243902,
      "grad_norm": 0.08432108908891678,
      "learning_rate": 4.89049515799151e-05,
      "loss": 0.0358,
      "step": 1164
    },
    {
      "epoch": 0.2841463414634146,
      "grad_norm": 0.06473860144615173,
      "learning_rate": 4.8903081525281334e-05,
      "loss": 0.0227,
      "step": 1165
    },
    {
      "epoch": 0.284390243902439,
      "grad_norm": 0.2230544090270996,
      "learning_rate": 4.8901209911044674e-05,
      "loss": 0.0372,
      "step": 1166
    },
    {
      "epoch": 0.2846341463414634,
      "grad_norm": 0.08854294568300247,
      "learning_rate": 4.889933673732723e-05,
      "loss": 0.0237,
      "step": 1167
    },
    {
      "epoch": 0.2848780487804878,
      "grad_norm": 0.09961708635091782,
      "learning_rate": 4.889746200425124e-05,
      "loss": 0.0482,
      "step": 1168
    },
    {
      "epoch": 0.28512195121951217,
      "grad_norm": 0.07699407637119293,
      "learning_rate": 4.8895585711939e-05,
      "loss": 0.0333,
      "step": 1169
    },
    {
      "epoch": 0.28536585365853656,
      "grad_norm": 0.13839192688465118,
      "learning_rate": 4.8893707860512936e-05,
      "loss": 0.0295,
      "step": 1170
    },
    {
      "epoch": 0.28560975609756095,
      "grad_norm": 0.15142926573753357,
      "learning_rate": 4.889182845009559e-05,
      "loss": 0.0356,
      "step": 1171
    },
    {
      "epoch": 0.28585365853658534,
      "grad_norm": 0.08287740498781204,
      "learning_rate": 4.8889947480809575e-05,
      "loss": 0.0224,
      "step": 1172
    },
    {
      "epoch": 0.28609756097560973,
      "grad_norm": 0.09748683869838715,
      "learning_rate": 4.888806495277762e-05,
      "loss": 0.0311,
      "step": 1173
    },
    {
      "epoch": 0.2863414634146341,
      "grad_norm": 0.0742173120379448,
      "learning_rate": 4.8886180866122554e-05,
      "loss": 0.0247,
      "step": 1174
    },
    {
      "epoch": 0.2865853658536585,
      "grad_norm": 0.21721670031547546,
      "learning_rate": 4.8884295220967316e-05,
      "loss": 0.0288,
      "step": 1175
    },
    {
      "epoch": 0.2868292682926829,
      "grad_norm": 0.11865323036909103,
      "learning_rate": 4.8882408017434926e-05,
      "loss": 0.0199,
      "step": 1176
    },
    {
      "epoch": 0.2870731707317073,
      "grad_norm": 0.11164847016334534,
      "learning_rate": 4.888051925564853e-05,
      "loss": 0.0246,
      "step": 1177
    },
    {
      "epoch": 0.2873170731707317,
      "grad_norm": 0.06726569682359695,
      "learning_rate": 4.887862893573135e-05,
      "loss": 0.0326,
      "step": 1178
    },
    {
      "epoch": 0.2875609756097561,
      "grad_norm": 0.05359959229826927,
      "learning_rate": 4.887673705780673e-05,
      "loss": 0.013,
      "step": 1179
    },
    {
      "epoch": 0.28780487804878047,
      "grad_norm": 0.07302433252334595,
      "learning_rate": 4.8874843621998126e-05,
      "loss": 0.0411,
      "step": 1180
    },
    {
      "epoch": 0.28804878048780486,
      "grad_norm": 0.38552260398864746,
      "learning_rate": 4.887294862842905e-05,
      "loss": 0.0422,
      "step": 1181
    },
    {
      "epoch": 0.28829268292682925,
      "grad_norm": 0.09908593446016312,
      "learning_rate": 4.8871052077223156e-05,
      "loss": 0.0232,
      "step": 1182
    },
    {
      "epoch": 0.28853658536585364,
      "grad_norm": 0.05753198638558388,
      "learning_rate": 4.88691539685042e-05,
      "loss": 0.018,
      "step": 1183
    },
    {
      "epoch": 0.288780487804878,
      "grad_norm": 0.16820059716701508,
      "learning_rate": 4.8867254302396013e-05,
      "loss": 0.0216,
      "step": 1184
    },
    {
      "epoch": 0.2890243902439024,
      "grad_norm": 0.3649316132068634,
      "learning_rate": 4.886535307902255e-05,
      "loss": 0.0263,
      "step": 1185
    },
    {
      "epoch": 0.2892682926829268,
      "grad_norm": 0.08874859660863876,
      "learning_rate": 4.886345029850785e-05,
      "loss": 0.0356,
      "step": 1186
    },
    {
      "epoch": 0.2895121951219512,
      "grad_norm": 0.10844262689352036,
      "learning_rate": 4.886154596097608e-05,
      "loss": 0.0356,
      "step": 1187
    },
    {
      "epoch": 0.2897560975609756,
      "grad_norm": 0.06684447079896927,
      "learning_rate": 4.885964006655148e-05,
      "loss": 0.0194,
      "step": 1188
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.10862737894058228,
      "learning_rate": 4.885773261535841e-05,
      "loss": 0.0248,
      "step": 1189
    },
    {
      "epoch": 0.29024390243902437,
      "grad_norm": 0.09377628564834595,
      "learning_rate": 4.8855823607521314e-05,
      "loss": 0.0223,
      "step": 1190
    },
    {
      "epoch": 0.29048780487804876,
      "grad_norm": 0.07680877298116684,
      "learning_rate": 4.8853913043164764e-05,
      "loss": 0.0329,
      "step": 1191
    },
    {
      "epoch": 0.29073170731707315,
      "grad_norm": 0.0731520801782608,
      "learning_rate": 4.885200092241341e-05,
      "loss": 0.0245,
      "step": 1192
    },
    {
      "epoch": 0.29097560975609754,
      "grad_norm": 0.09300768375396729,
      "learning_rate": 4.885008724539201e-05,
      "loss": 0.0405,
      "step": 1193
    },
    {
      "epoch": 0.29121951219512193,
      "grad_norm": 0.13023641705513,
      "learning_rate": 4.884817201222544e-05,
      "loss": 0.021,
      "step": 1194
    },
    {
      "epoch": 0.2914634146341463,
      "grad_norm": 0.10147964209318161,
      "learning_rate": 4.884625522303864e-05,
      "loss": 0.0372,
      "step": 1195
    },
    {
      "epoch": 0.2917073170731707,
      "grad_norm": 0.14419761300086975,
      "learning_rate": 4.88443368779567e-05,
      "loss": 0.0408,
      "step": 1196
    },
    {
      "epoch": 0.2919512195121951,
      "grad_norm": 0.19252894818782806,
      "learning_rate": 4.884241697710476e-05,
      "loss": 0.0243,
      "step": 1197
    },
    {
      "epoch": 0.2921951219512195,
      "grad_norm": 0.13572055101394653,
      "learning_rate": 4.884049552060811e-05,
      "loss": 0.0297,
      "step": 1198
    },
    {
      "epoch": 0.2924390243902439,
      "grad_norm": 0.14497169852256775,
      "learning_rate": 4.8838572508592104e-05,
      "loss": 0.0308,
      "step": 1199
    },
    {
      "epoch": 0.2926829268292683,
      "grad_norm": 0.35139793157577515,
      "learning_rate": 4.8836647941182226e-05,
      "loss": 0.0452,
      "step": 1200
    },
    {
      "epoch": 0.29292682926829267,
      "grad_norm": 0.07229922711849213,
      "learning_rate": 4.883472181850404e-05,
      "loss": 0.0119,
      "step": 1201
    },
    {
      "epoch": 0.29317073170731706,
      "grad_norm": 0.5601395964622498,
      "learning_rate": 4.8832794140683216e-05,
      "loss": 0.0236,
      "step": 1202
    },
    {
      "epoch": 0.29341463414634145,
      "grad_norm": 0.049110159277915955,
      "learning_rate": 4.883086490784553e-05,
      "loss": 0.0217,
      "step": 1203
    },
    {
      "epoch": 0.29365853658536584,
      "grad_norm": 0.14373597502708435,
      "learning_rate": 4.8828934120116864e-05,
      "loss": 0.0298,
      "step": 1204
    },
    {
      "epoch": 0.2939024390243902,
      "grad_norm": 0.22085759043693542,
      "learning_rate": 4.882700177762319e-05,
      "loss": 0.0242,
      "step": 1205
    },
    {
      "epoch": 0.2941463414634146,
      "grad_norm": 0.0883917585015297,
      "learning_rate": 4.8825067880490604e-05,
      "loss": 0.0404,
      "step": 1206
    },
    {
      "epoch": 0.294390243902439,
      "grad_norm": 0.1353839635848999,
      "learning_rate": 4.882313242884527e-05,
      "loss": 0.0416,
      "step": 1207
    },
    {
      "epoch": 0.2946341463414634,
      "grad_norm": 0.15722927451133728,
      "learning_rate": 4.882119542281347e-05,
      "loss": 0.0243,
      "step": 1208
    },
    {
      "epoch": 0.2948780487804878,
      "grad_norm": 0.19826017320156097,
      "learning_rate": 4.8819256862521604e-05,
      "loss": 0.0258,
      "step": 1209
    },
    {
      "epoch": 0.2951219512195122,
      "grad_norm": 0.2102249562740326,
      "learning_rate": 4.881731674809613e-05,
      "loss": 0.0372,
      "step": 1210
    },
    {
      "epoch": 0.29536585365853657,
      "grad_norm": 0.07559197396039963,
      "learning_rate": 4.881537507966366e-05,
      "loss": 0.028,
      "step": 1211
    },
    {
      "epoch": 0.29560975609756096,
      "grad_norm": 0.062386855483055115,
      "learning_rate": 4.8813431857350866e-05,
      "loss": 0.0318,
      "step": 1212
    },
    {
      "epoch": 0.29585365853658535,
      "grad_norm": 0.11228138953447342,
      "learning_rate": 4.8811487081284544e-05,
      "loss": 0.0473,
      "step": 1213
    },
    {
      "epoch": 0.29609756097560974,
      "grad_norm": 0.12319453060626984,
      "learning_rate": 4.880954075159159e-05,
      "loss": 0.02,
      "step": 1214
    },
    {
      "epoch": 0.29634146341463413,
      "grad_norm": 0.10525263845920563,
      "learning_rate": 4.880759286839898e-05,
      "loss": 0.037,
      "step": 1215
    },
    {
      "epoch": 0.2965853658536585,
      "grad_norm": 0.11031799018383026,
      "learning_rate": 4.8805643431833826e-05,
      "loss": 0.0151,
      "step": 1216
    },
    {
      "epoch": 0.2968292682926829,
      "grad_norm": 0.054681446403265,
      "learning_rate": 4.880369244202331e-05,
      "loss": 0.0177,
      "step": 1217
    },
    {
      "epoch": 0.2970731707317073,
      "grad_norm": 0.049273837357759476,
      "learning_rate": 4.8801739899094734e-05,
      "loss": 0.014,
      "step": 1218
    },
    {
      "epoch": 0.2973170731707317,
      "grad_norm": 0.09476175159215927,
      "learning_rate": 4.879978580317549e-05,
      "loss": 0.0468,
      "step": 1219
    },
    {
      "epoch": 0.2975609756097561,
      "grad_norm": 0.05705559253692627,
      "learning_rate": 4.8797830154393084e-05,
      "loss": 0.0232,
      "step": 1220
    },
    {
      "epoch": 0.2978048780487805,
      "grad_norm": 0.11853567510843277,
      "learning_rate": 4.8795872952875116e-05,
      "loss": 0.0432,
      "step": 1221
    },
    {
      "epoch": 0.29804878048780487,
      "grad_norm": 0.049038466066122055,
      "learning_rate": 4.879391419874928e-05,
      "loss": 0.0161,
      "step": 1222
    },
    {
      "epoch": 0.29829268292682926,
      "grad_norm": 0.06511827558279037,
      "learning_rate": 4.879195389214338e-05,
      "loss": 0.0285,
      "step": 1223
    },
    {
      "epoch": 0.29853658536585365,
      "grad_norm": 0.06176474690437317,
      "learning_rate": 4.8789992033185325e-05,
      "loss": 0.0304,
      "step": 1224
    },
    {
      "epoch": 0.29878048780487804,
      "grad_norm": 0.08972828835248947,
      "learning_rate": 4.8788028622003125e-05,
      "loss": 0.0466,
      "step": 1225
    },
    {
      "epoch": 0.2990243902439024,
      "grad_norm": 0.0663171038031578,
      "learning_rate": 4.878606365872487e-05,
      "loss": 0.0188,
      "step": 1226
    },
    {
      "epoch": 0.2992682926829268,
      "grad_norm": 0.11687083542346954,
      "learning_rate": 4.878409714347878e-05,
      "loss": 0.024,
      "step": 1227
    },
    {
      "epoch": 0.2995121951219512,
      "grad_norm": 0.1379970908164978,
      "learning_rate": 4.878212907639316e-05,
      "loss": 0.0187,
      "step": 1228
    },
    {
      "epoch": 0.2997560975609756,
      "grad_norm": 0.11578895896673203,
      "learning_rate": 4.878015945759642e-05,
      "loss": 0.0297,
      "step": 1229
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.28791743516921997,
      "learning_rate": 4.877818828721709e-05,
      "loss": 0.0328,
      "step": 1230
    },
    {
      "epoch": 0.3002439024390244,
      "grad_norm": 0.16810640692710876,
      "learning_rate": 4.877621556538374e-05,
      "loss": 0.0346,
      "step": 1231
    },
    {
      "epoch": 0.30048780487804877,
      "grad_norm": 0.08488497883081436,
      "learning_rate": 4.877424129222513e-05,
      "loss": 0.0274,
      "step": 1232
    },
    {
      "epoch": 0.30073170731707316,
      "grad_norm": 0.5316326022148132,
      "learning_rate": 4.877226546787005e-05,
      "loss": 0.0512,
      "step": 1233
    },
    {
      "epoch": 0.30097560975609755,
      "grad_norm": 0.17205660045146942,
      "learning_rate": 4.877028809244742e-05,
      "loss": 0.0241,
      "step": 1234
    },
    {
      "epoch": 0.30121951219512194,
      "grad_norm": 0.12238764017820358,
      "learning_rate": 4.8768309166086264e-05,
      "loss": 0.05,
      "step": 1235
    },
    {
      "epoch": 0.30146341463414633,
      "grad_norm": 0.11527471244335175,
      "learning_rate": 4.876632868891569e-05,
      "loss": 0.0304,
      "step": 1236
    },
    {
      "epoch": 0.3017073170731707,
      "grad_norm": 0.0689166784286499,
      "learning_rate": 4.876434666106493e-05,
      "loss": 0.0189,
      "step": 1237
    },
    {
      "epoch": 0.3019512195121951,
      "grad_norm": 0.09696642309427261,
      "learning_rate": 4.87623630826633e-05,
      "loss": 0.0219,
      "step": 1238
    },
    {
      "epoch": 0.3021951219512195,
      "grad_norm": 0.09912631660699844,
      "learning_rate": 4.876037795384022e-05,
      "loss": 0.0326,
      "step": 1239
    },
    {
      "epoch": 0.3024390243902439,
      "grad_norm": 0.03434412553906441,
      "learning_rate": 4.8758391274725215e-05,
      "loss": 0.0128,
      "step": 1240
    },
    {
      "epoch": 0.3026829268292683,
      "grad_norm": 0.060650356113910675,
      "learning_rate": 4.875640304544792e-05,
      "loss": 0.0184,
      "step": 1241
    },
    {
      "epoch": 0.3029268292682927,
      "grad_norm": 0.09263341128826141,
      "learning_rate": 4.875441326613804e-05,
      "loss": 0.0406,
      "step": 1242
    },
    {
      "epoch": 0.30317073170731706,
      "grad_norm": 0.06994151324033737,
      "learning_rate": 4.875242193692542e-05,
      "loss": 0.0378,
      "step": 1243
    },
    {
      "epoch": 0.30341463414634146,
      "grad_norm": 0.2691895067691803,
      "learning_rate": 4.875042905793998e-05,
      "loss": 0.0302,
      "step": 1244
    },
    {
      "epoch": 0.30365853658536585,
      "grad_norm": 0.06108788773417473,
      "learning_rate": 4.874843462931175e-05,
      "loss": 0.0209,
      "step": 1245
    },
    {
      "epoch": 0.30390243902439024,
      "grad_norm": 0.07622361928224564,
      "learning_rate": 4.8746438651170865e-05,
      "loss": 0.0338,
      "step": 1246
    },
    {
      "epoch": 0.3041463414634146,
      "grad_norm": 0.08917734771966934,
      "learning_rate": 4.874444112364755e-05,
      "loss": 0.0273,
      "step": 1247
    },
    {
      "epoch": 0.304390243902439,
      "grad_norm": 0.0849539190530777,
      "learning_rate": 4.874244204687214e-05,
      "loss": 0.0203,
      "step": 1248
    },
    {
      "epoch": 0.3046341463414634,
      "grad_norm": 0.08819451183080673,
      "learning_rate": 4.874044142097507e-05,
      "loss": 0.0328,
      "step": 1249
    },
    {
      "epoch": 0.3048780487804878,
      "grad_norm": 0.07548484206199646,
      "learning_rate": 4.873843924608688e-05,
      "loss": 0.0319,
      "step": 1250
    },
    {
      "epoch": 0.3051219512195122,
      "grad_norm": 0.05755815654993057,
      "learning_rate": 4.873643552233819e-05,
      "loss": 0.0217,
      "step": 1251
    },
    {
      "epoch": 0.3053658536585366,
      "grad_norm": 0.24267849326133728,
      "learning_rate": 4.8734430249859745e-05,
      "loss": 0.0246,
      "step": 1252
    },
    {
      "epoch": 0.30560975609756097,
      "grad_norm": 0.06478198617696762,
      "learning_rate": 4.8732423428782394e-05,
      "loss": 0.0279,
      "step": 1253
    },
    {
      "epoch": 0.30585365853658536,
      "grad_norm": 0.14856158196926117,
      "learning_rate": 4.873041505923706e-05,
      "loss": 0.0325,
      "step": 1254
    },
    {
      "epoch": 0.30609756097560975,
      "grad_norm": 0.23904713988304138,
      "learning_rate": 4.872840514135478e-05,
      "loss": 0.0172,
      "step": 1255
    },
    {
      "epoch": 0.30634146341463414,
      "grad_norm": 0.27662548422813416,
      "learning_rate": 4.8726393675266716e-05,
      "loss": 0.0347,
      "step": 1256
    },
    {
      "epoch": 0.30658536585365853,
      "grad_norm": 0.10043727606534958,
      "learning_rate": 4.8724380661104095e-05,
      "loss": 0.0432,
      "step": 1257
    },
    {
      "epoch": 0.3068292682926829,
      "grad_norm": 0.23615099489688873,
      "learning_rate": 4.872236609899827e-05,
      "loss": 0.0442,
      "step": 1258
    },
    {
      "epoch": 0.3070731707317073,
      "grad_norm": 0.25857874751091003,
      "learning_rate": 4.872034998908067e-05,
      "loss": 0.0293,
      "step": 1259
    },
    {
      "epoch": 0.3073170731707317,
      "grad_norm": 0.22604477405548096,
      "learning_rate": 4.871833233148285e-05,
      "loss": 0.0151,
      "step": 1260
    },
    {
      "epoch": 0.3075609756097561,
      "grad_norm": 0.10647431015968323,
      "learning_rate": 4.871631312633646e-05,
      "loss": 0.0372,
      "step": 1261
    },
    {
      "epoch": 0.3078048780487805,
      "grad_norm": 0.134582981467247,
      "learning_rate": 4.871429237377323e-05,
      "loss": 0.037,
      "step": 1262
    },
    {
      "epoch": 0.3080487804878049,
      "grad_norm": 0.09470267593860626,
      "learning_rate": 4.8712270073925027e-05,
      "loss": 0.0412,
      "step": 1263
    },
    {
      "epoch": 0.30829268292682926,
      "grad_norm": 0.13871121406555176,
      "learning_rate": 4.871024622692379e-05,
      "loss": 0.0443,
      "step": 1264
    },
    {
      "epoch": 0.30853658536585366,
      "grad_norm": 0.07221917808055878,
      "learning_rate": 4.8708220832901575e-05,
      "loss": 0.0291,
      "step": 1265
    },
    {
      "epoch": 0.30878048780487805,
      "grad_norm": 0.1279822438955307,
      "learning_rate": 4.870619389199052e-05,
      "loss": 0.0327,
      "step": 1266
    },
    {
      "epoch": 0.30902439024390244,
      "grad_norm": 0.0804905816912651,
      "learning_rate": 4.870416540432289e-05,
      "loss": 0.0274,
      "step": 1267
    },
    {
      "epoch": 0.3092682926829268,
      "grad_norm": 0.08274898678064346,
      "learning_rate": 4.870213537003103e-05,
      "loss": 0.0298,
      "step": 1268
    },
    {
      "epoch": 0.3095121951219512,
      "grad_norm": 0.1737087368965149,
      "learning_rate": 4.87001037892474e-05,
      "loss": 0.0146,
      "step": 1269
    },
    {
      "epoch": 0.3097560975609756,
      "grad_norm": 0.13401266932487488,
      "learning_rate": 4.869807066210455e-05,
      "loss": 0.0333,
      "step": 1270
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.11883721500635147,
      "learning_rate": 4.8696035988735134e-05,
      "loss": 0.0403,
      "step": 1271
    },
    {
      "epoch": 0.3102439024390244,
      "grad_norm": 0.08217667043209076,
      "learning_rate": 4.8693999769271917e-05,
      "loss": 0.012,
      "step": 1272
    },
    {
      "epoch": 0.3104878048780488,
      "grad_norm": 0.15225349366664886,
      "learning_rate": 4.8691962003847735e-05,
      "loss": 0.0355,
      "step": 1273
    },
    {
      "epoch": 0.31073170731707317,
      "grad_norm": 0.15648837387561798,
      "learning_rate": 4.8689922692595574e-05,
      "loss": 0.0411,
      "step": 1274
    },
    {
      "epoch": 0.31097560975609756,
      "grad_norm": 0.07863576710224152,
      "learning_rate": 4.868788183564847e-05,
      "loss": 0.0239,
      "step": 1275
    },
    {
      "epoch": 0.31121951219512195,
      "grad_norm": 0.05633903667330742,
      "learning_rate": 4.86858394331396e-05,
      "loss": 0.017,
      "step": 1276
    },
    {
      "epoch": 0.31146341463414634,
      "grad_norm": 0.16943177580833435,
      "learning_rate": 4.868379548520221e-05,
      "loss": 0.0326,
      "step": 1277
    },
    {
      "epoch": 0.31170731707317073,
      "grad_norm": 0.11273019760847092,
      "learning_rate": 4.868174999196967e-05,
      "loss": 0.0282,
      "step": 1278
    },
    {
      "epoch": 0.3119512195121951,
      "grad_norm": 0.08747953921556473,
      "learning_rate": 4.867970295357544e-05,
      "loss": 0.0326,
      "step": 1279
    },
    {
      "epoch": 0.3121951219512195,
      "grad_norm": 0.14371657371520996,
      "learning_rate": 4.8677654370153084e-05,
      "loss": 0.0348,
      "step": 1280
    },
    {
      "epoch": 0.3124390243902439,
      "grad_norm": 0.1241568848490715,
      "learning_rate": 4.867560424183626e-05,
      "loss": 0.0389,
      "step": 1281
    },
    {
      "epoch": 0.3126829268292683,
      "grad_norm": 0.16419275104999542,
      "learning_rate": 4.867355256875874e-05,
      "loss": 0.0232,
      "step": 1282
    },
    {
      "epoch": 0.3129268292682927,
      "grad_norm": 0.08895786106586456,
      "learning_rate": 4.867149935105439e-05,
      "loss": 0.0487,
      "step": 1283
    },
    {
      "epoch": 0.3131707317073171,
      "grad_norm": 0.15685218572616577,
      "learning_rate": 4.866944458885717e-05,
      "loss": 0.0283,
      "step": 1284
    },
    {
      "epoch": 0.31341463414634146,
      "grad_norm": 0.11533322930335999,
      "learning_rate": 4.8667388282301155e-05,
      "loss": 0.0345,
      "step": 1285
    },
    {
      "epoch": 0.31365853658536585,
      "grad_norm": 0.13841572403907776,
      "learning_rate": 4.866533043152049e-05,
      "loss": 0.0416,
      "step": 1286
    },
    {
      "epoch": 0.31390243902439025,
      "grad_norm": 0.07667781412601471,
      "learning_rate": 4.8663271036649475e-05,
      "loss": 0.0251,
      "step": 1287
    },
    {
      "epoch": 0.31414634146341464,
      "grad_norm": 0.10317526757717133,
      "learning_rate": 4.866121009782246e-05,
      "loss": 0.0332,
      "step": 1288
    },
    {
      "epoch": 0.314390243902439,
      "grad_norm": 0.0925494059920311,
      "learning_rate": 4.8659147615173926e-05,
      "loss": 0.033,
      "step": 1289
    },
    {
      "epoch": 0.3146341463414634,
      "grad_norm": 0.12064478546380997,
      "learning_rate": 4.865708358883844e-05,
      "loss": 0.0319,
      "step": 1290
    },
    {
      "epoch": 0.3148780487804878,
      "grad_norm": 0.13113164901733398,
      "learning_rate": 4.865501801895066e-05,
      "loss": 0.032,
      "step": 1291
    },
    {
      "epoch": 0.3151219512195122,
      "grad_norm": 0.08161883056163788,
      "learning_rate": 4.865295090564538e-05,
      "loss": 0.0273,
      "step": 1292
    },
    {
      "epoch": 0.3153658536585366,
      "grad_norm": 0.0772450640797615,
      "learning_rate": 4.865088224905746e-05,
      "loss": 0.0351,
      "step": 1293
    },
    {
      "epoch": 0.315609756097561,
      "grad_norm": 0.2319478839635849,
      "learning_rate": 4.864881204932187e-05,
      "loss": 0.061,
      "step": 1294
    },
    {
      "epoch": 0.31585365853658537,
      "grad_norm": 0.1937902271747589,
      "learning_rate": 4.864674030657369e-05,
      "loss": 0.027,
      "step": 1295
    },
    {
      "epoch": 0.31609756097560976,
      "grad_norm": 0.08162910491228104,
      "learning_rate": 4.8644667020948096e-05,
      "loss": 0.0226,
      "step": 1296
    },
    {
      "epoch": 0.31634146341463415,
      "grad_norm": 0.10357549041509628,
      "learning_rate": 4.864259219258036e-05,
      "loss": 0.0304,
      "step": 1297
    },
    {
      "epoch": 0.31658536585365854,
      "grad_norm": 0.12393786013126373,
      "learning_rate": 4.864051582160587e-05,
      "loss": 0.043,
      "step": 1298
    },
    {
      "epoch": 0.31682926829268293,
      "grad_norm": 0.4169774055480957,
      "learning_rate": 4.863843790816008e-05,
      "loss": 0.0313,
      "step": 1299
    },
    {
      "epoch": 0.3170731707317073,
      "grad_norm": 0.15682919323444366,
      "learning_rate": 4.863635845237859e-05,
      "loss": 0.0323,
      "step": 1300
    },
    {
      "epoch": 0.3173170731707317,
      "grad_norm": 0.14510409533977509,
      "learning_rate": 4.863427745439707e-05,
      "loss": 0.0274,
      "step": 1301
    },
    {
      "epoch": 0.3175609756097561,
      "grad_norm": 0.05933221057057381,
      "learning_rate": 4.863219491435129e-05,
      "loss": 0.0192,
      "step": 1302
    },
    {
      "epoch": 0.3178048780487805,
      "grad_norm": 0.10236558318138123,
      "learning_rate": 4.863011083237714e-05,
      "loss": 0.0403,
      "step": 1303
    },
    {
      "epoch": 0.3180487804878049,
      "grad_norm": 0.2569624185562134,
      "learning_rate": 4.862802520861059e-05,
      "loss": 0.0457,
      "step": 1304
    },
    {
      "epoch": 0.3182926829268293,
      "grad_norm": 0.06593765318393707,
      "learning_rate": 4.862593804318774e-05,
      "loss": 0.0252,
      "step": 1305
    },
    {
      "epoch": 0.31853658536585366,
      "grad_norm": 0.21631300449371338,
      "learning_rate": 4.862384933624475e-05,
      "loss": 0.0241,
      "step": 1306
    },
    {
      "epoch": 0.31878048780487805,
      "grad_norm": 0.07142005860805511,
      "learning_rate": 4.862175908791792e-05,
      "loss": 0.0217,
      "step": 1307
    },
    {
      "epoch": 0.31902439024390244,
      "grad_norm": 0.09176882356405258,
      "learning_rate": 4.861966729834362e-05,
      "loss": 0.0263,
      "step": 1308
    },
    {
      "epoch": 0.31926829268292684,
      "grad_norm": 0.13554032146930695,
      "learning_rate": 4.8617573967658326e-05,
      "loss": 0.0301,
      "step": 1309
    },
    {
      "epoch": 0.3195121951219512,
      "grad_norm": 0.1182204931974411,
      "learning_rate": 4.861547909599864e-05,
      "loss": 0.0258,
      "step": 1310
    },
    {
      "epoch": 0.3197560975609756,
      "grad_norm": 0.21849380433559418,
      "learning_rate": 4.861338268350123e-05,
      "loss": 0.0267,
      "step": 1311
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.10053891688585281,
      "learning_rate": 4.86112847303029e-05,
      "loss": 0.0379,
      "step": 1312
    },
    {
      "epoch": 0.3202439024390244,
      "grad_norm": 0.1166970431804657,
      "learning_rate": 4.860918523654051e-05,
      "loss": 0.0524,
      "step": 1313
    },
    {
      "epoch": 0.3204878048780488,
      "grad_norm": 0.13726645708084106,
      "learning_rate": 4.860708420235106e-05,
      "loss": 0.0247,
      "step": 1314
    },
    {
      "epoch": 0.3207317073170732,
      "grad_norm": 0.17796798050403595,
      "learning_rate": 4.860498162787164e-05,
      "loss": 0.0245,
      "step": 1315
    },
    {
      "epoch": 0.32097560975609757,
      "grad_norm": 0.10192126035690308,
      "learning_rate": 4.860287751323942e-05,
      "loss": 0.0438,
      "step": 1316
    },
    {
      "epoch": 0.32121951219512196,
      "grad_norm": 0.10176834464073181,
      "learning_rate": 4.8600771858591706e-05,
      "loss": 0.0186,
      "step": 1317
    },
    {
      "epoch": 0.32146341463414635,
      "grad_norm": 0.07945327460765839,
      "learning_rate": 4.8598664664065876e-05,
      "loss": 0.0281,
      "step": 1318
    },
    {
      "epoch": 0.32170731707317074,
      "grad_norm": 0.11878520995378494,
      "learning_rate": 4.8596555929799415e-05,
      "loss": 0.0167,
      "step": 1319
    },
    {
      "epoch": 0.32195121951219513,
      "grad_norm": 0.13341733813285828,
      "learning_rate": 4.859444565592992e-05,
      "loss": 0.0363,
      "step": 1320
    },
    {
      "epoch": 0.3221951219512195,
      "grad_norm": 0.5216742753982544,
      "learning_rate": 4.859233384259507e-05,
      "loss": 0.033,
      "step": 1321
    },
    {
      "epoch": 0.3224390243902439,
      "grad_norm": 0.18228115141391754,
      "learning_rate": 4.859022048993267e-05,
      "loss": 0.0581,
      "step": 1322
    },
    {
      "epoch": 0.3226829268292683,
      "grad_norm": 0.08518578112125397,
      "learning_rate": 4.858810559808059e-05,
      "loss": 0.0141,
      "step": 1323
    },
    {
      "epoch": 0.3229268292682927,
      "grad_norm": 0.065518319606781,
      "learning_rate": 4.8585989167176835e-05,
      "loss": 0.0179,
      "step": 1324
    },
    {
      "epoch": 0.3231707317073171,
      "grad_norm": 0.11183207482099533,
      "learning_rate": 4.8583871197359485e-05,
      "loss": 0.0284,
      "step": 1325
    },
    {
      "epoch": 0.3234146341463415,
      "grad_norm": 0.11760714650154114,
      "learning_rate": 4.858175168876674e-05,
      "loss": 0.0293,
      "step": 1326
    },
    {
      "epoch": 0.32365853658536586,
      "grad_norm": 0.13670769333839417,
      "learning_rate": 4.8579630641536886e-05,
      "loss": 0.0324,
      "step": 1327
    },
    {
      "epoch": 0.32390243902439025,
      "grad_norm": 0.10389803349971771,
      "learning_rate": 4.8577508055808327e-05,
      "loss": 0.0302,
      "step": 1328
    },
    {
      "epoch": 0.32414634146341464,
      "grad_norm": 0.119051992893219,
      "learning_rate": 4.8575383931719534e-05,
      "loss": 0.024,
      "step": 1329
    },
    {
      "epoch": 0.32439024390243903,
      "grad_norm": 0.09961675852537155,
      "learning_rate": 4.857325826940912e-05,
      "loss": 0.043,
      "step": 1330
    },
    {
      "epoch": 0.3246341463414634,
      "grad_norm": 0.08703102916479111,
      "learning_rate": 4.8571131069015755e-05,
      "loss": 0.031,
      "step": 1331
    },
    {
      "epoch": 0.3248780487804878,
      "grad_norm": 0.09230498224496841,
      "learning_rate": 4.856900233067826e-05,
      "loss": 0.0195,
      "step": 1332
    },
    {
      "epoch": 0.3251219512195122,
      "grad_norm": 0.1723952293395996,
      "learning_rate": 4.8566872054535506e-05,
      "loss": 0.0227,
      "step": 1333
    },
    {
      "epoch": 0.3253658536585366,
      "grad_norm": 0.22181425988674164,
      "learning_rate": 4.8564740240726495e-05,
      "loss": 0.0318,
      "step": 1334
    },
    {
      "epoch": 0.325609756097561,
      "grad_norm": 0.07410235702991486,
      "learning_rate": 4.8562606889390325e-05,
      "loss": 0.0285,
      "step": 1335
    },
    {
      "epoch": 0.3258536585365854,
      "grad_norm": 0.16949214041233063,
      "learning_rate": 4.856047200066619e-05,
      "loss": 0.0325,
      "step": 1336
    },
    {
      "epoch": 0.32609756097560977,
      "grad_norm": 0.13513366878032684,
      "learning_rate": 4.855833557469338e-05,
      "loss": 0.0207,
      "step": 1337
    },
    {
      "epoch": 0.32634146341463416,
      "grad_norm": 0.2131093442440033,
      "learning_rate": 4.8556197611611286e-05,
      "loss": 0.0316,
      "step": 1338
    },
    {
      "epoch": 0.32658536585365855,
      "grad_norm": 0.3280624449253082,
      "learning_rate": 4.855405811155942e-05,
      "loss": 0.0201,
      "step": 1339
    },
    {
      "epoch": 0.32682926829268294,
      "grad_norm": 0.17846836149692535,
      "learning_rate": 4.8551917074677364e-05,
      "loss": 0.0337,
      "step": 1340
    },
    {
      "epoch": 0.32707317073170733,
      "grad_norm": 0.10822341591119766,
      "learning_rate": 4.8549774501104816e-05,
      "loss": 0.0253,
      "step": 1341
    },
    {
      "epoch": 0.3273170731707317,
      "grad_norm": 0.12715689837932587,
      "learning_rate": 4.854763039098158e-05,
      "loss": 0.0342,
      "step": 1342
    },
    {
      "epoch": 0.3275609756097561,
      "grad_norm": 0.2586357593536377,
      "learning_rate": 4.854548474444755e-05,
      "loss": 0.031,
      "step": 1343
    },
    {
      "epoch": 0.3278048780487805,
      "grad_norm": 0.19222909212112427,
      "learning_rate": 4.854333756164271e-05,
      "loss": 0.0246,
      "step": 1344
    },
    {
      "epoch": 0.3280487804878049,
      "grad_norm": 0.124293752014637,
      "learning_rate": 4.8541188842707166e-05,
      "loss": 0.0287,
      "step": 1345
    },
    {
      "epoch": 0.3282926829268293,
      "grad_norm": 0.11485501378774643,
      "learning_rate": 4.853903858778113e-05,
      "loss": 0.027,
      "step": 1346
    },
    {
      "epoch": 0.3285365853658537,
      "grad_norm": 0.08409291505813599,
      "learning_rate": 4.853688679700488e-05,
      "loss": 0.0479,
      "step": 1347
    },
    {
      "epoch": 0.32878048780487806,
      "grad_norm": 0.06873028725385666,
      "learning_rate": 4.853473347051881e-05,
      "loss": 0.0335,
      "step": 1348
    },
    {
      "epoch": 0.32902439024390245,
      "grad_norm": 0.13420407474040985,
      "learning_rate": 4.8532578608463444e-05,
      "loss": 0.0212,
      "step": 1349
    },
    {
      "epoch": 0.32926829268292684,
      "grad_norm": 0.0813201442360878,
      "learning_rate": 4.853042221097935e-05,
      "loss": 0.0252,
      "step": 1350
    },
    {
      "epoch": 0.32951219512195123,
      "grad_norm": 0.053510040044784546,
      "learning_rate": 4.852826427820725e-05,
      "loss": 0.0207,
      "step": 1351
    },
    {
      "epoch": 0.3297560975609756,
      "grad_norm": 0.06056270748376846,
      "learning_rate": 4.8526104810287927e-05,
      "loss": 0.025,
      "step": 1352
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.06454046070575714,
      "learning_rate": 4.8523943807362284e-05,
      "loss": 0.0286,
      "step": 1353
    },
    {
      "epoch": 0.3302439024390244,
      "grad_norm": 0.08331809192895889,
      "learning_rate": 4.852178126957133e-05,
      "loss": 0.0355,
      "step": 1354
    },
    {
      "epoch": 0.3304878048780488,
      "grad_norm": 0.10998406261205673,
      "learning_rate": 4.8519617197056146e-05,
      "loss": 0.0237,
      "step": 1355
    },
    {
      "epoch": 0.3307317073170732,
      "grad_norm": 0.23392793536186218,
      "learning_rate": 4.851745158995794e-05,
      "loss": 0.0245,
      "step": 1356
    },
    {
      "epoch": 0.3309756097560976,
      "grad_norm": 0.07377934455871582,
      "learning_rate": 4.851528444841802e-05,
      "loss": 0.0246,
      "step": 1357
    },
    {
      "epoch": 0.33121951219512197,
      "grad_norm": 0.11096139252185822,
      "learning_rate": 4.851311577257778e-05,
      "loss": 0.039,
      "step": 1358
    },
    {
      "epoch": 0.33146341463414636,
      "grad_norm": 0.06497520208358765,
      "learning_rate": 4.8510945562578704e-05,
      "loss": 0.0301,
      "step": 1359
    },
    {
      "epoch": 0.33170731707317075,
      "grad_norm": 0.07564272731542587,
      "learning_rate": 4.8508773818562406e-05,
      "loss": 0.0308,
      "step": 1360
    },
    {
      "epoch": 0.33195121951219514,
      "grad_norm": 0.13838458061218262,
      "learning_rate": 4.850660054067059e-05,
      "loss": 0.0323,
      "step": 1361
    },
    {
      "epoch": 0.33219512195121953,
      "grad_norm": 0.05451663210988045,
      "learning_rate": 4.850442572904504e-05,
      "loss": 0.0251,
      "step": 1362
    },
    {
      "epoch": 0.3324390243902439,
      "grad_norm": 0.10707259178161621,
      "learning_rate": 4.850224938382767e-05,
      "loss": 0.0285,
      "step": 1363
    },
    {
      "epoch": 0.3326829268292683,
      "grad_norm": 0.23001304268836975,
      "learning_rate": 4.850007150516048e-05,
      "loss": 0.0191,
      "step": 1364
    },
    {
      "epoch": 0.3329268292682927,
      "grad_norm": 0.1810913234949112,
      "learning_rate": 4.849789209318556e-05,
      "loss": 0.046,
      "step": 1365
    },
    {
      "epoch": 0.3331707317073171,
      "grad_norm": 0.06791482865810394,
      "learning_rate": 4.849571114804512e-05,
      "loss": 0.0291,
      "step": 1366
    },
    {
      "epoch": 0.3334146341463415,
      "grad_norm": 0.0948842316865921,
      "learning_rate": 4.849352866988145e-05,
      "loss": 0.0244,
      "step": 1367
    },
    {
      "epoch": 0.3336585365853659,
      "grad_norm": 0.2111181765794754,
      "learning_rate": 4.849134465883696e-05,
      "loss": 0.0239,
      "step": 1368
    },
    {
      "epoch": 0.33390243902439026,
      "grad_norm": 0.06805156171321869,
      "learning_rate": 4.8489159115054136e-05,
      "loss": 0.024,
      "step": 1369
    },
    {
      "epoch": 0.33414634146341465,
      "grad_norm": 0.2064206749200821,
      "learning_rate": 4.8486972038675594e-05,
      "loss": 0.035,
      "step": 1370
    },
    {
      "epoch": 0.33439024390243904,
      "grad_norm": 0.23410992324352264,
      "learning_rate": 4.848478342984402e-05,
      "loss": 0.0333,
      "step": 1371
    },
    {
      "epoch": 0.33463414634146343,
      "grad_norm": 0.11273813247680664,
      "learning_rate": 4.8482593288702225e-05,
      "loss": 0.0399,
      "step": 1372
    },
    {
      "epoch": 0.3348780487804878,
      "grad_norm": 0.07493014633655548,
      "learning_rate": 4.8480401615393106e-05,
      "loss": 0.028,
      "step": 1373
    },
    {
      "epoch": 0.3351219512195122,
      "grad_norm": 0.14443473517894745,
      "learning_rate": 4.847820841005967e-05,
      "loss": 0.0256,
      "step": 1374
    },
    {
      "epoch": 0.3353658536585366,
      "grad_norm": 0.2007780820131302,
      "learning_rate": 4.8476013672844996e-05,
      "loss": 0.0509,
      "step": 1375
    },
    {
      "epoch": 0.335609756097561,
      "grad_norm": 0.3239659070968628,
      "learning_rate": 4.8473817403892296e-05,
      "loss": 0.0417,
      "step": 1376
    },
    {
      "epoch": 0.3358536585365854,
      "grad_norm": 0.08691176027059555,
      "learning_rate": 4.847161960334487e-05,
      "loss": 0.0317,
      "step": 1377
    },
    {
      "epoch": 0.3360975609756098,
      "grad_norm": 0.6652695536613464,
      "learning_rate": 4.846942027134613e-05,
      "loss": 0.0323,
      "step": 1378
    },
    {
      "epoch": 0.33634146341463417,
      "grad_norm": 0.30095189809799194,
      "learning_rate": 4.846721940803956e-05,
      "loss": 0.0277,
      "step": 1379
    },
    {
      "epoch": 0.33658536585365856,
      "grad_norm": 0.10208399593830109,
      "learning_rate": 4.846501701356877e-05,
      "loss": 0.0253,
      "step": 1380
    },
    {
      "epoch": 0.33682926829268295,
      "grad_norm": 0.06238224357366562,
      "learning_rate": 4.846281308807744e-05,
      "loss": 0.0235,
      "step": 1381
    },
    {
      "epoch": 0.33707317073170734,
      "grad_norm": 0.13175003230571747,
      "learning_rate": 4.846060763170939e-05,
      "loss": 0.0309,
      "step": 1382
    },
    {
      "epoch": 0.33731707317073173,
      "grad_norm": 0.06185385212302208,
      "learning_rate": 4.84584006446085e-05,
      "loss": 0.0203,
      "step": 1383
    },
    {
      "epoch": 0.3375609756097561,
      "grad_norm": 0.15542495250701904,
      "learning_rate": 4.84561921269188e-05,
      "loss": 0.0343,
      "step": 1384
    },
    {
      "epoch": 0.3378048780487805,
      "grad_norm": 0.11572644859552383,
      "learning_rate": 4.8453982078784355e-05,
      "loss": 0.0387,
      "step": 1385
    },
    {
      "epoch": 0.3380487804878049,
      "grad_norm": 0.23780322074890137,
      "learning_rate": 4.845177050034939e-05,
      "loss": 0.0365,
      "step": 1386
    },
    {
      "epoch": 0.3382926829268293,
      "grad_norm": 0.151912659406662,
      "learning_rate": 4.844955739175818e-05,
      "loss": 0.0305,
      "step": 1387
    },
    {
      "epoch": 0.3385365853658537,
      "grad_norm": 0.3176009953022003,
      "learning_rate": 4.844734275315514e-05,
      "loss": 0.0235,
      "step": 1388
    },
    {
      "epoch": 0.33878048780487807,
      "grad_norm": 0.15284590423107147,
      "learning_rate": 4.844512658468477e-05,
      "loss": 0.0264,
      "step": 1389
    },
    {
      "epoch": 0.33902439024390246,
      "grad_norm": 0.1531844586133957,
      "learning_rate": 4.8442908886491664e-05,
      "loss": 0.0711,
      "step": 1390
    },
    {
      "epoch": 0.33926829268292685,
      "grad_norm": 0.09075326472520828,
      "learning_rate": 4.8440689658720516e-05,
      "loss": 0.031,
      "step": 1391
    },
    {
      "epoch": 0.33951219512195124,
      "grad_norm": 0.08567249029874802,
      "learning_rate": 4.843846890151613e-05,
      "loss": 0.0246,
      "step": 1392
    },
    {
      "epoch": 0.33975609756097563,
      "grad_norm": 0.19732409715652466,
      "learning_rate": 4.8436246615023394e-05,
      "loss": 0.0361,
      "step": 1393
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.15116892755031586,
      "learning_rate": 4.8434022799387314e-05,
      "loss": 0.0187,
      "step": 1394
    },
    {
      "epoch": 0.3402439024390244,
      "grad_norm": 0.3697451949119568,
      "learning_rate": 4.8431797454752987e-05,
      "loss": 0.0366,
      "step": 1395
    },
    {
      "epoch": 0.3404878048780488,
      "grad_norm": 0.1685245931148529,
      "learning_rate": 4.8429570581265605e-05,
      "loss": 0.0277,
      "step": 1396
    },
    {
      "epoch": 0.3407317073170732,
      "grad_norm": 0.0761658251285553,
      "learning_rate": 4.842734217907047e-05,
      "loss": 0.0231,
      "step": 1397
    },
    {
      "epoch": 0.3409756097560976,
      "grad_norm": 0.3362036347389221,
      "learning_rate": 4.842511224831297e-05,
      "loss": 0.019,
      "step": 1398
    },
    {
      "epoch": 0.341219512195122,
      "grad_norm": 0.07443970441818237,
      "learning_rate": 4.842288078913862e-05,
      "loss": 0.0245,
      "step": 1399
    },
    {
      "epoch": 0.34146341463414637,
      "grad_norm": 0.05592573061585426,
      "learning_rate": 4.8420647801692985e-05,
      "loss": 0.0208,
      "step": 1400
    },
    {
      "epoch": 0.34170731707317076,
      "grad_norm": 0.09504919499158859,
      "learning_rate": 4.8418413286121786e-05,
      "loss": 0.0235,
      "step": 1401
    },
    {
      "epoch": 0.34195121951219515,
      "grad_norm": 0.20285367965698242,
      "learning_rate": 4.841617724257082e-05,
      "loss": 0.0298,
      "step": 1402
    },
    {
      "epoch": 0.34219512195121954,
      "grad_norm": 0.16780748963356018,
      "learning_rate": 4.8413939671185954e-05,
      "loss": 0.0368,
      "step": 1403
    },
    {
      "epoch": 0.34243902439024393,
      "grad_norm": 0.19016514718532562,
      "learning_rate": 4.8411700572113215e-05,
      "loss": 0.0213,
      "step": 1404
    },
    {
      "epoch": 0.3426829268292683,
      "grad_norm": 0.09481378644704819,
      "learning_rate": 4.840945994549868e-05,
      "loss": 0.0337,
      "step": 1405
    },
    {
      "epoch": 0.3429268292682927,
      "grad_norm": 0.07320298999547958,
      "learning_rate": 4.8407217791488544e-05,
      "loss": 0.0258,
      "step": 1406
    },
    {
      "epoch": 0.3431707317073171,
      "grad_norm": 0.3071841299533844,
      "learning_rate": 4.8404974110229104e-05,
      "loss": 0.0221,
      "step": 1407
    },
    {
      "epoch": 0.3434146341463415,
      "grad_norm": 0.1353205144405365,
      "learning_rate": 4.8402728901866757e-05,
      "loss": 0.0193,
      "step": 1408
    },
    {
      "epoch": 0.3436585365853659,
      "grad_norm": 0.11802594363689423,
      "learning_rate": 4.840048216654799e-05,
      "loss": 0.0196,
      "step": 1409
    },
    {
      "epoch": 0.3439024390243902,
      "grad_norm": 0.0899687185883522,
      "learning_rate": 4.83982339044194e-05,
      "loss": 0.0181,
      "step": 1410
    },
    {
      "epoch": 0.3441463414634146,
      "grad_norm": 0.07816177606582642,
      "learning_rate": 4.839598411562767e-05,
      "loss": 0.0204,
      "step": 1411
    },
    {
      "epoch": 0.344390243902439,
      "grad_norm": 0.14898686110973358,
      "learning_rate": 4.839373280031961e-05,
      "loss": 0.0224,
      "step": 1412
    },
    {
      "epoch": 0.3446341463414634,
      "grad_norm": 0.15012717247009277,
      "learning_rate": 4.839147995864209e-05,
      "loss": 0.0385,
      "step": 1413
    },
    {
      "epoch": 0.3448780487804878,
      "grad_norm": 0.06395524740219116,
      "learning_rate": 4.838922559074212e-05,
      "loss": 0.0172,
      "step": 1414
    },
    {
      "epoch": 0.34512195121951217,
      "grad_norm": 0.10339333862066269,
      "learning_rate": 4.838696969676677e-05,
      "loss": 0.03,
      "step": 1415
    },
    {
      "epoch": 0.34536585365853656,
      "grad_norm": 0.14564958214759827,
      "learning_rate": 4.838471227686324e-05,
      "loss": 0.0203,
      "step": 1416
    },
    {
      "epoch": 0.34560975609756095,
      "grad_norm": 0.15643811225891113,
      "learning_rate": 4.8382453331178835e-05,
      "loss": 0.0301,
      "step": 1417
    },
    {
      "epoch": 0.34585365853658534,
      "grad_norm": 0.20943105220794678,
      "learning_rate": 4.8380192859860915e-05,
      "loss": 0.0194,
      "step": 1418
    },
    {
      "epoch": 0.34609756097560973,
      "grad_norm": 0.18040622770786285,
      "learning_rate": 4.8377930863057e-05,
      "loss": 0.0333,
      "step": 1419
    },
    {
      "epoch": 0.3463414634146341,
      "grad_norm": 0.08758387714624405,
      "learning_rate": 4.8375667340914656e-05,
      "loss": 0.0242,
      "step": 1420
    },
    {
      "epoch": 0.3465853658536585,
      "grad_norm": 0.11596369743347168,
      "learning_rate": 4.837340229358158e-05,
      "loss": 0.0206,
      "step": 1421
    },
    {
      "epoch": 0.3468292682926829,
      "grad_norm": 0.129994735121727,
      "learning_rate": 4.837113572120555e-05,
      "loss": 0.016,
      "step": 1422
    },
    {
      "epoch": 0.3470731707317073,
      "grad_norm": 0.1723356693983078,
      "learning_rate": 4.8368867623934464e-05,
      "loss": 0.0209,
      "step": 1423
    },
    {
      "epoch": 0.3473170731707317,
      "grad_norm": 0.16783565282821655,
      "learning_rate": 4.836659800191631e-05,
      "loss": 0.0242,
      "step": 1424
    },
    {
      "epoch": 0.3475609756097561,
      "grad_norm": 0.13558533787727356,
      "learning_rate": 4.836432685529916e-05,
      "loss": 0.0254,
      "step": 1425
    },
    {
      "epoch": 0.34780487804878046,
      "grad_norm": 0.059992387890815735,
      "learning_rate": 4.8362054184231214e-05,
      "loss": 0.0209,
      "step": 1426
    },
    {
      "epoch": 0.34804878048780485,
      "grad_norm": 0.11379391700029373,
      "learning_rate": 4.835977998886075e-05,
      "loss": 0.0199,
      "step": 1427
    },
    {
      "epoch": 0.34829268292682924,
      "grad_norm": 0.08405002951622009,
      "learning_rate": 4.835750426933615e-05,
      "loss": 0.031,
      "step": 1428
    },
    {
      "epoch": 0.34853658536585364,
      "grad_norm": 0.24898913502693176,
      "learning_rate": 4.8355227025805904e-05,
      "loss": 0.0157,
      "step": 1429
    },
    {
      "epoch": 0.348780487804878,
      "grad_norm": 0.16291174292564392,
      "learning_rate": 4.8352948258418585e-05,
      "loss": 0.0278,
      "step": 1430
    },
    {
      "epoch": 0.3490243902439024,
      "grad_norm": 0.09795418381690979,
      "learning_rate": 4.8350667967322896e-05,
      "loss": 0.0399,
      "step": 1431
    },
    {
      "epoch": 0.3492682926829268,
      "grad_norm": 0.1434001475572586,
      "learning_rate": 4.834838615266759e-05,
      "loss": 0.0467,
      "step": 1432
    },
    {
      "epoch": 0.3495121951219512,
      "grad_norm": 0.1257786750793457,
      "learning_rate": 4.834610281460157e-05,
      "loss": 0.0359,
      "step": 1433
    },
    {
      "epoch": 0.3497560975609756,
      "grad_norm": 0.08273687213659286,
      "learning_rate": 4.834381795327381e-05,
      "loss": 0.0266,
      "step": 1434
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.10983386635780334,
      "learning_rate": 4.8341531568833395e-05,
      "loss": 0.0408,
      "step": 1435
    },
    {
      "epoch": 0.35024390243902437,
      "grad_norm": 0.05740403011441231,
      "learning_rate": 4.833924366142949e-05,
      "loss": 0.0178,
      "step": 1436
    },
    {
      "epoch": 0.35048780487804876,
      "grad_norm": 0.16349239647388458,
      "learning_rate": 4.8336954231211394e-05,
      "loss": 0.0238,
      "step": 1437
    },
    {
      "epoch": 0.35073170731707315,
      "grad_norm": 0.14118488132953644,
      "learning_rate": 4.8334663278328476e-05,
      "loss": 0.023,
      "step": 1438
    },
    {
      "epoch": 0.35097560975609754,
      "grad_norm": 0.1259012222290039,
      "learning_rate": 4.833237080293021e-05,
      "loss": 0.0283,
      "step": 1439
    },
    {
      "epoch": 0.35121951219512193,
      "grad_norm": 0.09978863596916199,
      "learning_rate": 4.833007680516618e-05,
      "loss": 0.0247,
      "step": 1440
    },
    {
      "epoch": 0.3514634146341463,
      "grad_norm": 0.2790239155292511,
      "learning_rate": 4.8327781285186056e-05,
      "loss": 0.0303,
      "step": 1441
    },
    {
      "epoch": 0.3517073170731707,
      "grad_norm": 0.43366947770118713,
      "learning_rate": 4.8325484243139614e-05,
      "loss": 0.0336,
      "step": 1442
    },
    {
      "epoch": 0.3519512195121951,
      "grad_norm": 0.3608267605304718,
      "learning_rate": 4.832318567917673e-05,
      "loss": 0.0247,
      "step": 1443
    },
    {
      "epoch": 0.3521951219512195,
      "grad_norm": 0.2192513793706894,
      "learning_rate": 4.832088559344738e-05,
      "loss": 0.021,
      "step": 1444
    },
    {
      "epoch": 0.3524390243902439,
      "grad_norm": 0.18421036005020142,
      "learning_rate": 4.8318583986101644e-05,
      "loss": 0.017,
      "step": 1445
    },
    {
      "epoch": 0.3526829268292683,
      "grad_norm": 0.06066730618476868,
      "learning_rate": 4.8316280857289685e-05,
      "loss": 0.0225,
      "step": 1446
    },
    {
      "epoch": 0.35292682926829266,
      "grad_norm": 0.07391420751810074,
      "learning_rate": 4.8313976207161775e-05,
      "loss": 0.0225,
      "step": 1447
    },
    {
      "epoch": 0.35317073170731705,
      "grad_norm": 0.06456930935382843,
      "learning_rate": 4.831167003586829e-05,
      "loss": 0.0188,
      "step": 1448
    },
    {
      "epoch": 0.35341463414634144,
      "grad_norm": 0.11431486904621124,
      "learning_rate": 4.8309362343559706e-05,
      "loss": 0.023,
      "step": 1449
    },
    {
      "epoch": 0.35365853658536583,
      "grad_norm": 0.07520320266485214,
      "learning_rate": 4.830705313038657e-05,
      "loss": 0.0227,
      "step": 1450
    },
    {
      "epoch": 0.3539024390243902,
      "grad_norm": 0.13443486392498016,
      "learning_rate": 4.830474239649958e-05,
      "loss": 0.0286,
      "step": 1451
    },
    {
      "epoch": 0.3541463414634146,
      "grad_norm": 0.07864189147949219,
      "learning_rate": 4.830243014204949e-05,
      "loss": 0.0234,
      "step": 1452
    },
    {
      "epoch": 0.354390243902439,
      "grad_norm": 0.09032206237316132,
      "learning_rate": 4.830011636718716e-05,
      "loss": 0.0276,
      "step": 1453
    },
    {
      "epoch": 0.3546341463414634,
      "grad_norm": 0.1876145303249359,
      "learning_rate": 4.829780107206358e-05,
      "loss": 0.0257,
      "step": 1454
    },
    {
      "epoch": 0.3548780487804878,
      "grad_norm": 0.05388433113694191,
      "learning_rate": 4.829548425682979e-05,
      "loss": 0.0208,
      "step": 1455
    },
    {
      "epoch": 0.3551219512195122,
      "grad_norm": 0.12083718180656433,
      "learning_rate": 4.829316592163697e-05,
      "loss": 0.0289,
      "step": 1456
    },
    {
      "epoch": 0.35536585365853657,
      "grad_norm": 0.14595568180084229,
      "learning_rate": 4.829084606663638e-05,
      "loss": 0.0398,
      "step": 1457
    },
    {
      "epoch": 0.35560975609756096,
      "grad_norm": 0.15086176991462708,
      "learning_rate": 4.828852469197939e-05,
      "loss": 0.015,
      "step": 1458
    },
    {
      "epoch": 0.35585365853658535,
      "grad_norm": 0.056523703038692474,
      "learning_rate": 4.828620179781744e-05,
      "loss": 0.0135,
      "step": 1459
    },
    {
      "epoch": 0.35609756097560974,
      "grad_norm": 0.1697097271680832,
      "learning_rate": 4.8283877384302125e-05,
      "loss": 0.0411,
      "step": 1460
    },
    {
      "epoch": 0.35634146341463413,
      "grad_norm": 0.08101744204759598,
      "learning_rate": 4.828155145158508e-05,
      "loss": 0.0208,
      "step": 1461
    },
    {
      "epoch": 0.3565853658536585,
      "grad_norm": 0.1012377217411995,
      "learning_rate": 4.827922399981809e-05,
      "loss": 0.0317,
      "step": 1462
    },
    {
      "epoch": 0.3568292682926829,
      "grad_norm": 0.054382264614105225,
      "learning_rate": 4.827689502915298e-05,
      "loss": 0.0175,
      "step": 1463
    },
    {
      "epoch": 0.3570731707317073,
      "grad_norm": 0.16935470700263977,
      "learning_rate": 4.827456453974174e-05,
      "loss": 0.0357,
      "step": 1464
    },
    {
      "epoch": 0.3573170731707317,
      "grad_norm": 0.07486085593700409,
      "learning_rate": 4.827223253173641e-05,
      "loss": 0.0321,
      "step": 1465
    },
    {
      "epoch": 0.3575609756097561,
      "grad_norm": 0.32469895482063293,
      "learning_rate": 4.8269899005289145e-05,
      "loss": 0.0213,
      "step": 1466
    },
    {
      "epoch": 0.3578048780487805,
      "grad_norm": 0.14407169818878174,
      "learning_rate": 4.826756396055221e-05,
      "loss": 0.0419,
      "step": 1467
    },
    {
      "epoch": 0.35804878048780486,
      "grad_norm": 0.09148398041725159,
      "learning_rate": 4.826522739767796e-05,
      "loss": 0.0222,
      "step": 1468
    },
    {
      "epoch": 0.35829268292682925,
      "grad_norm": 0.05154307186603546,
      "learning_rate": 4.8262889316818835e-05,
      "loss": 0.0151,
      "step": 1469
    },
    {
      "epoch": 0.35853658536585364,
      "grad_norm": 0.30612021684646606,
      "learning_rate": 4.82605497181274e-05,
      "loss": 0.0304,
      "step": 1470
    },
    {
      "epoch": 0.35878048780487803,
      "grad_norm": 0.0723474845290184,
      "learning_rate": 4.82582086017563e-05,
      "loss": 0.0347,
      "step": 1471
    },
    {
      "epoch": 0.3590243902439024,
      "grad_norm": 0.37797844409942627,
      "learning_rate": 4.82558659678583e-05,
      "loss": 0.0259,
      "step": 1472
    },
    {
      "epoch": 0.3592682926829268,
      "grad_norm": 0.2041770964860916,
      "learning_rate": 4.825352181658623e-05,
      "loss": 0.0254,
      "step": 1473
    },
    {
      "epoch": 0.3595121951219512,
      "grad_norm": 0.1531308889389038,
      "learning_rate": 4.825117614809305e-05,
      "loss": 0.0439,
      "step": 1474
    },
    {
      "epoch": 0.3597560975609756,
      "grad_norm": 0.08419086784124374,
      "learning_rate": 4.8248828962531806e-05,
      "loss": 0.0213,
      "step": 1475
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.15879406034946442,
      "learning_rate": 4.824648026005564e-05,
      "loss": 0.0264,
      "step": 1476
    },
    {
      "epoch": 0.3602439024390244,
      "grad_norm": 0.07354240864515305,
      "learning_rate": 4.82441300408178e-05,
      "loss": 0.0237,
      "step": 1477
    },
    {
      "epoch": 0.36048780487804877,
      "grad_norm": 0.0872136726975441,
      "learning_rate": 4.824177830497164e-05,
      "loss": 0.0269,
      "step": 1478
    },
    {
      "epoch": 0.36073170731707316,
      "grad_norm": 0.08392974734306335,
      "learning_rate": 4.823942505267059e-05,
      "loss": 0.0211,
      "step": 1479
    },
    {
      "epoch": 0.36097560975609755,
      "grad_norm": 0.11620943993330002,
      "learning_rate": 4.82370702840682e-05,
      "loss": 0.0279,
      "step": 1480
    },
    {
      "epoch": 0.36121951219512194,
      "grad_norm": 0.1816781461238861,
      "learning_rate": 4.8234713999318104e-05,
      "loss": 0.0217,
      "step": 1481
    },
    {
      "epoch": 0.36146341463414633,
      "grad_norm": 0.06304240971803665,
      "learning_rate": 4.8232356198574056e-05,
      "loss": 0.0179,
      "step": 1482
    },
    {
      "epoch": 0.3617073170731707,
      "grad_norm": 0.10086273401975632,
      "learning_rate": 4.822999688198988e-05,
      "loss": 0.038,
      "step": 1483
    },
    {
      "epoch": 0.3619512195121951,
      "grad_norm": 0.14063599705696106,
      "learning_rate": 4.8227636049719523e-05,
      "loss": 0.0341,
      "step": 1484
    },
    {
      "epoch": 0.3621951219512195,
      "grad_norm": 0.10137131810188293,
      "learning_rate": 4.8225273701917027e-05,
      "loss": 0.0183,
      "step": 1485
    },
    {
      "epoch": 0.3624390243902439,
      "grad_norm": 0.1621626913547516,
      "learning_rate": 4.822290983873651e-05,
      "loss": 0.0279,
      "step": 1486
    },
    {
      "epoch": 0.3626829268292683,
      "grad_norm": 0.24617180228233337,
      "learning_rate": 4.822054446033223e-05,
      "loss": 0.0378,
      "step": 1487
    },
    {
      "epoch": 0.36292682926829267,
      "grad_norm": 0.11720581352710724,
      "learning_rate": 4.82181775668585e-05,
      "loss": 0.0425,
      "step": 1488
    },
    {
      "epoch": 0.36317073170731706,
      "grad_norm": 0.6644333600997925,
      "learning_rate": 4.8215809158469766e-05,
      "loss": 0.0334,
      "step": 1489
    },
    {
      "epoch": 0.36341463414634145,
      "grad_norm": 0.26809796690940857,
      "learning_rate": 4.8213439235320554e-05,
      "loss": 0.0386,
      "step": 1490
    },
    {
      "epoch": 0.36365853658536584,
      "grad_norm": 0.11727805435657501,
      "learning_rate": 4.821106779756549e-05,
      "loss": 0.0326,
      "step": 1491
    },
    {
      "epoch": 0.36390243902439023,
      "grad_norm": 0.07517289370298386,
      "learning_rate": 4.8208694845359313e-05,
      "loss": 0.0182,
      "step": 1492
    },
    {
      "epoch": 0.3641463414634146,
      "grad_norm": 0.13299304246902466,
      "learning_rate": 4.820632037885685e-05,
      "loss": 0.0233,
      "step": 1493
    },
    {
      "epoch": 0.364390243902439,
      "grad_norm": 0.11073680967092514,
      "learning_rate": 4.820394439821302e-05,
      "loss": 0.0297,
      "step": 1494
    },
    {
      "epoch": 0.3646341463414634,
      "grad_norm": 0.1444990485906601,
      "learning_rate": 4.820156690358285e-05,
      "loss": 0.0264,
      "step": 1495
    },
    {
      "epoch": 0.3648780487804878,
      "grad_norm": 0.12325935810804367,
      "learning_rate": 4.819918789512147e-05,
      "loss": 0.0268,
      "step": 1496
    },
    {
      "epoch": 0.3651219512195122,
      "grad_norm": 0.16773664951324463,
      "learning_rate": 4.819680737298409e-05,
      "loss": 0.0239,
      "step": 1497
    },
    {
      "epoch": 0.3653658536585366,
      "grad_norm": 0.055609479546546936,
      "learning_rate": 4.8194425337326056e-05,
      "loss": 0.0182,
      "step": 1498
    },
    {
      "epoch": 0.36560975609756097,
      "grad_norm": 0.22601942718029022,
      "learning_rate": 4.819204178830277e-05,
      "loss": 0.0276,
      "step": 1499
    },
    {
      "epoch": 0.36585365853658536,
      "grad_norm": 0.1885606199502945,
      "learning_rate": 4.818965672606974e-05,
      "loss": 0.0233,
      "step": 1500
    },
    {
      "epoch": 0.36609756097560975,
      "grad_norm": 0.06096569448709488,
      "learning_rate": 4.8187270150782615e-05,
      "loss": 0.0212,
      "step": 1501
    },
    {
      "epoch": 0.36634146341463414,
      "grad_norm": 0.10461705178022385,
      "learning_rate": 4.8184882062597094e-05,
      "loss": 0.0248,
      "step": 1502
    },
    {
      "epoch": 0.36658536585365853,
      "grad_norm": 0.10370959341526031,
      "learning_rate": 4.8182492461668985e-05,
      "loss": 0.0203,
      "step": 1503
    },
    {
      "epoch": 0.3668292682926829,
      "grad_norm": 0.12038064748048782,
      "learning_rate": 4.818010134815422e-05,
      "loss": 0.017,
      "step": 1504
    },
    {
      "epoch": 0.3670731707317073,
      "grad_norm": 0.09397878497838974,
      "learning_rate": 4.81777087222088e-05,
      "loss": 0.0398,
      "step": 1505
    },
    {
      "epoch": 0.3673170731707317,
      "grad_norm": 0.06748203188180923,
      "learning_rate": 4.817531458398884e-05,
      "loss": 0.0194,
      "step": 1506
    },
    {
      "epoch": 0.3675609756097561,
      "grad_norm": 0.1258610337972641,
      "learning_rate": 4.817291893365055e-05,
      "loss": 0.0265,
      "step": 1507
    },
    {
      "epoch": 0.3678048780487805,
      "grad_norm": 0.10731464624404907,
      "learning_rate": 4.8170521771350235e-05,
      "loss": 0.0159,
      "step": 1508
    },
    {
      "epoch": 0.36804878048780487,
      "grad_norm": 0.1401723325252533,
      "learning_rate": 4.816812309724431e-05,
      "loss": 0.027,
      "step": 1509
    },
    {
      "epoch": 0.36829268292682926,
      "grad_norm": 0.08706218749284744,
      "learning_rate": 4.8165722911489273e-05,
      "loss": 0.0266,
      "step": 1510
    },
    {
      "epoch": 0.36853658536585365,
      "grad_norm": 0.16340439021587372,
      "learning_rate": 4.816332121424174e-05,
      "loss": 0.028,
      "step": 1511
    },
    {
      "epoch": 0.36878048780487804,
      "grad_norm": 0.07812804728746414,
      "learning_rate": 4.816091800565841e-05,
      "loss": 0.0179,
      "step": 1512
    },
    {
      "epoch": 0.36902439024390243,
      "grad_norm": 0.055257972329854965,
      "learning_rate": 4.815851328589607e-05,
      "loss": 0.0194,
      "step": 1513
    },
    {
      "epoch": 0.3692682926829268,
      "grad_norm": 0.06025717407464981,
      "learning_rate": 4.815610705511164e-05,
      "loss": 0.016,
      "step": 1514
    },
    {
      "epoch": 0.3695121951219512,
      "grad_norm": 0.17256687581539154,
      "learning_rate": 4.8153699313462116e-05,
      "loss": 0.0164,
      "step": 1515
    },
    {
      "epoch": 0.3697560975609756,
      "grad_norm": 0.14554521441459656,
      "learning_rate": 4.8151290061104584e-05,
      "loss": 0.0171,
      "step": 1516
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.17499925196170807,
      "learning_rate": 4.814887929819626e-05,
      "loss": 0.0236,
      "step": 1517
    },
    {
      "epoch": 0.3702439024390244,
      "grad_norm": 0.13850773870944977,
      "learning_rate": 4.8146467024894414e-05,
      "loss": 0.0265,
      "step": 1518
    },
    {
      "epoch": 0.3704878048780488,
      "grad_norm": 0.15038615465164185,
      "learning_rate": 4.814405324135646e-05,
      "loss": 0.0133,
      "step": 1519
    },
    {
      "epoch": 0.37073170731707317,
      "grad_norm": 0.12478325515985489,
      "learning_rate": 4.814163794773988e-05,
      "loss": 0.025,
      "step": 1520
    },
    {
      "epoch": 0.37097560975609756,
      "grad_norm": 0.08822638541460037,
      "learning_rate": 4.8139221144202276e-05,
      "loss": 0.0266,
      "step": 1521
    },
    {
      "epoch": 0.37121951219512195,
      "grad_norm": 0.27108922600746155,
      "learning_rate": 4.8136802830901315e-05,
      "loss": 0.0439,
      "step": 1522
    },
    {
      "epoch": 0.37146341463414634,
      "grad_norm": 0.17825815081596375,
      "learning_rate": 4.81343830079948e-05,
      "loss": 0.0427,
      "step": 1523
    },
    {
      "epoch": 0.37170731707317073,
      "grad_norm": 0.18467363715171814,
      "learning_rate": 4.813196167564063e-05,
      "loss": 0.0284,
      "step": 1524
    },
    {
      "epoch": 0.3719512195121951,
      "grad_norm": 0.16108335554599762,
      "learning_rate": 4.8129538833996756e-05,
      "loss": 0.0246,
      "step": 1525
    },
    {
      "epoch": 0.3721951219512195,
      "grad_norm": 0.07356856763362885,
      "learning_rate": 4.8127114483221285e-05,
      "loss": 0.0224,
      "step": 1526
    },
    {
      "epoch": 0.3724390243902439,
      "grad_norm": 0.0930153951048851,
      "learning_rate": 4.8124688623472395e-05,
      "loss": 0.0276,
      "step": 1527
    },
    {
      "epoch": 0.3726829268292683,
      "grad_norm": 0.11006055772304535,
      "learning_rate": 4.8122261254908364e-05,
      "loss": 0.0277,
      "step": 1528
    },
    {
      "epoch": 0.3729268292682927,
      "grad_norm": 0.10854705423116684,
      "learning_rate": 4.811983237768757e-05,
      "loss": 0.0217,
      "step": 1529
    },
    {
      "epoch": 0.37317073170731707,
      "grad_norm": 0.18424709141254425,
      "learning_rate": 4.8117401991968495e-05,
      "loss": 0.0299,
      "step": 1530
    },
    {
      "epoch": 0.37341463414634146,
      "grad_norm": 0.06160581111907959,
      "learning_rate": 4.81149700979097e-05,
      "loss": 0.0275,
      "step": 1531
    },
    {
      "epoch": 0.37365853658536585,
      "grad_norm": 0.08614891767501831,
      "learning_rate": 4.811253669566987e-05,
      "loss": 0.0219,
      "step": 1532
    },
    {
      "epoch": 0.37390243902439024,
      "grad_norm": 0.07344694435596466,
      "learning_rate": 4.811010178540778e-05,
      "loss": 0.0272,
      "step": 1533
    },
    {
      "epoch": 0.37414634146341463,
      "grad_norm": 0.05876484140753746,
      "learning_rate": 4.8107665367282286e-05,
      "loss": 0.0178,
      "step": 1534
    },
    {
      "epoch": 0.374390243902439,
      "grad_norm": 0.1719980388879776,
      "learning_rate": 4.810522744145237e-05,
      "loss": 0.0226,
      "step": 1535
    },
    {
      "epoch": 0.3746341463414634,
      "grad_norm": 0.1154366061091423,
      "learning_rate": 4.8102788008077106e-05,
      "loss": 0.028,
      "step": 1536
    },
    {
      "epoch": 0.3748780487804878,
      "grad_norm": 0.13048017024993896,
      "learning_rate": 4.810034706731563e-05,
      "loss": 0.0264,
      "step": 1537
    },
    {
      "epoch": 0.3751219512195122,
      "grad_norm": 0.10239435732364655,
      "learning_rate": 4.809790461932724e-05,
      "loss": 0.0296,
      "step": 1538
    },
    {
      "epoch": 0.3753658536585366,
      "grad_norm": 0.22385527193546295,
      "learning_rate": 4.8095460664271275e-05,
      "loss": 0.0241,
      "step": 1539
    },
    {
      "epoch": 0.375609756097561,
      "grad_norm": 0.07581501454114914,
      "learning_rate": 4.8093015202307215e-05,
      "loss": 0.033,
      "step": 1540
    },
    {
      "epoch": 0.37585365853658537,
      "grad_norm": 0.06650730967521667,
      "learning_rate": 4.809056823359459e-05,
      "loss": 0.0246,
      "step": 1541
    },
    {
      "epoch": 0.37609756097560976,
      "grad_norm": 0.14908112585544586,
      "learning_rate": 4.808811975829308e-05,
      "loss": 0.0298,
      "step": 1542
    },
    {
      "epoch": 0.37634146341463415,
      "grad_norm": 0.049446817487478256,
      "learning_rate": 4.808566977656245e-05,
      "loss": 0.0114,
      "step": 1543
    },
    {
      "epoch": 0.37658536585365854,
      "grad_norm": 0.1616130769252777,
      "learning_rate": 4.8083218288562526e-05,
      "loss": 0.0313,
      "step": 1544
    },
    {
      "epoch": 0.37682926829268293,
      "grad_norm": 0.10491165518760681,
      "learning_rate": 4.808076529445327e-05,
      "loss": 0.0175,
      "step": 1545
    },
    {
      "epoch": 0.3770731707317073,
      "grad_norm": 0.18671815097332,
      "learning_rate": 4.8078310794394746e-05,
      "loss": 0.0302,
      "step": 1546
    },
    {
      "epoch": 0.3773170731707317,
      "grad_norm": 0.12319904565811157,
      "learning_rate": 4.807585478854708e-05,
      "loss": 0.0203,
      "step": 1547
    },
    {
      "epoch": 0.3775609756097561,
      "grad_norm": 0.16616228222846985,
      "learning_rate": 4.807339727707054e-05,
      "loss": 0.0548,
      "step": 1548
    },
    {
      "epoch": 0.3778048780487805,
      "grad_norm": 0.12996311485767365,
      "learning_rate": 4.807093826012546e-05,
      "loss": 0.0331,
      "step": 1549
    },
    {
      "epoch": 0.3780487804878049,
      "grad_norm": 0.12385022640228271,
      "learning_rate": 4.806847773787229e-05,
      "loss": 0.0414,
      "step": 1550
    },
    {
      "epoch": 0.37829268292682927,
      "grad_norm": 0.1007767766714096,
      "learning_rate": 4.806601571047156e-05,
      "loss": 0.0173,
      "step": 1551
    },
    {
      "epoch": 0.37853658536585366,
      "grad_norm": 0.08182395249605179,
      "learning_rate": 4.806355217808392e-05,
      "loss": 0.0269,
      "step": 1552
    },
    {
      "epoch": 0.37878048780487805,
      "grad_norm": 0.08640345185995102,
      "learning_rate": 4.80610871408701e-05,
      "loss": 0.0258,
      "step": 1553
    },
    {
      "epoch": 0.37902439024390244,
      "grad_norm": 0.05232050642371178,
      "learning_rate": 4.805862059899095e-05,
      "loss": 0.0167,
      "step": 1554
    },
    {
      "epoch": 0.37926829268292683,
      "grad_norm": 0.18042631447315216,
      "learning_rate": 4.805615255260739e-05,
      "loss": 0.0446,
      "step": 1555
    },
    {
      "epoch": 0.3795121951219512,
      "grad_norm": 0.12613196671009064,
      "learning_rate": 4.805368300188046e-05,
      "loss": 0.0174,
      "step": 1556
    },
    {
      "epoch": 0.3797560975609756,
      "grad_norm": 0.1463857889175415,
      "learning_rate": 4.805121194697128e-05,
      "loss": 0.0406,
      "step": 1557
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.05287003889679909,
      "learning_rate": 4.80487393880411e-05,
      "loss": 0.0238,
      "step": 1558
    },
    {
      "epoch": 0.3802439024390244,
      "grad_norm": 0.0772218331694603,
      "learning_rate": 4.8046265325251225e-05,
      "loss": 0.0306,
      "step": 1559
    },
    {
      "epoch": 0.3804878048780488,
      "grad_norm": 0.07105099409818649,
      "learning_rate": 4.8043789758763094e-05,
      "loss": 0.0292,
      "step": 1560
    },
    {
      "epoch": 0.3807317073170732,
      "grad_norm": 0.18966889381408691,
      "learning_rate": 4.8041312688738214e-05,
      "loss": 0.04,
      "step": 1561
    },
    {
      "epoch": 0.38097560975609757,
      "grad_norm": 0.12876692414283752,
      "learning_rate": 4.8038834115338225e-05,
      "loss": 0.0291,
      "step": 1562
    },
    {
      "epoch": 0.38121951219512196,
      "grad_norm": 0.11617321521043777,
      "learning_rate": 4.803635403872484e-05,
      "loss": 0.0359,
      "step": 1563
    },
    {
      "epoch": 0.38146341463414635,
      "grad_norm": 0.4196639358997345,
      "learning_rate": 4.803387245905988e-05,
      "loss": 0.0337,
      "step": 1564
    },
    {
      "epoch": 0.38170731707317074,
      "grad_norm": 0.0604713037610054,
      "learning_rate": 4.803138937650524e-05,
      "loss": 0.0267,
      "step": 1565
    },
    {
      "epoch": 0.38195121951219513,
      "grad_norm": 0.1659936010837555,
      "learning_rate": 4.8028904791222964e-05,
      "loss": 0.0341,
      "step": 1566
    },
    {
      "epoch": 0.3821951219512195,
      "grad_norm": 0.1325719803571701,
      "learning_rate": 4.8026418703375144e-05,
      "loss": 0.0427,
      "step": 1567
    },
    {
      "epoch": 0.3824390243902439,
      "grad_norm": 0.06686173379421234,
      "learning_rate": 4.8023931113123985e-05,
      "loss": 0.0193,
      "step": 1568
    },
    {
      "epoch": 0.3826829268292683,
      "grad_norm": 0.2518824636936188,
      "learning_rate": 4.8021442020631814e-05,
      "loss": 0.0201,
      "step": 1569
    },
    {
      "epoch": 0.3829268292682927,
      "grad_norm": 0.06595061719417572,
      "learning_rate": 4.801895142606102e-05,
      "loss": 0.0219,
      "step": 1570
    },
    {
      "epoch": 0.3831707317073171,
      "grad_norm": 0.1863705962896347,
      "learning_rate": 4.801645932957413e-05,
      "loss": 0.0232,
      "step": 1571
    },
    {
      "epoch": 0.38341463414634147,
      "grad_norm": 0.15429089963436127,
      "learning_rate": 4.801396573133371e-05,
      "loss": 0.0273,
      "step": 1572
    },
    {
      "epoch": 0.38365853658536586,
      "grad_norm": 0.15854932367801666,
      "learning_rate": 4.8011470631502484e-05,
      "loss": 0.0355,
      "step": 1573
    },
    {
      "epoch": 0.38390243902439025,
      "grad_norm": 0.109199658036232,
      "learning_rate": 4.8008974030243246e-05,
      "loss": 0.0205,
      "step": 1574
    },
    {
      "epoch": 0.38414634146341464,
      "grad_norm": 0.10682208836078644,
      "learning_rate": 4.800647592771889e-05,
      "loss": 0.0361,
      "step": 1575
    },
    {
      "epoch": 0.38439024390243903,
      "grad_norm": 0.13389742374420166,
      "learning_rate": 4.800397632409241e-05,
      "loss": 0.0294,
      "step": 1576
    },
    {
      "epoch": 0.3846341463414634,
      "grad_norm": 0.07400460541248322,
      "learning_rate": 4.8001475219526894e-05,
      "loss": 0.0452,
      "step": 1577
    },
    {
      "epoch": 0.3848780487804878,
      "grad_norm": 0.12395978718996048,
      "learning_rate": 4.799897261418553e-05,
      "loss": 0.043,
      "step": 1578
    },
    {
      "epoch": 0.3851219512195122,
      "grad_norm": 0.040291860699653625,
      "learning_rate": 4.799646850823162e-05,
      "loss": 0.0128,
      "step": 1579
    },
    {
      "epoch": 0.3853658536585366,
      "grad_norm": 0.5326240658760071,
      "learning_rate": 4.799396290182853e-05,
      "loss": 0.0253,
      "step": 1580
    },
    {
      "epoch": 0.385609756097561,
      "grad_norm": 0.07237471640110016,
      "learning_rate": 4.799145579513976e-05,
      "loss": 0.0231,
      "step": 1581
    },
    {
      "epoch": 0.3858536585365854,
      "grad_norm": 0.05762994661927223,
      "learning_rate": 4.798894718832888e-05,
      "loss": 0.0175,
      "step": 1582
    },
    {
      "epoch": 0.38609756097560977,
      "grad_norm": 0.1106431633234024,
      "learning_rate": 4.798643708155956e-05,
      "loss": 0.0358,
      "step": 1583
    },
    {
      "epoch": 0.38634146341463416,
      "grad_norm": 0.24913683533668518,
      "learning_rate": 4.798392547499561e-05,
      "loss": 0.0204,
      "step": 1584
    },
    {
      "epoch": 0.38658536585365855,
      "grad_norm": 0.0468129925429821,
      "learning_rate": 4.798141236880087e-05,
      "loss": 0.0181,
      "step": 1585
    },
    {
      "epoch": 0.38682926829268294,
      "grad_norm": 0.05501923710107803,
      "learning_rate": 4.797889776313933e-05,
      "loss": 0.019,
      "step": 1586
    },
    {
      "epoch": 0.38707317073170733,
      "grad_norm": 0.058042012155056,
      "learning_rate": 4.797638165817505e-05,
      "loss": 0.0225,
      "step": 1587
    },
    {
      "epoch": 0.3873170731707317,
      "grad_norm": 0.09536876529455185,
      "learning_rate": 4.797386405407221e-05,
      "loss": 0.028,
      "step": 1588
    },
    {
      "epoch": 0.3875609756097561,
      "grad_norm": 0.1107344999909401,
      "learning_rate": 4.797134495099507e-05,
      "loss": 0.0219,
      "step": 1589
    },
    {
      "epoch": 0.3878048780487805,
      "grad_norm": 0.13530276715755463,
      "learning_rate": 4.796882434910799e-05,
      "loss": 0.0487,
      "step": 1590
    },
    {
      "epoch": 0.3880487804878049,
      "grad_norm": 0.061577215790748596,
      "learning_rate": 4.796630224857543e-05,
      "loss": 0.0174,
      "step": 1591
    },
    {
      "epoch": 0.3882926829268293,
      "grad_norm": 0.14218489825725555,
      "learning_rate": 4.796377864956198e-05,
      "loss": 0.035,
      "step": 1592
    },
    {
      "epoch": 0.38853658536585367,
      "grad_norm": 0.08374754339456558,
      "learning_rate": 4.7961253552232247e-05,
      "loss": 0.017,
      "step": 1593
    },
    {
      "epoch": 0.38878048780487806,
      "grad_norm": 0.06646028161048889,
      "learning_rate": 4.795872695675102e-05,
      "loss": 0.0239,
      "step": 1594
    },
    {
      "epoch": 0.38902439024390245,
      "grad_norm": 0.11044080555438995,
      "learning_rate": 4.795619886328314e-05,
      "loss": 0.025,
      "step": 1595
    },
    {
      "epoch": 0.38926829268292684,
      "grad_norm": 0.20146936178207397,
      "learning_rate": 4.7953669271993565e-05,
      "loss": 0.027,
      "step": 1596
    },
    {
      "epoch": 0.38951219512195123,
      "grad_norm": 0.0751294195652008,
      "learning_rate": 4.795113818304733e-05,
      "loss": 0.0163,
      "step": 1597
    },
    {
      "epoch": 0.3897560975609756,
      "grad_norm": 0.09326555579900742,
      "learning_rate": 4.79486055966096e-05,
      "loss": 0.022,
      "step": 1598
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.075792096555233,
      "learning_rate": 4.794607151284559e-05,
      "loss": 0.0155,
      "step": 1599
    },
    {
      "epoch": 0.3902439024390244,
      "grad_norm": 0.08860369771718979,
      "learning_rate": 4.7943535931920667e-05,
      "loss": 0.0184,
      "step": 1600
    },
    {
      "epoch": 0.3904878048780488,
      "grad_norm": 0.06576474010944366,
      "learning_rate": 4.794099885400027e-05,
      "loss": 0.0174,
      "step": 1601
    },
    {
      "epoch": 0.3907317073170732,
      "grad_norm": 0.11196883767843246,
      "learning_rate": 4.7938460279249916e-05,
      "loss": 0.0229,
      "step": 1602
    },
    {
      "epoch": 0.3909756097560976,
      "grad_norm": 0.06173502281308174,
      "learning_rate": 4.793592020783525e-05,
      "loss": 0.0167,
      "step": 1603
    },
    {
      "epoch": 0.39121951219512197,
      "grad_norm": 0.1403486728668213,
      "learning_rate": 4.793337863992201e-05,
      "loss": 0.0202,
      "step": 1604
    },
    {
      "epoch": 0.39146341463414636,
      "grad_norm": 0.08569186180830002,
      "learning_rate": 4.793083557567602e-05,
      "loss": 0.0459,
      "step": 1605
    },
    {
      "epoch": 0.39170731707317075,
      "grad_norm": 0.18055954575538635,
      "learning_rate": 4.7928291015263204e-05,
      "loss": 0.0373,
      "step": 1606
    },
    {
      "epoch": 0.39195121951219514,
      "grad_norm": 0.054044947028160095,
      "learning_rate": 4.792574495884959e-05,
      "loss": 0.0187,
      "step": 1607
    },
    {
      "epoch": 0.3921951219512195,
      "grad_norm": 0.14539003372192383,
      "learning_rate": 4.79231974066013e-05,
      "loss": 0.05,
      "step": 1608
    },
    {
      "epoch": 0.3924390243902439,
      "grad_norm": 0.09007604420185089,
      "learning_rate": 4.792064835868455e-05,
      "loss": 0.0232,
      "step": 1609
    },
    {
      "epoch": 0.3926829268292683,
      "grad_norm": 0.049980517476797104,
      "learning_rate": 4.791809781526567e-05,
      "loss": 0.0262,
      "step": 1610
    },
    {
      "epoch": 0.3929268292682927,
      "grad_norm": 0.07095050066709518,
      "learning_rate": 4.791554577651106e-05,
      "loss": 0.0108,
      "step": 1611
    },
    {
      "epoch": 0.3931707317073171,
      "grad_norm": 0.23661839962005615,
      "learning_rate": 4.791299224258724e-05,
      "loss": 0.0269,
      "step": 1612
    },
    {
      "epoch": 0.3934146341463415,
      "grad_norm": 0.13080869615077972,
      "learning_rate": 4.791043721366082e-05,
      "loss": 0.0287,
      "step": 1613
    },
    {
      "epoch": 0.39365853658536587,
      "grad_norm": 0.07897400856018066,
      "learning_rate": 4.790788068989851e-05,
      "loss": 0.0267,
      "step": 1614
    },
    {
      "epoch": 0.39390243902439026,
      "grad_norm": 0.127837672829628,
      "learning_rate": 4.790532267146711e-05,
      "loss": 0.0289,
      "step": 1615
    },
    {
      "epoch": 0.39414634146341465,
      "grad_norm": 0.12102463841438293,
      "learning_rate": 4.790276315853352e-05,
      "loss": 0.0301,
      "step": 1616
    },
    {
      "epoch": 0.39439024390243904,
      "grad_norm": 0.1215423047542572,
      "learning_rate": 4.790020215126476e-05,
      "loss": 0.0254,
      "step": 1617
    },
    {
      "epoch": 0.39463414634146343,
      "grad_norm": 0.12123601138591766,
      "learning_rate": 4.789763964982791e-05,
      "loss": 0.0211,
      "step": 1618
    },
    {
      "epoch": 0.3948780487804878,
      "grad_norm": 0.17209362983703613,
      "learning_rate": 4.789507565439016e-05,
      "loss": 0.0224,
      "step": 1619
    },
    {
      "epoch": 0.3951219512195122,
      "grad_norm": 0.06408003717660904,
      "learning_rate": 4.789251016511882e-05,
      "loss": 0.0247,
      "step": 1620
    },
    {
      "epoch": 0.3953658536585366,
      "grad_norm": 0.07598181068897247,
      "learning_rate": 4.788994318218127e-05,
      "loss": 0.0301,
      "step": 1621
    },
    {
      "epoch": 0.395609756097561,
      "grad_norm": 0.15005995333194733,
      "learning_rate": 4.7887374705745e-05,
      "loss": 0.0301,
      "step": 1622
    },
    {
      "epoch": 0.3958536585365854,
      "grad_norm": 0.12412454187870026,
      "learning_rate": 4.7884804735977594e-05,
      "loss": 0.0185,
      "step": 1623
    },
    {
      "epoch": 0.3960975609756098,
      "grad_norm": 0.18321824073791504,
      "learning_rate": 4.788223327304674e-05,
      "loss": 0.0317,
      "step": 1624
    },
    {
      "epoch": 0.39634146341463417,
      "grad_norm": 0.10570613294839859,
      "learning_rate": 4.78796603171202e-05,
      "loss": 0.0288,
      "step": 1625
    },
    {
      "epoch": 0.39658536585365856,
      "grad_norm": 0.1429528146982193,
      "learning_rate": 4.787708586836589e-05,
      "loss": 0.0285,
      "step": 1626
    },
    {
      "epoch": 0.39682926829268295,
      "grad_norm": 0.47287192940711975,
      "learning_rate": 4.787450992695174e-05,
      "loss": 0.0266,
      "step": 1627
    },
    {
      "epoch": 0.39707317073170734,
      "grad_norm": 0.08849785476922989,
      "learning_rate": 4.7871932493045857e-05,
      "loss": 0.0178,
      "step": 1628
    },
    {
      "epoch": 0.3973170731707317,
      "grad_norm": 0.07501569390296936,
      "learning_rate": 4.786935356681639e-05,
      "loss": 0.0313,
      "step": 1629
    },
    {
      "epoch": 0.3975609756097561,
      "grad_norm": 0.5281277298927307,
      "learning_rate": 4.786677314843161e-05,
      "loss": 0.0309,
      "step": 1630
    },
    {
      "epoch": 0.3978048780487805,
      "grad_norm": 0.06904592365026474,
      "learning_rate": 4.786419123805989e-05,
      "loss": 0.0271,
      "step": 1631
    },
    {
      "epoch": 0.3980487804878049,
      "grad_norm": 0.11145012080669403,
      "learning_rate": 4.7861607835869685e-05,
      "loss": 0.0246,
      "step": 1632
    },
    {
      "epoch": 0.3982926829268293,
      "grad_norm": 0.1408115178346634,
      "learning_rate": 4.7859022942029546e-05,
      "loss": 0.0178,
      "step": 1633
    },
    {
      "epoch": 0.3985365853658537,
      "grad_norm": 0.15964911878108978,
      "learning_rate": 4.7856436556708146e-05,
      "loss": 0.0206,
      "step": 1634
    },
    {
      "epoch": 0.39878048780487807,
      "grad_norm": 0.09613784402608871,
      "learning_rate": 4.785384868007423e-05,
      "loss": 0.0181,
      "step": 1635
    },
    {
      "epoch": 0.39902439024390246,
      "grad_norm": 0.13842034339904785,
      "learning_rate": 4.7851259312296645e-05,
      "loss": 0.0244,
      "step": 1636
    },
    {
      "epoch": 0.39926829268292685,
      "grad_norm": 0.09130162745714188,
      "learning_rate": 4.7848668453544355e-05,
      "loss": 0.0284,
      "step": 1637
    },
    {
      "epoch": 0.39951219512195124,
      "grad_norm": 0.08663074672222137,
      "learning_rate": 4.784607610398639e-05,
      "loss": 0.0395,
      "step": 1638
    },
    {
      "epoch": 0.39975609756097563,
      "grad_norm": 0.11608226597309113,
      "learning_rate": 4.784348226379189e-05,
      "loss": 0.0228,
      "step": 1639
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.10635068267583847,
      "learning_rate": 4.784088693313011e-05,
      "loss": 0.0277,
      "step": 1640
    },
    {
      "epoch": 0.4002439024390244,
      "grad_norm": 0.14121884107589722,
      "learning_rate": 4.783829011217038e-05,
      "loss": 0.0473,
      "step": 1641
    },
    {
      "epoch": 0.4004878048780488,
      "grad_norm": 0.07229798287153244,
      "learning_rate": 4.783569180108214e-05,
      "loss": 0.0189,
      "step": 1642
    },
    {
      "epoch": 0.4007317073170732,
      "grad_norm": 0.0955842062830925,
      "learning_rate": 4.7833092000034904e-05,
      "loss": 0.0306,
      "step": 1643
    },
    {
      "epoch": 0.4009756097560976,
      "grad_norm": 0.0749976709485054,
      "learning_rate": 4.783049070919832e-05,
      "loss": 0.0164,
      "step": 1644
    },
    {
      "epoch": 0.401219512195122,
      "grad_norm": 0.3214947581291199,
      "learning_rate": 4.782788792874211e-05,
      "loss": 0.0278,
      "step": 1645
    },
    {
      "epoch": 0.40146341463414636,
      "grad_norm": 0.08538640290498734,
      "learning_rate": 4.7825283658836095e-05,
      "loss": 0.0337,
      "step": 1646
    },
    {
      "epoch": 0.40170731707317076,
      "grad_norm": 0.11548733711242676,
      "learning_rate": 4.7822677899650196e-05,
      "loss": 0.0138,
      "step": 1647
    },
    {
      "epoch": 0.40195121951219515,
      "grad_norm": 0.13716928660869598,
      "learning_rate": 4.782007065135443e-05,
      "loss": 0.0342,
      "step": 1648
    },
    {
      "epoch": 0.40219512195121954,
      "grad_norm": 0.07311467826366425,
      "learning_rate": 4.781746191411891e-05,
      "loss": 0.0219,
      "step": 1649
    },
    {
      "epoch": 0.4024390243902439,
      "grad_norm": 0.048309553414583206,
      "learning_rate": 4.781485168811386e-05,
      "loss": 0.0189,
      "step": 1650
    },
    {
      "epoch": 0.4026829268292683,
      "grad_norm": 0.06941313296556473,
      "learning_rate": 4.781223997350958e-05,
      "loss": 0.0208,
      "step": 1651
    },
    {
      "epoch": 0.4029268292682927,
      "grad_norm": 0.09904994815587997,
      "learning_rate": 4.7809626770476464e-05,
      "loss": 0.021,
      "step": 1652
    },
    {
      "epoch": 0.4031707317073171,
      "grad_norm": 0.20937374234199524,
      "learning_rate": 4.780701207918504e-05,
      "loss": 0.0137,
      "step": 1653
    },
    {
      "epoch": 0.4034146341463415,
      "grad_norm": 0.0629688948392868,
      "learning_rate": 4.780439589980589e-05,
      "loss": 0.0218,
      "step": 1654
    },
    {
      "epoch": 0.4036585365853659,
      "grad_norm": 0.2661471664905548,
      "learning_rate": 4.7801778232509724e-05,
      "loss": 0.0351,
      "step": 1655
    },
    {
      "epoch": 0.40390243902439027,
      "grad_norm": 0.0789932906627655,
      "learning_rate": 4.7799159077467325e-05,
      "loss": 0.0176,
      "step": 1656
    },
    {
      "epoch": 0.40414634146341466,
      "grad_norm": 0.4744674265384674,
      "learning_rate": 4.77965384348496e-05,
      "loss": 0.0302,
      "step": 1657
    },
    {
      "epoch": 0.40439024390243905,
      "grad_norm": 0.07967628538608551,
      "learning_rate": 4.779391630482753e-05,
      "loss": 0.0173,
      "step": 1658
    },
    {
      "epoch": 0.40463414634146344,
      "grad_norm": 0.11402791738510132,
      "learning_rate": 4.779129268757219e-05,
      "loss": 0.0254,
      "step": 1659
    },
    {
      "epoch": 0.40487804878048783,
      "grad_norm": 0.6353245973587036,
      "learning_rate": 4.778866758325477e-05,
      "loss": 0.0231,
      "step": 1660
    },
    {
      "epoch": 0.4051219512195122,
      "grad_norm": 0.0974435955286026,
      "learning_rate": 4.778604099204656e-05,
      "loss": 0.0252,
      "step": 1661
    },
    {
      "epoch": 0.4053658536585366,
      "grad_norm": 0.06885118037462234,
      "learning_rate": 4.778341291411893e-05,
      "loss": 0.022,
      "step": 1662
    },
    {
      "epoch": 0.405609756097561,
      "grad_norm": 0.086799256503582,
      "learning_rate": 4.778078334964336e-05,
      "loss": 0.0163,
      "step": 1663
    },
    {
      "epoch": 0.4058536585365854,
      "grad_norm": 0.3838242292404175,
      "learning_rate": 4.77781522987914e-05,
      "loss": 0.0318,
      "step": 1664
    },
    {
      "epoch": 0.4060975609756098,
      "grad_norm": 0.14022591710090637,
      "learning_rate": 4.7775519761734746e-05,
      "loss": 0.0384,
      "step": 1665
    },
    {
      "epoch": 0.4063414634146341,
      "grad_norm": 0.1265956461429596,
      "learning_rate": 4.7772885738645145e-05,
      "loss": 0.0361,
      "step": 1666
    },
    {
      "epoch": 0.4065853658536585,
      "grad_norm": 0.30559927225112915,
      "learning_rate": 4.777025022969447e-05,
      "loss": 0.027,
      "step": 1667
    },
    {
      "epoch": 0.4068292682926829,
      "grad_norm": 0.09366206079721451,
      "learning_rate": 4.776761323505467e-05,
      "loss": 0.0222,
      "step": 1668
    },
    {
      "epoch": 0.4070731707317073,
      "grad_norm": 0.373849481344223,
      "learning_rate": 4.7764974754897795e-05,
      "loss": 0.0148,
      "step": 1669
    },
    {
      "epoch": 0.4073170731707317,
      "grad_norm": 0.04626798629760742,
      "learning_rate": 4.7762334789396016e-05,
      "loss": 0.0162,
      "step": 1670
    },
    {
      "epoch": 0.40756097560975607,
      "grad_norm": 0.06186157464981079,
      "learning_rate": 4.775969333872158e-05,
      "loss": 0.0212,
      "step": 1671
    },
    {
      "epoch": 0.40780487804878046,
      "grad_norm": 0.19089630246162415,
      "learning_rate": 4.775705040304682e-05,
      "loss": 0.0237,
      "step": 1672
    },
    {
      "epoch": 0.40804878048780485,
      "grad_norm": 0.12538793683052063,
      "learning_rate": 4.775440598254419e-05,
      "loss": 0.0248,
      "step": 1673
    },
    {
      "epoch": 0.40829268292682924,
      "grad_norm": 0.1700984388589859,
      "learning_rate": 4.775176007738623e-05,
      "loss": 0.0253,
      "step": 1674
    },
    {
      "epoch": 0.40853658536585363,
      "grad_norm": 0.11489147692918777,
      "learning_rate": 4.774911268774557e-05,
      "loss": 0.0329,
      "step": 1675
    },
    {
      "epoch": 0.408780487804878,
      "grad_norm": 0.06914446502923965,
      "learning_rate": 4.774646381379495e-05,
      "loss": 0.022,
      "step": 1676
    },
    {
      "epoch": 0.4090243902439024,
      "grad_norm": 0.24957503378391266,
      "learning_rate": 4.77438134557072e-05,
      "loss": 0.0296,
      "step": 1677
    },
    {
      "epoch": 0.4092682926829268,
      "grad_norm": 0.09179187566041946,
      "learning_rate": 4.774116161365525e-05,
      "loss": 0.0191,
      "step": 1678
    },
    {
      "epoch": 0.4095121951219512,
      "grad_norm": 0.04702409356832504,
      "learning_rate": 4.7738508287812125e-05,
      "loss": 0.0195,
      "step": 1679
    },
    {
      "epoch": 0.4097560975609756,
      "grad_norm": 0.2197006195783615,
      "learning_rate": 4.7735853478350937e-05,
      "loss": 0.02,
      "step": 1680
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.11327396333217621,
      "learning_rate": 4.7733197185444905e-05,
      "loss": 0.0253,
      "step": 1681
    },
    {
      "epoch": 0.41024390243902437,
      "grad_norm": 0.07501517981290817,
      "learning_rate": 4.773053940926736e-05,
      "loss": 0.017,
      "step": 1682
    },
    {
      "epoch": 0.41048780487804876,
      "grad_norm": 0.051157400012016296,
      "learning_rate": 4.77278801499917e-05,
      "loss": 0.0135,
      "step": 1683
    },
    {
      "epoch": 0.41073170731707315,
      "grad_norm": 0.0728573352098465,
      "learning_rate": 4.7725219407791436e-05,
      "loss": 0.0224,
      "step": 1684
    },
    {
      "epoch": 0.41097560975609754,
      "grad_norm": 0.10634351521730423,
      "learning_rate": 4.772255718284018e-05,
      "loss": 0.0158,
      "step": 1685
    },
    {
      "epoch": 0.41121951219512193,
      "grad_norm": 0.1548684686422348,
      "learning_rate": 4.7719893475311615e-05,
      "loss": 0.026,
      "step": 1686
    },
    {
      "epoch": 0.4114634146341463,
      "grad_norm": 0.14778125286102295,
      "learning_rate": 4.771722828537957e-05,
      "loss": 0.0274,
      "step": 1687
    },
    {
      "epoch": 0.4117073170731707,
      "grad_norm": 0.11838718503713608,
      "learning_rate": 4.771456161321791e-05,
      "loss": 0.0113,
      "step": 1688
    },
    {
      "epoch": 0.4119512195121951,
      "grad_norm": 0.06453876942396164,
      "learning_rate": 4.771189345900064e-05,
      "loss": 0.0205,
      "step": 1689
    },
    {
      "epoch": 0.4121951219512195,
      "grad_norm": 0.1770576536655426,
      "learning_rate": 4.770922382290186e-05,
      "loss": 0.0174,
      "step": 1690
    },
    {
      "epoch": 0.4124390243902439,
      "grad_norm": 0.06097151339054108,
      "learning_rate": 4.770655270509574e-05,
      "loss": 0.0175,
      "step": 1691
    },
    {
      "epoch": 0.41268292682926827,
      "grad_norm": 0.19706855714321136,
      "learning_rate": 4.770388010575657e-05,
      "loss": 0.0209,
      "step": 1692
    },
    {
      "epoch": 0.41292682926829266,
      "grad_norm": 0.09113837778568268,
      "learning_rate": 4.770120602505872e-05,
      "loss": 0.0228,
      "step": 1693
    },
    {
      "epoch": 0.41317073170731705,
      "grad_norm": 0.10321937501430511,
      "learning_rate": 4.7698530463176683e-05,
      "loss": 0.0194,
      "step": 1694
    },
    {
      "epoch": 0.41341463414634144,
      "grad_norm": 0.2839280366897583,
      "learning_rate": 4.7695853420285006e-05,
      "loss": 0.048,
      "step": 1695
    },
    {
      "epoch": 0.41365853658536583,
      "grad_norm": 0.16270650923252106,
      "learning_rate": 4.769317489655838e-05,
      "loss": 0.0454,
      "step": 1696
    },
    {
      "epoch": 0.4139024390243902,
      "grad_norm": 0.24652071297168732,
      "learning_rate": 4.769049489217157e-05,
      "loss": 0.0284,
      "step": 1697
    },
    {
      "epoch": 0.4141463414634146,
      "grad_norm": 0.0904785543680191,
      "learning_rate": 4.768781340729942e-05,
      "loss": 0.0203,
      "step": 1698
    },
    {
      "epoch": 0.414390243902439,
      "grad_norm": 0.355133056640625,
      "learning_rate": 4.76851304421169e-05,
      "loss": 0.0332,
      "step": 1699
    },
    {
      "epoch": 0.4146341463414634,
      "grad_norm": 0.12768325209617615,
      "learning_rate": 4.7682445996799064e-05,
      "loss": 0.0216,
      "step": 1700
    },
    {
      "epoch": 0.4148780487804878,
      "grad_norm": 0.5713050365447998,
      "learning_rate": 4.767976007152107e-05,
      "loss": 0.0322,
      "step": 1701
    },
    {
      "epoch": 0.4151219512195122,
      "grad_norm": 0.15598344802856445,
      "learning_rate": 4.7677072666458155e-05,
      "loss": 0.0335,
      "step": 1702
    },
    {
      "epoch": 0.41536585365853657,
      "grad_norm": 0.06595286726951599,
      "learning_rate": 4.767438378178567e-05,
      "loss": 0.0162,
      "step": 1703
    },
    {
      "epoch": 0.41560975609756096,
      "grad_norm": 0.17077140510082245,
      "learning_rate": 4.767169341767906e-05,
      "loss": 0.0389,
      "step": 1704
    },
    {
      "epoch": 0.41585365853658535,
      "grad_norm": 0.06879216432571411,
      "learning_rate": 4.7669001574313854e-05,
      "loss": 0.0278,
      "step": 1705
    },
    {
      "epoch": 0.41609756097560974,
      "grad_norm": 0.09727396816015244,
      "learning_rate": 4.76663082518657e-05,
      "loss": 0.0183,
      "step": 1706
    },
    {
      "epoch": 0.4163414634146341,
      "grad_norm": 0.05391637608408928,
      "learning_rate": 4.7663613450510314e-05,
      "loss": 0.0159,
      "step": 1707
    },
    {
      "epoch": 0.4165853658536585,
      "grad_norm": 0.04942589998245239,
      "learning_rate": 4.766091717042353e-05,
      "loss": 0.021,
      "step": 1708
    },
    {
      "epoch": 0.4168292682926829,
      "grad_norm": 0.05967221036553383,
      "learning_rate": 4.765821941178128e-05,
      "loss": 0.0204,
      "step": 1709
    },
    {
      "epoch": 0.4170731707317073,
      "grad_norm": 0.13763223588466644,
      "learning_rate": 4.7655520174759566e-05,
      "loss": 0.0161,
      "step": 1710
    },
    {
      "epoch": 0.4173170731707317,
      "grad_norm": 0.10237979888916016,
      "learning_rate": 4.765281945953453e-05,
      "loss": 0.0342,
      "step": 1711
    },
    {
      "epoch": 0.4175609756097561,
      "grad_norm": 0.06720155477523804,
      "learning_rate": 4.765011726628236e-05,
      "loss": 0.0241,
      "step": 1712
    },
    {
      "epoch": 0.41780487804878047,
      "grad_norm": 0.08361241966485977,
      "learning_rate": 4.764741359517937e-05,
      "loss": 0.0194,
      "step": 1713
    },
    {
      "epoch": 0.41804878048780486,
      "grad_norm": 0.1280464231967926,
      "learning_rate": 4.7644708446401986e-05,
      "loss": 0.023,
      "step": 1714
    },
    {
      "epoch": 0.41829268292682925,
      "grad_norm": 0.14306288957595825,
      "learning_rate": 4.764200182012669e-05,
      "loss": 0.0141,
      "step": 1715
    },
    {
      "epoch": 0.41853658536585364,
      "grad_norm": 0.08166682720184326,
      "learning_rate": 4.76392937165301e-05,
      "loss": 0.018,
      "step": 1716
    },
    {
      "epoch": 0.41878048780487803,
      "grad_norm": 0.15221400558948517,
      "learning_rate": 4.7636584135788895e-05,
      "loss": 0.043,
      "step": 1717
    },
    {
      "epoch": 0.4190243902439024,
      "grad_norm": 0.1168695017695427,
      "learning_rate": 4.7633873078079874e-05,
      "loss": 0.0268,
      "step": 1718
    },
    {
      "epoch": 0.4192682926829268,
      "grad_norm": 0.1065455824136734,
      "learning_rate": 4.7631160543579914e-05,
      "loss": 0.0169,
      "step": 1719
    },
    {
      "epoch": 0.4195121951219512,
      "grad_norm": 0.08807072788476944,
      "learning_rate": 4.762844653246602e-05,
      "loss": 0.0329,
      "step": 1720
    },
    {
      "epoch": 0.4197560975609756,
      "grad_norm": 0.13040505349636078,
      "learning_rate": 4.7625731044915255e-05,
      "loss": 0.0321,
      "step": 1721
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.14579075574874878,
      "learning_rate": 4.7623014081104814e-05,
      "loss": 0.0321,
      "step": 1722
    },
    {
      "epoch": 0.4202439024390244,
      "grad_norm": 0.0713152289390564,
      "learning_rate": 4.7620295641211946e-05,
      "loss": 0.0297,
      "step": 1723
    },
    {
      "epoch": 0.42048780487804877,
      "grad_norm": 0.08537931740283966,
      "learning_rate": 4.761757572541404e-05,
      "loss": 0.0087,
      "step": 1724
    },
    {
      "epoch": 0.42073170731707316,
      "grad_norm": 0.20714829862117767,
      "learning_rate": 4.761485433388856e-05,
      "loss": 0.0192,
      "step": 1725
    },
    {
      "epoch": 0.42097560975609755,
      "grad_norm": 0.11989296972751617,
      "learning_rate": 4.761213146681307e-05,
      "loss": 0.0331,
      "step": 1726
    },
    {
      "epoch": 0.42121951219512194,
      "grad_norm": 0.08917514234781265,
      "learning_rate": 4.760940712436521e-05,
      "loss": 0.0172,
      "step": 1727
    },
    {
      "epoch": 0.4214634146341463,
      "grad_norm": 0.11470288038253784,
      "learning_rate": 4.760668130672276e-05,
      "loss": 0.0214,
      "step": 1728
    },
    {
      "epoch": 0.4217073170731707,
      "grad_norm": 0.1444006860256195,
      "learning_rate": 4.7603954014063553e-05,
      "loss": 0.0254,
      "step": 1729
    },
    {
      "epoch": 0.4219512195121951,
      "grad_norm": 0.18377692997455597,
      "learning_rate": 4.760122524656555e-05,
      "loss": 0.0281,
      "step": 1730
    },
    {
      "epoch": 0.4221951219512195,
      "grad_norm": 0.12322971224784851,
      "learning_rate": 4.759849500440679e-05,
      "loss": 0.0253,
      "step": 1731
    },
    {
      "epoch": 0.4224390243902439,
      "grad_norm": 0.2693127989768982,
      "learning_rate": 4.759576328776541e-05,
      "loss": 0.031,
      "step": 1732
    },
    {
      "epoch": 0.4226829268292683,
      "grad_norm": 0.22763507068157196,
      "learning_rate": 4.7593030096819646e-05,
      "loss": 0.0172,
      "step": 1733
    },
    {
      "epoch": 0.42292682926829267,
      "grad_norm": 0.14267495274543762,
      "learning_rate": 4.7590295431747835e-05,
      "loss": 0.0302,
      "step": 1734
    },
    {
      "epoch": 0.42317073170731706,
      "grad_norm": 0.0874544307589531,
      "learning_rate": 4.75875592927284e-05,
      "loss": 0.019,
      "step": 1735
    },
    {
      "epoch": 0.42341463414634145,
      "grad_norm": 0.29323482513427734,
      "learning_rate": 4.7584821679939865e-05,
      "loss": 0.0251,
      "step": 1736
    },
    {
      "epoch": 0.42365853658536584,
      "grad_norm": 0.15849892795085907,
      "learning_rate": 4.758208259356086e-05,
      "loss": 0.0211,
      "step": 1737
    },
    {
      "epoch": 0.42390243902439023,
      "grad_norm": 0.15601268410682678,
      "learning_rate": 4.7579342033770094e-05,
      "loss": 0.0288,
      "step": 1738
    },
    {
      "epoch": 0.4241463414634146,
      "grad_norm": 0.1890571266412735,
      "learning_rate": 4.757660000074638e-05,
      "loss": 0.0282,
      "step": 1739
    },
    {
      "epoch": 0.424390243902439,
      "grad_norm": 0.15725399553775787,
      "learning_rate": 4.757385649466863e-05,
      "loss": 0.0235,
      "step": 1740
    },
    {
      "epoch": 0.4246341463414634,
      "grad_norm": 0.18621519207954407,
      "learning_rate": 4.757111151571585e-05,
      "loss": 0.0218,
      "step": 1741
    },
    {
      "epoch": 0.4248780487804878,
      "grad_norm": 0.10012644529342651,
      "learning_rate": 4.756836506406715e-05,
      "loss": 0.0215,
      "step": 1742
    },
    {
      "epoch": 0.4251219512195122,
      "grad_norm": 0.16044750809669495,
      "learning_rate": 4.7565617139901705e-05,
      "loss": 0.0426,
      "step": 1743
    },
    {
      "epoch": 0.4253658536585366,
      "grad_norm": 0.0737307220697403,
      "learning_rate": 4.756286774339882e-05,
      "loss": 0.0149,
      "step": 1744
    },
    {
      "epoch": 0.42560975609756097,
      "grad_norm": 0.08708487451076508,
      "learning_rate": 4.756011687473789e-05,
      "loss": 0.0193,
      "step": 1745
    },
    {
      "epoch": 0.42585365853658536,
      "grad_norm": 0.1937316507101059,
      "learning_rate": 4.75573645340984e-05,
      "loss": 0.0228,
      "step": 1746
    },
    {
      "epoch": 0.42609756097560975,
      "grad_norm": 0.2050163596868515,
      "learning_rate": 4.755461072165993e-05,
      "loss": 0.0171,
      "step": 1747
    },
    {
      "epoch": 0.42634146341463414,
      "grad_norm": 0.10111617296934128,
      "learning_rate": 4.7551855437602155e-05,
      "loss": 0.0321,
      "step": 1748
    },
    {
      "epoch": 0.4265853658536585,
      "grad_norm": 0.14204876124858856,
      "learning_rate": 4.7549098682104854e-05,
      "loss": 0.0354,
      "step": 1749
    },
    {
      "epoch": 0.4268292682926829,
      "grad_norm": 0.12072380632162094,
      "learning_rate": 4.754634045534789e-05,
      "loss": 0.0197,
      "step": 1750
    },
    {
      "epoch": 0.4270731707317073,
      "grad_norm": 0.14853675663471222,
      "learning_rate": 4.754358075751123e-05,
      "loss": 0.0215,
      "step": 1751
    },
    {
      "epoch": 0.4273170731707317,
      "grad_norm": 0.15348152816295624,
      "learning_rate": 4.7540819588774935e-05,
      "loss": 0.0217,
      "step": 1752
    },
    {
      "epoch": 0.4275609756097561,
      "grad_norm": 0.1712135225534439,
      "learning_rate": 4.753805694931917e-05,
      "loss": 0.027,
      "step": 1753
    },
    {
      "epoch": 0.4278048780487805,
      "grad_norm": 0.2457643449306488,
      "learning_rate": 4.7535292839324185e-05,
      "loss": 0.0184,
      "step": 1754
    },
    {
      "epoch": 0.42804878048780487,
      "grad_norm": 0.08105919510126114,
      "learning_rate": 4.7532527258970335e-05,
      "loss": 0.0367,
      "step": 1755
    },
    {
      "epoch": 0.42829268292682926,
      "grad_norm": 0.23265261948108673,
      "learning_rate": 4.7529760208438055e-05,
      "loss": 0.0341,
      "step": 1756
    },
    {
      "epoch": 0.42853658536585365,
      "grad_norm": 0.0943906158208847,
      "learning_rate": 4.752699168790789e-05,
      "loss": 0.0166,
      "step": 1757
    },
    {
      "epoch": 0.42878048780487804,
      "grad_norm": 0.15287967026233673,
      "learning_rate": 4.752422169756048e-05,
      "loss": 0.0427,
      "step": 1758
    },
    {
      "epoch": 0.42902439024390243,
      "grad_norm": 0.1438867449760437,
      "learning_rate": 4.752145023757656e-05,
      "loss": 0.0167,
      "step": 1759
    },
    {
      "epoch": 0.4292682926829268,
      "grad_norm": 0.112089604139328,
      "learning_rate": 4.751867730813695e-05,
      "loss": 0.0317,
      "step": 1760
    },
    {
      "epoch": 0.4295121951219512,
      "grad_norm": 0.10844040662050247,
      "learning_rate": 4.751590290942259e-05,
      "loss": 0.0284,
      "step": 1761
    },
    {
      "epoch": 0.4297560975609756,
      "grad_norm": 0.11151410639286041,
      "learning_rate": 4.751312704161449e-05,
      "loss": 0.0213,
      "step": 1762
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.18522243201732635,
      "learning_rate": 4.751034970489377e-05,
      "loss": 0.0241,
      "step": 1763
    },
    {
      "epoch": 0.4302439024390244,
      "grad_norm": 0.22866851091384888,
      "learning_rate": 4.7507570899441644e-05,
      "loss": 0.0132,
      "step": 1764
    },
    {
      "epoch": 0.4304878048780488,
      "grad_norm": 0.10500908643007278,
      "learning_rate": 4.750479062543941e-05,
      "loss": 0.0281,
      "step": 1765
    },
    {
      "epoch": 0.43073170731707316,
      "grad_norm": 0.07481814175844193,
      "learning_rate": 4.7502008883068496e-05,
      "loss": 0.0232,
      "step": 1766
    },
    {
      "epoch": 0.43097560975609756,
      "grad_norm": 0.08397755771875381,
      "learning_rate": 4.7499225672510376e-05,
      "loss": 0.0292,
      "step": 1767
    },
    {
      "epoch": 0.43121951219512195,
      "grad_norm": 0.05622956156730652,
      "learning_rate": 4.749644099394666e-05,
      "loss": 0.0254,
      "step": 1768
    },
    {
      "epoch": 0.43146341463414634,
      "grad_norm": 0.16748946905136108,
      "learning_rate": 4.7493654847559046e-05,
      "loss": 0.0272,
      "step": 1769
    },
    {
      "epoch": 0.4317073170731707,
      "grad_norm": 0.12662269175052643,
      "learning_rate": 4.74908672335293e-05,
      "loss": 0.0431,
      "step": 1770
    },
    {
      "epoch": 0.4319512195121951,
      "grad_norm": 0.24404646456241608,
      "learning_rate": 4.748807815203933e-05,
      "loss": 0.0138,
      "step": 1771
    },
    {
      "epoch": 0.4321951219512195,
      "grad_norm": 0.062362950295209885,
      "learning_rate": 4.74852876032711e-05,
      "loss": 0.0235,
      "step": 1772
    },
    {
      "epoch": 0.4324390243902439,
      "grad_norm": 0.08470822870731354,
      "learning_rate": 4.748249558740668e-05,
      "loss": 0.0284,
      "step": 1773
    },
    {
      "epoch": 0.4326829268292683,
      "grad_norm": 0.0960197001695633,
      "learning_rate": 4.747970210462825e-05,
      "loss": 0.0223,
      "step": 1774
    },
    {
      "epoch": 0.4329268292682927,
      "grad_norm": 0.05678973346948624,
      "learning_rate": 4.7476907155118084e-05,
      "loss": 0.0184,
      "step": 1775
    },
    {
      "epoch": 0.43317073170731707,
      "grad_norm": 0.1494075357913971,
      "learning_rate": 4.747411073905853e-05,
      "loss": 0.0317,
      "step": 1776
    },
    {
      "epoch": 0.43341463414634146,
      "grad_norm": 0.08840981870889664,
      "learning_rate": 4.7471312856632047e-05,
      "loss": 0.0216,
      "step": 1777
    },
    {
      "epoch": 0.43365853658536585,
      "grad_norm": 0.09779293090105057,
      "learning_rate": 4.7468513508021194e-05,
      "loss": 0.0214,
      "step": 1778
    },
    {
      "epoch": 0.43390243902439024,
      "grad_norm": 0.09080886095762253,
      "learning_rate": 4.7465712693408616e-05,
      "loss": 0.0191,
      "step": 1779
    },
    {
      "epoch": 0.43414634146341463,
      "grad_norm": 0.08553127199411392,
      "learning_rate": 4.7462910412977056e-05,
      "loss": 0.0216,
      "step": 1780
    },
    {
      "epoch": 0.434390243902439,
      "grad_norm": 0.14147056639194489,
      "learning_rate": 4.7460106666909364e-05,
      "loss": 0.027,
      "step": 1781
    },
    {
      "epoch": 0.4346341463414634,
      "grad_norm": 0.052641578018665314,
      "learning_rate": 4.7457301455388467e-05,
      "loss": 0.0237,
      "step": 1782
    },
    {
      "epoch": 0.4348780487804878,
      "grad_norm": 0.11560381203889847,
      "learning_rate": 4.7454494778597396e-05,
      "loss": 0.0281,
      "step": 1783
    },
    {
      "epoch": 0.4351219512195122,
      "grad_norm": 0.1603115350008011,
      "learning_rate": 4.745168663671928e-05,
      "loss": 0.0241,
      "step": 1784
    },
    {
      "epoch": 0.4353658536585366,
      "grad_norm": 0.08386199921369553,
      "learning_rate": 4.7448877029937346e-05,
      "loss": 0.0207,
      "step": 1785
    },
    {
      "epoch": 0.435609756097561,
      "grad_norm": 0.14304795861244202,
      "learning_rate": 4.744606595843491e-05,
      "loss": 0.0255,
      "step": 1786
    },
    {
      "epoch": 0.43585365853658536,
      "grad_norm": 0.13525477051734924,
      "learning_rate": 4.744325342239537e-05,
      "loss": 0.0314,
      "step": 1787
    },
    {
      "epoch": 0.43609756097560975,
      "grad_norm": 0.21715772151947021,
      "learning_rate": 4.744043942200227e-05,
      "loss": 0.0288,
      "step": 1788
    },
    {
      "epoch": 0.43634146341463415,
      "grad_norm": 0.08541940152645111,
      "learning_rate": 4.743762395743918e-05,
      "loss": 0.0199,
      "step": 1789
    },
    {
      "epoch": 0.43658536585365854,
      "grad_norm": 0.2397892326116562,
      "learning_rate": 4.743480702888982e-05,
      "loss": 0.0508,
      "step": 1790
    },
    {
      "epoch": 0.4368292682926829,
      "grad_norm": 0.0926101952791214,
      "learning_rate": 4.7431988636537985e-05,
      "loss": 0.0176,
      "step": 1791
    },
    {
      "epoch": 0.4370731707317073,
      "grad_norm": 0.0781627744436264,
      "learning_rate": 4.7429168780567555e-05,
      "loss": 0.0279,
      "step": 1792
    },
    {
      "epoch": 0.4373170731707317,
      "grad_norm": 0.25140759348869324,
      "learning_rate": 4.7426347461162527e-05,
      "loss": 0.0259,
      "step": 1793
    },
    {
      "epoch": 0.4375609756097561,
      "grad_norm": 0.22293531894683838,
      "learning_rate": 4.7423524678506984e-05,
      "loss": 0.0222,
      "step": 1794
    },
    {
      "epoch": 0.4378048780487805,
      "grad_norm": 0.08382739126682281,
      "learning_rate": 4.742070043278509e-05,
      "loss": 0.0123,
      "step": 1795
    },
    {
      "epoch": 0.4380487804878049,
      "grad_norm": 0.12510770559310913,
      "learning_rate": 4.7417874724181146e-05,
      "loss": 0.0095,
      "step": 1796
    },
    {
      "epoch": 0.43829268292682927,
      "grad_norm": 0.0785328820347786,
      "learning_rate": 4.741504755287949e-05,
      "loss": 0.0251,
      "step": 1797
    },
    {
      "epoch": 0.43853658536585366,
      "grad_norm": 0.18667250871658325,
      "learning_rate": 4.7412218919064604e-05,
      "loss": 0.0249,
      "step": 1798
    },
    {
      "epoch": 0.43878048780487805,
      "grad_norm": 0.05651319399476051,
      "learning_rate": 4.7409388822921044e-05,
      "loss": 0.0171,
      "step": 1799
    },
    {
      "epoch": 0.43902439024390244,
      "grad_norm": 0.07419181615114212,
      "learning_rate": 4.7406557264633466e-05,
      "loss": 0.033,
      "step": 1800
    },
    {
      "epoch": 0.43926829268292683,
      "grad_norm": 0.132607102394104,
      "learning_rate": 4.7403724244386624e-05,
      "loss": 0.0318,
      "step": 1801
    },
    {
      "epoch": 0.4395121951219512,
      "grad_norm": 0.18809647858142853,
      "learning_rate": 4.7400889762365355e-05,
      "loss": 0.0235,
      "step": 1802
    },
    {
      "epoch": 0.4397560975609756,
      "grad_norm": 0.09571754187345505,
      "learning_rate": 4.7398053818754604e-05,
      "loss": 0.0298,
      "step": 1803
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.14988617599010468,
      "learning_rate": 4.739521641373941e-05,
      "loss": 0.023,
      "step": 1804
    },
    {
      "epoch": 0.4402439024390244,
      "grad_norm": 0.04872482270002365,
      "learning_rate": 4.739237754750489e-05,
      "loss": 0.0134,
      "step": 1805
    },
    {
      "epoch": 0.4404878048780488,
      "grad_norm": 0.04965207725763321,
      "learning_rate": 4.7389537220236304e-05,
      "loss": 0.0151,
      "step": 1806
    },
    {
      "epoch": 0.4407317073170732,
      "grad_norm": 0.10979445278644562,
      "learning_rate": 4.738669543211895e-05,
      "loss": 0.0237,
      "step": 1807
    },
    {
      "epoch": 0.44097560975609756,
      "grad_norm": 0.11746162176132202,
      "learning_rate": 4.738385218333825e-05,
      "loss": 0.0248,
      "step": 1808
    },
    {
      "epoch": 0.44121951219512195,
      "grad_norm": 0.26254406571388245,
      "learning_rate": 4.738100747407972e-05,
      "loss": 0.054,
      "step": 1809
    },
    {
      "epoch": 0.44146341463414634,
      "grad_norm": 0.12347974628210068,
      "learning_rate": 4.7378161304528965e-05,
      "loss": 0.0331,
      "step": 1810
    },
    {
      "epoch": 0.44170731707317074,
      "grad_norm": 0.5386393070220947,
      "learning_rate": 4.7375313674871694e-05,
      "loss": 0.0351,
      "step": 1811
    },
    {
      "epoch": 0.4419512195121951,
      "grad_norm": 0.10899235308170319,
      "learning_rate": 4.73724645852937e-05,
      "loss": 0.0219,
      "step": 1812
    },
    {
      "epoch": 0.4421951219512195,
      "grad_norm": 0.18900229036808014,
      "learning_rate": 4.736961403598088e-05,
      "loss": 0.0243,
      "step": 1813
    },
    {
      "epoch": 0.4424390243902439,
      "grad_norm": 0.11084817349910736,
      "learning_rate": 4.736676202711922e-05,
      "loss": 0.0159,
      "step": 1814
    },
    {
      "epoch": 0.4426829268292683,
      "grad_norm": 0.16855750977993011,
      "learning_rate": 4.736390855889481e-05,
      "loss": 0.0187,
      "step": 1815
    },
    {
      "epoch": 0.4429268292682927,
      "grad_norm": 0.04320624843239784,
      "learning_rate": 4.7361053631493834e-05,
      "loss": 0.0182,
      "step": 1816
    },
    {
      "epoch": 0.4431707317073171,
      "grad_norm": 0.3993412256240845,
      "learning_rate": 4.7358197245102565e-05,
      "loss": 0.0244,
      "step": 1817
    },
    {
      "epoch": 0.44341463414634147,
      "grad_norm": 0.16397279500961304,
      "learning_rate": 4.7355339399907365e-05,
      "loss": 0.0248,
      "step": 1818
    },
    {
      "epoch": 0.44365853658536586,
      "grad_norm": 0.14534755051136017,
      "learning_rate": 4.735248009609471e-05,
      "loss": 0.0262,
      "step": 1819
    },
    {
      "epoch": 0.44390243902439025,
      "grad_norm": 0.15214598178863525,
      "learning_rate": 4.734961933385116e-05,
      "loss": 0.0235,
      "step": 1820
    },
    {
      "epoch": 0.44414634146341464,
      "grad_norm": 0.08154363930225372,
      "learning_rate": 4.7346757113363354e-05,
      "loss": 0.0252,
      "step": 1821
    },
    {
      "epoch": 0.44439024390243903,
      "grad_norm": 0.1755860149860382,
      "learning_rate": 4.734389343481807e-05,
      "loss": 0.0262,
      "step": 1822
    },
    {
      "epoch": 0.4446341463414634,
      "grad_norm": 0.26193270087242126,
      "learning_rate": 4.734102829840213e-05,
      "loss": 0.0228,
      "step": 1823
    },
    {
      "epoch": 0.4448780487804878,
      "grad_norm": 0.10034897178411484,
      "learning_rate": 4.733816170430249e-05,
      "loss": 0.0198,
      "step": 1824
    },
    {
      "epoch": 0.4451219512195122,
      "grad_norm": 0.13027727603912354,
      "learning_rate": 4.733529365270618e-05,
      "loss": 0.0249,
      "step": 1825
    },
    {
      "epoch": 0.4453658536585366,
      "grad_norm": 0.09716751426458359,
      "learning_rate": 4.7332424143800336e-05,
      "loss": 0.0216,
      "step": 1826
    },
    {
      "epoch": 0.445609756097561,
      "grad_norm": 0.09100314974784851,
      "learning_rate": 4.7329553177772185e-05,
      "loss": 0.0386,
      "step": 1827
    },
    {
      "epoch": 0.4458536585365854,
      "grad_norm": 0.07405496388673782,
      "learning_rate": 4.7326680754809044e-05,
      "loss": 0.0301,
      "step": 1828
    },
    {
      "epoch": 0.44609756097560976,
      "grad_norm": 0.09008386731147766,
      "learning_rate": 4.732380687509833e-05,
      "loss": 0.0208,
      "step": 1829
    },
    {
      "epoch": 0.44634146341463415,
      "grad_norm": 0.11795507371425629,
      "learning_rate": 4.732093153882756e-05,
      "loss": 0.0505,
      "step": 1830
    },
    {
      "epoch": 0.44658536585365854,
      "grad_norm": 0.05231195315718651,
      "learning_rate": 4.7318054746184335e-05,
      "loss": 0.0109,
      "step": 1831
    },
    {
      "epoch": 0.44682926829268294,
      "grad_norm": 0.14322669804096222,
      "learning_rate": 4.731517649735637e-05,
      "loss": 0.0189,
      "step": 1832
    },
    {
      "epoch": 0.4470731707317073,
      "grad_norm": 0.08903160691261292,
      "learning_rate": 4.731229679253144e-05,
      "loss": 0.0289,
      "step": 1833
    },
    {
      "epoch": 0.4473170731707317,
      "grad_norm": 0.0648583397269249,
      "learning_rate": 4.730941563189746e-05,
      "loss": 0.0214,
      "step": 1834
    },
    {
      "epoch": 0.4475609756097561,
      "grad_norm": 0.14522084593772888,
      "learning_rate": 4.73065330156424e-05,
      "loss": 0.0307,
      "step": 1835
    },
    {
      "epoch": 0.4478048780487805,
      "grad_norm": 0.11920298635959625,
      "learning_rate": 4.7303648943954343e-05,
      "loss": 0.0528,
      "step": 1836
    },
    {
      "epoch": 0.4480487804878049,
      "grad_norm": 0.0825643539428711,
      "learning_rate": 4.730076341702148e-05,
      "loss": 0.0304,
      "step": 1837
    },
    {
      "epoch": 0.4482926829268293,
      "grad_norm": 0.1025085598230362,
      "learning_rate": 4.7297876435032064e-05,
      "loss": 0.0293,
      "step": 1838
    },
    {
      "epoch": 0.44853658536585367,
      "grad_norm": 0.0936683863401413,
      "learning_rate": 4.7294987998174475e-05,
      "loss": 0.0161,
      "step": 1839
    },
    {
      "epoch": 0.44878048780487806,
      "grad_norm": 0.10747788101434708,
      "learning_rate": 4.729209810663717e-05,
      "loss": 0.0308,
      "step": 1840
    },
    {
      "epoch": 0.44902439024390245,
      "grad_norm": 0.08232904970645905,
      "learning_rate": 4.728920676060871e-05,
      "loss": 0.0376,
      "step": 1841
    },
    {
      "epoch": 0.44926829268292684,
      "grad_norm": 0.17727984488010406,
      "learning_rate": 4.728631396027773e-05,
      "loss": 0.0437,
      "step": 1842
    },
    {
      "epoch": 0.44951219512195123,
      "grad_norm": 0.19956204295158386,
      "learning_rate": 4.7283419705832996e-05,
      "loss": 0.026,
      "step": 1843
    },
    {
      "epoch": 0.4497560975609756,
      "grad_norm": 0.0774981901049614,
      "learning_rate": 4.728052399746335e-05,
      "loss": 0.0183,
      "step": 1844
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.10949227213859558,
      "learning_rate": 4.727762683535772e-05,
      "loss": 0.0216,
      "step": 1845
    },
    {
      "epoch": 0.4502439024390244,
      "grad_norm": 0.1164698675274849,
      "learning_rate": 4.727472821970513e-05,
      "loss": 0.0313,
      "step": 1846
    },
    {
      "epoch": 0.4504878048780488,
      "grad_norm": 0.10301966220140457,
      "learning_rate": 4.727182815069471e-05,
      "loss": 0.0338,
      "step": 1847
    },
    {
      "epoch": 0.4507317073170732,
      "grad_norm": 0.09467169642448425,
      "learning_rate": 4.7268926628515694e-05,
      "loss": 0.0177,
      "step": 1848
    },
    {
      "epoch": 0.4509756097560976,
      "grad_norm": 0.07365435361862183,
      "learning_rate": 4.726602365335738e-05,
      "loss": 0.0188,
      "step": 1849
    },
    {
      "epoch": 0.45121951219512196,
      "grad_norm": 0.09025079011917114,
      "learning_rate": 4.7263119225409186e-05,
      "loss": 0.0242,
      "step": 1850
    },
    {
      "epoch": 0.45146341463414635,
      "grad_norm": 0.15171876549720764,
      "learning_rate": 4.7260213344860616e-05,
      "loss": 0.0398,
      "step": 1851
    },
    {
      "epoch": 0.45170731707317074,
      "grad_norm": 0.14831511676311493,
      "learning_rate": 4.7257306011901276e-05,
      "loss": 0.0494,
      "step": 1852
    },
    {
      "epoch": 0.45195121951219513,
      "grad_norm": 0.11887188255786896,
      "learning_rate": 4.725439722672085e-05,
      "loss": 0.0353,
      "step": 1853
    },
    {
      "epoch": 0.4521951219512195,
      "grad_norm": 0.11952187865972519,
      "learning_rate": 4.725148698950913e-05,
      "loss": 0.023,
      "step": 1854
    },
    {
      "epoch": 0.4524390243902439,
      "grad_norm": 0.09339961409568787,
      "learning_rate": 4.724857530045601e-05,
      "loss": 0.0274,
      "step": 1855
    },
    {
      "epoch": 0.4526829268292683,
      "grad_norm": 0.07709366828203201,
      "learning_rate": 4.724566215975146e-05,
      "loss": 0.0245,
      "step": 1856
    },
    {
      "epoch": 0.4529268292682927,
      "grad_norm": 0.10515719652175903,
      "learning_rate": 4.724274756758555e-05,
      "loss": 0.0417,
      "step": 1857
    },
    {
      "epoch": 0.4531707317073171,
      "grad_norm": 0.13886748254299164,
      "learning_rate": 4.723983152414845e-05,
      "loss": 0.0093,
      "step": 1858
    },
    {
      "epoch": 0.4534146341463415,
      "grad_norm": 0.1336698979139328,
      "learning_rate": 4.723691402963043e-05,
      "loss": 0.0088,
      "step": 1859
    },
    {
      "epoch": 0.45365853658536587,
      "grad_norm": 0.06364718079566956,
      "learning_rate": 4.723399508422185e-05,
      "loss": 0.0218,
      "step": 1860
    },
    {
      "epoch": 0.45390243902439026,
      "grad_norm": 0.1027328222990036,
      "learning_rate": 4.723107468811314e-05,
      "loss": 0.0547,
      "step": 1861
    },
    {
      "epoch": 0.45414634146341465,
      "grad_norm": 0.19892343878746033,
      "learning_rate": 4.7228152841494875e-05,
      "loss": 0.0144,
      "step": 1862
    },
    {
      "epoch": 0.45439024390243904,
      "grad_norm": 0.11532475054264069,
      "learning_rate": 4.722522954455768e-05,
      "loss": 0.0101,
      "step": 1863
    },
    {
      "epoch": 0.45463414634146343,
      "grad_norm": 0.08941292762756348,
      "learning_rate": 4.7222304797492295e-05,
      "loss": 0.0262,
      "step": 1864
    },
    {
      "epoch": 0.4548780487804878,
      "grad_norm": 0.23568759858608246,
      "learning_rate": 4.721937860048955e-05,
      "loss": 0.027,
      "step": 1865
    },
    {
      "epoch": 0.4551219512195122,
      "grad_norm": 0.09260242432355881,
      "learning_rate": 4.721645095374038e-05,
      "loss": 0.0207,
      "step": 1866
    },
    {
      "epoch": 0.4553658536585366,
      "grad_norm": 0.14237792789936066,
      "learning_rate": 4.721352185743578e-05,
      "loss": 0.023,
      "step": 1867
    },
    {
      "epoch": 0.455609756097561,
      "grad_norm": 0.05953288450837135,
      "learning_rate": 4.7210591311766895e-05,
      "loss": 0.0226,
      "step": 1868
    },
    {
      "epoch": 0.4558536585365854,
      "grad_norm": 0.22108076512813568,
      "learning_rate": 4.720765931692491e-05,
      "loss": 0.0264,
      "step": 1869
    },
    {
      "epoch": 0.4560975609756098,
      "grad_norm": 0.029334373772144318,
      "learning_rate": 4.720472587310115e-05,
      "loss": 0.0077,
      "step": 1870
    },
    {
      "epoch": 0.45634146341463416,
      "grad_norm": 0.0987539291381836,
      "learning_rate": 4.720179098048699e-05,
      "loss": 0.0261,
      "step": 1871
    },
    {
      "epoch": 0.45658536585365855,
      "grad_norm": 0.09184344112873077,
      "learning_rate": 4.7198854639273947e-05,
      "loss": 0.0157,
      "step": 1872
    },
    {
      "epoch": 0.45682926829268294,
      "grad_norm": 0.08083284646272659,
      "learning_rate": 4.719591684965359e-05,
      "loss": 0.0309,
      "step": 1873
    },
    {
      "epoch": 0.45707317073170733,
      "grad_norm": 0.09728077054023743,
      "learning_rate": 4.71929776118176e-05,
      "loss": 0.0173,
      "step": 1874
    },
    {
      "epoch": 0.4573170731707317,
      "grad_norm": 0.1783587783575058,
      "learning_rate": 4.719003692595777e-05,
      "loss": 0.0163,
      "step": 1875
    },
    {
      "epoch": 0.4575609756097561,
      "grad_norm": 0.07658834755420685,
      "learning_rate": 4.7187094792265956e-05,
      "loss": 0.0227,
      "step": 1876
    },
    {
      "epoch": 0.4578048780487805,
      "grad_norm": 0.09128313511610031,
      "learning_rate": 4.718415121093413e-05,
      "loss": 0.0349,
      "step": 1877
    },
    {
      "epoch": 0.4580487804878049,
      "grad_norm": 0.13914945721626282,
      "learning_rate": 4.7181206182154345e-05,
      "loss": 0.0674,
      "step": 1878
    },
    {
      "epoch": 0.4582926829268293,
      "grad_norm": 0.16650702059268951,
      "learning_rate": 4.717825970611877e-05,
      "loss": 0.0397,
      "step": 1879
    },
    {
      "epoch": 0.4585365853658537,
      "grad_norm": 0.08238612860441208,
      "learning_rate": 4.717531178301964e-05,
      "loss": 0.028,
      "step": 1880
    },
    {
      "epoch": 0.45878048780487807,
      "grad_norm": 0.191552996635437,
      "learning_rate": 4.71723624130493e-05,
      "loss": 0.0143,
      "step": 1881
    },
    {
      "epoch": 0.45902439024390246,
      "grad_norm": 0.2784513831138611,
      "learning_rate": 4.716941159640018e-05,
      "loss": 0.0334,
      "step": 1882
    },
    {
      "epoch": 0.45926829268292685,
      "grad_norm": 0.09886037558317184,
      "learning_rate": 4.7166459333264826e-05,
      "loss": 0.0233,
      "step": 1883
    },
    {
      "epoch": 0.45951219512195124,
      "grad_norm": 0.06410695612430573,
      "learning_rate": 4.7163505623835866e-05,
      "loss": 0.0304,
      "step": 1884
    },
    {
      "epoch": 0.45975609756097563,
      "grad_norm": 0.09581122547388077,
      "learning_rate": 4.7160550468306004e-05,
      "loss": 0.0252,
      "step": 1885
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.1596062332391739,
      "learning_rate": 4.7157593866868065e-05,
      "loss": 0.0205,
      "step": 1886
    },
    {
      "epoch": 0.4602439024390244,
      "grad_norm": 0.15101809799671173,
      "learning_rate": 4.715463581971496e-05,
      "loss": 0.0275,
      "step": 1887
    },
    {
      "epoch": 0.4604878048780488,
      "grad_norm": 0.08530742675065994,
      "learning_rate": 4.7151676327039684e-05,
      "loss": 0.0205,
      "step": 1888
    },
    {
      "epoch": 0.4607317073170732,
      "grad_norm": 0.11088153719902039,
      "learning_rate": 4.714871538903535e-05,
      "loss": 0.0461,
      "step": 1889
    },
    {
      "epoch": 0.4609756097560976,
      "grad_norm": 0.07742783427238464,
      "learning_rate": 4.7145753005895136e-05,
      "loss": 0.0142,
      "step": 1890
    },
    {
      "epoch": 0.461219512195122,
      "grad_norm": 0.09354519098997116,
      "learning_rate": 4.714278917781233e-05,
      "loss": 0.0307,
      "step": 1891
    },
    {
      "epoch": 0.46146341463414636,
      "grad_norm": 0.0819544866681099,
      "learning_rate": 4.7139823904980316e-05,
      "loss": 0.0272,
      "step": 1892
    },
    {
      "epoch": 0.46170731707317075,
      "grad_norm": 0.2951980531215668,
      "learning_rate": 4.713685718759258e-05,
      "loss": 0.0204,
      "step": 1893
    },
    {
      "epoch": 0.46195121951219514,
      "grad_norm": 0.056172821670770645,
      "learning_rate": 4.713388902584267e-05,
      "loss": 0.0244,
      "step": 1894
    },
    {
      "epoch": 0.46219512195121953,
      "grad_norm": 0.04769296944141388,
      "learning_rate": 4.713091941992426e-05,
      "loss": 0.007,
      "step": 1895
    },
    {
      "epoch": 0.4624390243902439,
      "grad_norm": 0.3706819415092468,
      "learning_rate": 4.7127948370031105e-05,
      "loss": 0.0184,
      "step": 1896
    },
    {
      "epoch": 0.4626829268292683,
      "grad_norm": 0.13379761576652527,
      "learning_rate": 4.712497587635706e-05,
      "loss": 0.0153,
      "step": 1897
    },
    {
      "epoch": 0.4629268292682927,
      "grad_norm": 0.091789610683918,
      "learning_rate": 4.7122001939096074e-05,
      "loss": 0.0198,
      "step": 1898
    },
    {
      "epoch": 0.4631707317073171,
      "grad_norm": 0.10923204571008682,
      "learning_rate": 4.711902655844218e-05,
      "loss": 0.0167,
      "step": 1899
    },
    {
      "epoch": 0.4634146341463415,
      "grad_norm": 0.1224682554602623,
      "learning_rate": 4.7116049734589515e-05,
      "loss": 0.0243,
      "step": 1900
    },
    {
      "epoch": 0.4636585365853659,
      "grad_norm": 0.05523558706045151,
      "learning_rate": 4.7113071467732314e-05,
      "loss": 0.0172,
      "step": 1901
    },
    {
      "epoch": 0.46390243902439027,
      "grad_norm": 0.054138630628585815,
      "learning_rate": 4.711009175806489e-05,
      "loss": 0.0176,
      "step": 1902
    },
    {
      "epoch": 0.46414634146341466,
      "grad_norm": 0.18992957472801208,
      "learning_rate": 4.710711060578167e-05,
      "loss": 0.0168,
      "step": 1903
    },
    {
      "epoch": 0.46439024390243905,
      "grad_norm": 0.15511469542980194,
      "learning_rate": 4.710412801107715e-05,
      "loss": 0.0228,
      "step": 1904
    },
    {
      "epoch": 0.46463414634146344,
      "grad_norm": 0.11803114414215088,
      "learning_rate": 4.710114397414595e-05,
      "loss": 0.029,
      "step": 1905
    },
    {
      "epoch": 0.46487804878048783,
      "grad_norm": 0.17757633328437805,
      "learning_rate": 4.709815849518276e-05,
      "loss": 0.0372,
      "step": 1906
    },
    {
      "epoch": 0.4651219512195122,
      "grad_norm": 0.10632028430700302,
      "learning_rate": 4.709517157438238e-05,
      "loss": 0.035,
      "step": 1907
    },
    {
      "epoch": 0.4653658536585366,
      "grad_norm": 0.48032939434051514,
      "learning_rate": 4.7092183211939695e-05,
      "loss": 0.0363,
      "step": 1908
    },
    {
      "epoch": 0.465609756097561,
      "grad_norm": 0.11815349012613297,
      "learning_rate": 4.708919340804968e-05,
      "loss": 0.0331,
      "step": 1909
    },
    {
      "epoch": 0.4658536585365854,
      "grad_norm": 0.0853654146194458,
      "learning_rate": 4.708620216290743e-05,
      "loss": 0.0247,
      "step": 1910
    },
    {
      "epoch": 0.4660975609756098,
      "grad_norm": 0.10102696716785431,
      "learning_rate": 4.708320947670809e-05,
      "loss": 0.0196,
      "step": 1911
    },
    {
      "epoch": 0.46634146341463417,
      "grad_norm": 0.14785751700401306,
      "learning_rate": 4.708021534964694e-05,
      "loss": 0.0235,
      "step": 1912
    },
    {
      "epoch": 0.46658536585365856,
      "grad_norm": 0.10580465197563171,
      "learning_rate": 4.707721978191933e-05,
      "loss": 0.0251,
      "step": 1913
    },
    {
      "epoch": 0.46682926829268295,
      "grad_norm": 0.17431235313415527,
      "learning_rate": 4.707422277372071e-05,
      "loss": 0.0251,
      "step": 1914
    },
    {
      "epoch": 0.46707317073170734,
      "grad_norm": 0.08427580446004868,
      "learning_rate": 4.7071224325246635e-05,
      "loss": 0.0216,
      "step": 1915
    },
    {
      "epoch": 0.46731707317073173,
      "grad_norm": 0.0341794453561306,
      "learning_rate": 4.7068224436692745e-05,
      "loss": 0.01,
      "step": 1916
    },
    {
      "epoch": 0.4675609756097561,
      "grad_norm": 0.12972919642925262,
      "learning_rate": 4.7065223108254755e-05,
      "loss": 0.0176,
      "step": 1917
    },
    {
      "epoch": 0.4678048780487805,
      "grad_norm": 0.07915113866329193,
      "learning_rate": 4.706222034012852e-05,
      "loss": 0.0306,
      "step": 1918
    },
    {
      "epoch": 0.4680487804878049,
      "grad_norm": 0.2515382170677185,
      "learning_rate": 4.705921613250993e-05,
      "loss": 0.0251,
      "step": 1919
    },
    {
      "epoch": 0.4682926829268293,
      "grad_norm": 0.13433289527893066,
      "learning_rate": 4.705621048559503e-05,
      "loss": 0.0322,
      "step": 1920
    },
    {
      "epoch": 0.4685365853658537,
      "grad_norm": 0.05188175290822983,
      "learning_rate": 4.705320339957991e-05,
      "loss": 0.0157,
      "step": 1921
    },
    {
      "epoch": 0.468780487804878,
      "grad_norm": 0.3541993200778961,
      "learning_rate": 4.705019487466078e-05,
      "loss": 0.0336,
      "step": 1922
    },
    {
      "epoch": 0.4690243902439024,
      "grad_norm": 0.07041564583778381,
      "learning_rate": 4.7047184911033946e-05,
      "loss": 0.0193,
      "step": 1923
    },
    {
      "epoch": 0.4692682926829268,
      "grad_norm": 0.10780952125787735,
      "learning_rate": 4.704417350889578e-05,
      "loss": 0.0266,
      "step": 1924
    },
    {
      "epoch": 0.4695121951219512,
      "grad_norm": 0.04530118778347969,
      "learning_rate": 4.7041160668442776e-05,
      "loss": 0.0117,
      "step": 1925
    },
    {
      "epoch": 0.4697560975609756,
      "grad_norm": 0.22557170689105988,
      "learning_rate": 4.703814638987151e-05,
      "loss": 0.032,
      "step": 1926
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.09328322857618332,
      "learning_rate": 4.703513067337867e-05,
      "loss": 0.0268,
      "step": 1927
    },
    {
      "epoch": 0.47024390243902436,
      "grad_norm": 0.36104580760002136,
      "learning_rate": 4.7032113519161e-05,
      "loss": 0.024,
      "step": 1928
    },
    {
      "epoch": 0.47048780487804875,
      "grad_norm": 0.11444571614265442,
      "learning_rate": 4.702909492741537e-05,
      "loss": 0.0316,
      "step": 1929
    },
    {
      "epoch": 0.47073170731707314,
      "grad_norm": 0.32784610986709595,
      "learning_rate": 4.702607489833873e-05,
      "loss": 0.0282,
      "step": 1930
    },
    {
      "epoch": 0.47097560975609754,
      "grad_norm": 0.08867786824703217,
      "learning_rate": 4.702305343212814e-05,
      "loss": 0.0269,
      "step": 1931
    },
    {
      "epoch": 0.4712195121951219,
      "grad_norm": 0.18020205199718475,
      "learning_rate": 4.702003052898073e-05,
      "loss": 0.0211,
      "step": 1932
    },
    {
      "epoch": 0.4714634146341463,
      "grad_norm": 0.1801781952381134,
      "learning_rate": 4.701700618909373e-05,
      "loss": 0.0243,
      "step": 1933
    },
    {
      "epoch": 0.4717073170731707,
      "grad_norm": 0.056821059435606,
      "learning_rate": 4.7013980412664483e-05,
      "loss": 0.0167,
      "step": 1934
    },
    {
      "epoch": 0.4719512195121951,
      "grad_norm": 0.10780054330825806,
      "learning_rate": 4.70109531998904e-05,
      "loss": 0.0351,
      "step": 1935
    },
    {
      "epoch": 0.4721951219512195,
      "grad_norm": 0.04460321366786957,
      "learning_rate": 4.700792455096902e-05,
      "loss": 0.0119,
      "step": 1936
    },
    {
      "epoch": 0.4724390243902439,
      "grad_norm": 0.04872490093111992,
      "learning_rate": 4.700489446609792e-05,
      "loss": 0.017,
      "step": 1937
    },
    {
      "epoch": 0.47268292682926827,
      "grad_norm": 0.06182572618126869,
      "learning_rate": 4.700186294547482e-05,
      "loss": 0.0287,
      "step": 1938
    },
    {
      "epoch": 0.47292682926829266,
      "grad_norm": 0.06660064309835434,
      "learning_rate": 4.699882998929752e-05,
      "loss": 0.0221,
      "step": 1939
    },
    {
      "epoch": 0.47317073170731705,
      "grad_norm": 0.08342568576335907,
      "learning_rate": 4.699579559776391e-05,
      "loss": 0.0131,
      "step": 1940
    },
    {
      "epoch": 0.47341463414634144,
      "grad_norm": 0.24985027313232422,
      "learning_rate": 4.699275977107197e-05,
      "loss": 0.024,
      "step": 1941
    },
    {
      "epoch": 0.47365853658536583,
      "grad_norm": 0.31367287039756775,
      "learning_rate": 4.6989722509419786e-05,
      "loss": 0.0449,
      "step": 1942
    },
    {
      "epoch": 0.4739024390243902,
      "grad_norm": 0.18976332247257233,
      "learning_rate": 4.698668381300552e-05,
      "loss": 0.0177,
      "step": 1943
    },
    {
      "epoch": 0.4741463414634146,
      "grad_norm": 0.12516476213932037,
      "learning_rate": 4.6983643682027446e-05,
      "loss": 0.0373,
      "step": 1944
    },
    {
      "epoch": 0.474390243902439,
      "grad_norm": 0.07144607603549957,
      "learning_rate": 4.698060211668392e-05,
      "loss": 0.0305,
      "step": 1945
    },
    {
      "epoch": 0.4746341463414634,
      "grad_norm": 0.10892362892627716,
      "learning_rate": 4.697755911717339e-05,
      "loss": 0.0182,
      "step": 1946
    },
    {
      "epoch": 0.4748780487804878,
      "grad_norm": 0.04146796092391014,
      "learning_rate": 4.6974514683694414e-05,
      "loss": 0.0137,
      "step": 1947
    },
    {
      "epoch": 0.4751219512195122,
      "grad_norm": 0.15611425042152405,
      "learning_rate": 4.697146881644562e-05,
      "loss": 0.0439,
      "step": 1948
    },
    {
      "epoch": 0.47536585365853656,
      "grad_norm": 0.09069513529539108,
      "learning_rate": 4.696842151562575e-05,
      "loss": 0.0319,
      "step": 1949
    },
    {
      "epoch": 0.47560975609756095,
      "grad_norm": 0.08687165379524231,
      "learning_rate": 4.6965372781433626e-05,
      "loss": 0.0295,
      "step": 1950
    },
    {
      "epoch": 0.47585365853658534,
      "grad_norm": 0.08977989107370377,
      "learning_rate": 4.696232261406818e-05,
      "loss": 0.0234,
      "step": 1951
    },
    {
      "epoch": 0.47609756097560973,
      "grad_norm": 0.0852498933672905,
      "learning_rate": 4.69592710137284e-05,
      "loss": 0.0236,
      "step": 1952
    },
    {
      "epoch": 0.4763414634146341,
      "grad_norm": 0.2116905003786087,
      "learning_rate": 4.695621798061342e-05,
      "loss": 0.0214,
      "step": 1953
    },
    {
      "epoch": 0.4765853658536585,
      "grad_norm": 0.06552613526582718,
      "learning_rate": 4.695316351492243e-05,
      "loss": 0.0253,
      "step": 1954
    },
    {
      "epoch": 0.4768292682926829,
      "grad_norm": 0.08464900404214859,
      "learning_rate": 4.695010761685472e-05,
      "loss": 0.0175,
      "step": 1955
    },
    {
      "epoch": 0.4770731707317073,
      "grad_norm": 0.05880690738558769,
      "learning_rate": 4.69470502866097e-05,
      "loss": 0.0178,
      "step": 1956
    },
    {
      "epoch": 0.4773170731707317,
      "grad_norm": 0.19460271298885345,
      "learning_rate": 4.694399152438682e-05,
      "loss": 0.0388,
      "step": 1957
    },
    {
      "epoch": 0.4775609756097561,
      "grad_norm": 0.12606194615364075,
      "learning_rate": 4.694093133038568e-05,
      "loss": 0.018,
      "step": 1958
    },
    {
      "epoch": 0.47780487804878047,
      "grad_norm": 0.09325084090232849,
      "learning_rate": 4.693786970480592e-05,
      "loss": 0.0375,
      "step": 1959
    },
    {
      "epoch": 0.47804878048780486,
      "grad_norm": 0.06070927530527115,
      "learning_rate": 4.693480664784734e-05,
      "loss": 0.0223,
      "step": 1960
    },
    {
      "epoch": 0.47829268292682925,
      "grad_norm": 0.18101485073566437,
      "learning_rate": 4.693174215970976e-05,
      "loss": 0.0194,
      "step": 1961
    },
    {
      "epoch": 0.47853658536585364,
      "grad_norm": 0.20116713643074036,
      "learning_rate": 4.6928676240593154e-05,
      "loss": 0.0228,
      "step": 1962
    },
    {
      "epoch": 0.47878048780487803,
      "grad_norm": 0.11284894496202469,
      "learning_rate": 4.692560889069755e-05,
      "loss": 0.0161,
      "step": 1963
    },
    {
      "epoch": 0.4790243902439024,
      "grad_norm": 0.10375723242759705,
      "learning_rate": 4.692254011022309e-05,
      "loss": 0.0201,
      "step": 1964
    },
    {
      "epoch": 0.4792682926829268,
      "grad_norm": 0.1425531506538391,
      "learning_rate": 4.6919469899369994e-05,
      "loss": 0.0314,
      "step": 1965
    },
    {
      "epoch": 0.4795121951219512,
      "grad_norm": 0.17649054527282715,
      "learning_rate": 4.691639825833859e-05,
      "loss": 0.0292,
      "step": 1966
    },
    {
      "epoch": 0.4797560975609756,
      "grad_norm": 0.08071261644363403,
      "learning_rate": 4.691332518732929e-05,
      "loss": 0.0261,
      "step": 1967
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.11581575870513916,
      "learning_rate": 4.691025068654261e-05,
      "loss": 0.0309,
      "step": 1968
    },
    {
      "epoch": 0.4802439024390244,
      "grad_norm": 0.1390351802110672,
      "learning_rate": 4.690717475617915e-05,
      "loss": 0.0165,
      "step": 1969
    },
    {
      "epoch": 0.48048780487804876,
      "grad_norm": 0.09530986845493317,
      "learning_rate": 4.69040973964396e-05,
      "loss": 0.0356,
      "step": 1970
    },
    {
      "epoch": 0.48073170731707315,
      "grad_norm": 0.10079192370176315,
      "learning_rate": 4.690101860752474e-05,
      "loss": 0.0295,
      "step": 1971
    },
    {
      "epoch": 0.48097560975609754,
      "grad_norm": 0.09801854193210602,
      "learning_rate": 4.689793838963547e-05,
      "loss": 0.0199,
      "step": 1972
    },
    {
      "epoch": 0.48121951219512193,
      "grad_norm": 0.11758079379796982,
      "learning_rate": 4.689485674297275e-05,
      "loss": 0.0121,
      "step": 1973
    },
    {
      "epoch": 0.4814634146341463,
      "grad_norm": 0.7169950604438782,
      "learning_rate": 4.689177366773766e-05,
      "loss": 0.0141,
      "step": 1974
    },
    {
      "epoch": 0.4817073170731707,
      "grad_norm": 0.06727031618356705,
      "learning_rate": 4.688868916413135e-05,
      "loss": 0.0288,
      "step": 1975
    },
    {
      "epoch": 0.4819512195121951,
      "grad_norm": 0.21607215702533722,
      "learning_rate": 4.688560323235508e-05,
      "loss": 0.0364,
      "step": 1976
    },
    {
      "epoch": 0.4821951219512195,
      "grad_norm": 0.144358828663826,
      "learning_rate": 4.68825158726102e-05,
      "loss": 0.0219,
      "step": 1977
    },
    {
      "epoch": 0.4824390243902439,
      "grad_norm": 0.12609051167964935,
      "learning_rate": 4.687942708509815e-05,
      "loss": 0.0274,
      "step": 1978
    },
    {
      "epoch": 0.4826829268292683,
      "grad_norm": 0.2194156050682068,
      "learning_rate": 4.687633687002046e-05,
      "loss": 0.0162,
      "step": 1979
    },
    {
      "epoch": 0.48292682926829267,
      "grad_norm": 0.09009821712970734,
      "learning_rate": 4.687324522757876e-05,
      "loss": 0.0177,
      "step": 1980
    },
    {
      "epoch": 0.48317073170731706,
      "grad_norm": 0.09389238804578781,
      "learning_rate": 4.687015215797477e-05,
      "loss": 0.0187,
      "step": 1981
    },
    {
      "epoch": 0.48341463414634145,
      "grad_norm": 0.46271830797195435,
      "learning_rate": 4.68670576614103e-05,
      "loss": 0.0296,
      "step": 1982
    },
    {
      "epoch": 0.48365853658536584,
      "grad_norm": 0.10099533945322037,
      "learning_rate": 4.686396173808726e-05,
      "loss": 0.0207,
      "step": 1983
    },
    {
      "epoch": 0.48390243902439023,
      "grad_norm": 0.10047435760498047,
      "learning_rate": 4.6860864388207646e-05,
      "loss": 0.0256,
      "step": 1984
    },
    {
      "epoch": 0.4841463414634146,
      "grad_norm": 0.18310554325580597,
      "learning_rate": 4.685776561197356e-05,
      "loss": 0.0235,
      "step": 1985
    },
    {
      "epoch": 0.484390243902439,
      "grad_norm": 0.26506972312927246,
      "learning_rate": 4.6854665409587174e-05,
      "loss": 0.0355,
      "step": 1986
    },
    {
      "epoch": 0.4846341463414634,
      "grad_norm": 0.09233416616916656,
      "learning_rate": 4.685156378125077e-05,
      "loss": 0.0357,
      "step": 1987
    },
    {
      "epoch": 0.4848780487804878,
      "grad_norm": 0.10645739734172821,
      "learning_rate": 4.6848460727166734e-05,
      "loss": 0.0289,
      "step": 1988
    },
    {
      "epoch": 0.4851219512195122,
      "grad_norm": 0.14115668833255768,
      "learning_rate": 4.684535624753752e-05,
      "loss": 0.013,
      "step": 1989
    },
    {
      "epoch": 0.4853658536585366,
      "grad_norm": 0.2431759387254715,
      "learning_rate": 4.684225034256568e-05,
      "loss": 0.0248,
      "step": 1990
    },
    {
      "epoch": 0.48560975609756096,
      "grad_norm": 0.45199060440063477,
      "learning_rate": 4.683914301245387e-05,
      "loss": 0.0192,
      "step": 1991
    },
    {
      "epoch": 0.48585365853658535,
      "grad_norm": 0.28502383828163147,
      "learning_rate": 4.683603425740484e-05,
      "loss": 0.0257,
      "step": 1992
    },
    {
      "epoch": 0.48609756097560974,
      "grad_norm": 0.14638862013816833,
      "learning_rate": 4.683292407762142e-05,
      "loss": 0.0354,
      "step": 1993
    },
    {
      "epoch": 0.48634146341463413,
      "grad_norm": 0.0673191025853157,
      "learning_rate": 4.682981247330653e-05,
      "loss": 0.0279,
      "step": 1994
    },
    {
      "epoch": 0.4865853658536585,
      "grad_norm": 0.09012953191995621,
      "learning_rate": 4.682669944466322e-05,
      "loss": 0.0323,
      "step": 1995
    },
    {
      "epoch": 0.4868292682926829,
      "grad_norm": 0.11670802533626556,
      "learning_rate": 4.6823584991894575e-05,
      "loss": 0.035,
      "step": 1996
    },
    {
      "epoch": 0.4870731707317073,
      "grad_norm": 0.14236858487129211,
      "learning_rate": 4.682046911520383e-05,
      "loss": 0.0318,
      "step": 1997
    },
    {
      "epoch": 0.4873170731707317,
      "grad_norm": 0.18680351972579956,
      "learning_rate": 4.6817351814794266e-05,
      "loss": 0.0322,
      "step": 1998
    },
    {
      "epoch": 0.4875609756097561,
      "grad_norm": 0.1109517440199852,
      "learning_rate": 4.681423309086929e-05,
      "loss": 0.0281,
      "step": 1999
    },
    {
      "epoch": 0.4878048780487805,
      "grad_norm": 0.08355040103197098,
      "learning_rate": 4.681111294363238e-05,
      "loss": 0.0309,
      "step": 2000
    },
    {
      "epoch": 0.48804878048780487,
      "grad_norm": 0.19508256018161774,
      "learning_rate": 4.680799137328713e-05,
      "loss": 0.031,
      "step": 2001
    },
    {
      "epoch": 0.48829268292682926,
      "grad_norm": 0.09460815787315369,
      "learning_rate": 4.680486838003719e-05,
      "loss": 0.0235,
      "step": 2002
    },
    {
      "epoch": 0.48853658536585365,
      "grad_norm": 0.1083107441663742,
      "learning_rate": 4.680174396408635e-05,
      "loss": 0.0226,
      "step": 2003
    },
    {
      "epoch": 0.48878048780487804,
      "grad_norm": 0.1748424470424652,
      "learning_rate": 4.679861812563845e-05,
      "loss": 0.0429,
      "step": 2004
    },
    {
      "epoch": 0.48902439024390243,
      "grad_norm": 0.07784546911716461,
      "learning_rate": 4.679549086489746e-05,
      "loss": 0.0167,
      "step": 2005
    },
    {
      "epoch": 0.4892682926829268,
      "grad_norm": 0.1281282603740692,
      "learning_rate": 4.6792362182067407e-05,
      "loss": 0.0333,
      "step": 2006
    },
    {
      "epoch": 0.4895121951219512,
      "grad_norm": 0.4150938391685486,
      "learning_rate": 4.678923207735243e-05,
      "loss": 0.0527,
      "step": 2007
    },
    {
      "epoch": 0.4897560975609756,
      "grad_norm": 0.15190985798835754,
      "learning_rate": 4.678610055095677e-05,
      "loss": 0.0362,
      "step": 2008
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.09734489768743515,
      "learning_rate": 4.678296760308474e-05,
      "loss": 0.0213,
      "step": 2009
    },
    {
      "epoch": 0.4902439024390244,
      "grad_norm": 0.06918016821146011,
      "learning_rate": 4.6779833233940755e-05,
      "loss": 0.0223,
      "step": 2010
    },
    {
      "epoch": 0.49048780487804877,
      "grad_norm": 0.08720856159925461,
      "learning_rate": 4.677669744372933e-05,
      "loss": 0.023,
      "step": 2011
    },
    {
      "epoch": 0.49073170731707316,
      "grad_norm": 0.1852778196334839,
      "learning_rate": 4.6773560232655054e-05,
      "loss": 0.0376,
      "step": 2012
    },
    {
      "epoch": 0.49097560975609755,
      "grad_norm": 0.09206950664520264,
      "learning_rate": 4.6770421600922645e-05,
      "loss": 0.0201,
      "step": 2013
    },
    {
      "epoch": 0.49121951219512194,
      "grad_norm": 0.12908312678337097,
      "learning_rate": 4.6767281548736854e-05,
      "loss": 0.0174,
      "step": 2014
    },
    {
      "epoch": 0.49146341463414633,
      "grad_norm": 0.10706533491611481,
      "learning_rate": 4.676414007630259e-05,
      "loss": 0.0298,
      "step": 2015
    },
    {
      "epoch": 0.4917073170731707,
      "grad_norm": 0.051922447979450226,
      "learning_rate": 4.6760997183824805e-05,
      "loss": 0.0241,
      "step": 2016
    },
    {
      "epoch": 0.4919512195121951,
      "grad_norm": 0.11662372201681137,
      "learning_rate": 4.675785287150858e-05,
      "loss": 0.0339,
      "step": 2017
    },
    {
      "epoch": 0.4921951219512195,
      "grad_norm": 0.2772550582885742,
      "learning_rate": 4.675470713955905e-05,
      "loss": 0.0281,
      "step": 2018
    },
    {
      "epoch": 0.4924390243902439,
      "grad_norm": 0.17917227745056152,
      "learning_rate": 4.6751559988181483e-05,
      "loss": 0.0435,
      "step": 2019
    },
    {
      "epoch": 0.4926829268292683,
      "grad_norm": 0.16437287628650665,
      "learning_rate": 4.674841141758122e-05,
      "loss": 0.0231,
      "step": 2020
    },
    {
      "epoch": 0.4929268292682927,
      "grad_norm": 0.11065120995044708,
      "learning_rate": 4.674526142796368e-05,
      "loss": 0.0218,
      "step": 2021
    },
    {
      "epoch": 0.49317073170731707,
      "grad_norm": 0.45432180166244507,
      "learning_rate": 4.6742110019534404e-05,
      "loss": 0.0401,
      "step": 2022
    },
    {
      "epoch": 0.49341463414634146,
      "grad_norm": 0.10606098175048828,
      "learning_rate": 4.673895719249901e-05,
      "loss": 0.0313,
      "step": 2023
    },
    {
      "epoch": 0.49365853658536585,
      "grad_norm": 0.1384350061416626,
      "learning_rate": 4.6735802947063215e-05,
      "loss": 0.0226,
      "step": 2024
    },
    {
      "epoch": 0.49390243902439024,
      "grad_norm": 0.09415778517723083,
      "learning_rate": 4.6732647283432805e-05,
      "loss": 0.0157,
      "step": 2025
    },
    {
      "epoch": 0.49414634146341463,
      "grad_norm": 0.2634938955307007,
      "learning_rate": 4.67294902018137e-05,
      "loss": 0.0333,
      "step": 2026
    },
    {
      "epoch": 0.494390243902439,
      "grad_norm": 0.052552830427885056,
      "learning_rate": 4.6726331702411875e-05,
      "loss": 0.0167,
      "step": 2027
    },
    {
      "epoch": 0.4946341463414634,
      "grad_norm": 0.08739317208528519,
      "learning_rate": 4.672317178543341e-05,
      "loss": 0.0281,
      "step": 2028
    },
    {
      "epoch": 0.4948780487804878,
      "grad_norm": 0.18551556766033173,
      "learning_rate": 4.6720010451084496e-05,
      "loss": 0.0307,
      "step": 2029
    },
    {
      "epoch": 0.4951219512195122,
      "grad_norm": 0.2461480051279068,
      "learning_rate": 4.6716847699571384e-05,
      "loss": 0.0204,
      "step": 2030
    },
    {
      "epoch": 0.4953658536585366,
      "grad_norm": 0.06290294229984283,
      "learning_rate": 4.671368353110045e-05,
      "loss": 0.0299,
      "step": 2031
    },
    {
      "epoch": 0.49560975609756097,
      "grad_norm": 0.17394249141216278,
      "learning_rate": 4.6710517945878134e-05,
      "loss": 0.0218,
      "step": 2032
    },
    {
      "epoch": 0.49585365853658536,
      "grad_norm": 0.1468116044998169,
      "learning_rate": 4.670735094411098e-05,
      "loss": 0.0277,
      "step": 2033
    },
    {
      "epoch": 0.49609756097560975,
      "grad_norm": 0.07111505419015884,
      "learning_rate": 4.670418252600564e-05,
      "loss": 0.022,
      "step": 2034
    },
    {
      "epoch": 0.49634146341463414,
      "grad_norm": 0.08330868184566498,
      "learning_rate": 4.6701012691768825e-05,
      "loss": 0.0304,
      "step": 2035
    },
    {
      "epoch": 0.49658536585365853,
      "grad_norm": 0.1623065322637558,
      "learning_rate": 4.6697841441607374e-05,
      "loss": 0.0223,
      "step": 2036
    },
    {
      "epoch": 0.4968292682926829,
      "grad_norm": 0.08242923766374588,
      "learning_rate": 4.669466877572818e-05,
      "loss": 0.0145,
      "step": 2037
    },
    {
      "epoch": 0.4970731707317073,
      "grad_norm": 0.9853605031967163,
      "learning_rate": 4.6691494694338266e-05,
      "loss": 0.0175,
      "step": 2038
    },
    {
      "epoch": 0.4973170731707317,
      "grad_norm": 0.13241645693778992,
      "learning_rate": 4.6688319197644733e-05,
      "loss": 0.0349,
      "step": 2039
    },
    {
      "epoch": 0.4975609756097561,
      "grad_norm": 0.06974104791879654,
      "learning_rate": 4.668514228585476e-05,
      "loss": 0.0208,
      "step": 2040
    },
    {
      "epoch": 0.4978048780487805,
      "grad_norm": 0.09275460988283157,
      "learning_rate": 4.668196395917564e-05,
      "loss": 0.0399,
      "step": 2041
    },
    {
      "epoch": 0.4980487804878049,
      "grad_norm": 0.4472772777080536,
      "learning_rate": 4.667878421781475e-05,
      "loss": 0.0232,
      "step": 2042
    },
    {
      "epoch": 0.49829268292682927,
      "grad_norm": 0.08681056648492813,
      "learning_rate": 4.667560306197955e-05,
      "loss": 0.014,
      "step": 2043
    },
    {
      "epoch": 0.49853658536585366,
      "grad_norm": 0.08585071563720703,
      "learning_rate": 4.667242049187761e-05,
      "loss": 0.017,
      "step": 2044
    },
    {
      "epoch": 0.49878048780487805,
      "grad_norm": 0.060390762984752655,
      "learning_rate": 4.6669236507716565e-05,
      "loss": 0.0214,
      "step": 2045
    },
    {
      "epoch": 0.49902439024390244,
      "grad_norm": 0.07203741371631622,
      "learning_rate": 4.666605110970419e-05,
      "loss": 0.0124,
      "step": 2046
    },
    {
      "epoch": 0.49926829268292683,
      "grad_norm": 0.173043891787529,
      "learning_rate": 4.66628642980483e-05,
      "loss": 0.0244,
      "step": 2047
    },
    {
      "epoch": 0.4995121951219512,
      "grad_norm": 0.28995969891548157,
      "learning_rate": 4.665967607295683e-05,
      "loss": 0.0251,
      "step": 2048
    },
    {
      "epoch": 0.4997560975609756,
      "grad_norm": 0.08693791925907135,
      "learning_rate": 4.6656486434637804e-05,
      "loss": 0.0209,
      "step": 2049
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.3413475751876831,
      "learning_rate": 4.665329538329933e-05,
      "loss": 0.0378,
      "step": 2050
    },
    {
      "epoch": 0.5002439024390244,
      "grad_norm": 0.5229198932647705,
      "learning_rate": 4.665010291914963e-05,
      "loss": 0.024,
      "step": 2051
    },
    {
      "epoch": 0.5004878048780488,
      "grad_norm": 0.11322229355573654,
      "learning_rate": 4.6646909042396994e-05,
      "loss": 0.0184,
      "step": 2052
    },
    {
      "epoch": 0.5007317073170732,
      "grad_norm": 0.3841400444507599,
      "learning_rate": 4.6643713753249796e-05,
      "loss": 0.0294,
      "step": 2053
    },
    {
      "epoch": 0.5009756097560976,
      "grad_norm": 0.1389080435037613,
      "learning_rate": 4.664051705191654e-05,
      "loss": 0.0276,
      "step": 2054
    },
    {
      "epoch": 0.501219512195122,
      "grad_norm": 0.12667809426784515,
      "learning_rate": 4.66373189386058e-05,
      "loss": 0.0272,
      "step": 2055
    },
    {
      "epoch": 0.5014634146341463,
      "grad_norm": 0.08099202066659927,
      "learning_rate": 4.663411941352623e-05,
      "loss": 0.0191,
      "step": 2056
    },
    {
      "epoch": 0.5017073170731707,
      "grad_norm": 0.11591658741235733,
      "learning_rate": 4.66309184768866e-05,
      "loss": 0.0238,
      "step": 2057
    },
    {
      "epoch": 0.5019512195121951,
      "grad_norm": 0.166861891746521,
      "learning_rate": 4.662771612889575e-05,
      "loss": 0.0186,
      "step": 2058
    },
    {
      "epoch": 0.5021951219512195,
      "grad_norm": 0.19380179047584534,
      "learning_rate": 4.662451236976264e-05,
      "loss": 0.0467,
      "step": 2059
    },
    {
      "epoch": 0.5024390243902439,
      "grad_norm": 0.32236698269844055,
      "learning_rate": 4.66213071996963e-05,
      "loss": 0.0221,
      "step": 2060
    },
    {
      "epoch": 0.5026829268292683,
      "grad_norm": 0.1573042869567871,
      "learning_rate": 4.6618100618905846e-05,
      "loss": 0.0426,
      "step": 2061
    },
    {
      "epoch": 0.5029268292682927,
      "grad_norm": 0.11044332385063171,
      "learning_rate": 4.661489262760051e-05,
      "loss": 0.0295,
      "step": 2062
    },
    {
      "epoch": 0.5031707317073171,
      "grad_norm": 0.2480406016111374,
      "learning_rate": 4.66116832259896e-05,
      "loss": 0.0348,
      "step": 2063
    },
    {
      "epoch": 0.5034146341463415,
      "grad_norm": 0.1743476539850235,
      "learning_rate": 4.6608472414282516e-05,
      "loss": 0.0206,
      "step": 2064
    },
    {
      "epoch": 0.5036585365853659,
      "grad_norm": 0.557049036026001,
      "learning_rate": 4.660526019268876e-05,
      "loss": 0.0299,
      "step": 2065
    },
    {
      "epoch": 0.5039024390243902,
      "grad_norm": 0.11373678594827652,
      "learning_rate": 4.6602046561417914e-05,
      "loss": 0.0412,
      "step": 2066
    },
    {
      "epoch": 0.5041463414634146,
      "grad_norm": 0.11116135120391846,
      "learning_rate": 4.659883152067967e-05,
      "loss": 0.0208,
      "step": 2067
    },
    {
      "epoch": 0.504390243902439,
      "grad_norm": 0.04135744273662567,
      "learning_rate": 4.6595615070683774e-05,
      "loss": 0.0106,
      "step": 2068
    },
    {
      "epoch": 0.5046341463414634,
      "grad_norm": 0.13869120180606842,
      "learning_rate": 4.6592397211640116e-05,
      "loss": 0.0188,
      "step": 2069
    },
    {
      "epoch": 0.5048780487804878,
      "grad_norm": 0.13406643271446228,
      "learning_rate": 4.6589177943758636e-05,
      "loss": 0.0274,
      "step": 2070
    },
    {
      "epoch": 0.5051219512195122,
      "grad_norm": 0.06013214960694313,
      "learning_rate": 4.6585957267249384e-05,
      "loss": 0.0292,
      "step": 2071
    },
    {
      "epoch": 0.5053658536585366,
      "grad_norm": 0.08704870194196701,
      "learning_rate": 4.6582735182322506e-05,
      "loss": 0.0518,
      "step": 2072
    },
    {
      "epoch": 0.505609756097561,
      "grad_norm": 0.20693756639957428,
      "learning_rate": 4.657951168918823e-05,
      "loss": 0.019,
      "step": 2073
    },
    {
      "epoch": 0.5058536585365854,
      "grad_norm": 0.08348217606544495,
      "learning_rate": 4.657628678805687e-05,
      "loss": 0.0279,
      "step": 2074
    },
    {
      "epoch": 0.5060975609756098,
      "grad_norm": 0.1353682577610016,
      "learning_rate": 4.657306047913885e-05,
      "loss": 0.0307,
      "step": 2075
    },
    {
      "epoch": 0.5063414634146342,
      "grad_norm": 0.13663026690483093,
      "learning_rate": 4.6569832762644685e-05,
      "loss": 0.0222,
      "step": 2076
    },
    {
      "epoch": 0.5065853658536585,
      "grad_norm": 0.06858164072036743,
      "learning_rate": 4.6566603638784954e-05,
      "loss": 0.0152,
      "step": 2077
    },
    {
      "epoch": 0.5068292682926829,
      "grad_norm": 0.1295187920331955,
      "learning_rate": 4.656337310777036e-05,
      "loss": 0.0208,
      "step": 2078
    },
    {
      "epoch": 0.5070731707317073,
      "grad_norm": 0.05983632802963257,
      "learning_rate": 4.6560141169811675e-05,
      "loss": 0.0253,
      "step": 2079
    },
    {
      "epoch": 0.5073170731707317,
      "grad_norm": 0.16515325009822845,
      "learning_rate": 4.655690782511979e-05,
      "loss": 0.0256,
      "step": 2080
    },
    {
      "epoch": 0.5075609756097561,
      "grad_norm": 0.4126884639263153,
      "learning_rate": 4.6553673073905665e-05,
      "loss": 0.0294,
      "step": 2081
    },
    {
      "epoch": 0.5078048780487805,
      "grad_norm": 0.09645494818687439,
      "learning_rate": 4.655043691638035e-05,
      "loss": 0.032,
      "step": 2082
    },
    {
      "epoch": 0.5080487804878049,
      "grad_norm": 0.131587952375412,
      "learning_rate": 4.654719935275499e-05,
      "loss": 0.0178,
      "step": 2083
    },
    {
      "epoch": 0.5082926829268293,
      "grad_norm": 0.09981709718704224,
      "learning_rate": 4.654396038324085e-05,
      "loss": 0.0192,
      "step": 2084
    },
    {
      "epoch": 0.5085365853658537,
      "grad_norm": 0.166396826505661,
      "learning_rate": 4.654072000804923e-05,
      "loss": 0.0329,
      "step": 2085
    },
    {
      "epoch": 0.5087804878048781,
      "grad_norm": 0.04396998509764671,
      "learning_rate": 4.6537478227391584e-05,
      "loss": 0.0126,
      "step": 2086
    },
    {
      "epoch": 0.5090243902439024,
      "grad_norm": 0.07194063812494278,
      "learning_rate": 4.6534235041479415e-05,
      "loss": 0.0296,
      "step": 2087
    },
    {
      "epoch": 0.5092682926829268,
      "grad_norm": 0.2333107590675354,
      "learning_rate": 4.6530990450524334e-05,
      "loss": 0.0257,
      "step": 2088
    },
    {
      "epoch": 0.5095121951219512,
      "grad_norm": 0.09909851104021072,
      "learning_rate": 4.652774445473803e-05,
      "loss": 0.0166,
      "step": 2089
    },
    {
      "epoch": 0.5097560975609756,
      "grad_norm": 0.09957145154476166,
      "learning_rate": 4.652449705433231e-05,
      "loss": 0.025,
      "step": 2090
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.2615372836589813,
      "learning_rate": 4.652124824951904e-05,
      "loss": 0.0257,
      "step": 2091
    },
    {
      "epoch": 0.5102439024390244,
      "grad_norm": 0.12492632865905762,
      "learning_rate": 4.6517998040510224e-05,
      "loss": 0.0264,
      "step": 2092
    },
    {
      "epoch": 0.5104878048780488,
      "grad_norm": 0.14504851400852203,
      "learning_rate": 4.6514746427517894e-05,
      "loss": 0.0278,
      "step": 2093
    },
    {
      "epoch": 0.5107317073170732,
      "grad_norm": 0.1464705765247345,
      "learning_rate": 4.6511493410754226e-05,
      "loss": 0.0172,
      "step": 2094
    },
    {
      "epoch": 0.5109756097560976,
      "grad_norm": 0.07964461296796799,
      "learning_rate": 4.6508238990431465e-05,
      "loss": 0.0211,
      "step": 2095
    },
    {
      "epoch": 0.511219512195122,
      "grad_norm": 0.15278147161006927,
      "learning_rate": 4.650498316676196e-05,
      "loss": 0.0309,
      "step": 2096
    },
    {
      "epoch": 0.5114634146341464,
      "grad_norm": 0.1496315598487854,
      "learning_rate": 4.650172593995813e-05,
      "loss": 0.0422,
      "step": 2097
    },
    {
      "epoch": 0.5117073170731707,
      "grad_norm": 0.06146504357457161,
      "learning_rate": 4.6498467310232504e-05,
      "loss": 0.029,
      "step": 2098
    },
    {
      "epoch": 0.5119512195121951,
      "grad_norm": 0.07014019042253494,
      "learning_rate": 4.64952072777977e-05,
      "loss": 0.0224,
      "step": 2099
    },
    {
      "epoch": 0.5121951219512195,
      "grad_norm": 0.09881599992513657,
      "learning_rate": 4.6491945842866424e-05,
      "loss": 0.0227,
      "step": 2100
    },
    {
      "epoch": 0.5124390243902439,
      "grad_norm": 0.16636517643928528,
      "learning_rate": 4.648868300565148e-05,
      "loss": 0.0314,
      "step": 2101
    },
    {
      "epoch": 0.5126829268292683,
      "grad_norm": 0.24750427901744843,
      "learning_rate": 4.648541876636575e-05,
      "loss": 0.0369,
      "step": 2102
    },
    {
      "epoch": 0.5129268292682927,
      "grad_norm": 0.09046167880296707,
      "learning_rate": 4.648215312522222e-05,
      "loss": 0.0254,
      "step": 2103
    },
    {
      "epoch": 0.5131707317073171,
      "grad_norm": 0.376814603805542,
      "learning_rate": 4.647888608243395e-05,
      "loss": 0.0298,
      "step": 2104
    },
    {
      "epoch": 0.5134146341463415,
      "grad_norm": 0.07647864520549774,
      "learning_rate": 4.647561763821413e-05,
      "loss": 0.0272,
      "step": 2105
    },
    {
      "epoch": 0.5136585365853659,
      "grad_norm": 0.18157146871089935,
      "learning_rate": 4.6472347792775995e-05,
      "loss": 0.0336,
      "step": 2106
    },
    {
      "epoch": 0.5139024390243903,
      "grad_norm": 0.15110617876052856,
      "learning_rate": 4.64690765463329e-05,
      "loss": 0.0225,
      "step": 2107
    },
    {
      "epoch": 0.5141463414634146,
      "grad_norm": 0.12481272220611572,
      "learning_rate": 4.6465803899098294e-05,
      "loss": 0.0442,
      "step": 2108
    },
    {
      "epoch": 0.514390243902439,
      "grad_norm": 0.17673243582248688,
      "learning_rate": 4.646252985128569e-05,
      "loss": 0.0339,
      "step": 2109
    },
    {
      "epoch": 0.5146341463414634,
      "grad_norm": 0.08689065277576447,
      "learning_rate": 4.645925440310871e-05,
      "loss": 0.027,
      "step": 2110
    },
    {
      "epoch": 0.5148780487804878,
      "grad_norm": 0.08751308172941208,
      "learning_rate": 4.645597755478108e-05,
      "loss": 0.0218,
      "step": 2111
    },
    {
      "epoch": 0.5151219512195122,
      "grad_norm": 0.07743973284959793,
      "learning_rate": 4.64526993065166e-05,
      "loss": 0.0156,
      "step": 2112
    },
    {
      "epoch": 0.5153658536585366,
      "grad_norm": 0.08129065483808517,
      "learning_rate": 4.644941965852916e-05,
      "loss": 0.026,
      "step": 2113
    },
    {
      "epoch": 0.515609756097561,
      "grad_norm": 0.1333666890859604,
      "learning_rate": 4.644613861103275e-05,
      "loss": 0.024,
      "step": 2114
    },
    {
      "epoch": 0.5158536585365854,
      "grad_norm": 0.08876222372055054,
      "learning_rate": 4.6442856164241445e-05,
      "loss": 0.0278,
      "step": 2115
    },
    {
      "epoch": 0.5160975609756098,
      "grad_norm": 0.11753271520137787,
      "learning_rate": 4.643957231836942e-05,
      "loss": 0.0331,
      "step": 2116
    },
    {
      "epoch": 0.5163414634146342,
      "grad_norm": 0.08972925692796707,
      "learning_rate": 4.643628707363093e-05,
      "loss": 0.0244,
      "step": 2117
    },
    {
      "epoch": 0.5165853658536586,
      "grad_norm": 0.06514707207679749,
      "learning_rate": 4.643300043024035e-05,
      "loss": 0.0271,
      "step": 2118
    },
    {
      "epoch": 0.5168292682926829,
      "grad_norm": 0.06523288041353226,
      "learning_rate": 4.642971238841208e-05,
      "loss": 0.0169,
      "step": 2119
    },
    {
      "epoch": 0.5170731707317073,
      "grad_norm": 0.07742521911859512,
      "learning_rate": 4.6426422948360693e-05,
      "loss": 0.035,
      "step": 2120
    },
    {
      "epoch": 0.5173170731707317,
      "grad_norm": 0.14291340112686157,
      "learning_rate": 4.642313211030081e-05,
      "loss": 0.0416,
      "step": 2121
    },
    {
      "epoch": 0.5175609756097561,
      "grad_norm": 0.11738905310630798,
      "learning_rate": 4.641983987444713e-05,
      "loss": 0.0432,
      "step": 2122
    },
    {
      "epoch": 0.5178048780487805,
      "grad_norm": 0.08194523304700851,
      "learning_rate": 4.641654624101446e-05,
      "loss": 0.0225,
      "step": 2123
    },
    {
      "epoch": 0.5180487804878049,
      "grad_norm": 0.07442121207714081,
      "learning_rate": 4.641325121021772e-05,
      "loss": 0.027,
      "step": 2124
    },
    {
      "epoch": 0.5182926829268293,
      "grad_norm": 0.06796634197235107,
      "learning_rate": 4.640995478227189e-05,
      "loss": 0.0259,
      "step": 2125
    },
    {
      "epoch": 0.5185365853658537,
      "grad_norm": 0.2026270478963852,
      "learning_rate": 4.6406656957392054e-05,
      "loss": 0.0303,
      "step": 2126
    },
    {
      "epoch": 0.5187804878048781,
      "grad_norm": 0.1421714574098587,
      "learning_rate": 4.640335773579338e-05,
      "loss": 0.0334,
      "step": 2127
    },
    {
      "epoch": 0.5190243902439025,
      "grad_norm": 0.15101629495620728,
      "learning_rate": 4.6400057117691134e-05,
      "loss": 0.0238,
      "step": 2128
    },
    {
      "epoch": 0.5192682926829268,
      "grad_norm": 0.08023660629987717,
      "learning_rate": 4.639675510330068e-05,
      "loss": 0.0149,
      "step": 2129
    },
    {
      "epoch": 0.5195121951219512,
      "grad_norm": 0.07542529702186584,
      "learning_rate": 4.639345169283745e-05,
      "loss": 0.0197,
      "step": 2130
    },
    {
      "epoch": 0.5197560975609756,
      "grad_norm": 0.07833084464073181,
      "learning_rate": 4.6390146886516996e-05,
      "loss": 0.0259,
      "step": 2131
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.18029966950416565,
      "learning_rate": 4.6386840684554935e-05,
      "loss": 0.0275,
      "step": 2132
    },
    {
      "epoch": 0.5202439024390244,
      "grad_norm": 0.2600460350513458,
      "learning_rate": 4.6383533087167e-05,
      "loss": 0.0126,
      "step": 2133
    },
    {
      "epoch": 0.5204878048780488,
      "grad_norm": 0.11756341904401779,
      "learning_rate": 4.638022409456898e-05,
      "loss": 0.027,
      "step": 2134
    },
    {
      "epoch": 0.5207317073170732,
      "grad_norm": 0.09799931198358536,
      "learning_rate": 4.637691370697679e-05,
      "loss": 0.0257,
      "step": 2135
    },
    {
      "epoch": 0.5209756097560976,
      "grad_norm": 0.08304000645875931,
      "learning_rate": 4.637360192460642e-05,
      "loss": 0.0096,
      "step": 2136
    },
    {
      "epoch": 0.521219512195122,
      "grad_norm": 0.23041002452373505,
      "learning_rate": 4.637028874767396e-05,
      "loss": 0.0267,
      "step": 2137
    },
    {
      "epoch": 0.5214634146341464,
      "grad_norm": 0.10608162730932236,
      "learning_rate": 4.636697417639558e-05,
      "loss": 0.0271,
      "step": 2138
    },
    {
      "epoch": 0.5217073170731708,
      "grad_norm": 0.08439772576093674,
      "learning_rate": 4.636365821098755e-05,
      "loss": 0.0158,
      "step": 2139
    },
    {
      "epoch": 0.5219512195121951,
      "grad_norm": 0.2026282399892807,
      "learning_rate": 4.636034085166622e-05,
      "loss": 0.0202,
      "step": 2140
    },
    {
      "epoch": 0.5221951219512195,
      "grad_norm": 0.0875084325671196,
      "learning_rate": 4.6357022098648036e-05,
      "loss": 0.0284,
      "step": 2141
    },
    {
      "epoch": 0.5224390243902439,
      "grad_norm": 0.0648781955242157,
      "learning_rate": 4.635370195214954e-05,
      "loss": 0.0155,
      "step": 2142
    },
    {
      "epoch": 0.5226829268292683,
      "grad_norm": 0.09579617530107498,
      "learning_rate": 4.635038041238736e-05,
      "loss": 0.0294,
      "step": 2143
    },
    {
      "epoch": 0.5229268292682927,
      "grad_norm": 0.12288804352283478,
      "learning_rate": 4.634705747957822e-05,
      "loss": 0.0155,
      "step": 2144
    },
    {
      "epoch": 0.5231707317073171,
      "grad_norm": 0.1936415582895279,
      "learning_rate": 4.6343733153938926e-05,
      "loss": 0.0242,
      "step": 2145
    },
    {
      "epoch": 0.5234146341463415,
      "grad_norm": 0.38405391573905945,
      "learning_rate": 4.634040743568639e-05,
      "loss": 0.0328,
      "step": 2146
    },
    {
      "epoch": 0.5236585365853659,
      "grad_norm": 0.0717989057302475,
      "learning_rate": 4.63370803250376e-05,
      "loss": 0.0253,
      "step": 2147
    },
    {
      "epoch": 0.5239024390243903,
      "grad_norm": 0.05390574783086777,
      "learning_rate": 4.633375182220963e-05,
      "loss": 0.0246,
      "step": 2148
    },
    {
      "epoch": 0.5241463414634147,
      "grad_norm": 0.14577853679656982,
      "learning_rate": 4.6330421927419664e-05,
      "loss": 0.0263,
      "step": 2149
    },
    {
      "epoch": 0.524390243902439,
      "grad_norm": 0.13428764045238495,
      "learning_rate": 4.6327090640884974e-05,
      "loss": 0.0275,
      "step": 2150
    },
    {
      "epoch": 0.5246341463414634,
      "grad_norm": 0.10482803732156754,
      "learning_rate": 4.63237579628229e-05,
      "loss": 0.0286,
      "step": 2151
    },
    {
      "epoch": 0.5248780487804878,
      "grad_norm": 0.06066740304231644,
      "learning_rate": 4.6320423893450896e-05,
      "loss": 0.0115,
      "step": 2152
    },
    {
      "epoch": 0.5251219512195122,
      "grad_norm": 0.19861984252929688,
      "learning_rate": 4.6317088432986496e-05,
      "loss": 0.022,
      "step": 2153
    },
    {
      "epoch": 0.5253658536585366,
      "grad_norm": 0.09360574185848236,
      "learning_rate": 4.631375158164735e-05,
      "loss": 0.026,
      "step": 2154
    },
    {
      "epoch": 0.525609756097561,
      "grad_norm": 0.09010875225067139,
      "learning_rate": 4.631041333965115e-05,
      "loss": 0.0248,
      "step": 2155
    },
    {
      "epoch": 0.5258536585365854,
      "grad_norm": 0.053289107978343964,
      "learning_rate": 4.630707370721572e-05,
      "loss": 0.0228,
      "step": 2156
    },
    {
      "epoch": 0.5260975609756098,
      "grad_norm": 0.198626309633255,
      "learning_rate": 4.630373268455895e-05,
      "loss": 0.0372,
      "step": 2157
    },
    {
      "epoch": 0.5263414634146342,
      "grad_norm": 0.051839955151081085,
      "learning_rate": 4.630039027189885e-05,
      "loss": 0.0155,
      "step": 2158
    },
    {
      "epoch": 0.5265853658536586,
      "grad_norm": 0.17719364166259766,
      "learning_rate": 4.629704646945349e-05,
      "loss": 0.0242,
      "step": 2159
    },
    {
      "epoch": 0.526829268292683,
      "grad_norm": 0.1472800076007843,
      "learning_rate": 4.629370127744105e-05,
      "loss": 0.0151,
      "step": 2160
    },
    {
      "epoch": 0.5270731707317073,
      "grad_norm": 0.2706950306892395,
      "learning_rate": 4.629035469607977e-05,
      "loss": 0.0408,
      "step": 2161
    },
    {
      "epoch": 0.5273170731707317,
      "grad_norm": 0.10427731275558472,
      "learning_rate": 4.628700672558804e-05,
      "loss": 0.0267,
      "step": 2162
    },
    {
      "epoch": 0.5275609756097561,
      "grad_norm": 0.13194961845874786,
      "learning_rate": 4.628365736618428e-05,
      "loss": 0.0471,
      "step": 2163
    },
    {
      "epoch": 0.5278048780487805,
      "grad_norm": 0.1523641049861908,
      "learning_rate": 4.628030661808703e-05,
      "loss": 0.0451,
      "step": 2164
    },
    {
      "epoch": 0.5280487804878049,
      "grad_norm": 0.14611485600471497,
      "learning_rate": 4.627695448151492e-05,
      "loss": 0.0351,
      "step": 2165
    },
    {
      "epoch": 0.5282926829268293,
      "grad_norm": 0.34005260467529297,
      "learning_rate": 4.6273600956686665e-05,
      "loss": 0.0359,
      "step": 2166
    },
    {
      "epoch": 0.5285365853658537,
      "grad_norm": 0.08483342826366425,
      "learning_rate": 4.6270246043821075e-05,
      "loss": 0.0381,
      "step": 2167
    },
    {
      "epoch": 0.5287804878048781,
      "grad_norm": 0.08216068893671036,
      "learning_rate": 4.6266889743137044e-05,
      "loss": 0.0262,
      "step": 2168
    },
    {
      "epoch": 0.5290243902439025,
      "grad_norm": 0.0807107612490654,
      "learning_rate": 4.626353205485355e-05,
      "loss": 0.019,
      "step": 2169
    },
    {
      "epoch": 0.5292682926829269,
      "grad_norm": 0.06937962770462036,
      "learning_rate": 4.6260172979189696e-05,
      "loss": 0.029,
      "step": 2170
    },
    {
      "epoch": 0.5295121951219512,
      "grad_norm": 0.17484335601329803,
      "learning_rate": 4.625681251636464e-05,
      "loss": 0.0265,
      "step": 2171
    },
    {
      "epoch": 0.5297560975609756,
      "grad_norm": 0.09441212564706802,
      "learning_rate": 4.625345066659763e-05,
      "loss": 0.0275,
      "step": 2172
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.11225994676351547,
      "learning_rate": 4.6250087430108034e-05,
      "loss": 0.0299,
      "step": 2173
    },
    {
      "epoch": 0.5302439024390244,
      "grad_norm": 0.15962447226047516,
      "learning_rate": 4.624672280711529e-05,
      "loss": 0.0268,
      "step": 2174
    },
    {
      "epoch": 0.5304878048780488,
      "grad_norm": 0.11246422678232193,
      "learning_rate": 4.624335679783892e-05,
      "loss": 0.0171,
      "step": 2175
    },
    {
      "epoch": 0.5307317073170732,
      "grad_norm": 0.10511457175016403,
      "learning_rate": 4.623998940249855e-05,
      "loss": 0.0165,
      "step": 2176
    },
    {
      "epoch": 0.5309756097560976,
      "grad_norm": 0.2873905301094055,
      "learning_rate": 4.62366206213139e-05,
      "loss": 0.0239,
      "step": 2177
    },
    {
      "epoch": 0.531219512195122,
      "grad_norm": 0.04714205116033554,
      "learning_rate": 4.623325045450476e-05,
      "loss": 0.0103,
      "step": 2178
    },
    {
      "epoch": 0.5314634146341464,
      "grad_norm": 0.07309943437576294,
      "learning_rate": 4.6229878902291035e-05,
      "loss": 0.0144,
      "step": 2179
    },
    {
      "epoch": 0.5317073170731708,
      "grad_norm": 0.07058784365653992,
      "learning_rate": 4.62265059648927e-05,
      "loss": 0.0339,
      "step": 2180
    },
    {
      "epoch": 0.5319512195121952,
      "grad_norm": 0.3631051182746887,
      "learning_rate": 4.6223131642529825e-05,
      "loss": 0.0296,
      "step": 2181
    },
    {
      "epoch": 0.5321951219512195,
      "grad_norm": 0.07330179959535599,
      "learning_rate": 4.621975593542258e-05,
      "loss": 0.0159,
      "step": 2182
    },
    {
      "epoch": 0.5324390243902439,
      "grad_norm": 0.21016104519367218,
      "learning_rate": 4.621637884379123e-05,
      "loss": 0.0275,
      "step": 2183
    },
    {
      "epoch": 0.5326829268292683,
      "grad_norm": 0.11544192582368851,
      "learning_rate": 4.621300036785611e-05,
      "loss": 0.0424,
      "step": 2184
    },
    {
      "epoch": 0.5329268292682927,
      "grad_norm": 0.09633168578147888,
      "learning_rate": 4.620962050783765e-05,
      "loss": 0.0319,
      "step": 2185
    },
    {
      "epoch": 0.5331707317073171,
      "grad_norm": 0.12845967710018158,
      "learning_rate": 4.6206239263956376e-05,
      "loss": 0.0196,
      "step": 2186
    },
    {
      "epoch": 0.5334146341463415,
      "grad_norm": 0.0833376869559288,
      "learning_rate": 4.6202856636432914e-05,
      "loss": 0.03,
      "step": 2187
    },
    {
      "epoch": 0.5336585365853659,
      "grad_norm": 0.09186487644910812,
      "learning_rate": 4.6199472625487964e-05,
      "loss": 0.0162,
      "step": 2188
    },
    {
      "epoch": 0.5339024390243903,
      "grad_norm": 0.14302338659763336,
      "learning_rate": 4.6196087231342324e-05,
      "loss": 0.0326,
      "step": 2189
    },
    {
      "epoch": 0.5341463414634147,
      "grad_norm": 0.13682742416858673,
      "learning_rate": 4.619270045421688e-05,
      "loss": 0.0254,
      "step": 2190
    },
    {
      "epoch": 0.534390243902439,
      "grad_norm": 0.12708893418312073,
      "learning_rate": 4.618931229433261e-05,
      "loss": 0.0239,
      "step": 2191
    },
    {
      "epoch": 0.5346341463414634,
      "grad_norm": 0.07528810203075409,
      "learning_rate": 4.618592275191057e-05,
      "loss": 0.0227,
      "step": 2192
    },
    {
      "epoch": 0.5348780487804878,
      "grad_norm": 0.05342726781964302,
      "learning_rate": 4.618253182717194e-05,
      "loss": 0.0104,
      "step": 2193
    },
    {
      "epoch": 0.5351219512195122,
      "grad_norm": 0.10555940121412277,
      "learning_rate": 4.617913952033794e-05,
      "loss": 0.023,
      "step": 2194
    },
    {
      "epoch": 0.5353658536585366,
      "grad_norm": 0.09142229706048965,
      "learning_rate": 4.617574583162993e-05,
      "loss": 0.0183,
      "step": 2195
    },
    {
      "epoch": 0.535609756097561,
      "grad_norm": 0.11243695765733719,
      "learning_rate": 4.617235076126933e-05,
      "loss": 0.0271,
      "step": 2196
    },
    {
      "epoch": 0.5358536585365854,
      "grad_norm": 0.1860312968492508,
      "learning_rate": 4.616895430947766e-05,
      "loss": 0.0296,
      "step": 2197
    },
    {
      "epoch": 0.5360975609756098,
      "grad_norm": 0.20071952044963837,
      "learning_rate": 4.616555647647652e-05,
      "loss": 0.0326,
      "step": 2198
    },
    {
      "epoch": 0.5363414634146342,
      "grad_norm": 0.07764583081007004,
      "learning_rate": 4.616215726248762e-05,
      "loss": 0.0222,
      "step": 2199
    },
    {
      "epoch": 0.5365853658536586,
      "grad_norm": 0.13539114594459534,
      "learning_rate": 4.615875666773274e-05,
      "loss": 0.0246,
      "step": 2200
    },
    {
      "epoch": 0.536829268292683,
      "grad_norm": 0.11865827441215515,
      "learning_rate": 4.6155354692433764e-05,
      "loss": 0.0261,
      "step": 2201
    },
    {
      "epoch": 0.5370731707317074,
      "grad_norm": 0.12694872915744781,
      "learning_rate": 4.615195133681265e-05,
      "loss": 0.0245,
      "step": 2202
    },
    {
      "epoch": 0.5373170731707317,
      "grad_norm": 0.06537243723869324,
      "learning_rate": 4.614854660109147e-05,
      "loss": 0.0182,
      "step": 2203
    },
    {
      "epoch": 0.5375609756097561,
      "grad_norm": 0.12018007040023804,
      "learning_rate": 4.614514048549235e-05,
      "loss": 0.0257,
      "step": 2204
    },
    {
      "epoch": 0.5378048780487805,
      "grad_norm": 0.20723937451839447,
      "learning_rate": 4.614173299023756e-05,
      "loss": 0.0346,
      "step": 2205
    },
    {
      "epoch": 0.5380487804878049,
      "grad_norm": 0.13843192160129547,
      "learning_rate": 4.613832411554941e-05,
      "loss": 0.0326,
      "step": 2206
    },
    {
      "epoch": 0.5382926829268293,
      "grad_norm": 0.06106239929795265,
      "learning_rate": 4.613491386165033e-05,
      "loss": 0.0322,
      "step": 2207
    },
    {
      "epoch": 0.5385365853658537,
      "grad_norm": 0.1596447229385376,
      "learning_rate": 4.613150222876281e-05,
      "loss": 0.0189,
      "step": 2208
    },
    {
      "epoch": 0.5387804878048781,
      "grad_norm": 0.0835636630654335,
      "learning_rate": 4.612808921710946e-05,
      "loss": 0.0199,
      "step": 2209
    },
    {
      "epoch": 0.5390243902439025,
      "grad_norm": 0.12621748447418213,
      "learning_rate": 4.612467482691297e-05,
      "loss": 0.0195,
      "step": 2210
    },
    {
      "epoch": 0.5392682926829269,
      "grad_norm": 0.05989360436797142,
      "learning_rate": 4.612125905839612e-05,
      "loss": 0.0234,
      "step": 2211
    },
    {
      "epoch": 0.5395121951219513,
      "grad_norm": 0.1625448614358902,
      "learning_rate": 4.611784191178177e-05,
      "loss": 0.0293,
      "step": 2212
    },
    {
      "epoch": 0.5397560975609756,
      "grad_norm": 0.14196303486824036,
      "learning_rate": 4.611442338729289e-05,
      "loss": 0.0281,
      "step": 2213
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.25986921787261963,
      "learning_rate": 4.6111003485152514e-05,
      "loss": 0.027,
      "step": 2214
    },
    {
      "epoch": 0.5402439024390244,
      "grad_norm": 0.08458716422319412,
      "learning_rate": 4.610758220558379e-05,
      "loss": 0.0209,
      "step": 2215
    },
    {
      "epoch": 0.5404878048780488,
      "grad_norm": 0.08340855687856674,
      "learning_rate": 4.6104159548809946e-05,
      "loss": 0.044,
      "step": 2216
    },
    {
      "epoch": 0.5407317073170732,
      "grad_norm": 0.07554570585489273,
      "learning_rate": 4.6100735515054294e-05,
      "loss": 0.0158,
      "step": 2217
    },
    {
      "epoch": 0.5409756097560976,
      "grad_norm": 0.11650244891643524,
      "learning_rate": 4.609731010454025e-05,
      "loss": 0.0283,
      "step": 2218
    },
    {
      "epoch": 0.541219512195122,
      "grad_norm": 0.05773840472102165,
      "learning_rate": 4.6093883317491296e-05,
      "loss": 0.0177,
      "step": 2219
    },
    {
      "epoch": 0.5414634146341464,
      "grad_norm": 0.22917893528938293,
      "learning_rate": 4.609045515413104e-05,
      "loss": 0.0231,
      "step": 2220
    },
    {
      "epoch": 0.5417073170731708,
      "grad_norm": 0.08212130516767502,
      "learning_rate": 4.6087025614683146e-05,
      "loss": 0.0099,
      "step": 2221
    },
    {
      "epoch": 0.5419512195121952,
      "grad_norm": 0.23237332701683044,
      "learning_rate": 4.608359469937138e-05,
      "loss": 0.0262,
      "step": 2222
    },
    {
      "epoch": 0.5421951219512195,
      "grad_norm": 0.1014355942606926,
      "learning_rate": 4.608016240841961e-05,
      "loss": 0.0258,
      "step": 2223
    },
    {
      "epoch": 0.5424390243902439,
      "grad_norm": 0.15538382530212402,
      "learning_rate": 4.607672874205178e-05,
      "loss": 0.0242,
      "step": 2224
    },
    {
      "epoch": 0.5426829268292683,
      "grad_norm": 0.104115329682827,
      "learning_rate": 4.60732937004919e-05,
      "loss": 0.0165,
      "step": 2225
    },
    {
      "epoch": 0.5429268292682927,
      "grad_norm": 0.08755960315465927,
      "learning_rate": 4.606985728396414e-05,
      "loss": 0.0176,
      "step": 2226
    },
    {
      "epoch": 0.5431707317073171,
      "grad_norm": 0.09795054793357849,
      "learning_rate": 4.606641949269269e-05,
      "loss": 0.0254,
      "step": 2227
    },
    {
      "epoch": 0.5434146341463415,
      "grad_norm": 0.2274048626422882,
      "learning_rate": 4.606298032690185e-05,
      "loss": 0.0297,
      "step": 2228
    },
    {
      "epoch": 0.5436585365853659,
      "grad_norm": 0.21804679930210114,
      "learning_rate": 4.605953978681603e-05,
      "loss": 0.0237,
      "step": 2229
    },
    {
      "epoch": 0.5439024390243903,
      "grad_norm": 0.1345876306295395,
      "learning_rate": 4.6056097872659705e-05,
      "loss": 0.0323,
      "step": 2230
    },
    {
      "epoch": 0.5441463414634147,
      "grad_norm": 0.12432289123535156,
      "learning_rate": 4.605265458465745e-05,
      "loss": 0.0324,
      "step": 2231
    },
    {
      "epoch": 0.5443902439024391,
      "grad_norm": 0.07223035395145416,
      "learning_rate": 4.6049209923033945e-05,
      "loss": 0.0217,
      "step": 2232
    },
    {
      "epoch": 0.5446341463414635,
      "grad_norm": 0.0746883749961853,
      "learning_rate": 4.604576388801392e-05,
      "loss": 0.0186,
      "step": 2233
    },
    {
      "epoch": 0.5448780487804878,
      "grad_norm": 0.11187522113323212,
      "learning_rate": 4.6042316479822225e-05,
      "loss": 0.0144,
      "step": 2234
    },
    {
      "epoch": 0.5451219512195122,
      "grad_norm": 0.11650554835796356,
      "learning_rate": 4.6038867698683794e-05,
      "loss": 0.0311,
      "step": 2235
    },
    {
      "epoch": 0.5453658536585366,
      "grad_norm": 0.16229824721813202,
      "learning_rate": 4.603541754482367e-05,
      "loss": 0.0404,
      "step": 2236
    },
    {
      "epoch": 0.545609756097561,
      "grad_norm": 0.056247346103191376,
      "learning_rate": 4.6031966018466934e-05,
      "loss": 0.0122,
      "step": 2237
    },
    {
      "epoch": 0.5458536585365854,
      "grad_norm": 0.40223878622055054,
      "learning_rate": 4.6028513119838794e-05,
      "loss": 0.0334,
      "step": 2238
    },
    {
      "epoch": 0.5460975609756098,
      "grad_norm": 0.10399314761161804,
      "learning_rate": 4.602505884916456e-05,
      "loss": 0.031,
      "step": 2239
    },
    {
      "epoch": 0.5463414634146342,
      "grad_norm": 0.062022916972637177,
      "learning_rate": 4.6021603206669596e-05,
      "loss": 0.0171,
      "step": 2240
    },
    {
      "epoch": 0.5465853658536586,
      "grad_norm": 0.11562466621398926,
      "learning_rate": 4.601814619257937e-05,
      "loss": 0.0336,
      "step": 2241
    },
    {
      "epoch": 0.546829268292683,
      "grad_norm": 0.1064271554350853,
      "learning_rate": 4.601468780711946e-05,
      "loss": 0.0245,
      "step": 2242
    },
    {
      "epoch": 0.5470731707317074,
      "grad_norm": 0.07248934358358383,
      "learning_rate": 4.601122805051549e-05,
      "loss": 0.03,
      "step": 2243
    },
    {
      "epoch": 0.5473170731707317,
      "grad_norm": 0.061852384358644485,
      "learning_rate": 4.600776692299321e-05,
      "loss": 0.0154,
      "step": 2244
    },
    {
      "epoch": 0.5475609756097561,
      "grad_norm": 0.10299518704414368,
      "learning_rate": 4.600430442477846e-05,
      "loss": 0.0188,
      "step": 2245
    },
    {
      "epoch": 0.5478048780487805,
      "grad_norm": 0.2005782276391983,
      "learning_rate": 4.6000840556097145e-05,
      "loss": 0.0219,
      "step": 2246
    },
    {
      "epoch": 0.5480487804878049,
      "grad_norm": 0.0879698097705841,
      "learning_rate": 4.5997375317175274e-05,
      "loss": 0.032,
      "step": 2247
    },
    {
      "epoch": 0.5482926829268293,
      "grad_norm": 0.08540213108062744,
      "learning_rate": 4.599390870823893e-05,
      "loss": 0.0234,
      "step": 2248
    },
    {
      "epoch": 0.5485365853658537,
      "grad_norm": 0.06323462724685669,
      "learning_rate": 4.599044072951433e-05,
      "loss": 0.013,
      "step": 2249
    },
    {
      "epoch": 0.5487804878048781,
      "grad_norm": 0.06891758739948273,
      "learning_rate": 4.5986971381227716e-05,
      "loss": 0.0189,
      "step": 2250
    },
    {
      "epoch": 0.5490243902439025,
      "grad_norm": 0.230922132730484,
      "learning_rate": 4.598350066360547e-05,
      "loss": 0.034,
      "step": 2251
    },
    {
      "epoch": 0.5492682926829269,
      "grad_norm": 0.1443430781364441,
      "learning_rate": 4.5980028576874045e-05,
      "loss": 0.0282,
      "step": 2252
    },
    {
      "epoch": 0.5495121951219513,
      "grad_norm": 0.10934348404407501,
      "learning_rate": 4.597655512125998e-05,
      "loss": 0.0196,
      "step": 2253
    },
    {
      "epoch": 0.5497560975609757,
      "grad_norm": 0.04816159978508949,
      "learning_rate": 4.597308029698991e-05,
      "loss": 0.0149,
      "step": 2254
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.14212708175182343,
      "learning_rate": 4.596960410429056e-05,
      "loss": 0.025,
      "step": 2255
    },
    {
      "epoch": 0.5502439024390244,
      "grad_norm": 0.10848482698202133,
      "learning_rate": 4.5966126543388723e-05,
      "loss": 0.0227,
      "step": 2256
    },
    {
      "epoch": 0.5504878048780488,
      "grad_norm": 0.05663327872753143,
      "learning_rate": 4.5962647614511325e-05,
      "loss": 0.0168,
      "step": 2257
    },
    {
      "epoch": 0.5507317073170732,
      "grad_norm": 0.08950848877429962,
      "learning_rate": 4.595916731788534e-05,
      "loss": 0.023,
      "step": 2258
    },
    {
      "epoch": 0.5509756097560976,
      "grad_norm": 0.12667657434940338,
      "learning_rate": 4.5955685653737843e-05,
      "loss": 0.0233,
      "step": 2259
    },
    {
      "epoch": 0.551219512195122,
      "grad_norm": 0.11229623109102249,
      "learning_rate": 4.595220262229601e-05,
      "loss": 0.0158,
      "step": 2260
    },
    {
      "epoch": 0.5514634146341464,
      "grad_norm": 0.09307607263326645,
      "learning_rate": 4.594871822378711e-05,
      "loss": 0.0187,
      "step": 2261
    },
    {
      "epoch": 0.5517073170731708,
      "grad_norm": 0.1309003382921219,
      "learning_rate": 4.594523245843847e-05,
      "loss": 0.0378,
      "step": 2262
    },
    {
      "epoch": 0.5519512195121952,
      "grad_norm": 0.0748634859919548,
      "learning_rate": 4.5941745326477524e-05,
      "loss": 0.0274,
      "step": 2263
    },
    {
      "epoch": 0.5521951219512196,
      "grad_norm": 0.08771297335624695,
      "learning_rate": 4.593825682813181e-05,
      "loss": 0.0386,
      "step": 2264
    },
    {
      "epoch": 0.552439024390244,
      "grad_norm": 0.06144571676850319,
      "learning_rate": 4.5934766963628936e-05,
      "loss": 0.0206,
      "step": 2265
    },
    {
      "epoch": 0.5526829268292683,
      "grad_norm": 0.08480913192033768,
      "learning_rate": 4.59312757331966e-05,
      "loss": 0.0282,
      "step": 2266
    },
    {
      "epoch": 0.5529268292682927,
      "grad_norm": 0.0961984395980835,
      "learning_rate": 4.592778313706261e-05,
      "loss": 0.0295,
      "step": 2267
    },
    {
      "epoch": 0.5531707317073171,
      "grad_norm": 0.06881128251552582,
      "learning_rate": 4.5924289175454825e-05,
      "loss": 0.0243,
      "step": 2268
    },
    {
      "epoch": 0.5534146341463415,
      "grad_norm": 0.0867084339261055,
      "learning_rate": 4.592079384860123e-05,
      "loss": 0.0134,
      "step": 2269
    },
    {
      "epoch": 0.5536585365853659,
      "grad_norm": 0.06728498637676239,
      "learning_rate": 4.591729715672988e-05,
      "loss": 0.0268,
      "step": 2270
    },
    {
      "epoch": 0.5539024390243903,
      "grad_norm": 0.07459589838981628,
      "learning_rate": 4.5913799100068924e-05,
      "loss": 0.0264,
      "step": 2271
    },
    {
      "epoch": 0.5541463414634147,
      "grad_norm": 0.2123943418264389,
      "learning_rate": 4.591029967884661e-05,
      "loss": 0.0447,
      "step": 2272
    },
    {
      "epoch": 0.5543902439024391,
      "grad_norm": 0.1163407564163208,
      "learning_rate": 4.5906798893291246e-05,
      "loss": 0.0346,
      "step": 2273
    },
    {
      "epoch": 0.5546341463414635,
      "grad_norm": 0.1105702593922615,
      "learning_rate": 4.590329674363126e-05,
      "loss": 0.0186,
      "step": 2274
    },
    {
      "epoch": 0.5548780487804879,
      "grad_norm": 0.15207290649414062,
      "learning_rate": 4.589979323009514e-05,
      "loss": 0.0125,
      "step": 2275
    },
    {
      "epoch": 0.5551219512195122,
      "grad_norm": 0.23283089697360992,
      "learning_rate": 4.589628835291151e-05,
      "loss": 0.0261,
      "step": 2276
    },
    {
      "epoch": 0.5553658536585366,
      "grad_norm": 0.11509841680526733,
      "learning_rate": 4.5892782112309026e-05,
      "loss": 0.0347,
      "step": 2277
    },
    {
      "epoch": 0.555609756097561,
      "grad_norm": 0.08827510476112366,
      "learning_rate": 4.5889274508516464e-05,
      "loss": 0.0274,
      "step": 2278
    },
    {
      "epoch": 0.5558536585365854,
      "grad_norm": 0.14642617106437683,
      "learning_rate": 4.588576554176269e-05,
      "loss": 0.0227,
      "step": 2279
    },
    {
      "epoch": 0.5560975609756098,
      "grad_norm": 0.23148897290229797,
      "learning_rate": 4.588225521227666e-05,
      "loss": 0.0248,
      "step": 2280
    },
    {
      "epoch": 0.5563414634146342,
      "grad_norm": 0.04619672894477844,
      "learning_rate": 4.58787435202874e-05,
      "loss": 0.0124,
      "step": 2281
    },
    {
      "epoch": 0.5565853658536586,
      "grad_norm": 0.06259119510650635,
      "learning_rate": 4.587523046602404e-05,
      "loss": 0.0141,
      "step": 2282
    },
    {
      "epoch": 0.556829268292683,
      "grad_norm": 0.13687992095947266,
      "learning_rate": 4.587171604971579e-05,
      "loss": 0.021,
      "step": 2283
    },
    {
      "epoch": 0.5570731707317074,
      "grad_norm": 0.14631372690200806,
      "learning_rate": 4.5868200271591976e-05,
      "loss": 0.0243,
      "step": 2284
    },
    {
      "epoch": 0.5573170731707318,
      "grad_norm": 0.13473892211914062,
      "learning_rate": 4.5864683131881974e-05,
      "loss": 0.0208,
      "step": 2285
    },
    {
      "epoch": 0.5575609756097561,
      "grad_norm": 0.16015537083148956,
      "learning_rate": 4.586116463081527e-05,
      "loss": 0.0341,
      "step": 2286
    },
    {
      "epoch": 0.5578048780487805,
      "grad_norm": 0.07230209559202194,
      "learning_rate": 4.5857644768621433e-05,
      "loss": 0.0183,
      "step": 2287
    },
    {
      "epoch": 0.5580487804878049,
      "grad_norm": 0.05799463018774986,
      "learning_rate": 4.585412354553013e-05,
      "loss": 0.0149,
      "step": 2288
    },
    {
      "epoch": 0.5582926829268293,
      "grad_norm": 0.2859099209308624,
      "learning_rate": 4.5850600961771106e-05,
      "loss": 0.0286,
      "step": 2289
    },
    {
      "epoch": 0.5585365853658537,
      "grad_norm": 0.14023399353027344,
      "learning_rate": 4.584707701757419e-05,
      "loss": 0.0264,
      "step": 2290
    },
    {
      "epoch": 0.5587804878048781,
      "grad_norm": 0.083562932908535,
      "learning_rate": 4.584355171316933e-05,
      "loss": 0.0307,
      "step": 2291
    },
    {
      "epoch": 0.5590243902439025,
      "grad_norm": 0.12971334159374237,
      "learning_rate": 4.584002504878652e-05,
      "loss": 0.0237,
      "step": 2292
    },
    {
      "epoch": 0.5592682926829269,
      "grad_norm": 0.08043120801448822,
      "learning_rate": 4.5836497024655875e-05,
      "loss": 0.026,
      "step": 2293
    },
    {
      "epoch": 0.5595121951219513,
      "grad_norm": 0.07125264406204224,
      "learning_rate": 4.5832967641007595e-05,
      "loss": 0.019,
      "step": 2294
    },
    {
      "epoch": 0.5597560975609757,
      "grad_norm": 0.17901970446109772,
      "learning_rate": 4.5829436898071945e-05,
      "loss": 0.0283,
      "step": 2295
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.14484041929244995,
      "learning_rate": 4.58259047960793e-05,
      "loss": 0.029,
      "step": 2296
    },
    {
      "epoch": 0.5602439024390244,
      "grad_norm": 0.09751410037279129,
      "learning_rate": 4.5822371335260124e-05,
      "loss": 0.0215,
      "step": 2297
    },
    {
      "epoch": 0.5604878048780488,
      "grad_norm": 0.09170465916395187,
      "learning_rate": 4.5818836515844956e-05,
      "loss": 0.0191,
      "step": 2298
    },
    {
      "epoch": 0.5607317073170732,
      "grad_norm": 0.05066576972603798,
      "learning_rate": 4.581530033806445e-05,
      "loss": 0.0157,
      "step": 2299
    },
    {
      "epoch": 0.5609756097560976,
      "grad_norm": 0.09080880880355835,
      "learning_rate": 4.5811762802149305e-05,
      "loss": 0.031,
      "step": 2300
    },
    {
      "epoch": 0.561219512195122,
      "grad_norm": 0.06855907291173935,
      "learning_rate": 4.580822390833036e-05,
      "loss": 0.0222,
      "step": 2301
    },
    {
      "epoch": 0.5614634146341464,
      "grad_norm": 0.07169223576784134,
      "learning_rate": 4.58046836568385e-05,
      "loss": 0.0107,
      "step": 2302
    },
    {
      "epoch": 0.5617073170731708,
      "grad_norm": 0.09186140447854996,
      "learning_rate": 4.580114204790472e-05,
      "loss": 0.0257,
      "step": 2303
    },
    {
      "epoch": 0.5619512195121952,
      "grad_norm": 0.12475664168596268,
      "learning_rate": 4.57975990817601e-05,
      "loss": 0.0289,
      "step": 2304
    },
    {
      "epoch": 0.5621951219512196,
      "grad_norm": 0.08997761458158493,
      "learning_rate": 4.5794054758635806e-05,
      "loss": 0.0358,
      "step": 2305
    },
    {
      "epoch": 0.562439024390244,
      "grad_norm": 0.3612936735153198,
      "learning_rate": 4.57905090787631e-05,
      "loss": 0.0165,
      "step": 2306
    },
    {
      "epoch": 0.5626829268292682,
      "grad_norm": 0.06607850641012192,
      "learning_rate": 4.5786962042373314e-05,
      "loss": 0.0155,
      "step": 2307
    },
    {
      "epoch": 0.5629268292682926,
      "grad_norm": 0.13219572603702545,
      "learning_rate": 4.57834136496979e-05,
      "loss": 0.0269,
      "step": 2308
    },
    {
      "epoch": 0.563170731707317,
      "grad_norm": 0.09666384756565094,
      "learning_rate": 4.577986390096837e-05,
      "loss": 0.0198,
      "step": 2309
    },
    {
      "epoch": 0.5634146341463414,
      "grad_norm": 0.12361585348844528,
      "learning_rate": 4.577631279641632e-05,
      "loss": 0.025,
      "step": 2310
    },
    {
      "epoch": 0.5636585365853658,
      "grad_norm": 0.12981173396110535,
      "learning_rate": 4.5772760336273476e-05,
      "loss": 0.0259,
      "step": 2311
    },
    {
      "epoch": 0.5639024390243902,
      "grad_norm": 0.08514000475406647,
      "learning_rate": 4.576920652077161e-05,
      "loss": 0.0249,
      "step": 2312
    },
    {
      "epoch": 0.5641463414634146,
      "grad_norm": 0.1801127940416336,
      "learning_rate": 4.576565135014259e-05,
      "loss": 0.0476,
      "step": 2313
    },
    {
      "epoch": 0.564390243902439,
      "grad_norm": 0.08266548812389374,
      "learning_rate": 4.576209482461839e-05,
      "loss": 0.0072,
      "step": 2314
    },
    {
      "epoch": 0.5646341463414634,
      "grad_norm": 0.13272717595100403,
      "learning_rate": 4.5758536944431075e-05,
      "loss": 0.028,
      "step": 2315
    },
    {
      "epoch": 0.5648780487804878,
      "grad_norm": 0.13995404541492462,
      "learning_rate": 4.5754977709812765e-05,
      "loss": 0.0258,
      "step": 2316
    },
    {
      "epoch": 0.5651219512195121,
      "grad_norm": 0.19365796446800232,
      "learning_rate": 4.575141712099569e-05,
      "loss": 0.0363,
      "step": 2317
    },
    {
      "epoch": 0.5653658536585365,
      "grad_norm": 0.1399799883365631,
      "learning_rate": 4.574785517821218e-05,
      "loss": 0.0322,
      "step": 2318
    },
    {
      "epoch": 0.5656097560975609,
      "grad_norm": 0.26547572016716003,
      "learning_rate": 4.574429188169464e-05,
      "loss": 0.0291,
      "step": 2319
    },
    {
      "epoch": 0.5658536585365853,
      "grad_norm": 0.32652217149734497,
      "learning_rate": 4.5740727231675554e-05,
      "loss": 0.0499,
      "step": 2320
    },
    {
      "epoch": 0.5660975609756097,
      "grad_norm": 0.0601353719830513,
      "learning_rate": 4.5737161228387506e-05,
      "loss": 0.0166,
      "step": 2321
    },
    {
      "epoch": 0.5663414634146341,
      "grad_norm": 0.09763084352016449,
      "learning_rate": 4.573359387206318e-05,
      "loss": 0.025,
      "step": 2322
    },
    {
      "epoch": 0.5665853658536585,
      "grad_norm": 0.08370570838451385,
      "learning_rate": 4.573002516293532e-05,
      "loss": 0.0254,
      "step": 2323
    },
    {
      "epoch": 0.5668292682926829,
      "grad_norm": 0.0537208691239357,
      "learning_rate": 4.5726455101236775e-05,
      "loss": 0.0162,
      "step": 2324
    },
    {
      "epoch": 0.5670731707317073,
      "grad_norm": 0.07783530652523041,
      "learning_rate": 4.572288368720048e-05,
      "loss": 0.0365,
      "step": 2325
    },
    {
      "epoch": 0.5673170731707317,
      "grad_norm": 0.05860377848148346,
      "learning_rate": 4.5719310921059475e-05,
      "loss": 0.0147,
      "step": 2326
    },
    {
      "epoch": 0.567560975609756,
      "grad_norm": 0.10003139823675156,
      "learning_rate": 4.571573680304686e-05,
      "loss": 0.0219,
      "step": 2327
    },
    {
      "epoch": 0.5678048780487804,
      "grad_norm": 0.08014297485351562,
      "learning_rate": 4.571216133339583e-05,
      "loss": 0.0141,
      "step": 2328
    },
    {
      "epoch": 0.5680487804878048,
      "grad_norm": 0.06059807166457176,
      "learning_rate": 4.5708584512339683e-05,
      "loss": 0.0151,
      "step": 2329
    },
    {
      "epoch": 0.5682926829268292,
      "grad_norm": 0.07935824245214462,
      "learning_rate": 4.570500634011178e-05,
      "loss": 0.0175,
      "step": 2330
    },
    {
      "epoch": 0.5685365853658536,
      "grad_norm": 0.06255754828453064,
      "learning_rate": 4.570142681694561e-05,
      "loss": 0.0082,
      "step": 2331
    },
    {
      "epoch": 0.568780487804878,
      "grad_norm": 0.1330605149269104,
      "learning_rate": 4.569784594307471e-05,
      "loss": 0.0276,
      "step": 2332
    },
    {
      "epoch": 0.5690243902439024,
      "grad_norm": 0.10267077386379242,
      "learning_rate": 4.5694263718732724e-05,
      "loss": 0.0308,
      "step": 2333
    },
    {
      "epoch": 0.5692682926829268,
      "grad_norm": 0.09618613868951797,
      "learning_rate": 4.5690680144153384e-05,
      "loss": 0.013,
      "step": 2334
    },
    {
      "epoch": 0.5695121951219512,
      "grad_norm": 0.07141481339931488,
      "learning_rate": 4.568709521957051e-05,
      "loss": 0.0163,
      "step": 2335
    },
    {
      "epoch": 0.5697560975609756,
      "grad_norm": 0.07110710442066193,
      "learning_rate": 4.568350894521799e-05,
      "loss": 0.0102,
      "step": 2336
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.13438652455806732,
      "learning_rate": 4.567992132132983e-05,
      "loss": 0.0265,
      "step": 2337
    },
    {
      "epoch": 0.5702439024390243,
      "grad_norm": 0.11873634904623032,
      "learning_rate": 4.567633234814012e-05,
      "loss": 0.0253,
      "step": 2338
    },
    {
      "epoch": 0.5704878048780487,
      "grad_norm": 0.07559461891651154,
      "learning_rate": 4.567274202588301e-05,
      "loss": 0.0217,
      "step": 2339
    },
    {
      "epoch": 0.5707317073170731,
      "grad_norm": 0.06122070923447609,
      "learning_rate": 4.566915035479278e-05,
      "loss": 0.0158,
      "step": 2340
    },
    {
      "epoch": 0.5709756097560975,
      "grad_norm": 0.045358192175626755,
      "learning_rate": 4.566555733510376e-05,
      "loss": 0.0123,
      "step": 2341
    },
    {
      "epoch": 0.5712195121951219,
      "grad_norm": 0.11785534769296646,
      "learning_rate": 4.566196296705038e-05,
      "loss": 0.018,
      "step": 2342
    },
    {
      "epoch": 0.5714634146341463,
      "grad_norm": 0.08961819857358932,
      "learning_rate": 4.565836725086717e-05,
      "loss": 0.0275,
      "step": 2343
    },
    {
      "epoch": 0.5717073170731707,
      "grad_norm": 0.13568423688411713,
      "learning_rate": 4.565477018678875e-05,
      "loss": 0.0401,
      "step": 2344
    },
    {
      "epoch": 0.5719512195121951,
      "grad_norm": 0.12657326459884644,
      "learning_rate": 4.565117177504979e-05,
      "loss": 0.0448,
      "step": 2345
    },
    {
      "epoch": 0.5721951219512195,
      "grad_norm": 0.09806042909622192,
      "learning_rate": 4.56475720158851e-05,
      "loss": 0.0295,
      "step": 2346
    },
    {
      "epoch": 0.5724390243902439,
      "grad_norm": 0.1947782039642334,
      "learning_rate": 4.564397090952954e-05,
      "loss": 0.0506,
      "step": 2347
    },
    {
      "epoch": 0.5726829268292682,
      "grad_norm": 0.057144880294799805,
      "learning_rate": 4.564036845621809e-05,
      "loss": 0.0164,
      "step": 2348
    },
    {
      "epoch": 0.5729268292682926,
      "grad_norm": 0.25228244066238403,
      "learning_rate": 4.563676465618577e-05,
      "loss": 0.0268,
      "step": 2349
    },
    {
      "epoch": 0.573170731707317,
      "grad_norm": 0.13540510833263397,
      "learning_rate": 4.563315950966773e-05,
      "loss": 0.0351,
      "step": 2350
    },
    {
      "epoch": 0.5734146341463414,
      "grad_norm": 0.10701654106378555,
      "learning_rate": 4.562955301689921e-05,
      "loss": 0.0271,
      "step": 2351
    },
    {
      "epoch": 0.5736585365853658,
      "grad_norm": 0.18510356545448303,
      "learning_rate": 4.5625945178115494e-05,
      "loss": 0.039,
      "step": 2352
    },
    {
      "epoch": 0.5739024390243902,
      "grad_norm": 0.050573330372571945,
      "learning_rate": 4.5622335993552e-05,
      "loss": 0.0193,
      "step": 2353
    },
    {
      "epoch": 0.5741463414634146,
      "grad_norm": 0.06896176189184189,
      "learning_rate": 4.5618725463444224e-05,
      "loss": 0.0159,
      "step": 2354
    },
    {
      "epoch": 0.574390243902439,
      "grad_norm": 0.13846656680107117,
      "learning_rate": 4.561511358802771e-05,
      "loss": 0.0353,
      "step": 2355
    },
    {
      "epoch": 0.5746341463414634,
      "grad_norm": 0.09738145023584366,
      "learning_rate": 4.561150036753816e-05,
      "loss": 0.0317,
      "step": 2356
    },
    {
      "epoch": 0.5748780487804878,
      "grad_norm": 0.07598548382520676,
      "learning_rate": 4.560788580221131e-05,
      "loss": 0.0261,
      "step": 2357
    },
    {
      "epoch": 0.5751219512195122,
      "grad_norm": 0.06829703599214554,
      "learning_rate": 4.5604269892282994e-05,
      "loss": 0.0195,
      "step": 2358
    },
    {
      "epoch": 0.5753658536585365,
      "grad_norm": 0.08382679522037506,
      "learning_rate": 4.560065263798915e-05,
      "loss": 0.0327,
      "step": 2359
    },
    {
      "epoch": 0.5756097560975609,
      "grad_norm": 0.06279656291007996,
      "learning_rate": 4.559703403956577e-05,
      "loss": 0.0173,
      "step": 2360
    },
    {
      "epoch": 0.5758536585365853,
      "grad_norm": 0.13262559473514557,
      "learning_rate": 4.5593414097248986e-05,
      "loss": 0.0192,
      "step": 2361
    },
    {
      "epoch": 0.5760975609756097,
      "grad_norm": 0.1293351650238037,
      "learning_rate": 4.558979281127497e-05,
      "loss": 0.0304,
      "step": 2362
    },
    {
      "epoch": 0.5763414634146341,
      "grad_norm": 0.13375848531723022,
      "learning_rate": 4.558617018188001e-05,
      "loss": 0.0174,
      "step": 2363
    },
    {
      "epoch": 0.5765853658536585,
      "grad_norm": 0.04649058356881142,
      "learning_rate": 4.558254620930045e-05,
      "loss": 0.0103,
      "step": 2364
    },
    {
      "epoch": 0.5768292682926829,
      "grad_norm": 0.11347890645265579,
      "learning_rate": 4.5578920893772774e-05,
      "loss": 0.0413,
      "step": 2365
    },
    {
      "epoch": 0.5770731707317073,
      "grad_norm": 0.20667119324207306,
      "learning_rate": 4.5575294235533504e-05,
      "loss": 0.0266,
      "step": 2366
    },
    {
      "epoch": 0.5773170731707317,
      "grad_norm": 0.10554724186658859,
      "learning_rate": 4.5571666234819274e-05,
      "loss": 0.0362,
      "step": 2367
    },
    {
      "epoch": 0.577560975609756,
      "grad_norm": 0.15129229426383972,
      "learning_rate": 4.5568036891866794e-05,
      "loss": 0.0502,
      "step": 2368
    },
    {
      "epoch": 0.5778048780487804,
      "grad_norm": 0.054140083491802216,
      "learning_rate": 4.5564406206912866e-05,
      "loss": 0.0141,
      "step": 2369
    },
    {
      "epoch": 0.5780487804878048,
      "grad_norm": 0.10369285941123962,
      "learning_rate": 4.5560774180194396e-05,
      "loss": 0.0198,
      "step": 2370
    },
    {
      "epoch": 0.5782926829268292,
      "grad_norm": 0.21025902032852173,
      "learning_rate": 4.555714081194835e-05,
      "loss": 0.0351,
      "step": 2371
    },
    {
      "epoch": 0.5785365853658536,
      "grad_norm": 0.19022983312606812,
      "learning_rate": 4.5553506102411806e-05,
      "loss": 0.0294,
      "step": 2372
    },
    {
      "epoch": 0.578780487804878,
      "grad_norm": 0.07874516397714615,
      "learning_rate": 4.5549870051821896e-05,
      "loss": 0.025,
      "step": 2373
    },
    {
      "epoch": 0.5790243902439024,
      "grad_norm": 0.08342819660902023,
      "learning_rate": 4.5546232660415884e-05,
      "loss": 0.0224,
      "step": 2374
    },
    {
      "epoch": 0.5792682926829268,
      "grad_norm": 0.11378549039363861,
      "learning_rate": 4.5542593928431086e-05,
      "loss": 0.0189,
      "step": 2375
    },
    {
      "epoch": 0.5795121951219512,
      "grad_norm": 0.2914466857910156,
      "learning_rate": 4.553895385610492e-05,
      "loss": 0.0296,
      "step": 2376
    },
    {
      "epoch": 0.5797560975609756,
      "grad_norm": 0.0812133252620697,
      "learning_rate": 4.553531244367489e-05,
      "loss": 0.0278,
      "step": 2377
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.3359367847442627,
      "learning_rate": 4.5531669691378587e-05,
      "loss": 0.0466,
      "step": 2378
    },
    {
      "epoch": 0.5802439024390244,
      "grad_norm": 0.08450967818498611,
      "learning_rate": 4.55280255994537e-05,
      "loss": 0.0238,
      "step": 2379
    },
    {
      "epoch": 0.5804878048780487,
      "grad_norm": 0.08416763693094254,
      "learning_rate": 4.552438016813797e-05,
      "loss": 0.0228,
      "step": 2380
    },
    {
      "epoch": 0.5807317073170731,
      "grad_norm": 0.07356517761945724,
      "learning_rate": 4.5520733397669266e-05,
      "loss": 0.0192,
      "step": 2381
    },
    {
      "epoch": 0.5809756097560975,
      "grad_norm": 0.2304045855998993,
      "learning_rate": 4.551708528828555e-05,
      "loss": 0.0299,
      "step": 2382
    },
    {
      "epoch": 0.5812195121951219,
      "grad_norm": 0.12562263011932373,
      "learning_rate": 4.55134358402248e-05,
      "loss": 0.0285,
      "step": 2383
    },
    {
      "epoch": 0.5814634146341463,
      "grad_norm": 0.07150665670633316,
      "learning_rate": 4.5509785053725174e-05,
      "loss": 0.0205,
      "step": 2384
    },
    {
      "epoch": 0.5817073170731707,
      "grad_norm": 0.05922887101769447,
      "learning_rate": 4.550613292902486e-05,
      "loss": 0.0235,
      "step": 2385
    },
    {
      "epoch": 0.5819512195121951,
      "grad_norm": 0.056699302047491074,
      "learning_rate": 4.5502479466362134e-05,
      "loss": 0.0189,
      "step": 2386
    },
    {
      "epoch": 0.5821951219512195,
      "grad_norm": 0.09882400184869766,
      "learning_rate": 4.5498824665975404e-05,
      "loss": 0.0159,
      "step": 2387
    },
    {
      "epoch": 0.5824390243902439,
      "grad_norm": 0.19442792236804962,
      "learning_rate": 4.5495168528103105e-05,
      "loss": 0.0279,
      "step": 2388
    },
    {
      "epoch": 0.5826829268292683,
      "grad_norm": 0.10417953878641129,
      "learning_rate": 4.549151105298381e-05,
      "loss": 0.0158,
      "step": 2389
    },
    {
      "epoch": 0.5829268292682926,
      "grad_norm": 0.17339174449443817,
      "learning_rate": 4.548785224085615e-05,
      "loss": 0.0295,
      "step": 2390
    },
    {
      "epoch": 0.583170731707317,
      "grad_norm": 0.27876991033554077,
      "learning_rate": 4.548419209195884e-05,
      "loss": 0.0327,
      "step": 2391
    },
    {
      "epoch": 0.5834146341463414,
      "grad_norm": 0.09331878274679184,
      "learning_rate": 4.5480530606530717e-05,
      "loss": 0.0248,
      "step": 2392
    },
    {
      "epoch": 0.5836585365853658,
      "grad_norm": 0.27187132835388184,
      "learning_rate": 4.547686778481066e-05,
      "loss": 0.0273,
      "step": 2393
    },
    {
      "epoch": 0.5839024390243902,
      "grad_norm": 0.06319154053926468,
      "learning_rate": 4.5473203627037666e-05,
      "loss": 0.0136,
      "step": 2394
    },
    {
      "epoch": 0.5841463414634146,
      "grad_norm": 0.13620585203170776,
      "learning_rate": 4.546953813345081e-05,
      "loss": 0.0443,
      "step": 2395
    },
    {
      "epoch": 0.584390243902439,
      "grad_norm": 0.11707046627998352,
      "learning_rate": 4.5465871304289256e-05,
      "loss": 0.0372,
      "step": 2396
    },
    {
      "epoch": 0.5846341463414634,
      "grad_norm": 0.11191399395465851,
      "learning_rate": 4.546220313979225e-05,
      "loss": 0.0337,
      "step": 2397
    },
    {
      "epoch": 0.5848780487804878,
      "grad_norm": 0.06561297178268433,
      "learning_rate": 4.545853364019914e-05,
      "loss": 0.025,
      "step": 2398
    },
    {
      "epoch": 0.5851219512195122,
      "grad_norm": 0.1427825391292572,
      "learning_rate": 4.545486280574933e-05,
      "loss": 0.0215,
      "step": 2399
    },
    {
      "epoch": 0.5853658536585366,
      "grad_norm": 0.27884146571159363,
      "learning_rate": 4.545119063668234e-05,
      "loss": 0.028,
      "step": 2400
    },
    {
      "epoch": 0.5856097560975609,
      "grad_norm": 0.12874658405780792,
      "learning_rate": 4.544751713323777e-05,
      "loss": 0.03,
      "step": 2401
    },
    {
      "epoch": 0.5858536585365853,
      "grad_norm": 0.08490384370088577,
      "learning_rate": 4.54438422956553e-05,
      "loss": 0.0235,
      "step": 2402
    },
    {
      "epoch": 0.5860975609756097,
      "grad_norm": 0.09942396730184555,
      "learning_rate": 4.544016612417471e-05,
      "loss": 0.0213,
      "step": 2403
    },
    {
      "epoch": 0.5863414634146341,
      "grad_norm": 0.06794010102748871,
      "learning_rate": 4.543648861903585e-05,
      "loss": 0.0241,
      "step": 2404
    },
    {
      "epoch": 0.5865853658536585,
      "grad_norm": 0.09413854777812958,
      "learning_rate": 4.543280978047867e-05,
      "loss": 0.0151,
      "step": 2405
    },
    {
      "epoch": 0.5868292682926829,
      "grad_norm": 0.11072945594787598,
      "learning_rate": 4.542912960874321e-05,
      "loss": 0.02,
      "step": 2406
    },
    {
      "epoch": 0.5870731707317073,
      "grad_norm": 0.05346957966685295,
      "learning_rate": 4.542544810406957e-05,
      "loss": 0.0228,
      "step": 2407
    },
    {
      "epoch": 0.5873170731707317,
      "grad_norm": 0.13154885172843933,
      "learning_rate": 4.542176526669797e-05,
      "loss": 0.0338,
      "step": 2408
    },
    {
      "epoch": 0.5875609756097561,
      "grad_norm": 0.07015088945627213,
      "learning_rate": 4.541808109686871e-05,
      "loss": 0.0209,
      "step": 2409
    },
    {
      "epoch": 0.5878048780487805,
      "grad_norm": 0.13516810536384583,
      "learning_rate": 4.541439559482217e-05,
      "loss": 0.0268,
      "step": 2410
    },
    {
      "epoch": 0.5880487804878048,
      "grad_norm": 0.07595551013946533,
      "learning_rate": 4.5410708760798804e-05,
      "loss": 0.0131,
      "step": 2411
    },
    {
      "epoch": 0.5882926829268292,
      "grad_norm": 0.06680954992771149,
      "learning_rate": 4.540702059503917e-05,
      "loss": 0.0248,
      "step": 2412
    },
    {
      "epoch": 0.5885365853658536,
      "grad_norm": 0.15391625463962555,
      "learning_rate": 4.540333109778392e-05,
      "loss": 0.0155,
      "step": 2413
    },
    {
      "epoch": 0.588780487804878,
      "grad_norm": 0.08171463012695312,
      "learning_rate": 4.539964026927378e-05,
      "loss": 0.0177,
      "step": 2414
    },
    {
      "epoch": 0.5890243902439024,
      "grad_norm": 0.07227716594934464,
      "learning_rate": 4.539594810974955e-05,
      "loss": 0.0204,
      "step": 2415
    },
    {
      "epoch": 0.5892682926829268,
      "grad_norm": 0.12847279012203217,
      "learning_rate": 4.539225461945216e-05,
      "loss": 0.014,
      "step": 2416
    },
    {
      "epoch": 0.5895121951219512,
      "grad_norm": 0.2253817915916443,
      "learning_rate": 4.5388559798622564e-05,
      "loss": 0.0469,
      "step": 2417
    },
    {
      "epoch": 0.5897560975609756,
      "grad_norm": 0.06625636667013168,
      "learning_rate": 4.5384863647501874e-05,
      "loss": 0.0259,
      "step": 2418
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.0729508101940155,
      "learning_rate": 4.538116616633122e-05,
      "loss": 0.0362,
      "step": 2419
    },
    {
      "epoch": 0.5902439024390244,
      "grad_norm": 0.2346467673778534,
      "learning_rate": 4.537746735535188e-05,
      "loss": 0.0298,
      "step": 2420
    },
    {
      "epoch": 0.5904878048780487,
      "grad_norm": 0.17021548748016357,
      "learning_rate": 4.537376721480517e-05,
      "loss": 0.0403,
      "step": 2421
    },
    {
      "epoch": 0.5907317073170731,
      "grad_norm": 0.04495996981859207,
      "learning_rate": 4.537006574493252e-05,
      "loss": 0.0145,
      "step": 2422
    },
    {
      "epoch": 0.5909756097560975,
      "grad_norm": 0.051194414496421814,
      "learning_rate": 4.536636294597544e-05,
      "loss": 0.0165,
      "step": 2423
    },
    {
      "epoch": 0.5912195121951219,
      "grad_norm": 0.22976912558078766,
      "learning_rate": 4.536265881817552e-05,
      "loss": 0.0413,
      "step": 2424
    },
    {
      "epoch": 0.5914634146341463,
      "grad_norm": 0.08990532159805298,
      "learning_rate": 4.5358953361774455e-05,
      "loss": 0.0198,
      "step": 2425
    },
    {
      "epoch": 0.5917073170731707,
      "grad_norm": 0.20318053662776947,
      "learning_rate": 4.5355246577014004e-05,
      "loss": 0.0187,
      "step": 2426
    },
    {
      "epoch": 0.5919512195121951,
      "grad_norm": 0.1652327924966812,
      "learning_rate": 4.535153846413603e-05,
      "loss": 0.0311,
      "step": 2427
    },
    {
      "epoch": 0.5921951219512195,
      "grad_norm": 0.2553219199180603,
      "learning_rate": 4.534782902338247e-05,
      "loss": 0.0256,
      "step": 2428
    },
    {
      "epoch": 0.5924390243902439,
      "grad_norm": 0.15524066984653473,
      "learning_rate": 4.534411825499536e-05,
      "loss": 0.0223,
      "step": 2429
    },
    {
      "epoch": 0.5926829268292683,
      "grad_norm": 0.22583121061325073,
      "learning_rate": 4.534040615921681e-05,
      "loss": 0.03,
      "step": 2430
    },
    {
      "epoch": 0.5929268292682927,
      "grad_norm": 0.06964588910341263,
      "learning_rate": 4.5336692736289035e-05,
      "loss": 0.0259,
      "step": 2431
    },
    {
      "epoch": 0.593170731707317,
      "grad_norm": 0.07056163251399994,
      "learning_rate": 4.5332977986454304e-05,
      "loss": 0.0246,
      "step": 2432
    },
    {
      "epoch": 0.5934146341463414,
      "grad_norm": 0.10285711288452148,
      "learning_rate": 4.5329261909955006e-05,
      "loss": 0.0328,
      "step": 2433
    },
    {
      "epoch": 0.5936585365853658,
      "grad_norm": 0.33232614398002625,
      "learning_rate": 4.53255445070336e-05,
      "loss": 0.0255,
      "step": 2434
    },
    {
      "epoch": 0.5939024390243902,
      "grad_norm": 0.11402502655982971,
      "learning_rate": 4.532182577793265e-05,
      "loss": 0.0245,
      "step": 2435
    },
    {
      "epoch": 0.5941463414634146,
      "grad_norm": 0.10326194018125534,
      "learning_rate": 4.531810572289477e-05,
      "loss": 0.0287,
      "step": 2436
    },
    {
      "epoch": 0.594390243902439,
      "grad_norm": 0.08432629704475403,
      "learning_rate": 4.531438434216269e-05,
      "loss": 0.0162,
      "step": 2437
    },
    {
      "epoch": 0.5946341463414634,
      "grad_norm": 0.22487516701221466,
      "learning_rate": 4.5310661635979225e-05,
      "loss": 0.0233,
      "step": 2438
    },
    {
      "epoch": 0.5948780487804878,
      "grad_norm": 0.13121947646141052,
      "learning_rate": 4.530693760458726e-05,
      "loss": 0.0229,
      "step": 2439
    },
    {
      "epoch": 0.5951219512195122,
      "grad_norm": 0.19495820999145508,
      "learning_rate": 4.5303212248229785e-05,
      "loss": 0.0304,
      "step": 2440
    },
    {
      "epoch": 0.5953658536585366,
      "grad_norm": 0.22580061852931976,
      "learning_rate": 4.529948556714987e-05,
      "loss": 0.0283,
      "step": 2441
    },
    {
      "epoch": 0.595609756097561,
      "grad_norm": 0.07491092383861542,
      "learning_rate": 4.529575756159066e-05,
      "loss": 0.0117,
      "step": 2442
    },
    {
      "epoch": 0.5958536585365853,
      "grad_norm": 0.10453334450721741,
      "learning_rate": 4.52920282317954e-05,
      "loss": 0.0238,
      "step": 2443
    },
    {
      "epoch": 0.5960975609756097,
      "grad_norm": 0.1544305980205536,
      "learning_rate": 4.528829757800741e-05,
      "loss": 0.0207,
      "step": 2444
    },
    {
      "epoch": 0.5963414634146341,
      "grad_norm": 0.1296950727701187,
      "learning_rate": 4.528456560047012e-05,
      "loss": 0.0174,
      "step": 2445
    },
    {
      "epoch": 0.5965853658536585,
      "grad_norm": 0.13866233825683594,
      "learning_rate": 4.5280832299427024e-05,
      "loss": 0.0309,
      "step": 2446
    },
    {
      "epoch": 0.5968292682926829,
      "grad_norm": 0.191437229514122,
      "learning_rate": 4.527709767512171e-05,
      "loss": 0.0321,
      "step": 2447
    },
    {
      "epoch": 0.5970731707317073,
      "grad_norm": 0.1608213186264038,
      "learning_rate": 4.527336172779783e-05,
      "loss": 0.0419,
      "step": 2448
    },
    {
      "epoch": 0.5973170731707317,
      "grad_norm": 0.1673319786787033,
      "learning_rate": 4.526962445769917e-05,
      "loss": 0.0193,
      "step": 2449
    },
    {
      "epoch": 0.5975609756097561,
      "grad_norm": 0.09813437610864639,
      "learning_rate": 4.526588586506956e-05,
      "loss": 0.0213,
      "step": 2450
    },
    {
      "epoch": 0.5978048780487805,
      "grad_norm": 0.12001455575227737,
      "learning_rate": 4.5262145950152945e-05,
      "loss": 0.0212,
      "step": 2451
    },
    {
      "epoch": 0.5980487804878049,
      "grad_norm": 0.22353370487689972,
      "learning_rate": 4.525840471319333e-05,
      "loss": 0.0304,
      "step": 2452
    },
    {
      "epoch": 0.5982926829268292,
      "grad_norm": 0.12086055427789688,
      "learning_rate": 4.525466215443483e-05,
      "loss": 0.0149,
      "step": 2453
    },
    {
      "epoch": 0.5985365853658536,
      "grad_norm": 0.09716174751520157,
      "learning_rate": 4.525091827412162e-05,
      "loss": 0.0184,
      "step": 2454
    },
    {
      "epoch": 0.598780487804878,
      "grad_norm": 0.12701916694641113,
      "learning_rate": 4.524717307249799e-05,
      "loss": 0.0195,
      "step": 2455
    },
    {
      "epoch": 0.5990243902439024,
      "grad_norm": 0.06670814007520676,
      "learning_rate": 4.524342654980831e-05,
      "loss": 0.0295,
      "step": 2456
    },
    {
      "epoch": 0.5992682926829268,
      "grad_norm": 0.14202271401882172,
      "learning_rate": 4.5239678706297007e-05,
      "loss": 0.0166,
      "step": 2457
    },
    {
      "epoch": 0.5995121951219512,
      "grad_norm": 0.17139452695846558,
      "learning_rate": 4.523592954220863e-05,
      "loss": 0.0275,
      "step": 2458
    },
    {
      "epoch": 0.5997560975609756,
      "grad_norm": 0.07297323644161224,
      "learning_rate": 4.5232179057787806e-05,
      "loss": 0.0215,
      "step": 2459
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.1044456958770752,
      "learning_rate": 4.5228427253279225e-05,
      "loss": 0.0252,
      "step": 2460
    },
    {
      "epoch": 0.6002439024390244,
      "grad_norm": 0.22457018494606018,
      "learning_rate": 4.52246741289277e-05,
      "loss": 0.0362,
      "step": 2461
    },
    {
      "epoch": 0.6004878048780488,
      "grad_norm": 0.13225127756595612,
      "learning_rate": 4.52209196849781e-05,
      "loss": 0.0422,
      "step": 2462
    },
    {
      "epoch": 0.6007317073170731,
      "grad_norm": 0.09015771746635437,
      "learning_rate": 4.5217163921675385e-05,
      "loss": 0.029,
      "step": 2463
    },
    {
      "epoch": 0.6009756097560975,
      "grad_norm": 0.1424359530210495,
      "learning_rate": 4.5213406839264625e-05,
      "loss": 0.0247,
      "step": 2464
    },
    {
      "epoch": 0.6012195121951219,
      "grad_norm": 0.17565703392028809,
      "learning_rate": 4.520964843799094e-05,
      "loss": 0.0267,
      "step": 2465
    },
    {
      "epoch": 0.6014634146341463,
      "grad_norm": 0.09913894534111023,
      "learning_rate": 4.5205888718099565e-05,
      "loss": 0.0248,
      "step": 2466
    },
    {
      "epoch": 0.6017073170731707,
      "grad_norm": 0.08222576975822449,
      "learning_rate": 4.5202127679835805e-05,
      "loss": 0.0243,
      "step": 2467
    },
    {
      "epoch": 0.6019512195121951,
      "grad_norm": 0.09820651262998581,
      "learning_rate": 4.519836532344506e-05,
      "loss": 0.0355,
      "step": 2468
    },
    {
      "epoch": 0.6021951219512195,
      "grad_norm": 0.06566977500915527,
      "learning_rate": 4.519460164917281e-05,
      "loss": 0.0304,
      "step": 2469
    },
    {
      "epoch": 0.6024390243902439,
      "grad_norm": 0.08682214468717575,
      "learning_rate": 4.519083665726463e-05,
      "loss": 0.034,
      "step": 2470
    },
    {
      "epoch": 0.6026829268292683,
      "grad_norm": 0.07853454351425171,
      "learning_rate": 4.518707034796616e-05,
      "loss": 0.0286,
      "step": 2471
    },
    {
      "epoch": 0.6029268292682927,
      "grad_norm": 0.098074771463871,
      "learning_rate": 4.518330272152315e-05,
      "loss": 0.0218,
      "step": 2472
    },
    {
      "epoch": 0.603170731707317,
      "grad_norm": 0.06617078185081482,
      "learning_rate": 4.517953377818143e-05,
      "loss": 0.0358,
      "step": 2473
    },
    {
      "epoch": 0.6034146341463414,
      "grad_norm": 0.22717037796974182,
      "learning_rate": 4.51757635181869e-05,
      "loss": 0.0268,
      "step": 2474
    },
    {
      "epoch": 0.6036585365853658,
      "grad_norm": 0.09757610410451889,
      "learning_rate": 4.5171991941785565e-05,
      "loss": 0.0296,
      "step": 2475
    },
    {
      "epoch": 0.6039024390243902,
      "grad_norm": 0.09524326026439667,
      "learning_rate": 4.5168219049223515e-05,
      "loss": 0.0263,
      "step": 2476
    },
    {
      "epoch": 0.6041463414634146,
      "grad_norm": 0.0689486414194107,
      "learning_rate": 4.516444484074691e-05,
      "loss": 0.0309,
      "step": 2477
    },
    {
      "epoch": 0.604390243902439,
      "grad_norm": 0.0630028247833252,
      "learning_rate": 4.5160669316602e-05,
      "loss": 0.0242,
      "step": 2478
    },
    {
      "epoch": 0.6046341463414634,
      "grad_norm": 0.2868584096431732,
      "learning_rate": 4.5156892477035145e-05,
      "loss": 0.0293,
      "step": 2479
    },
    {
      "epoch": 0.6048780487804878,
      "grad_norm": 0.12669824063777924,
      "learning_rate": 4.515311432229276e-05,
      "loss": 0.0267,
      "step": 2480
    },
    {
      "epoch": 0.6051219512195122,
      "grad_norm": 0.04653724282979965,
      "learning_rate": 4.5149334852621364e-05,
      "loss": 0.0164,
      "step": 2481
    },
    {
      "epoch": 0.6053658536585366,
      "grad_norm": 0.07319501787424088,
      "learning_rate": 4.514555406826754e-05,
      "loss": 0.0211,
      "step": 2482
    },
    {
      "epoch": 0.605609756097561,
      "grad_norm": 0.109773650765419,
      "learning_rate": 4.514177196947799e-05,
      "loss": 0.0433,
      "step": 2483
    },
    {
      "epoch": 0.6058536585365853,
      "grad_norm": 0.07353310286998749,
      "learning_rate": 4.513798855649948e-05,
      "loss": 0.0284,
      "step": 2484
    },
    {
      "epoch": 0.6060975609756097,
      "grad_norm": 0.10944069921970367,
      "learning_rate": 4.513420382957887e-05,
      "loss": 0.0288,
      "step": 2485
    },
    {
      "epoch": 0.6063414634146341,
      "grad_norm": 0.08408980071544647,
      "learning_rate": 4.513041778896309e-05,
      "loss": 0.0188,
      "step": 2486
    },
    {
      "epoch": 0.6065853658536585,
      "grad_norm": 0.20152024924755096,
      "learning_rate": 4.512663043489918e-05,
      "loss": 0.0385,
      "step": 2487
    },
    {
      "epoch": 0.6068292682926829,
      "grad_norm": 0.11292234063148499,
      "learning_rate": 4.512284176763424e-05,
      "loss": 0.0341,
      "step": 2488
    },
    {
      "epoch": 0.6070731707317073,
      "grad_norm": 0.059175897389650345,
      "learning_rate": 4.511905178741548e-05,
      "loss": 0.0173,
      "step": 2489
    },
    {
      "epoch": 0.6073170731707317,
      "grad_norm": 0.11307670921087265,
      "learning_rate": 4.511526049449018e-05,
      "loss": 0.0214,
      "step": 2490
    },
    {
      "epoch": 0.6075609756097561,
      "grad_norm": 0.11993318796157837,
      "learning_rate": 4.511146788910571e-05,
      "loss": 0.0341,
      "step": 2491
    },
    {
      "epoch": 0.6078048780487805,
      "grad_norm": 0.14058448374271393,
      "learning_rate": 4.5107673971509536e-05,
      "loss": 0.0148,
      "step": 2492
    },
    {
      "epoch": 0.6080487804878049,
      "grad_norm": 0.03952505439519882,
      "learning_rate": 4.510387874194918e-05,
      "loss": 0.0125,
      "step": 2493
    },
    {
      "epoch": 0.6082926829268293,
      "grad_norm": 0.11326964944601059,
      "learning_rate": 4.5100082200672286e-05,
      "loss": 0.024,
      "step": 2494
    },
    {
      "epoch": 0.6085365853658536,
      "grad_norm": 0.10949347168207169,
      "learning_rate": 4.509628434792656e-05,
      "loss": 0.016,
      "step": 2495
    },
    {
      "epoch": 0.608780487804878,
      "grad_norm": 0.13590764999389648,
      "learning_rate": 4.50924851839598e-05,
      "loss": 0.0212,
      "step": 2496
    },
    {
      "epoch": 0.6090243902439024,
      "grad_norm": 0.05296959728002548,
      "learning_rate": 4.508868470901989e-05,
      "loss": 0.0185,
      "step": 2497
    },
    {
      "epoch": 0.6092682926829268,
      "grad_norm": 0.060758233070373535,
      "learning_rate": 4.508488292335481e-05,
      "loss": 0.0134,
      "step": 2498
    },
    {
      "epoch": 0.6095121951219512,
      "grad_norm": 0.13882379233837128,
      "learning_rate": 4.5081079827212593e-05,
      "loss": 0.0139,
      "step": 2499
    },
    {
      "epoch": 0.6097560975609756,
      "grad_norm": 0.10028313100337982,
      "learning_rate": 4.507727542084139e-05,
      "loss": 0.0407,
      "step": 2500
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.13933426141738892,
      "learning_rate": 4.5073469704489435e-05,
      "loss": 0.0422,
      "step": 2501
    },
    {
      "epoch": 0.6102439024390244,
      "grad_norm": 0.08366963267326355,
      "learning_rate": 4.5069662678405034e-05,
      "loss": 0.0104,
      "step": 2502
    },
    {
      "epoch": 0.6104878048780488,
      "grad_norm": 0.26644667983055115,
      "learning_rate": 4.506585434283658e-05,
      "loss": 0.026,
      "step": 2503
    },
    {
      "epoch": 0.6107317073170732,
      "grad_norm": 0.08988336473703384,
      "learning_rate": 4.506204469803256e-05,
      "loss": 0.0291,
      "step": 2504
    },
    {
      "epoch": 0.6109756097560975,
      "grad_norm": 0.16748110949993134,
      "learning_rate": 4.505823374424153e-05,
      "loss": 0.0156,
      "step": 2505
    },
    {
      "epoch": 0.6112195121951219,
      "grad_norm": 0.10009809583425522,
      "learning_rate": 4.505442148171216e-05,
      "loss": 0.0223,
      "step": 2506
    },
    {
      "epoch": 0.6114634146341463,
      "grad_norm": 0.09150601178407669,
      "learning_rate": 4.5050607910693177e-05,
      "loss": 0.0199,
      "step": 2507
    },
    {
      "epoch": 0.6117073170731707,
      "grad_norm": 0.12077008932828903,
      "learning_rate": 4.504679303143341e-05,
      "loss": 0.0358,
      "step": 2508
    },
    {
      "epoch": 0.6119512195121951,
      "grad_norm": 0.062316421419382095,
      "learning_rate": 4.504297684418177e-05,
      "loss": 0.0166,
      "step": 2509
    },
    {
      "epoch": 0.6121951219512195,
      "grad_norm": 0.11349215358495712,
      "learning_rate": 4.503915934918724e-05,
      "loss": 0.042,
      "step": 2510
    },
    {
      "epoch": 0.6124390243902439,
      "grad_norm": 0.39347732067108154,
      "learning_rate": 4.503534054669892e-05,
      "loss": 0.0303,
      "step": 2511
    },
    {
      "epoch": 0.6126829268292683,
      "grad_norm": 0.08233920484781265,
      "learning_rate": 4.503152043696596e-05,
      "loss": 0.0153,
      "step": 2512
    },
    {
      "epoch": 0.6129268292682927,
      "grad_norm": 0.19515147805213928,
      "learning_rate": 4.5027699020237616e-05,
      "loss": 0.0207,
      "step": 2513
    },
    {
      "epoch": 0.6131707317073171,
      "grad_norm": 0.11468930542469025,
      "learning_rate": 4.502387629676321e-05,
      "loss": 0.041,
      "step": 2514
    },
    {
      "epoch": 0.6134146341463415,
      "grad_norm": 0.08407667279243469,
      "learning_rate": 4.502005226679218e-05,
      "loss": 0.0397,
      "step": 2515
    },
    {
      "epoch": 0.6136585365853658,
      "grad_norm": 0.09844071418046951,
      "learning_rate": 4.501622693057403e-05,
      "loss": 0.0185,
      "step": 2516
    },
    {
      "epoch": 0.6139024390243902,
      "grad_norm": 0.15633055567741394,
      "learning_rate": 4.501240028835835e-05,
      "loss": 0.0304,
      "step": 2517
    },
    {
      "epoch": 0.6141463414634146,
      "grad_norm": 0.09238705784082413,
      "learning_rate": 4.5008572340394804e-05,
      "loss": 0.0068,
      "step": 2518
    },
    {
      "epoch": 0.614390243902439,
      "grad_norm": 0.06510132551193237,
      "learning_rate": 4.500474308693318e-05,
      "loss": 0.0229,
      "step": 2519
    },
    {
      "epoch": 0.6146341463414634,
      "grad_norm": 0.16097338497638702,
      "learning_rate": 4.5000912528223296e-05,
      "loss": 0.0402,
      "step": 2520
    },
    {
      "epoch": 0.6148780487804878,
      "grad_norm": 0.07972925156354904,
      "learning_rate": 4.499708066451511e-05,
      "loss": 0.0212,
      "step": 2521
    },
    {
      "epoch": 0.6151219512195122,
      "grad_norm": 0.04715669900178909,
      "learning_rate": 4.499324749605862e-05,
      "loss": 0.0118,
      "step": 2522
    },
    {
      "epoch": 0.6153658536585366,
      "grad_norm": 0.08110851794481277,
      "learning_rate": 4.498941302310394e-05,
      "loss": 0.0175,
      "step": 2523
    },
    {
      "epoch": 0.615609756097561,
      "grad_norm": 0.08277898281812668,
      "learning_rate": 4.498557724590125e-05,
      "loss": 0.0182,
      "step": 2524
    },
    {
      "epoch": 0.6158536585365854,
      "grad_norm": 0.06089381128549576,
      "learning_rate": 4.498174016470083e-05,
      "loss": 0.0189,
      "step": 2525
    },
    {
      "epoch": 0.6160975609756097,
      "grad_norm": 0.06882679462432861,
      "learning_rate": 4.497790177975303e-05,
      "loss": 0.0275,
      "step": 2526
    },
    {
      "epoch": 0.6163414634146341,
      "grad_norm": 0.08601531386375427,
      "learning_rate": 4.49740620913083e-05,
      "loss": 0.0326,
      "step": 2527
    },
    {
      "epoch": 0.6165853658536585,
      "grad_norm": 0.1545161008834839,
      "learning_rate": 4.497022109961717e-05,
      "loss": 0.0277,
      "step": 2528
    },
    {
      "epoch": 0.6168292682926829,
      "grad_norm": 0.04287504404783249,
      "learning_rate": 4.496637880493024e-05,
      "loss": 0.0206,
      "step": 2529
    },
    {
      "epoch": 0.6170731707317073,
      "grad_norm": 0.21080973744392395,
      "learning_rate": 4.496253520749822e-05,
      "loss": 0.0316,
      "step": 2530
    },
    {
      "epoch": 0.6173170731707317,
      "grad_norm": 0.07938336580991745,
      "learning_rate": 4.495869030757188e-05,
      "loss": 0.0102,
      "step": 2531
    },
    {
      "epoch": 0.6175609756097561,
      "grad_norm": 0.07713321596384048,
      "learning_rate": 4.495484410540211e-05,
      "loss": 0.0324,
      "step": 2532
    },
    {
      "epoch": 0.6178048780487805,
      "grad_norm": 0.08483057469129562,
      "learning_rate": 4.495099660123985e-05,
      "loss": 0.0415,
      "step": 2533
    },
    {
      "epoch": 0.6180487804878049,
      "grad_norm": 0.1666218638420105,
      "learning_rate": 4.494714779533613e-05,
      "loss": 0.0253,
      "step": 2534
    },
    {
      "epoch": 0.6182926829268293,
      "grad_norm": 0.13754034042358398,
      "learning_rate": 4.494329768794208e-05,
      "loss": 0.0253,
      "step": 2535
    },
    {
      "epoch": 0.6185365853658537,
      "grad_norm": 0.07501509040594101,
      "learning_rate": 4.493944627930891e-05,
      "loss": 0.019,
      "step": 2536
    },
    {
      "epoch": 0.618780487804878,
      "grad_norm": 0.12841174006462097,
      "learning_rate": 4.493559356968791e-05,
      "loss": 0.0213,
      "step": 2537
    },
    {
      "epoch": 0.6190243902439024,
      "grad_norm": 0.1330200433731079,
      "learning_rate": 4.4931739559330464e-05,
      "loss": 0.0139,
      "step": 2538
    },
    {
      "epoch": 0.6192682926829268,
      "grad_norm": 0.10844232141971588,
      "learning_rate": 4.492788424848802e-05,
      "loss": 0.0246,
      "step": 2539
    },
    {
      "epoch": 0.6195121951219512,
      "grad_norm": 0.048554807901382446,
      "learning_rate": 4.492402763741214e-05,
      "loss": 0.0203,
      "step": 2540
    },
    {
      "epoch": 0.6197560975609756,
      "grad_norm": 0.10678230226039886,
      "learning_rate": 4.492016972635444e-05,
      "loss": 0.0441,
      "step": 2541
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.07367012649774551,
      "learning_rate": 4.4916310515566664e-05,
      "loss": 0.0355,
      "step": 2542
    },
    {
      "epoch": 0.6202439024390244,
      "grad_norm": 0.056279078125953674,
      "learning_rate": 4.491245000530059e-05,
      "loss": 0.014,
      "step": 2543
    },
    {
      "epoch": 0.6204878048780488,
      "grad_norm": 0.06068821996450424,
      "learning_rate": 4.4908588195808104e-05,
      "loss": 0.0161,
      "step": 2544
    },
    {
      "epoch": 0.6207317073170732,
      "grad_norm": 0.08115771412849426,
      "learning_rate": 4.490472508734119e-05,
      "loss": 0.0157,
      "step": 2545
    },
    {
      "epoch": 0.6209756097560976,
      "grad_norm": 0.11256670951843262,
      "learning_rate": 4.49008606801519e-05,
      "loss": 0.0269,
      "step": 2546
    },
    {
      "epoch": 0.621219512195122,
      "grad_norm": 0.24115927517414093,
      "learning_rate": 4.489699497449237e-05,
      "loss": 0.0376,
      "step": 2547
    },
    {
      "epoch": 0.6214634146341463,
      "grad_norm": 0.06850282847881317,
      "learning_rate": 4.489312797061482e-05,
      "loss": 0.0169,
      "step": 2548
    },
    {
      "epoch": 0.6217073170731707,
      "grad_norm": 0.20248690247535706,
      "learning_rate": 4.488925966877158e-05,
      "loss": 0.033,
      "step": 2549
    },
    {
      "epoch": 0.6219512195121951,
      "grad_norm": 0.3027724325656891,
      "learning_rate": 4.4885390069215035e-05,
      "loss": 0.0401,
      "step": 2550
    },
    {
      "epoch": 0.6221951219512195,
      "grad_norm": 0.08251439779996872,
      "learning_rate": 4.4881519172197664e-05,
      "loss": 0.0324,
      "step": 2551
    },
    {
      "epoch": 0.6224390243902439,
      "grad_norm": 0.14234021306037903,
      "learning_rate": 4.487764697797202e-05,
      "loss": 0.0165,
      "step": 2552
    },
    {
      "epoch": 0.6226829268292683,
      "grad_norm": 0.11502142995595932,
      "learning_rate": 4.4873773486790767e-05,
      "loss": 0.0291,
      "step": 2553
    },
    {
      "epoch": 0.6229268292682927,
      "grad_norm": 0.08097919821739197,
      "learning_rate": 4.4869898698906634e-05,
      "loss": 0.0232,
      "step": 2554
    },
    {
      "epoch": 0.6231707317073171,
      "grad_norm": 0.14815406501293182,
      "learning_rate": 4.486602261457243e-05,
      "loss": 0.0358,
      "step": 2555
    },
    {
      "epoch": 0.6234146341463415,
      "grad_norm": 0.07185477018356323,
      "learning_rate": 4.486214523404107e-05,
      "loss": 0.0128,
      "step": 2556
    },
    {
      "epoch": 0.6236585365853659,
      "grad_norm": 0.1940746158361435,
      "learning_rate": 4.4858266557565554e-05,
      "loss": 0.0238,
      "step": 2557
    },
    {
      "epoch": 0.6239024390243902,
      "grad_norm": 0.1415645331144333,
      "learning_rate": 4.485438658539892e-05,
      "loss": 0.0263,
      "step": 2558
    },
    {
      "epoch": 0.6241463414634146,
      "grad_norm": 0.2585467994213104,
      "learning_rate": 4.485050531779434e-05,
      "loss": 0.0276,
      "step": 2559
    },
    {
      "epoch": 0.624390243902439,
      "grad_norm": 0.12165314704179764,
      "learning_rate": 4.4846622755005066e-05,
      "loss": 0.0238,
      "step": 2560
    },
    {
      "epoch": 0.6246341463414634,
      "grad_norm": 0.1708063930273056,
      "learning_rate": 4.484273889728441e-05,
      "loss": 0.0301,
      "step": 2561
    },
    {
      "epoch": 0.6248780487804878,
      "grad_norm": 0.10353211313486099,
      "learning_rate": 4.483885374488579e-05,
      "loss": 0.0324,
      "step": 2562
    },
    {
      "epoch": 0.6251219512195122,
      "grad_norm": 0.2000882923603058,
      "learning_rate": 4.483496729806269e-05,
      "loss": 0.0221,
      "step": 2563
    },
    {
      "epoch": 0.6253658536585366,
      "grad_norm": 0.0717478096485138,
      "learning_rate": 4.483107955706869e-05,
      "loss": 0.0244,
      "step": 2564
    },
    {
      "epoch": 0.625609756097561,
      "grad_norm": 0.1496017426252365,
      "learning_rate": 4.482719052215747e-05,
      "loss": 0.0229,
      "step": 2565
    },
    {
      "epoch": 0.6258536585365854,
      "grad_norm": 0.08430787175893784,
      "learning_rate": 4.4823300193582755e-05,
      "loss": 0.0201,
      "step": 2566
    },
    {
      "epoch": 0.6260975609756098,
      "grad_norm": 0.10507240891456604,
      "learning_rate": 4.481940857159839e-05,
      "loss": 0.0248,
      "step": 2567
    },
    {
      "epoch": 0.6263414634146341,
      "grad_norm": 0.0681295096874237,
      "learning_rate": 4.4815515656458285e-05,
      "loss": 0.0235,
      "step": 2568
    },
    {
      "epoch": 0.6265853658536585,
      "grad_norm": 0.10606836527585983,
      "learning_rate": 4.481162144841645e-05,
      "loss": 0.0349,
      "step": 2569
    },
    {
      "epoch": 0.6268292682926829,
      "grad_norm": 0.12058183550834656,
      "learning_rate": 4.480772594772697e-05,
      "loss": 0.0242,
      "step": 2570
    },
    {
      "epoch": 0.6270731707317073,
      "grad_norm": 0.10780992358922958,
      "learning_rate": 4.4803829154644e-05,
      "loss": 0.017,
      "step": 2571
    },
    {
      "epoch": 0.6273170731707317,
      "grad_norm": 0.06557880342006683,
      "learning_rate": 4.4799931069421807e-05,
      "loss": 0.0184,
      "step": 2572
    },
    {
      "epoch": 0.6275609756097561,
      "grad_norm": 0.07848238945007324,
      "learning_rate": 4.479603169231472e-05,
      "loss": 0.0239,
      "step": 2573
    },
    {
      "epoch": 0.6278048780487805,
      "grad_norm": 0.14899089932441711,
      "learning_rate": 4.479213102357717e-05,
      "loss": 0.0252,
      "step": 2574
    },
    {
      "epoch": 0.6280487804878049,
      "grad_norm": 0.36463451385498047,
      "learning_rate": 4.478822906346366e-05,
      "loss": 0.0193,
      "step": 2575
    },
    {
      "epoch": 0.6282926829268293,
      "grad_norm": 0.1421005129814148,
      "learning_rate": 4.478432581222878e-05,
      "loss": 0.0321,
      "step": 2576
    },
    {
      "epoch": 0.6285365853658537,
      "grad_norm": 0.14988037943840027,
      "learning_rate": 4.478042127012721e-05,
      "loss": 0.0391,
      "step": 2577
    },
    {
      "epoch": 0.628780487804878,
      "grad_norm": 0.1680183857679367,
      "learning_rate": 4.4776515437413704e-05,
      "loss": 0.0237,
      "step": 2578
    },
    {
      "epoch": 0.6290243902439024,
      "grad_norm": 0.18393613398075104,
      "learning_rate": 4.477260831434311e-05,
      "loss": 0.027,
      "step": 2579
    },
    {
      "epoch": 0.6292682926829268,
      "grad_norm": 0.0950455293059349,
      "learning_rate": 4.4768699901170354e-05,
      "loss": 0.0129,
      "step": 2580
    },
    {
      "epoch": 0.6295121951219512,
      "grad_norm": 0.11388465017080307,
      "learning_rate": 4.4764790198150444e-05,
      "loss": 0.0214,
      "step": 2581
    },
    {
      "epoch": 0.6297560975609756,
      "grad_norm": 0.1898421049118042,
      "learning_rate": 4.476087920553848e-05,
      "loss": 0.0177,
      "step": 2582
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.06285226345062256,
      "learning_rate": 4.475696692358964e-05,
      "loss": 0.0218,
      "step": 2583
    },
    {
      "epoch": 0.6302439024390244,
      "grad_norm": 0.06483826786279678,
      "learning_rate": 4.4753053352559184e-05,
      "loss": 0.0149,
      "step": 2584
    },
    {
      "epoch": 0.6304878048780488,
      "grad_norm": 0.06384715437889099,
      "learning_rate": 4.4749138492702476e-05,
      "loss": 0.0208,
      "step": 2585
    },
    {
      "epoch": 0.6307317073170732,
      "grad_norm": 0.08357904106378555,
      "learning_rate": 4.4745222344274935e-05,
      "loss": 0.0203,
      "step": 2586
    },
    {
      "epoch": 0.6309756097560976,
      "grad_norm": 0.04820484668016434,
      "learning_rate": 4.474130490753208e-05,
      "loss": 0.0223,
      "step": 2587
    },
    {
      "epoch": 0.631219512195122,
      "grad_norm": 0.06916678696870804,
      "learning_rate": 4.473738618272952e-05,
      "loss": 0.021,
      "step": 2588
    },
    {
      "epoch": 0.6314634146341463,
      "grad_norm": 0.09985070675611496,
      "learning_rate": 4.473346617012293e-05,
      "loss": 0.0206,
      "step": 2589
    },
    {
      "epoch": 0.6317073170731707,
      "grad_norm": 0.09505439549684525,
      "learning_rate": 4.4729544869968085e-05,
      "loss": 0.0209,
      "step": 2590
    },
    {
      "epoch": 0.6319512195121951,
      "grad_norm": 0.09876152127981186,
      "learning_rate": 4.472562228252084e-05,
      "loss": 0.0225,
      "step": 2591
    },
    {
      "epoch": 0.6321951219512195,
      "grad_norm": 0.08247986435890198,
      "learning_rate": 4.472169840803711e-05,
      "loss": 0.019,
      "step": 2592
    },
    {
      "epoch": 0.6324390243902439,
      "grad_norm": 0.14540649950504303,
      "learning_rate": 4.4717773246772946e-05,
      "loss": 0.0244,
      "step": 2593
    },
    {
      "epoch": 0.6326829268292683,
      "grad_norm": 0.07356473803520203,
      "learning_rate": 4.471384679898444e-05,
      "loss": 0.0173,
      "step": 2594
    },
    {
      "epoch": 0.6329268292682927,
      "grad_norm": 0.05292367935180664,
      "learning_rate": 4.4709919064927775e-05,
      "loss": 0.0131,
      "step": 2595
    },
    {
      "epoch": 0.6331707317073171,
      "grad_norm": 0.1134941577911377,
      "learning_rate": 4.470599004485924e-05,
      "loss": 0.0212,
      "step": 2596
    },
    {
      "epoch": 0.6334146341463415,
      "grad_norm": 0.24002127349376678,
      "learning_rate": 4.4702059739035164e-05,
      "loss": 0.0362,
      "step": 2597
    },
    {
      "epoch": 0.6336585365853659,
      "grad_norm": 0.08136212825775146,
      "learning_rate": 4.469812814771201e-05,
      "loss": 0.0196,
      "step": 2598
    },
    {
      "epoch": 0.6339024390243903,
      "grad_norm": 0.09734704345464706,
      "learning_rate": 4.46941952711463e-05,
      "loss": 0.031,
      "step": 2599
    },
    {
      "epoch": 0.6341463414634146,
      "grad_norm": 0.17729710042476654,
      "learning_rate": 4.469026110959463e-05,
      "loss": 0.0215,
      "step": 2600
    },
    {
      "epoch": 0.634390243902439,
      "grad_norm": 0.1412843018770218,
      "learning_rate": 4.468632566331371e-05,
      "loss": 0.0391,
      "step": 2601
    },
    {
      "epoch": 0.6346341463414634,
      "grad_norm": 0.0723918080329895,
      "learning_rate": 4.46823889325603e-05,
      "loss": 0.0201,
      "step": 2602
    },
    {
      "epoch": 0.6348780487804878,
      "grad_norm": 0.3372848331928253,
      "learning_rate": 4.467845091759127e-05,
      "loss": 0.0075,
      "step": 2603
    },
    {
      "epoch": 0.6351219512195122,
      "grad_norm": 0.1454036384820938,
      "learning_rate": 4.467451161866356e-05,
      "loss": 0.0119,
      "step": 2604
    },
    {
      "epoch": 0.6353658536585366,
      "grad_norm": 0.10802903026342392,
      "learning_rate": 4.467057103603419e-05,
      "loss": 0.0215,
      "step": 2605
    },
    {
      "epoch": 0.635609756097561,
      "grad_norm": 0.06744035333395004,
      "learning_rate": 4.466662916996028e-05,
      "loss": 0.0244,
      "step": 2606
    },
    {
      "epoch": 0.6358536585365854,
      "grad_norm": 0.17293012142181396,
      "learning_rate": 4.466268602069902e-05,
      "loss": 0.0189,
      "step": 2607
    },
    {
      "epoch": 0.6360975609756098,
      "grad_norm": 0.09133976697921753,
      "learning_rate": 4.4658741588507696e-05,
      "loss": 0.0387,
      "step": 2608
    },
    {
      "epoch": 0.6363414634146342,
      "grad_norm": 0.06634502857923508,
      "learning_rate": 4.4654795873643664e-05,
      "loss": 0.0153,
      "step": 2609
    },
    {
      "epoch": 0.6365853658536585,
      "grad_norm": 0.11424973607063293,
      "learning_rate": 4.4650848876364364e-05,
      "loss": 0.0224,
      "step": 2610
    },
    {
      "epoch": 0.6368292682926829,
      "grad_norm": 0.048943620175123215,
      "learning_rate": 4.4646900596927334e-05,
      "loss": 0.0189,
      "step": 2611
    },
    {
      "epoch": 0.6370731707317073,
      "grad_norm": 0.21014823019504547,
      "learning_rate": 4.464295103559019e-05,
      "loss": 0.0334,
      "step": 2612
    },
    {
      "epoch": 0.6373170731707317,
      "grad_norm": 0.04665818065404892,
      "learning_rate": 4.463900019261062e-05,
      "loss": 0.016,
      "step": 2613
    },
    {
      "epoch": 0.6375609756097561,
      "grad_norm": 0.0963093638420105,
      "learning_rate": 4.463504806824641e-05,
      "loss": 0.0259,
      "step": 2614
    },
    {
      "epoch": 0.6378048780487805,
      "grad_norm": 0.09291539341211319,
      "learning_rate": 4.463109466275542e-05,
      "loss": 0.0294,
      "step": 2615
    },
    {
      "epoch": 0.6380487804878049,
      "grad_norm": 0.07420840114355087,
      "learning_rate": 4.46271399763956e-05,
      "loss": 0.0224,
      "step": 2616
    },
    {
      "epoch": 0.6382926829268293,
      "grad_norm": 0.11882700026035309,
      "learning_rate": 4.4623184009424976e-05,
      "loss": 0.0304,
      "step": 2617
    },
    {
      "epoch": 0.6385365853658537,
      "grad_norm": 0.14056800305843353,
      "learning_rate": 4.461922676210168e-05,
      "loss": 0.0311,
      "step": 2618
    },
    {
      "epoch": 0.6387804878048781,
      "grad_norm": 0.2535862624645233,
      "learning_rate": 4.461526823468389e-05,
      "loss": 0.0166,
      "step": 2619
    },
    {
      "epoch": 0.6390243902439025,
      "grad_norm": 0.19730830192565918,
      "learning_rate": 4.46113084274299e-05,
      "loss": 0.0304,
      "step": 2620
    },
    {
      "epoch": 0.6392682926829268,
      "grad_norm": 0.10965779423713684,
      "learning_rate": 4.460734734059806e-05,
      "loss": 0.0197,
      "step": 2621
    },
    {
      "epoch": 0.6395121951219512,
      "grad_norm": 0.0842147022485733,
      "learning_rate": 4.4603384974446845e-05,
      "loss": 0.0172,
      "step": 2622
    },
    {
      "epoch": 0.6397560975609756,
      "grad_norm": 0.08212854713201523,
      "learning_rate": 4.459942132923476e-05,
      "loss": 0.0358,
      "step": 2623
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.09092115610837936,
      "learning_rate": 4.4595456405220436e-05,
      "loss": 0.0286,
      "step": 2624
    },
    {
      "epoch": 0.6402439024390244,
      "grad_norm": 0.07281389832496643,
      "learning_rate": 4.459149020266258e-05,
      "loss": 0.0233,
      "step": 2625
    },
    {
      "epoch": 0.6404878048780488,
      "grad_norm": 0.10350412875413895,
      "learning_rate": 4.458752272181995e-05,
      "loss": 0.025,
      "step": 2626
    },
    {
      "epoch": 0.6407317073170732,
      "grad_norm": 0.16154925525188446,
      "learning_rate": 4.458355396295143e-05,
      "loss": 0.0368,
      "step": 2627
    },
    {
      "epoch": 0.6409756097560976,
      "grad_norm": 0.12247058749198914,
      "learning_rate": 4.4579583926315966e-05,
      "loss": 0.0295,
      "step": 2628
    },
    {
      "epoch": 0.641219512195122,
      "grad_norm": 0.1124800443649292,
      "learning_rate": 4.457561261217259e-05,
      "loss": 0.026,
      "step": 2629
    },
    {
      "epoch": 0.6414634146341464,
      "grad_norm": 0.1635790467262268,
      "learning_rate": 4.4571640020780417e-05,
      "loss": 0.0418,
      "step": 2630
    },
    {
      "epoch": 0.6417073170731707,
      "grad_norm": 0.1537037342786789,
      "learning_rate": 4.4567666152398646e-05,
      "loss": 0.0407,
      "step": 2631
    },
    {
      "epoch": 0.6419512195121951,
      "grad_norm": 0.10911949723958969,
      "learning_rate": 4.4563691007286554e-05,
      "loss": 0.0232,
      "step": 2632
    },
    {
      "epoch": 0.6421951219512195,
      "grad_norm": 0.0916990414261818,
      "learning_rate": 4.455971458570353e-05,
      "loss": 0.0131,
      "step": 2633
    },
    {
      "epoch": 0.6424390243902439,
      "grad_norm": 0.07133939117193222,
      "learning_rate": 4.455573688790899e-05,
      "loss": 0.0271,
      "step": 2634
    },
    {
      "epoch": 0.6426829268292683,
      "grad_norm": 0.13463827967643738,
      "learning_rate": 4.4551757914162495e-05,
      "loss": 0.0097,
      "step": 2635
    },
    {
      "epoch": 0.6429268292682927,
      "grad_norm": 0.1820220798254013,
      "learning_rate": 4.454777766472365e-05,
      "loss": 0.0245,
      "step": 2636
    },
    {
      "epoch": 0.6431707317073171,
      "grad_norm": 0.11339829117059708,
      "learning_rate": 4.4543796139852154e-05,
      "loss": 0.0198,
      "step": 2637
    },
    {
      "epoch": 0.6434146341463415,
      "grad_norm": 0.22140313684940338,
      "learning_rate": 4.4539813339807784e-05,
      "loss": 0.0273,
      "step": 2638
    },
    {
      "epoch": 0.6436585365853659,
      "grad_norm": 0.17581899464130402,
      "learning_rate": 4.453582926485041e-05,
      "loss": 0.0216,
      "step": 2639
    },
    {
      "epoch": 0.6439024390243903,
      "grad_norm": 0.15474148094654083,
      "learning_rate": 4.453184391523999e-05,
      "loss": 0.0128,
      "step": 2640
    },
    {
      "epoch": 0.6441463414634147,
      "grad_norm": 0.13526809215545654,
      "learning_rate": 4.452785729123654e-05,
      "loss": 0.0191,
      "step": 2641
    },
    {
      "epoch": 0.644390243902439,
      "grad_norm": 0.06283698230981827,
      "learning_rate": 4.452386939310018e-05,
      "loss": 0.0239,
      "step": 2642
    },
    {
      "epoch": 0.6446341463414634,
      "grad_norm": 0.2281905710697174,
      "learning_rate": 4.451988022109112e-05,
      "loss": 0.0445,
      "step": 2643
    },
    {
      "epoch": 0.6448780487804878,
      "grad_norm": 0.07037029415369034,
      "learning_rate": 4.451588977546962e-05,
      "loss": 0.0118,
      "step": 2644
    },
    {
      "epoch": 0.6451219512195122,
      "grad_norm": 0.09220408648252487,
      "learning_rate": 4.451189805649607e-05,
      "loss": 0.0105,
      "step": 2645
    },
    {
      "epoch": 0.6453658536585366,
      "grad_norm": 0.08282908797264099,
      "learning_rate": 4.450790506443089e-05,
      "loss": 0.0142,
      "step": 2646
    },
    {
      "epoch": 0.645609756097561,
      "grad_norm": 0.32411426305770874,
      "learning_rate": 4.450391079953463e-05,
      "loss": 0.0269,
      "step": 2647
    },
    {
      "epoch": 0.6458536585365854,
      "grad_norm": 0.08558429032564163,
      "learning_rate": 4.44999152620679e-05,
      "loss": 0.0202,
      "step": 2648
    },
    {
      "epoch": 0.6460975609756098,
      "grad_norm": 0.17206890881061554,
      "learning_rate": 4.4495918452291394e-05,
      "loss": 0.0142,
      "step": 2649
    },
    {
      "epoch": 0.6463414634146342,
      "grad_norm": 0.07719145715236664,
      "learning_rate": 4.449192037046589e-05,
      "loss": 0.0222,
      "step": 2650
    },
    {
      "epoch": 0.6465853658536586,
      "grad_norm": 0.3336523771286011,
      "learning_rate": 4.4487921016852244e-05,
      "loss": 0.0409,
      "step": 2651
    },
    {
      "epoch": 0.646829268292683,
      "grad_norm": 0.13974151015281677,
      "learning_rate": 4.448392039171142e-05,
      "loss": 0.0235,
      "step": 2652
    },
    {
      "epoch": 0.6470731707317073,
      "grad_norm": 0.5743406414985657,
      "learning_rate": 4.447991849530444e-05,
      "loss": 0.0226,
      "step": 2653
    },
    {
      "epoch": 0.6473170731707317,
      "grad_norm": 0.16742363572120667,
      "learning_rate": 4.44759153278924e-05,
      "loss": 0.0389,
      "step": 2654
    },
    {
      "epoch": 0.6475609756097561,
      "grad_norm": 0.19697527587413788,
      "learning_rate": 4.447191088973651e-05,
      "loss": 0.0321,
      "step": 2655
    },
    {
      "epoch": 0.6478048780487805,
      "grad_norm": 0.11671962589025497,
      "learning_rate": 4.4467905181098045e-05,
      "loss": 0.0247,
      "step": 2656
    },
    {
      "epoch": 0.6480487804878049,
      "grad_norm": 0.07456022500991821,
      "learning_rate": 4.4463898202238367e-05,
      "loss": 0.0157,
      "step": 2657
    },
    {
      "epoch": 0.6482926829268293,
      "grad_norm": 0.10430427640676498,
      "learning_rate": 4.445988995341892e-05,
      "loss": 0.0421,
      "step": 2658
    },
    {
      "epoch": 0.6485365853658537,
      "grad_norm": 0.08072159439325333,
      "learning_rate": 4.445588043490121e-05,
      "loss": 0.0148,
      "step": 2659
    },
    {
      "epoch": 0.6487804878048781,
      "grad_norm": 0.13305266201496124,
      "learning_rate": 4.4451869646946874e-05,
      "loss": 0.0262,
      "step": 2660
    },
    {
      "epoch": 0.6490243902439025,
      "grad_norm": 0.14787207543849945,
      "learning_rate": 4.444785758981759e-05,
      "loss": 0.026,
      "step": 2661
    },
    {
      "epoch": 0.6492682926829269,
      "grad_norm": 0.2156011015176773,
      "learning_rate": 4.444384426377513e-05,
      "loss": 0.0429,
      "step": 2662
    },
    {
      "epoch": 0.6495121951219512,
      "grad_norm": 0.08661087602376938,
      "learning_rate": 4.4439829669081354e-05,
      "loss": 0.0188,
      "step": 2663
    },
    {
      "epoch": 0.6497560975609756,
      "grad_norm": 0.0675710141658783,
      "learning_rate": 4.4435813805998214e-05,
      "loss": 0.0146,
      "step": 2664
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.29219529032707214,
      "learning_rate": 4.443179667478771e-05,
      "loss": 0.0329,
      "step": 2665
    },
    {
      "epoch": 0.6502439024390244,
      "grad_norm": 0.14826570451259613,
      "learning_rate": 4.442777827571197e-05,
      "loss": 0.0255,
      "step": 2666
    },
    {
      "epoch": 0.6504878048780488,
      "grad_norm": 0.2112606167793274,
      "learning_rate": 4.442375860903316e-05,
      "loss": 0.033,
      "step": 2667
    },
    {
      "epoch": 0.6507317073170732,
      "grad_norm": 0.07843682169914246,
      "learning_rate": 4.441973767501357e-05,
      "loss": 0.0301,
      "step": 2668
    },
    {
      "epoch": 0.6509756097560976,
      "grad_norm": 0.08173134177923203,
      "learning_rate": 4.441571547391554e-05,
      "loss": 0.0286,
      "step": 2669
    },
    {
      "epoch": 0.651219512195122,
      "grad_norm": 0.2051895409822464,
      "learning_rate": 4.4411692006001525e-05,
      "loss": 0.0242,
      "step": 2670
    },
    {
      "epoch": 0.6514634146341464,
      "grad_norm": 0.1731904149055481,
      "learning_rate": 4.4407667271534024e-05,
      "loss": 0.0283,
      "step": 2671
    },
    {
      "epoch": 0.6517073170731708,
      "grad_norm": 0.17071311175823212,
      "learning_rate": 4.4403641270775645e-05,
      "loss": 0.0267,
      "step": 2672
    },
    {
      "epoch": 0.6519512195121951,
      "grad_norm": 0.3318713903427124,
      "learning_rate": 4.439961400398908e-05,
      "loss": 0.0316,
      "step": 2673
    },
    {
      "epoch": 0.6521951219512195,
      "grad_norm": 0.1304808109998703,
      "learning_rate": 4.4395585471437076e-05,
      "loss": 0.0171,
      "step": 2674
    },
    {
      "epoch": 0.6524390243902439,
      "grad_norm": 0.06217082217335701,
      "learning_rate": 4.43915556733825e-05,
      "loss": 0.0219,
      "step": 2675
    },
    {
      "epoch": 0.6526829268292683,
      "grad_norm": 0.18658408522605896,
      "learning_rate": 4.438752461008828e-05,
      "loss": 0.0243,
      "step": 2676
    },
    {
      "epoch": 0.6529268292682927,
      "grad_norm": 0.0648287758231163,
      "learning_rate": 4.438349228181744e-05,
      "loss": 0.0254,
      "step": 2677
    },
    {
      "epoch": 0.6531707317073171,
      "grad_norm": 0.11105340719223022,
      "learning_rate": 4.437945868883306e-05,
      "loss": 0.0272,
      "step": 2678
    },
    {
      "epoch": 0.6534146341463415,
      "grad_norm": 0.10063909739255905,
      "learning_rate": 4.437542383139832e-05,
      "loss": 0.0139,
      "step": 2679
    },
    {
      "epoch": 0.6536585365853659,
      "grad_norm": 0.388335257768631,
      "learning_rate": 4.43713877097765e-05,
      "loss": 0.0299,
      "step": 2680
    },
    {
      "epoch": 0.6539024390243903,
      "grad_norm": 0.10041352361440659,
      "learning_rate": 4.4367350324230925e-05,
      "loss": 0.0156,
      "step": 2681
    },
    {
      "epoch": 0.6541463414634147,
      "grad_norm": 0.08912048488855362,
      "learning_rate": 4.436331167502504e-05,
      "loss": 0.0285,
      "step": 2682
    },
    {
      "epoch": 0.654390243902439,
      "grad_norm": 0.10277240723371506,
      "learning_rate": 4.435927176242234e-05,
      "loss": 0.0106,
      "step": 2683
    },
    {
      "epoch": 0.6546341463414634,
      "grad_norm": 0.2245536744594574,
      "learning_rate": 4.435523058668642e-05,
      "loss": 0.0317,
      "step": 2684
    },
    {
      "epoch": 0.6548780487804878,
      "grad_norm": 0.1003563180565834,
      "learning_rate": 4.435118814808096e-05,
      "loss": 0.0182,
      "step": 2685
    },
    {
      "epoch": 0.6551219512195122,
      "grad_norm": 0.08975261449813843,
      "learning_rate": 4.43471444468697e-05,
      "loss": 0.0261,
      "step": 2686
    },
    {
      "epoch": 0.6553658536585366,
      "grad_norm": 0.08483146876096725,
      "learning_rate": 4.434309948331651e-05,
      "loss": 0.0429,
      "step": 2687
    },
    {
      "epoch": 0.655609756097561,
      "grad_norm": 0.06313078105449677,
      "learning_rate": 4.433905325768528e-05,
      "loss": 0.0218,
      "step": 2688
    },
    {
      "epoch": 0.6558536585365854,
      "grad_norm": 0.17001502215862274,
      "learning_rate": 4.4335005770240034e-05,
      "loss": 0.0283,
      "step": 2689
    },
    {
      "epoch": 0.6560975609756098,
      "grad_norm": 0.06514214724302292,
      "learning_rate": 4.4330957021244855e-05,
      "loss": 0.021,
      "step": 2690
    },
    {
      "epoch": 0.6563414634146342,
      "grad_norm": 0.14027339220046997,
      "learning_rate": 4.43269070109639e-05,
      "loss": 0.0195,
      "step": 2691
    },
    {
      "epoch": 0.6565853658536586,
      "grad_norm": 0.16071435809135437,
      "learning_rate": 4.432285573966144e-05,
      "loss": 0.0131,
      "step": 2692
    },
    {
      "epoch": 0.656829268292683,
      "grad_norm": 0.1053902804851532,
      "learning_rate": 4.431880320760179e-05,
      "loss": 0.0289,
      "step": 2693
    },
    {
      "epoch": 0.6570731707317073,
      "grad_norm": 0.15335212647914886,
      "learning_rate": 4.431474941504937e-05,
      "loss": 0.0141,
      "step": 2694
    },
    {
      "epoch": 0.6573170731707317,
      "grad_norm": 0.12387299537658691,
      "learning_rate": 4.431069436226867e-05,
      "loss": 0.0228,
      "step": 2695
    },
    {
      "epoch": 0.6575609756097561,
      "grad_norm": 0.1474774181842804,
      "learning_rate": 4.430663804952429e-05,
      "loss": 0.017,
      "step": 2696
    },
    {
      "epoch": 0.6578048780487805,
      "grad_norm": 0.09169146418571472,
      "learning_rate": 4.430258047708088e-05,
      "loss": 0.0204,
      "step": 2697
    },
    {
      "epoch": 0.6580487804878049,
      "grad_norm": 0.08888591080904007,
      "learning_rate": 4.429852164520318e-05,
      "loss": 0.0145,
      "step": 2698
    },
    {
      "epoch": 0.6582926829268293,
      "grad_norm": 0.23801901936531067,
      "learning_rate": 4.429446155415602e-05,
      "loss": 0.0171,
      "step": 2699
    },
    {
      "epoch": 0.6585365853658537,
      "grad_norm": 0.14786089956760406,
      "learning_rate": 4.4290400204204314e-05,
      "loss": 0.0195,
      "step": 2700
    },
    {
      "epoch": 0.6587804878048781,
      "grad_norm": 0.183431014418602,
      "learning_rate": 4.428633759561305e-05,
      "loss": 0.0174,
      "step": 2701
    },
    {
      "epoch": 0.6590243902439025,
      "grad_norm": 0.09335832297801971,
      "learning_rate": 4.428227372864729e-05,
      "loss": 0.0208,
      "step": 2702
    },
    {
      "epoch": 0.6592682926829269,
      "grad_norm": 0.06034990772604942,
      "learning_rate": 4.4278208603572206e-05,
      "loss": 0.0158,
      "step": 2703
    },
    {
      "epoch": 0.6595121951219513,
      "grad_norm": 0.11773587018251419,
      "learning_rate": 4.427414222065302e-05,
      "loss": 0.0272,
      "step": 2704
    },
    {
      "epoch": 0.6597560975609756,
      "grad_norm": 0.09516213089227676,
      "learning_rate": 4.427007458015507e-05,
      "loss": 0.0439,
      "step": 2705
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.06967159360647202,
      "learning_rate": 4.4266005682343735e-05,
      "loss": 0.0253,
      "step": 2706
    },
    {
      "epoch": 0.6602439024390244,
      "grad_norm": 0.06331352889537811,
      "learning_rate": 4.426193552748451e-05,
      "loss": 0.0275,
      "step": 2707
    },
    {
      "epoch": 0.6604878048780488,
      "grad_norm": 0.1333850920200348,
      "learning_rate": 4.425786411584296e-05,
      "loss": 0.0308,
      "step": 2708
    },
    {
      "epoch": 0.6607317073170732,
      "grad_norm": 0.10187741369009018,
      "learning_rate": 4.425379144768473e-05,
      "loss": 0.0178,
      "step": 2709
    },
    {
      "epoch": 0.6609756097560976,
      "grad_norm": 0.26380455493927,
      "learning_rate": 4.4249717523275546e-05,
      "loss": 0.0356,
      "step": 2710
    },
    {
      "epoch": 0.661219512195122,
      "grad_norm": 0.21362394094467163,
      "learning_rate": 4.424564234288123e-05,
      "loss": 0.0304,
      "step": 2711
    },
    {
      "epoch": 0.6614634146341464,
      "grad_norm": 0.20048287510871887,
      "learning_rate": 4.424156590676767e-05,
      "loss": 0.0231,
      "step": 2712
    },
    {
      "epoch": 0.6617073170731708,
      "grad_norm": 0.06097536161541939,
      "learning_rate": 4.423748821520083e-05,
      "loss": 0.0248,
      "step": 2713
    },
    {
      "epoch": 0.6619512195121952,
      "grad_norm": 0.12994244694709778,
      "learning_rate": 4.423340926844677e-05,
      "loss": 0.0309,
      "step": 2714
    },
    {
      "epoch": 0.6621951219512195,
      "grad_norm": 0.0625300332903862,
      "learning_rate": 4.422932906677165e-05,
      "loss": 0.0176,
      "step": 2715
    },
    {
      "epoch": 0.6624390243902439,
      "grad_norm": 0.20290017127990723,
      "learning_rate": 4.422524761044167e-05,
      "loss": 0.0347,
      "step": 2716
    },
    {
      "epoch": 0.6626829268292683,
      "grad_norm": 0.11740957200527191,
      "learning_rate": 4.422116489972313e-05,
      "loss": 0.0132,
      "step": 2717
    },
    {
      "epoch": 0.6629268292682927,
      "grad_norm": 0.062389522790908813,
      "learning_rate": 4.4217080934882426e-05,
      "loss": 0.0101,
      "step": 2718
    },
    {
      "epoch": 0.6631707317073171,
      "grad_norm": 0.3327019214630127,
      "learning_rate": 4.421299571618602e-05,
      "loss": 0.0413,
      "step": 2719
    },
    {
      "epoch": 0.6634146341463415,
      "grad_norm": 0.04117678105831146,
      "learning_rate": 4.420890924390046e-05,
      "loss": 0.0068,
      "step": 2720
    },
    {
      "epoch": 0.6636585365853659,
      "grad_norm": 0.18088123202323914,
      "learning_rate": 4.420482151829237e-05,
      "loss": 0.0248,
      "step": 2721
    },
    {
      "epoch": 0.6639024390243903,
      "grad_norm": 0.12225464731454849,
      "learning_rate": 4.420073253962848e-05,
      "loss": 0.0245,
      "step": 2722
    },
    {
      "epoch": 0.6641463414634147,
      "grad_norm": 0.21596165001392365,
      "learning_rate": 4.419664230817556e-05,
      "loss": 0.0299,
      "step": 2723
    },
    {
      "epoch": 0.6643902439024391,
      "grad_norm": 0.20008091628551483,
      "learning_rate": 4.419255082420051e-05,
      "loss": 0.0111,
      "step": 2724
    },
    {
      "epoch": 0.6646341463414634,
      "grad_norm": 0.07235671579837799,
      "learning_rate": 4.4188458087970264e-05,
      "loss": 0.0142,
      "step": 2725
    },
    {
      "epoch": 0.6648780487804878,
      "grad_norm": 0.19119524955749512,
      "learning_rate": 4.418436409975186e-05,
      "loss": 0.026,
      "step": 2726
    },
    {
      "epoch": 0.6651219512195122,
      "grad_norm": 0.14022046327590942,
      "learning_rate": 4.418026885981244e-05,
      "loss": 0.0232,
      "step": 2727
    },
    {
      "epoch": 0.6653658536585366,
      "grad_norm": 0.12465721368789673,
      "learning_rate": 4.417617236841919e-05,
      "loss": 0.0421,
      "step": 2728
    },
    {
      "epoch": 0.665609756097561,
      "grad_norm": 0.3237025737762451,
      "learning_rate": 4.41720746258394e-05,
      "loss": 0.0241,
      "step": 2729
    },
    {
      "epoch": 0.6658536585365854,
      "grad_norm": 0.10197614133358002,
      "learning_rate": 4.416797563234043e-05,
      "loss": 0.0251,
      "step": 2730
    },
    {
      "epoch": 0.6660975609756098,
      "grad_norm": 0.2461416870355606,
      "learning_rate": 4.416387538818972e-05,
      "loss": 0.0265,
      "step": 2731
    },
    {
      "epoch": 0.6663414634146342,
      "grad_norm": 0.07100838422775269,
      "learning_rate": 4.415977389365482e-05,
      "loss": 0.0154,
      "step": 2732
    },
    {
      "epoch": 0.6665853658536586,
      "grad_norm": 0.457499235868454,
      "learning_rate": 4.4155671149003316e-05,
      "loss": 0.0401,
      "step": 2733
    },
    {
      "epoch": 0.666829268292683,
      "grad_norm": 0.11272621154785156,
      "learning_rate": 4.4151567154502925e-05,
      "loss": 0.0269,
      "step": 2734
    },
    {
      "epoch": 0.6670731707317074,
      "grad_norm": 0.14532367885112762,
      "learning_rate": 4.4147461910421393e-05,
      "loss": 0.0191,
      "step": 2735
    },
    {
      "epoch": 0.6673170731707317,
      "grad_norm": 0.07737790048122406,
      "learning_rate": 4.4143355417026594e-05,
      "loss": 0.0221,
      "step": 2736
    },
    {
      "epoch": 0.6675609756097561,
      "grad_norm": 0.10651779919862747,
      "learning_rate": 4.413924767458645e-05,
      "loss": 0.0318,
      "step": 2737
    },
    {
      "epoch": 0.6678048780487805,
      "grad_norm": 0.2757721245288849,
      "learning_rate": 4.413513868336899e-05,
      "loss": 0.036,
      "step": 2738
    },
    {
      "epoch": 0.6680487804878049,
      "grad_norm": 0.047268033027648926,
      "learning_rate": 4.413102844364232e-05,
      "loss": 0.0165,
      "step": 2739
    },
    {
      "epoch": 0.6682926829268293,
      "grad_norm": 0.3068912625312805,
      "learning_rate": 4.4126916955674596e-05,
      "loss": 0.0206,
      "step": 2740
    },
    {
      "epoch": 0.6685365853658537,
      "grad_norm": 0.2989177405834198,
      "learning_rate": 4.41228042197341e-05,
      "loss": 0.0263,
      "step": 2741
    },
    {
      "epoch": 0.6687804878048781,
      "grad_norm": 0.1409805864095688,
      "learning_rate": 4.4118690236089164e-05,
      "loss": 0.0119,
      "step": 2742
    },
    {
      "epoch": 0.6690243902439025,
      "grad_norm": 0.5973832607269287,
      "learning_rate": 4.411457500500822e-05,
      "loss": 0.0302,
      "step": 2743
    },
    {
      "epoch": 0.6692682926829269,
      "grad_norm": 0.29726046323776245,
      "learning_rate": 4.411045852675978e-05,
      "loss": 0.0375,
      "step": 2744
    },
    {
      "epoch": 0.6695121951219513,
      "grad_norm": 0.1772417277097702,
      "learning_rate": 4.410634080161241e-05,
      "loss": 0.0254,
      "step": 2745
    },
    {
      "epoch": 0.6697560975609756,
      "grad_norm": 0.07065972685813904,
      "learning_rate": 4.41022218298348e-05,
      "loss": 0.0218,
      "step": 2746
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.14186465740203857,
      "learning_rate": 4.40981016116957e-05,
      "loss": 0.0207,
      "step": 2747
    },
    {
      "epoch": 0.6702439024390244,
      "grad_norm": 0.5636337995529175,
      "learning_rate": 4.4093980147463923e-05,
      "loss": 0.0241,
      "step": 2748
    },
    {
      "epoch": 0.6704878048780488,
      "grad_norm": 0.08500095456838608,
      "learning_rate": 4.40898574374084e-05,
      "loss": 0.0127,
      "step": 2749
    },
    {
      "epoch": 0.6707317073170732,
      "grad_norm": 0.06214519217610359,
      "learning_rate": 4.408573348179812e-05,
      "loss": 0.0233,
      "step": 2750
    },
    {
      "epoch": 0.6709756097560976,
      "grad_norm": 0.13279233872890472,
      "learning_rate": 4.4081608280902156e-05,
      "loss": 0.0214,
      "step": 2751
    },
    {
      "epoch": 0.671219512195122,
      "grad_norm": 0.11859974265098572,
      "learning_rate": 4.4077481834989664e-05,
      "loss": 0.0222,
      "step": 2752
    },
    {
      "epoch": 0.6714634146341464,
      "grad_norm": 0.14360497891902924,
      "learning_rate": 4.407335414432988e-05,
      "loss": 0.0246,
      "step": 2753
    },
    {
      "epoch": 0.6717073170731708,
      "grad_norm": 0.10320679098367691,
      "learning_rate": 4.406922520919213e-05,
      "loss": 0.0141,
      "step": 2754
    },
    {
      "epoch": 0.6719512195121952,
      "grad_norm": 0.12323708832263947,
      "learning_rate": 4.4065095029845815e-05,
      "loss": 0.0338,
      "step": 2755
    },
    {
      "epoch": 0.6721951219512196,
      "grad_norm": 0.08315909653902054,
      "learning_rate": 4.406096360656041e-05,
      "loss": 0.02,
      "step": 2756
    },
    {
      "epoch": 0.672439024390244,
      "grad_norm": 0.2668542265892029,
      "learning_rate": 4.4056830939605487e-05,
      "loss": 0.0175,
      "step": 2757
    },
    {
      "epoch": 0.6726829268292683,
      "grad_norm": 0.09548933058977127,
      "learning_rate": 4.405269702925068e-05,
      "loss": 0.0257,
      "step": 2758
    },
    {
      "epoch": 0.6729268292682927,
      "grad_norm": 0.09032097458839417,
      "learning_rate": 4.404856187576572e-05,
      "loss": 0.0348,
      "step": 2759
    },
    {
      "epoch": 0.6731707317073171,
      "grad_norm": 0.08857779204845428,
      "learning_rate": 4.40444254794204e-05,
      "loss": 0.0191,
      "step": 2760
    },
    {
      "epoch": 0.6734146341463415,
      "grad_norm": 0.07456384599208832,
      "learning_rate": 4.404028784048463e-05,
      "loss": 0.022,
      "step": 2761
    },
    {
      "epoch": 0.6736585365853659,
      "grad_norm": 0.12713183462619781,
      "learning_rate": 4.4036148959228365e-05,
      "loss": 0.0182,
      "step": 2762
    },
    {
      "epoch": 0.6739024390243903,
      "grad_norm": 0.10041103512048721,
      "learning_rate": 4.4032008835921654e-05,
      "loss": 0.0183,
      "step": 2763
    },
    {
      "epoch": 0.6741463414634147,
      "grad_norm": 0.04112803563475609,
      "learning_rate": 4.402786747083463e-05,
      "loss": 0.0139,
      "step": 2764
    },
    {
      "epoch": 0.6743902439024391,
      "grad_norm": 0.08722274005413055,
      "learning_rate": 4.4023724864237497e-05,
      "loss": 0.0238,
      "step": 2765
    },
    {
      "epoch": 0.6746341463414635,
      "grad_norm": 0.07162756472826004,
      "learning_rate": 4.401958101640056e-05,
      "loss": 0.0119,
      "step": 2766
    },
    {
      "epoch": 0.6748780487804878,
      "grad_norm": 0.13326990604400635,
      "learning_rate": 4.4015435927594184e-05,
      "loss": 0.0191,
      "step": 2767
    },
    {
      "epoch": 0.6751219512195122,
      "grad_norm": 0.11894131451845169,
      "learning_rate": 4.4011289598088825e-05,
      "loss": 0.0336,
      "step": 2768
    },
    {
      "epoch": 0.6753658536585366,
      "grad_norm": 0.13838061690330505,
      "learning_rate": 4.4007142028155025e-05,
      "loss": 0.0248,
      "step": 2769
    },
    {
      "epoch": 0.675609756097561,
      "grad_norm": 0.10670343786478043,
      "learning_rate": 4.400299321806338e-05,
      "loss": 0.0125,
      "step": 2770
    },
    {
      "epoch": 0.6758536585365854,
      "grad_norm": 0.1387498676776886,
      "learning_rate": 4.3998843168084616e-05,
      "loss": 0.0209,
      "step": 2771
    },
    {
      "epoch": 0.6760975609756098,
      "grad_norm": 0.12570366263389587,
      "learning_rate": 4.399469187848949e-05,
      "loss": 0.0237,
      "step": 2772
    },
    {
      "epoch": 0.6763414634146342,
      "grad_norm": 0.15814101696014404,
      "learning_rate": 4.3990539349548866e-05,
      "loss": 0.0472,
      "step": 2773
    },
    {
      "epoch": 0.6765853658536586,
      "grad_norm": 0.11160648614168167,
      "learning_rate": 4.398638558153369e-05,
      "loss": 0.015,
      "step": 2774
    },
    {
      "epoch": 0.676829268292683,
      "grad_norm": 0.06582051515579224,
      "learning_rate": 4.3982230574714974e-05,
      "loss": 0.0231,
      "step": 2775
    },
    {
      "epoch": 0.6770731707317074,
      "grad_norm": 0.24306334555149078,
      "learning_rate": 4.397807432936382e-05,
      "loss": 0.0572,
      "step": 2776
    },
    {
      "epoch": 0.6773170731707318,
      "grad_norm": 0.1555919200181961,
      "learning_rate": 4.397391684575142e-05,
      "loss": 0.0176,
      "step": 2777
    },
    {
      "epoch": 0.6775609756097561,
      "grad_norm": 0.08849075436592102,
      "learning_rate": 4.396975812414903e-05,
      "loss": 0.0188,
      "step": 2778
    },
    {
      "epoch": 0.6778048780487805,
      "grad_norm": 0.09076616168022156,
      "learning_rate": 4.396559816482799e-05,
      "loss": 0.0362,
      "step": 2779
    },
    {
      "epoch": 0.6780487804878049,
      "grad_norm": 0.07952841371297836,
      "learning_rate": 4.396143696805973e-05,
      "loss": 0.0283,
      "step": 2780
    },
    {
      "epoch": 0.6782926829268293,
      "grad_norm": 0.24233652651309967,
      "learning_rate": 4.395727453411576e-05,
      "loss": 0.0274,
      "step": 2781
    },
    {
      "epoch": 0.6785365853658537,
      "grad_norm": 0.18402165174484253,
      "learning_rate": 4.3953110863267653e-05,
      "loss": 0.0236,
      "step": 2782
    },
    {
      "epoch": 0.6787804878048781,
      "grad_norm": 0.12769222259521484,
      "learning_rate": 4.394894595578709e-05,
      "loss": 0.0413,
      "step": 2783
    },
    {
      "epoch": 0.6790243902439025,
      "grad_norm": 0.20792247354984283,
      "learning_rate": 4.39447798119458e-05,
      "loss": 0.0345,
      "step": 2784
    },
    {
      "epoch": 0.6792682926829269,
      "grad_norm": 0.08749617636203766,
      "learning_rate": 4.394061243201564e-05,
      "loss": 0.0242,
      "step": 2785
    },
    {
      "epoch": 0.6795121951219513,
      "grad_norm": 0.10771621763706207,
      "learning_rate": 4.393644381626849e-05,
      "loss": 0.0546,
      "step": 2786
    },
    {
      "epoch": 0.6797560975609757,
      "grad_norm": 0.2762834429740906,
      "learning_rate": 4.3932273964976355e-05,
      "loss": 0.0256,
      "step": 2787
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.21485474705696106,
      "learning_rate": 4.392810287841129e-05,
      "loss": 0.0304,
      "step": 2788
    },
    {
      "epoch": 0.6802439024390244,
      "grad_norm": 0.1872374713420868,
      "learning_rate": 4.3923930556845474e-05,
      "loss": 0.0236,
      "step": 2789
    },
    {
      "epoch": 0.6804878048780488,
      "grad_norm": 0.16769613325595856,
      "learning_rate": 4.391975700055111e-05,
      "loss": 0.0175,
      "step": 2790
    },
    {
      "epoch": 0.6807317073170732,
      "grad_norm": 0.1849857121706009,
      "learning_rate": 4.391558220980053e-05,
      "loss": 0.0445,
      "step": 2791
    },
    {
      "epoch": 0.6809756097560976,
      "grad_norm": 0.11975625157356262,
      "learning_rate": 4.3911406184866104e-05,
      "loss": 0.0176,
      "step": 2792
    },
    {
      "epoch": 0.681219512195122,
      "grad_norm": 0.0824839174747467,
      "learning_rate": 4.390722892602033e-05,
      "loss": 0.0222,
      "step": 2793
    },
    {
      "epoch": 0.6814634146341464,
      "grad_norm": 0.07015187293291092,
      "learning_rate": 4.390305043353574e-05,
      "loss": 0.0171,
      "step": 2794
    },
    {
      "epoch": 0.6817073170731708,
      "grad_norm": 0.09349953383207321,
      "learning_rate": 4.389887070768498e-05,
      "loss": 0.0185,
      "step": 2795
    },
    {
      "epoch": 0.6819512195121952,
      "grad_norm": 0.10788089036941528,
      "learning_rate": 4.389468974874076e-05,
      "loss": 0.0267,
      "step": 2796
    },
    {
      "epoch": 0.6821951219512196,
      "grad_norm": 0.14678405225276947,
      "learning_rate": 4.389050755697588e-05,
      "loss": 0.0253,
      "step": 2797
    },
    {
      "epoch": 0.682439024390244,
      "grad_norm": 0.32598716020584106,
      "learning_rate": 4.388632413266321e-05,
      "loss": 0.0216,
      "step": 2798
    },
    {
      "epoch": 0.6826829268292683,
      "grad_norm": 0.07244909554719925,
      "learning_rate": 4.3882139476075706e-05,
      "loss": 0.019,
      "step": 2799
    },
    {
      "epoch": 0.6829268292682927,
      "grad_norm": 0.07541834563016891,
      "learning_rate": 4.3877953587486406e-05,
      "loss": 0.0204,
      "step": 2800
    },
    {
      "epoch": 0.6831707317073171,
      "grad_norm": 0.08866874873638153,
      "learning_rate": 4.387376646716842e-05,
      "loss": 0.0134,
      "step": 2801
    },
    {
      "epoch": 0.6834146341463415,
      "grad_norm": 0.07280273735523224,
      "learning_rate": 4.386957811539496e-05,
      "loss": 0.0179,
      "step": 2802
    },
    {
      "epoch": 0.6836585365853659,
      "grad_norm": 0.10100152343511581,
      "learning_rate": 4.386538853243929e-05,
      "loss": 0.0162,
      "step": 2803
    },
    {
      "epoch": 0.6839024390243903,
      "grad_norm": 0.0738959014415741,
      "learning_rate": 4.386119771857477e-05,
      "loss": 0.0197,
      "step": 2804
    },
    {
      "epoch": 0.6841463414634147,
      "grad_norm": 0.08565394580364227,
      "learning_rate": 4.385700567407483e-05,
      "loss": 0.0231,
      "step": 2805
    },
    {
      "epoch": 0.6843902439024391,
      "grad_norm": 0.2618192434310913,
      "learning_rate": 4.3852812399213005e-05,
      "loss": 0.0284,
      "step": 2806
    },
    {
      "epoch": 0.6846341463414635,
      "grad_norm": 0.08395444601774216,
      "learning_rate": 4.3848617894262887e-05,
      "loss": 0.0243,
      "step": 2807
    },
    {
      "epoch": 0.6848780487804879,
      "grad_norm": 0.2057250291109085,
      "learning_rate": 4.3844422159498144e-05,
      "loss": 0.0303,
      "step": 2808
    },
    {
      "epoch": 0.6851219512195122,
      "grad_norm": 0.12440529465675354,
      "learning_rate": 4.384022519519255e-05,
      "loss": 0.0127,
      "step": 2809
    },
    {
      "epoch": 0.6853658536585366,
      "grad_norm": 0.16391566395759583,
      "learning_rate": 4.383602700161994e-05,
      "loss": 0.0269,
      "step": 2810
    },
    {
      "epoch": 0.685609756097561,
      "grad_norm": 0.1376635730266571,
      "learning_rate": 4.383182757905422e-05,
      "loss": 0.0298,
      "step": 2811
    },
    {
      "epoch": 0.6858536585365854,
      "grad_norm": 0.09906429797410965,
      "learning_rate": 4.3827626927769415e-05,
      "loss": 0.046,
      "step": 2812
    },
    {
      "epoch": 0.6860975609756098,
      "grad_norm": 0.04333268105983734,
      "learning_rate": 4.382342504803958e-05,
      "loss": 0.0141,
      "step": 2813
    },
    {
      "epoch": 0.6863414634146342,
      "grad_norm": 0.10757653415203094,
      "learning_rate": 4.381922194013889e-05,
      "loss": 0.0595,
      "step": 2814
    },
    {
      "epoch": 0.6865853658536586,
      "grad_norm": 0.04540105536580086,
      "learning_rate": 4.3815017604341575e-05,
      "loss": 0.011,
      "step": 2815
    },
    {
      "epoch": 0.686829268292683,
      "grad_norm": 0.11613104492425919,
      "learning_rate": 4.3810812040921954e-05,
      "loss": 0.0242,
      "step": 2816
    },
    {
      "epoch": 0.6870731707317074,
      "grad_norm": 0.15777462720870972,
      "learning_rate": 4.380660525015444e-05,
      "loss": 0.0327,
      "step": 2817
    },
    {
      "epoch": 0.6873170731707318,
      "grad_norm": 0.10378101468086243,
      "learning_rate": 4.380239723231351e-05,
      "loss": 0.0164,
      "step": 2818
    },
    {
      "epoch": 0.687560975609756,
      "grad_norm": 0.14253190159797668,
      "learning_rate": 4.3798187987673716e-05,
      "loss": 0.0322,
      "step": 2819
    },
    {
      "epoch": 0.6878048780487804,
      "grad_norm": 0.08835023641586304,
      "learning_rate": 4.37939775165097e-05,
      "loss": 0.0244,
      "step": 2820
    },
    {
      "epoch": 0.6880487804878048,
      "grad_norm": 0.12340760231018066,
      "learning_rate": 4.378976581909619e-05,
      "loss": 0.0257,
      "step": 2821
    },
    {
      "epoch": 0.6882926829268292,
      "grad_norm": 0.2819058895111084,
      "learning_rate": 4.378555289570798e-05,
      "loss": 0.0216,
      "step": 2822
    },
    {
      "epoch": 0.6885365853658536,
      "grad_norm": 0.08064301311969757,
      "learning_rate": 4.378133874661995e-05,
      "loss": 0.0211,
      "step": 2823
    },
    {
      "epoch": 0.688780487804878,
      "grad_norm": 0.07659069448709488,
      "learning_rate": 4.3777123372107063e-05,
      "loss": 0.0169,
      "step": 2824
    },
    {
      "epoch": 0.6890243902439024,
      "grad_norm": 0.3090682029724121,
      "learning_rate": 4.3772906772444356e-05,
      "loss": 0.0276,
      "step": 2825
    },
    {
      "epoch": 0.6892682926829268,
      "grad_norm": 0.16469885408878326,
      "learning_rate": 4.3768688947906955e-05,
      "loss": 0.0311,
      "step": 2826
    },
    {
      "epoch": 0.6895121951219512,
      "grad_norm": 0.07057120651006699,
      "learning_rate": 4.376446989877006e-05,
      "loss": 0.0161,
      "step": 2827
    },
    {
      "epoch": 0.6897560975609756,
      "grad_norm": 0.08130305260419846,
      "learning_rate": 4.3760249625308936e-05,
      "loss": 0.0204,
      "step": 2828
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.1211773082613945,
      "learning_rate": 4.3756028127798965e-05,
      "loss": 0.0234,
      "step": 2829
    },
    {
      "epoch": 0.6902439024390243,
      "grad_norm": 0.10257436335086823,
      "learning_rate": 4.3751805406515564e-05,
      "loss": 0.0293,
      "step": 2830
    },
    {
      "epoch": 0.6904878048780487,
      "grad_norm": 0.047323957085609436,
      "learning_rate": 4.374758146173428e-05,
      "loss": 0.011,
      "step": 2831
    },
    {
      "epoch": 0.6907317073170731,
      "grad_norm": 0.19704559445381165,
      "learning_rate": 4.374335629373069e-05,
      "loss": 0.0252,
      "step": 2832
    },
    {
      "epoch": 0.6909756097560975,
      "grad_norm": 0.099314846098423,
      "learning_rate": 4.373912990278048e-05,
      "loss": 0.0178,
      "step": 2833
    },
    {
      "epoch": 0.6912195121951219,
      "grad_norm": 0.10251231491565704,
      "learning_rate": 4.373490228915941e-05,
      "loss": 0.0239,
      "step": 2834
    },
    {
      "epoch": 0.6914634146341463,
      "grad_norm": 0.06151127070188522,
      "learning_rate": 4.3730673453143325e-05,
      "loss": 0.0189,
      "step": 2835
    },
    {
      "epoch": 0.6917073170731707,
      "grad_norm": 0.2225726693868637,
      "learning_rate": 4.372644339500813e-05,
      "loss": 0.0251,
      "step": 2836
    },
    {
      "epoch": 0.6919512195121951,
      "grad_norm": 0.24252508580684662,
      "learning_rate": 4.372221211502983e-05,
      "loss": 0.0281,
      "step": 2837
    },
    {
      "epoch": 0.6921951219512195,
      "grad_norm": 0.08045367896556854,
      "learning_rate": 4.37179796134845e-05,
      "loss": 0.0204,
      "step": 2838
    },
    {
      "epoch": 0.6924390243902439,
      "grad_norm": 0.0635477602481842,
      "learning_rate": 4.371374589064832e-05,
      "loss": 0.0171,
      "step": 2839
    },
    {
      "epoch": 0.6926829268292682,
      "grad_norm": 0.06968093663454056,
      "learning_rate": 4.37095109467975e-05,
      "loss": 0.0259,
      "step": 2840
    },
    {
      "epoch": 0.6929268292682926,
      "grad_norm": 0.07912079989910126,
      "learning_rate": 4.3705274782208356e-05,
      "loss": 0.0172,
      "step": 2841
    },
    {
      "epoch": 0.693170731707317,
      "grad_norm": 0.1209021657705307,
      "learning_rate": 4.3701037397157305e-05,
      "loss": 0.0232,
      "step": 2842
    },
    {
      "epoch": 0.6934146341463414,
      "grad_norm": 0.1498020589351654,
      "learning_rate": 4.369679879192081e-05,
      "loss": 0.025,
      "step": 2843
    },
    {
      "epoch": 0.6936585365853658,
      "grad_norm": 0.12519071996212006,
      "learning_rate": 4.3692558966775434e-05,
      "loss": 0.019,
      "step": 2844
    },
    {
      "epoch": 0.6939024390243902,
      "grad_norm": 0.06542699038982391,
      "learning_rate": 4.36883179219978e-05,
      "loss": 0.0173,
      "step": 2845
    },
    {
      "epoch": 0.6941463414634146,
      "grad_norm": 0.09833639860153198,
      "learning_rate": 4.3684075657864634e-05,
      "loss": 0.0223,
      "step": 2846
    },
    {
      "epoch": 0.694390243902439,
      "grad_norm": 0.16707397997379303,
      "learning_rate": 4.3679832174652734e-05,
      "loss": 0.0256,
      "step": 2847
    },
    {
      "epoch": 0.6946341463414634,
      "grad_norm": 0.2054612785577774,
      "learning_rate": 4.367558747263897e-05,
      "loss": 0.0278,
      "step": 2848
    },
    {
      "epoch": 0.6948780487804878,
      "grad_norm": 0.1503181904554367,
      "learning_rate": 4.3671341552100285e-05,
      "loss": 0.0372,
      "step": 2849
    },
    {
      "epoch": 0.6951219512195121,
      "grad_norm": 0.11490991711616516,
      "learning_rate": 4.3667094413313736e-05,
      "loss": 0.0283,
      "step": 2850
    },
    {
      "epoch": 0.6953658536585365,
      "grad_norm": 0.06069398298859596,
      "learning_rate": 4.3662846056556414e-05,
      "loss": 0.0195,
      "step": 2851
    },
    {
      "epoch": 0.6956097560975609,
      "grad_norm": 0.09907983988523483,
      "learning_rate": 4.365859648210552e-05,
      "loss": 0.0283,
      "step": 2852
    },
    {
      "epoch": 0.6958536585365853,
      "grad_norm": 0.10910740494728088,
      "learning_rate": 4.3654345690238315e-05,
      "loss": 0.0264,
      "step": 2853
    },
    {
      "epoch": 0.6960975609756097,
      "grad_norm": 0.15961357951164246,
      "learning_rate": 4.365009368123218e-05,
      "loss": 0.0382,
      "step": 2854
    },
    {
      "epoch": 0.6963414634146341,
      "grad_norm": 0.12568892538547516,
      "learning_rate": 4.364584045536452e-05,
      "loss": 0.0369,
      "step": 2855
    },
    {
      "epoch": 0.6965853658536585,
      "grad_norm": 0.12419421225786209,
      "learning_rate": 4.3641586012912835e-05,
      "loss": 0.0207,
      "step": 2856
    },
    {
      "epoch": 0.6968292682926829,
      "grad_norm": 0.12660221755504608,
      "learning_rate": 4.3637330354154747e-05,
      "loss": 0.0317,
      "step": 2857
    },
    {
      "epoch": 0.6970731707317073,
      "grad_norm": 0.19786107540130615,
      "learning_rate": 4.36330734793679e-05,
      "loss": 0.0291,
      "step": 2858
    },
    {
      "epoch": 0.6973170731707317,
      "grad_norm": 0.08181026577949524,
      "learning_rate": 4.362881538883006e-05,
      "loss": 0.0372,
      "step": 2859
    },
    {
      "epoch": 0.697560975609756,
      "grad_norm": 0.558894693851471,
      "learning_rate": 4.3624556082819035e-05,
      "loss": 0.0346,
      "step": 2860
    },
    {
      "epoch": 0.6978048780487804,
      "grad_norm": 0.14400973916053772,
      "learning_rate": 4.362029556161274e-05,
      "loss": 0.0219,
      "step": 2861
    },
    {
      "epoch": 0.6980487804878048,
      "grad_norm": 0.10021387040615082,
      "learning_rate": 4.3616033825489166e-05,
      "loss": 0.0229,
      "step": 2862
    },
    {
      "epoch": 0.6982926829268292,
      "grad_norm": 0.055508650839328766,
      "learning_rate": 4.3611770874726364e-05,
      "loss": 0.0154,
      "step": 2863
    },
    {
      "epoch": 0.6985365853658536,
      "grad_norm": 0.1615445464849472,
      "learning_rate": 4.3607506709602504e-05,
      "loss": 0.0213,
      "step": 2864
    },
    {
      "epoch": 0.698780487804878,
      "grad_norm": 0.13165287673473358,
      "learning_rate": 4.360324133039578e-05,
      "loss": 0.0199,
      "step": 2865
    },
    {
      "epoch": 0.6990243902439024,
      "grad_norm": 0.09376206248998642,
      "learning_rate": 4.359897473738452e-05,
      "loss": 0.0215,
      "step": 2866
    },
    {
      "epoch": 0.6992682926829268,
      "grad_norm": 0.07925625145435333,
      "learning_rate": 4.3594706930847096e-05,
      "loss": 0.0247,
      "step": 2867
    },
    {
      "epoch": 0.6995121951219512,
      "grad_norm": 0.11458347737789154,
      "learning_rate": 4.359043791106196e-05,
      "loss": 0.0271,
      "step": 2868
    },
    {
      "epoch": 0.6997560975609756,
      "grad_norm": 0.21483995020389557,
      "learning_rate": 4.3586167678307666e-05,
      "loss": 0.024,
      "step": 2869
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.15298430621623993,
      "learning_rate": 4.358189623286283e-05,
      "loss": 0.0236,
      "step": 2870
    },
    {
      "epoch": 0.7002439024390243,
      "grad_norm": 0.12538819015026093,
      "learning_rate": 4.3577623575006145e-05,
      "loss": 0.0204,
      "step": 2871
    },
    {
      "epoch": 0.7004878048780487,
      "grad_norm": 0.10671728849411011,
      "learning_rate": 4.35733497050164e-05,
      "loss": 0.0356,
      "step": 2872
    },
    {
      "epoch": 0.7007317073170731,
      "grad_norm": 0.1841171681880951,
      "learning_rate": 4.356907462317244e-05,
      "loss": 0.0205,
      "step": 2873
    },
    {
      "epoch": 0.7009756097560975,
      "grad_norm": 0.17864446341991425,
      "learning_rate": 4.35647983297532e-05,
      "loss": 0.0257,
      "step": 2874
    },
    {
      "epoch": 0.7012195121951219,
      "grad_norm": 0.16128890216350555,
      "learning_rate": 4.356052082503771e-05,
      "loss": 0.0269,
      "step": 2875
    },
    {
      "epoch": 0.7014634146341463,
      "grad_norm": 0.12755799293518066,
      "learning_rate": 4.355624210930506e-05,
      "loss": 0.0245,
      "step": 2876
    },
    {
      "epoch": 0.7017073170731707,
      "grad_norm": 0.15356183052062988,
      "learning_rate": 4.355196218283441e-05,
      "loss": 0.032,
      "step": 2877
    },
    {
      "epoch": 0.7019512195121951,
      "grad_norm": 0.09538568556308746,
      "learning_rate": 4.354768104590502e-05,
      "loss": 0.0303,
      "step": 2878
    },
    {
      "epoch": 0.7021951219512195,
      "grad_norm": 0.11785297095775604,
      "learning_rate": 4.354339869879622e-05,
      "loss": 0.0203,
      "step": 2879
    },
    {
      "epoch": 0.7024390243902439,
      "grad_norm": 0.2926008403301239,
      "learning_rate": 4.353911514178743e-05,
      "loss": 0.0398,
      "step": 2880
    },
    {
      "epoch": 0.7026829268292683,
      "grad_norm": 0.23668847978115082,
      "learning_rate": 4.3534830375158134e-05,
      "loss": 0.0425,
      "step": 2881
    },
    {
      "epoch": 0.7029268292682926,
      "grad_norm": 0.05951268970966339,
      "learning_rate": 4.3530544399187885e-05,
      "loss": 0.023,
      "step": 2882
    },
    {
      "epoch": 0.703170731707317,
      "grad_norm": 0.1091085821390152,
      "learning_rate": 4.352625721415634e-05,
      "loss": 0.0381,
      "step": 2883
    },
    {
      "epoch": 0.7034146341463414,
      "grad_norm": 0.06947923451662064,
      "learning_rate": 4.352196882034324e-05,
      "loss": 0.0235,
      "step": 2884
    },
    {
      "epoch": 0.7036585365853658,
      "grad_norm": 0.1827416718006134,
      "learning_rate": 4.351767921802836e-05,
      "loss": 0.0149,
      "step": 2885
    },
    {
      "epoch": 0.7039024390243902,
      "grad_norm": 0.07939478009939194,
      "learning_rate": 4.351338840749161e-05,
      "loss": 0.0323,
      "step": 2886
    },
    {
      "epoch": 0.7041463414634146,
      "grad_norm": 0.1746824085712433,
      "learning_rate": 4.3509096389012934e-05,
      "loss": 0.0395,
      "step": 2887
    },
    {
      "epoch": 0.704390243902439,
      "grad_norm": 0.06930152326822281,
      "learning_rate": 4.3504803162872386e-05,
      "loss": 0.0166,
      "step": 2888
    },
    {
      "epoch": 0.7046341463414634,
      "grad_norm": 0.06602274626493454,
      "learning_rate": 4.350050872935008e-05,
      "loss": 0.0211,
      "step": 2889
    },
    {
      "epoch": 0.7048780487804878,
      "grad_norm": 0.2138175666332245,
      "learning_rate": 4.3496213088726214e-05,
      "loss": 0.0292,
      "step": 2890
    },
    {
      "epoch": 0.7051219512195122,
      "grad_norm": 0.07770854234695435,
      "learning_rate": 4.349191624128106e-05,
      "loss": 0.017,
      "step": 2891
    },
    {
      "epoch": 0.7053658536585365,
      "grad_norm": 0.22116442024707794,
      "learning_rate": 4.3487618187294986e-05,
      "loss": 0.0201,
      "step": 2892
    },
    {
      "epoch": 0.7056097560975609,
      "grad_norm": 0.13805033266544342,
      "learning_rate": 4.348331892704843e-05,
      "loss": 0.0346,
      "step": 2893
    },
    {
      "epoch": 0.7058536585365853,
      "grad_norm": 0.06711729615926743,
      "learning_rate": 4.347901846082188e-05,
      "loss": 0.0203,
      "step": 2894
    },
    {
      "epoch": 0.7060975609756097,
      "grad_norm": 0.18473759293556213,
      "learning_rate": 4.3474716788895956e-05,
      "loss": 0.0188,
      "step": 2895
    },
    {
      "epoch": 0.7063414634146341,
      "grad_norm": 0.24037966132164001,
      "learning_rate": 4.347041391155132e-05,
      "loss": 0.0173,
      "step": 2896
    },
    {
      "epoch": 0.7065853658536585,
      "grad_norm": 0.1040089800953865,
      "learning_rate": 4.346610982906871e-05,
      "loss": 0.023,
      "step": 2897
    },
    {
      "epoch": 0.7068292682926829,
      "grad_norm": 0.15137869119644165,
      "learning_rate": 4.346180454172897e-05,
      "loss": 0.0212,
      "step": 2898
    },
    {
      "epoch": 0.7070731707317073,
      "grad_norm": 0.17371122539043427,
      "learning_rate": 4.345749804981301e-05,
      "loss": 0.0176,
      "step": 2899
    },
    {
      "epoch": 0.7073170731707317,
      "grad_norm": 0.07285387814044952,
      "learning_rate": 4.345319035360178e-05,
      "loss": 0.0193,
      "step": 2900
    },
    {
      "epoch": 0.7075609756097561,
      "grad_norm": 0.076250359416008,
      "learning_rate": 4.3448881453376396e-05,
      "loss": 0.0233,
      "step": 2901
    },
    {
      "epoch": 0.7078048780487805,
      "grad_norm": 0.04850218817591667,
      "learning_rate": 4.3444571349417965e-05,
      "loss": 0.021,
      "step": 2902
    },
    {
      "epoch": 0.7080487804878048,
      "grad_norm": 0.09627529978752136,
      "learning_rate": 4.3440260042007713e-05,
      "loss": 0.0301,
      "step": 2903
    },
    {
      "epoch": 0.7082926829268292,
      "grad_norm": 0.0926816537976265,
      "learning_rate": 4.3435947531426944e-05,
      "loss": 0.0134,
      "step": 2904
    },
    {
      "epoch": 0.7085365853658536,
      "grad_norm": 0.12245242297649384,
      "learning_rate": 4.3431633817957054e-05,
      "loss": 0.0173,
      "step": 2905
    },
    {
      "epoch": 0.708780487804878,
      "grad_norm": 0.2213621884584427,
      "learning_rate": 4.342731890187946e-05,
      "loss": 0.0309,
      "step": 2906
    },
    {
      "epoch": 0.7090243902439024,
      "grad_norm": 0.1276291012763977,
      "learning_rate": 4.342300278347573e-05,
      "loss": 0.0286,
      "step": 2907
    },
    {
      "epoch": 0.7092682926829268,
      "grad_norm": 0.059201430529356,
      "learning_rate": 4.341868546302746e-05,
      "loss": 0.011,
      "step": 2908
    },
    {
      "epoch": 0.7095121951219512,
      "grad_norm": 0.08568914979696274,
      "learning_rate": 4.3414366940816356e-05,
      "loss": 0.0175,
      "step": 2909
    },
    {
      "epoch": 0.7097560975609756,
      "grad_norm": 0.349842369556427,
      "learning_rate": 4.3410047217124185e-05,
      "loss": 0.0212,
      "step": 2910
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.130062535405159,
      "learning_rate": 4.340572629223278e-05,
      "loss": 0.0194,
      "step": 2911
    },
    {
      "epoch": 0.7102439024390244,
      "grad_norm": 0.10053396970033646,
      "learning_rate": 4.340140416642409e-05,
      "loss": 0.0292,
      "step": 2912
    },
    {
      "epoch": 0.7104878048780487,
      "grad_norm": 0.2966141700744629,
      "learning_rate": 4.33970808399801e-05,
      "loss": 0.0267,
      "step": 2913
    },
    {
      "epoch": 0.7107317073170731,
      "grad_norm": 0.1997937560081482,
      "learning_rate": 4.3392756313182906e-05,
      "loss": 0.0324,
      "step": 2914
    },
    {
      "epoch": 0.7109756097560975,
      "grad_norm": 0.18167486786842346,
      "learning_rate": 4.338843058631467e-05,
      "loss": 0.0554,
      "step": 2915
    },
    {
      "epoch": 0.7112195121951219,
      "grad_norm": 0.15266939997673035,
      "learning_rate": 4.338410365965763e-05,
      "loss": 0.0319,
      "step": 2916
    },
    {
      "epoch": 0.7114634146341463,
      "grad_norm": 0.0743866115808487,
      "learning_rate": 4.337977553349411e-05,
      "loss": 0.0222,
      "step": 2917
    },
    {
      "epoch": 0.7117073170731707,
      "grad_norm": 0.23903480172157288,
      "learning_rate": 4.33754462081065e-05,
      "loss": 0.0136,
      "step": 2918
    },
    {
      "epoch": 0.7119512195121951,
      "grad_norm": 0.2360626459121704,
      "learning_rate": 4.337111568377728e-05,
      "loss": 0.0406,
      "step": 2919
    },
    {
      "epoch": 0.7121951219512195,
      "grad_norm": 0.292905330657959,
      "learning_rate": 4.336678396078899e-05,
      "loss": 0.0411,
      "step": 2920
    },
    {
      "epoch": 0.7124390243902439,
      "grad_norm": 0.06886197626590729,
      "learning_rate": 4.3362451039424284e-05,
      "loss": 0.0169,
      "step": 2921
    },
    {
      "epoch": 0.7126829268292683,
      "grad_norm": 0.1302928924560547,
      "learning_rate": 4.3358116919965856e-05,
      "loss": 0.0204,
      "step": 2922
    },
    {
      "epoch": 0.7129268292682926,
      "grad_norm": 0.11279483139514923,
      "learning_rate": 4.3353781602696495e-05,
      "loss": 0.0325,
      "step": 2923
    },
    {
      "epoch": 0.713170731707317,
      "grad_norm": 0.06359643489122391,
      "learning_rate": 4.334944508789908e-05,
      "loss": 0.0177,
      "step": 2924
    },
    {
      "epoch": 0.7134146341463414,
      "grad_norm": 0.10749316215515137,
      "learning_rate": 4.3345107375856544e-05,
      "loss": 0.0209,
      "step": 2925
    },
    {
      "epoch": 0.7136585365853658,
      "grad_norm": 0.16625718772411346,
      "learning_rate": 4.334076846685191e-05,
      "loss": 0.0219,
      "step": 2926
    },
    {
      "epoch": 0.7139024390243902,
      "grad_norm": 0.07554911077022552,
      "learning_rate": 4.333642836116828e-05,
      "loss": 0.0218,
      "step": 2927
    },
    {
      "epoch": 0.7141463414634146,
      "grad_norm": 0.08356142789125443,
      "learning_rate": 4.333208705908883e-05,
      "loss": 0.0309,
      "step": 2928
    },
    {
      "epoch": 0.714390243902439,
      "grad_norm": 0.056744880974292755,
      "learning_rate": 4.3327744560896824e-05,
      "loss": 0.0168,
      "step": 2929
    },
    {
      "epoch": 0.7146341463414634,
      "grad_norm": 0.0780254453420639,
      "learning_rate": 4.332340086687559e-05,
      "loss": 0.0205,
      "step": 2930
    },
    {
      "epoch": 0.7148780487804878,
      "grad_norm": 0.10244002193212509,
      "learning_rate": 4.331905597730854e-05,
      "loss": 0.0219,
      "step": 2931
    },
    {
      "epoch": 0.7151219512195122,
      "grad_norm": 0.10761820524930954,
      "learning_rate": 4.3314709892479164e-05,
      "loss": 0.0104,
      "step": 2932
    },
    {
      "epoch": 0.7153658536585366,
      "grad_norm": 0.07846490293741226,
      "learning_rate": 4.331036261267104e-05,
      "loss": 0.024,
      "step": 2933
    },
    {
      "epoch": 0.715609756097561,
      "grad_norm": 0.16362792253494263,
      "learning_rate": 4.3306014138167804e-05,
      "loss": 0.0355,
      "step": 2934
    },
    {
      "epoch": 0.7158536585365853,
      "grad_norm": 0.15721014142036438,
      "learning_rate": 4.33016644692532e-05,
      "loss": 0.0367,
      "step": 2935
    },
    {
      "epoch": 0.7160975609756097,
      "grad_norm": 0.14348486065864563,
      "learning_rate": 4.3297313606210996e-05,
      "loss": 0.0429,
      "step": 2936
    },
    {
      "epoch": 0.7163414634146341,
      "grad_norm": 0.1311040073633194,
      "learning_rate": 4.3292961549325105e-05,
      "loss": 0.019,
      "step": 2937
    },
    {
      "epoch": 0.7165853658536585,
      "grad_norm": 0.15911833941936493,
      "learning_rate": 4.328860829887947e-05,
      "loss": 0.0218,
      "step": 2938
    },
    {
      "epoch": 0.7168292682926829,
      "grad_norm": 0.14460940659046173,
      "learning_rate": 4.328425385515813e-05,
      "loss": 0.026,
      "step": 2939
    },
    {
      "epoch": 0.7170731707317073,
      "grad_norm": 0.07564105838537216,
      "learning_rate": 4.32798982184452e-05,
      "loss": 0.0181,
      "step": 2940
    },
    {
      "epoch": 0.7173170731707317,
      "grad_norm": 0.18430978059768677,
      "learning_rate": 4.327554138902487e-05,
      "loss": 0.0389,
      "step": 2941
    },
    {
      "epoch": 0.7175609756097561,
      "grad_norm": 0.7320347428321838,
      "learning_rate": 4.3271183367181424e-05,
      "loss": 0.0327,
      "step": 2942
    },
    {
      "epoch": 0.7178048780487805,
      "grad_norm": 0.15046000480651855,
      "learning_rate": 4.326682415319918e-05,
      "loss": 0.0273,
      "step": 2943
    },
    {
      "epoch": 0.7180487804878048,
      "grad_norm": 0.10943054407835007,
      "learning_rate": 4.326246374736259e-05,
      "loss": 0.0315,
      "step": 2944
    },
    {
      "epoch": 0.7182926829268292,
      "grad_norm": 0.14032255113124847,
      "learning_rate": 4.3258102149956135e-05,
      "loss": 0.0386,
      "step": 2945
    },
    {
      "epoch": 0.7185365853658536,
      "grad_norm": 0.14771756529808044,
      "learning_rate": 4.325373936126442e-05,
      "loss": 0.0342,
      "step": 2946
    },
    {
      "epoch": 0.718780487804878,
      "grad_norm": 0.10816432535648346,
      "learning_rate": 4.324937538157209e-05,
      "loss": 0.0195,
      "step": 2947
    },
    {
      "epoch": 0.7190243902439024,
      "grad_norm": 0.1776992827653885,
      "learning_rate": 4.3245010211163875e-05,
      "loss": 0.0293,
      "step": 2948
    },
    {
      "epoch": 0.7192682926829268,
      "grad_norm": 0.1312994360923767,
      "learning_rate": 4.324064385032461e-05,
      "loss": 0.0156,
      "step": 2949
    },
    {
      "epoch": 0.7195121951219512,
      "grad_norm": 0.24004578590393066,
      "learning_rate": 4.323627629933916e-05,
      "loss": 0.0192,
      "step": 2950
    },
    {
      "epoch": 0.7197560975609756,
      "grad_norm": 0.08899835497140884,
      "learning_rate": 4.3231907558492517e-05,
      "loss": 0.0149,
      "step": 2951
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.06432131677865982,
      "learning_rate": 4.322753762806971e-05,
      "loss": 0.0188,
      "step": 2952
    },
    {
      "epoch": 0.7202439024390244,
      "grad_norm": 0.09161815792322159,
      "learning_rate": 4.322316650835589e-05,
      "loss": 0.023,
      "step": 2953
    },
    {
      "epoch": 0.7204878048780488,
      "grad_norm": 0.10165710747241974,
      "learning_rate": 4.321879419963623e-05,
      "loss": 0.0266,
      "step": 2954
    },
    {
      "epoch": 0.7207317073170731,
      "grad_norm": 0.14144055545330048,
      "learning_rate": 4.3214420702196024e-05,
      "loss": 0.0186,
      "step": 2955
    },
    {
      "epoch": 0.7209756097560975,
      "grad_norm": 0.11404304206371307,
      "learning_rate": 4.3210046016320625e-05,
      "loss": 0.0129,
      "step": 2956
    },
    {
      "epoch": 0.7212195121951219,
      "grad_norm": 0.11868345737457275,
      "learning_rate": 4.320567014229547e-05,
      "loss": 0.0187,
      "step": 2957
    },
    {
      "epoch": 0.7214634146341463,
      "grad_norm": 0.19331224262714386,
      "learning_rate": 4.320129308040607e-05,
      "loss": 0.0252,
      "step": 2958
    },
    {
      "epoch": 0.7217073170731707,
      "grad_norm": 0.12528155744075775,
      "learning_rate": 4.319691483093802e-05,
      "loss": 0.0345,
      "step": 2959
    },
    {
      "epoch": 0.7219512195121951,
      "grad_norm": 0.13812944293022156,
      "learning_rate": 4.319253539417698e-05,
      "loss": 0.0355,
      "step": 2960
    },
    {
      "epoch": 0.7221951219512195,
      "grad_norm": 0.09738929569721222,
      "learning_rate": 4.3188154770408695e-05,
      "loss": 0.026,
      "step": 2961
    },
    {
      "epoch": 0.7224390243902439,
      "grad_norm": 0.14429324865341187,
      "learning_rate": 4.3183772959919e-05,
      "loss": 0.0261,
      "step": 2962
    },
    {
      "epoch": 0.7226829268292683,
      "grad_norm": 0.07077109813690186,
      "learning_rate": 4.3179389962993776e-05,
      "loss": 0.015,
      "step": 2963
    },
    {
      "epoch": 0.7229268292682927,
      "grad_norm": 0.0710759237408638,
      "learning_rate": 4.3175005779919016e-05,
      "loss": 0.0194,
      "step": 2964
    },
    {
      "epoch": 0.723170731707317,
      "grad_norm": 0.12256079912185669,
      "learning_rate": 4.317062041098077e-05,
      "loss": 0.0273,
      "step": 2965
    },
    {
      "epoch": 0.7234146341463414,
      "grad_norm": 0.08508030325174332,
      "learning_rate": 4.316623385646516e-05,
      "loss": 0.0153,
      "step": 2966
    },
    {
      "epoch": 0.7236585365853658,
      "grad_norm": 0.0603698305785656,
      "learning_rate": 4.31618461166584e-05,
      "loss": 0.0121,
      "step": 2967
    },
    {
      "epoch": 0.7239024390243902,
      "grad_norm": 0.08170006424188614,
      "learning_rate": 4.3157457191846786e-05,
      "loss": 0.0242,
      "step": 2968
    },
    {
      "epoch": 0.7241463414634146,
      "grad_norm": 0.07226407527923584,
      "learning_rate": 4.3153067082316676e-05,
      "loss": 0.0211,
      "step": 2969
    },
    {
      "epoch": 0.724390243902439,
      "grad_norm": 0.06825871020555496,
      "learning_rate": 4.314867578835451e-05,
      "loss": 0.0212,
      "step": 2970
    },
    {
      "epoch": 0.7246341463414634,
      "grad_norm": 0.09874400496482849,
      "learning_rate": 4.3144283310246816e-05,
      "loss": 0.0224,
      "step": 2971
    },
    {
      "epoch": 0.7248780487804878,
      "grad_norm": 0.14643605053424835,
      "learning_rate": 4.313988964828016e-05,
      "loss": 0.0266,
      "step": 2972
    },
    {
      "epoch": 0.7251219512195122,
      "grad_norm": 0.1376686841249466,
      "learning_rate": 4.313549480274125e-05,
      "loss": 0.0295,
      "step": 2973
    },
    {
      "epoch": 0.7253658536585366,
      "grad_norm": 0.1372448354959488,
      "learning_rate": 4.3131098773916825e-05,
      "loss": 0.0427,
      "step": 2974
    },
    {
      "epoch": 0.725609756097561,
      "grad_norm": 0.1107206642627716,
      "learning_rate": 4.312670156209371e-05,
      "loss": 0.0266,
      "step": 2975
    },
    {
      "epoch": 0.7258536585365853,
      "grad_norm": 0.1122516319155693,
      "learning_rate": 4.31223031675588e-05,
      "loss": 0.0289,
      "step": 2976
    },
    {
      "epoch": 0.7260975609756097,
      "grad_norm": 0.05942337214946747,
      "learning_rate": 4.311790359059909e-05,
      "loss": 0.0215,
      "step": 2977
    },
    {
      "epoch": 0.7263414634146341,
      "grad_norm": 0.06382431089878082,
      "learning_rate": 4.311350283150163e-05,
      "loss": 0.025,
      "step": 2978
    },
    {
      "epoch": 0.7265853658536585,
      "grad_norm": 0.24376928806304932,
      "learning_rate": 4.3109100890553565e-05,
      "loss": 0.0314,
      "step": 2979
    },
    {
      "epoch": 0.7268292682926829,
      "grad_norm": 0.08029654622077942,
      "learning_rate": 4.310469776804211e-05,
      "loss": 0.0349,
      "step": 2980
    },
    {
      "epoch": 0.7270731707317073,
      "grad_norm": 0.11593250185251236,
      "learning_rate": 4.310029346425454e-05,
      "loss": 0.0215,
      "step": 2981
    },
    {
      "epoch": 0.7273170731707317,
      "grad_norm": 0.16975745558738708,
      "learning_rate": 4.309588797947824e-05,
      "loss": 0.0208,
      "step": 2982
    },
    {
      "epoch": 0.7275609756097561,
      "grad_norm": 0.15825875103473663,
      "learning_rate": 4.309148131400064e-05,
      "loss": 0.0421,
      "step": 2983
    },
    {
      "epoch": 0.7278048780487805,
      "grad_norm": 0.15407264232635498,
      "learning_rate": 4.308707346810927e-05,
      "loss": 0.0421,
      "step": 2984
    },
    {
      "epoch": 0.7280487804878049,
      "grad_norm": 0.16863110661506653,
      "learning_rate": 4.308266444209173e-05,
      "loss": 0.0146,
      "step": 2985
    },
    {
      "epoch": 0.7282926829268292,
      "grad_norm": 0.12924692034721375,
      "learning_rate": 4.3078254236235685e-05,
      "loss": 0.0296,
      "step": 2986
    },
    {
      "epoch": 0.7285365853658536,
      "grad_norm": 0.2712832987308502,
      "learning_rate": 4.30738428508289e-05,
      "loss": 0.0357,
      "step": 2987
    },
    {
      "epoch": 0.728780487804878,
      "grad_norm": 0.049846403300762177,
      "learning_rate": 4.30694302861592e-05,
      "loss": 0.0088,
      "step": 2988
    },
    {
      "epoch": 0.7290243902439024,
      "grad_norm": 0.07719260454177856,
      "learning_rate": 4.306501654251448e-05,
      "loss": 0.0211,
      "step": 2989
    },
    {
      "epoch": 0.7292682926829268,
      "grad_norm": 0.11739751696586609,
      "learning_rate": 4.306060162018274e-05,
      "loss": 0.0284,
      "step": 2990
    },
    {
      "epoch": 0.7295121951219512,
      "grad_norm": 0.07629674673080444,
      "learning_rate": 4.3056185519452023e-05,
      "loss": 0.0192,
      "step": 2991
    },
    {
      "epoch": 0.7297560975609756,
      "grad_norm": 0.07269036024808884,
      "learning_rate": 4.305176824061049e-05,
      "loss": 0.0163,
      "step": 2992
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.25692132115364075,
      "learning_rate": 4.304734978394633e-05,
      "loss": 0.0378,
      "step": 2993
    },
    {
      "epoch": 0.7302439024390244,
      "grad_norm": 0.25693416595458984,
      "learning_rate": 4.304293014974785e-05,
      "loss": 0.0292,
      "step": 2994
    },
    {
      "epoch": 0.7304878048780488,
      "grad_norm": 0.16417735815048218,
      "learning_rate": 4.303850933830341e-05,
      "loss": 0.032,
      "step": 2995
    },
    {
      "epoch": 0.7307317073170732,
      "grad_norm": 0.24509522318840027,
      "learning_rate": 4.303408734990146e-05,
      "loss": 0.0384,
      "step": 2996
    },
    {
      "epoch": 0.7309756097560975,
      "grad_norm": 0.26398786902427673,
      "learning_rate": 4.302966418483052e-05,
      "loss": 0.0254,
      "step": 2997
    },
    {
      "epoch": 0.7312195121951219,
      "grad_norm": 0.08259434252977371,
      "learning_rate": 4.302523984337918e-05,
      "loss": 0.0173,
      "step": 2998
    },
    {
      "epoch": 0.7314634146341463,
      "grad_norm": 0.1194801777601242,
      "learning_rate": 4.3020814325836124e-05,
      "loss": 0.0285,
      "step": 2999
    },
    {
      "epoch": 0.7317073170731707,
      "grad_norm": 0.11574102938175201,
      "learning_rate": 4.301638763249011e-05,
      "loss": 0.0258,
      "step": 3000
    },
    {
      "epoch": 0.7319512195121951,
      "grad_norm": 0.11126626282930374,
      "learning_rate": 4.301195976362994e-05,
      "loss": 0.022,
      "step": 3001
    },
    {
      "epoch": 0.7321951219512195,
      "grad_norm": 0.08472460508346558,
      "learning_rate": 4.300753071954454e-05,
      "loss": 0.0237,
      "step": 3002
    },
    {
      "epoch": 0.7324390243902439,
      "grad_norm": 0.11610787361860275,
      "learning_rate": 4.300310050052289e-05,
      "loss": 0.0432,
      "step": 3003
    },
    {
      "epoch": 0.7326829268292683,
      "grad_norm": 0.1900857537984848,
      "learning_rate": 4.299866910685405e-05,
      "loss": 0.0284,
      "step": 3004
    },
    {
      "epoch": 0.7329268292682927,
      "grad_norm": 0.14213038980960846,
      "learning_rate": 4.2994236538827135e-05,
      "loss": 0.0294,
      "step": 3005
    },
    {
      "epoch": 0.7331707317073171,
      "grad_norm": 0.2850675582885742,
      "learning_rate": 4.298980279673138e-05,
      "loss": 0.0209,
      "step": 3006
    },
    {
      "epoch": 0.7334146341463414,
      "grad_norm": 0.3739531934261322,
      "learning_rate": 4.298536788085607e-05,
      "loss": 0.0289,
      "step": 3007
    },
    {
      "epoch": 0.7336585365853658,
      "grad_norm": 0.09893776476383209,
      "learning_rate": 4.2980931791490556e-05,
      "loss": 0.029,
      "step": 3008
    },
    {
      "epoch": 0.7339024390243902,
      "grad_norm": 0.15154966711997986,
      "learning_rate": 4.297649452892429e-05,
      "loss": 0.032,
      "step": 3009
    },
    {
      "epoch": 0.7341463414634146,
      "grad_norm": 0.22341659665107727,
      "learning_rate": 4.297205609344679e-05,
      "loss": 0.0451,
      "step": 3010
    },
    {
      "epoch": 0.734390243902439,
      "grad_norm": 0.05737011507153511,
      "learning_rate": 4.296761648534765e-05,
      "loss": 0.0224,
      "step": 3011
    },
    {
      "epoch": 0.7346341463414634,
      "grad_norm": 0.1336614340543747,
      "learning_rate": 4.2963175704916533e-05,
      "loss": 0.019,
      "step": 3012
    },
    {
      "epoch": 0.7348780487804878,
      "grad_norm": 0.06498057395219803,
      "learning_rate": 4.2958733752443195e-05,
      "loss": 0.0253,
      "step": 3013
    },
    {
      "epoch": 0.7351219512195122,
      "grad_norm": 0.18448887765407562,
      "learning_rate": 4.295429062821745e-05,
      "loss": 0.0281,
      "step": 3014
    },
    {
      "epoch": 0.7353658536585366,
      "grad_norm": 0.10794948041439056,
      "learning_rate": 4.294984633252921e-05,
      "loss": 0.015,
      "step": 3015
    },
    {
      "epoch": 0.735609756097561,
      "grad_norm": 0.05962115526199341,
      "learning_rate": 4.294540086566845e-05,
      "loss": 0.0152,
      "step": 3016
    },
    {
      "epoch": 0.7358536585365854,
      "grad_norm": 0.12315703183412552,
      "learning_rate": 4.294095422792521e-05,
      "loss": 0.0189,
      "step": 3017
    },
    {
      "epoch": 0.7360975609756097,
      "grad_norm": 0.22833745181560516,
      "learning_rate": 4.293650641958964e-05,
      "loss": 0.0287,
      "step": 3018
    },
    {
      "epoch": 0.7363414634146341,
      "grad_norm": 0.09376689791679382,
      "learning_rate": 4.2932057440951926e-05,
      "loss": 0.0254,
      "step": 3019
    },
    {
      "epoch": 0.7365853658536585,
      "grad_norm": 0.06911785155534744,
      "learning_rate": 4.292760729230236e-05,
      "loss": 0.034,
      "step": 3020
    },
    {
      "epoch": 0.7368292682926829,
      "grad_norm": 0.050299957394599915,
      "learning_rate": 4.29231559739313e-05,
      "loss": 0.0153,
      "step": 3021
    },
    {
      "epoch": 0.7370731707317073,
      "grad_norm": 0.13776621222496033,
      "learning_rate": 4.291870348612919e-05,
      "loss": 0.032,
      "step": 3022
    },
    {
      "epoch": 0.7373170731707317,
      "grad_norm": 0.124097540974617,
      "learning_rate": 4.291424982918652e-05,
      "loss": 0.0266,
      "step": 3023
    },
    {
      "epoch": 0.7375609756097561,
      "grad_norm": 0.2725966274738312,
      "learning_rate": 4.2909795003393905e-05,
      "loss": 0.0372,
      "step": 3024
    },
    {
      "epoch": 0.7378048780487805,
      "grad_norm": 0.32363054156303406,
      "learning_rate": 4.290533900904198e-05,
      "loss": 0.0284,
      "step": 3025
    },
    {
      "epoch": 0.7380487804878049,
      "grad_norm": 0.08045300841331482,
      "learning_rate": 4.29008818464215e-05,
      "loss": 0.0171,
      "step": 3026
    },
    {
      "epoch": 0.7382926829268293,
      "grad_norm": 0.11016327142715454,
      "learning_rate": 4.289642351582328e-05,
      "loss": 0.034,
      "step": 3027
    },
    {
      "epoch": 0.7385365853658536,
      "grad_norm": 0.15589386224746704,
      "learning_rate": 4.2891964017538214e-05,
      "loss": 0.0266,
      "step": 3028
    },
    {
      "epoch": 0.738780487804878,
      "grad_norm": 0.12195415794849396,
      "learning_rate": 4.288750335185727e-05,
      "loss": 0.0265,
      "step": 3029
    },
    {
      "epoch": 0.7390243902439024,
      "grad_norm": 0.059934649616479874,
      "learning_rate": 4.288304151907149e-05,
      "loss": 0.0281,
      "step": 3030
    },
    {
      "epoch": 0.7392682926829268,
      "grad_norm": 0.10366591066122055,
      "learning_rate": 4.287857851947199e-05,
      "loss": 0.0196,
      "step": 3031
    },
    {
      "epoch": 0.7395121951219512,
      "grad_norm": 0.1798464059829712,
      "learning_rate": 4.287411435334998e-05,
      "loss": 0.0254,
      "step": 3032
    },
    {
      "epoch": 0.7397560975609756,
      "grad_norm": 0.10317665338516235,
      "learning_rate": 4.286964902099673e-05,
      "loss": 0.0269,
      "step": 3033
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.22556738555431366,
      "learning_rate": 4.286518252270357e-05,
      "loss": 0.0198,
      "step": 3034
    },
    {
      "epoch": 0.7402439024390244,
      "grad_norm": 0.2297428548336029,
      "learning_rate": 4.2860714858761954e-05,
      "loss": 0.0258,
      "step": 3035
    },
    {
      "epoch": 0.7404878048780488,
      "grad_norm": 0.17924636602401733,
      "learning_rate": 4.285624602946337e-05,
      "loss": 0.0232,
      "step": 3036
    },
    {
      "epoch": 0.7407317073170732,
      "grad_norm": 0.15297271311283112,
      "learning_rate": 4.285177603509939e-05,
      "loss": 0.0283,
      "step": 3037
    },
    {
      "epoch": 0.7409756097560976,
      "grad_norm": 0.13867062330245972,
      "learning_rate": 4.284730487596168e-05,
      "loss": 0.0216,
      "step": 3038
    },
    {
      "epoch": 0.7412195121951219,
      "grad_norm": 0.09327128529548645,
      "learning_rate": 4.2842832552341955e-05,
      "loss": 0.0167,
      "step": 3039
    },
    {
      "epoch": 0.7414634146341463,
      "grad_norm": 0.06604575365781784,
      "learning_rate": 4.283835906453203e-05,
      "loss": 0.0146,
      "step": 3040
    },
    {
      "epoch": 0.7417073170731707,
      "grad_norm": 0.09096112847328186,
      "learning_rate": 4.283388441282379e-05,
      "loss": 0.0306,
      "step": 3041
    },
    {
      "epoch": 0.7419512195121951,
      "grad_norm": 0.193472757935524,
      "learning_rate": 4.282940859750918e-05,
      "loss": 0.0262,
      "step": 3042
    },
    {
      "epoch": 0.7421951219512195,
      "grad_norm": 0.08864924311637878,
      "learning_rate": 4.282493161888024e-05,
      "loss": 0.0233,
      "step": 3043
    },
    {
      "epoch": 0.7424390243902439,
      "grad_norm": 0.12405131012201309,
      "learning_rate": 4.282045347722908e-05,
      "loss": 0.0284,
      "step": 3044
    },
    {
      "epoch": 0.7426829268292683,
      "grad_norm": 0.05004110932350159,
      "learning_rate": 4.2815974172847895e-05,
      "loss": 0.0172,
      "step": 3045
    },
    {
      "epoch": 0.7429268292682927,
      "grad_norm": 0.13653451204299927,
      "learning_rate": 4.281149370602893e-05,
      "loss": 0.0148,
      "step": 3046
    },
    {
      "epoch": 0.7431707317073171,
      "grad_norm": 0.08591111749410629,
      "learning_rate": 4.280701207706452e-05,
      "loss": 0.0138,
      "step": 3047
    },
    {
      "epoch": 0.7434146341463415,
      "grad_norm": 0.13090479373931885,
      "learning_rate": 4.280252928624708e-05,
      "loss": 0.0219,
      "step": 3048
    },
    {
      "epoch": 0.7436585365853658,
      "grad_norm": 0.20078325271606445,
      "learning_rate": 4.279804533386912e-05,
      "loss": 0.0266,
      "step": 3049
    },
    {
      "epoch": 0.7439024390243902,
      "grad_norm": 0.06194296479225159,
      "learning_rate": 4.279356022022317e-05,
      "loss": 0.0236,
      "step": 3050
    },
    {
      "epoch": 0.7441463414634146,
      "grad_norm": 0.0704025998711586,
      "learning_rate": 4.27890739456019e-05,
      "loss": 0.017,
      "step": 3051
    },
    {
      "epoch": 0.744390243902439,
      "grad_norm": 0.06390771269798279,
      "learning_rate": 4.2784586510298006e-05,
      "loss": 0.0225,
      "step": 3052
    },
    {
      "epoch": 0.7446341463414634,
      "grad_norm": 0.07102520018815994,
      "learning_rate": 4.278009791460429e-05,
      "loss": 0.0175,
      "step": 3053
    },
    {
      "epoch": 0.7448780487804878,
      "grad_norm": 0.0837373435497284,
      "learning_rate": 4.2775608158813615e-05,
      "loss": 0.0324,
      "step": 3054
    },
    {
      "epoch": 0.7451219512195122,
      "grad_norm": 0.09725489467382431,
      "learning_rate": 4.277111724321892e-05,
      "loss": 0.0139,
      "step": 3055
    },
    {
      "epoch": 0.7453658536585366,
      "grad_norm": 0.10967395454645157,
      "learning_rate": 4.276662516811323e-05,
      "loss": 0.0263,
      "step": 3056
    },
    {
      "epoch": 0.745609756097561,
      "grad_norm": 0.08833866566419601,
      "learning_rate": 4.276213193378963e-05,
      "loss": 0.0292,
      "step": 3057
    },
    {
      "epoch": 0.7458536585365854,
      "grad_norm": 0.10584928840398788,
      "learning_rate": 4.2757637540541305e-05,
      "loss": 0.0243,
      "step": 3058
    },
    {
      "epoch": 0.7460975609756098,
      "grad_norm": 0.1434919834136963,
      "learning_rate": 4.275314198866149e-05,
      "loss": 0.0168,
      "step": 3059
    },
    {
      "epoch": 0.7463414634146341,
      "grad_norm": 0.1742110550403595,
      "learning_rate": 4.27486452784435e-05,
      "loss": 0.0279,
      "step": 3060
    },
    {
      "epoch": 0.7465853658536585,
      "grad_norm": 0.19901001453399658,
      "learning_rate": 4.274414741018076e-05,
      "loss": 0.0257,
      "step": 3061
    },
    {
      "epoch": 0.7468292682926829,
      "grad_norm": 0.11158207803964615,
      "learning_rate": 4.27396483841667e-05,
      "loss": 0.0257,
      "step": 3062
    },
    {
      "epoch": 0.7470731707317073,
      "grad_norm": 0.3455585539340973,
      "learning_rate": 4.27351482006949e-05,
      "loss": 0.0326,
      "step": 3063
    },
    {
      "epoch": 0.7473170731707317,
      "grad_norm": 0.10387977212667465,
      "learning_rate": 4.2730646860058965e-05,
      "loss": 0.0202,
      "step": 3064
    },
    {
      "epoch": 0.7475609756097561,
      "grad_norm": 0.14503611624240875,
      "learning_rate": 4.27261443625526e-05,
      "loss": 0.0172,
      "step": 3065
    },
    {
      "epoch": 0.7478048780487805,
      "grad_norm": 0.12680529057979584,
      "learning_rate": 4.2721640708469596e-05,
      "loss": 0.0221,
      "step": 3066
    },
    {
      "epoch": 0.7480487804878049,
      "grad_norm": 0.07502129673957825,
      "learning_rate": 4.2717135898103776e-05,
      "loss": 0.0223,
      "step": 3067
    },
    {
      "epoch": 0.7482926829268293,
      "grad_norm": 0.09746849536895752,
      "learning_rate": 4.271262993174908e-05,
      "loss": 0.0324,
      "step": 3068
    },
    {
      "epoch": 0.7485365853658537,
      "grad_norm": 0.169325590133667,
      "learning_rate": 4.2708122809699495e-05,
      "loss": 0.0276,
      "step": 3069
    },
    {
      "epoch": 0.748780487804878,
      "grad_norm": 0.09198304265737534,
      "learning_rate": 4.270361453224911e-05,
      "loss": 0.0271,
      "step": 3070
    },
    {
      "epoch": 0.7490243902439024,
      "grad_norm": 0.07146856188774109,
      "learning_rate": 4.2699105099692084e-05,
      "loss": 0.0179,
      "step": 3071
    },
    {
      "epoch": 0.7492682926829268,
      "grad_norm": 0.1050381138920784,
      "learning_rate": 4.269459451232262e-05,
      "loss": 0.0215,
      "step": 3072
    },
    {
      "epoch": 0.7495121951219512,
      "grad_norm": 0.05979809910058975,
      "learning_rate": 4.269008277043504e-05,
      "loss": 0.013,
      "step": 3073
    },
    {
      "epoch": 0.7497560975609756,
      "grad_norm": 0.14984031021595,
      "learning_rate": 4.2685569874323704e-05,
      "loss": 0.0209,
      "step": 3074
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.12619315087795258,
      "learning_rate": 4.268105582428308e-05,
      "loss": 0.0195,
      "step": 3075
    },
    {
      "epoch": 0.7502439024390244,
      "grad_norm": 0.16664725542068481,
      "learning_rate": 4.26765406206077e-05,
      "loss": 0.0224,
      "step": 3076
    },
    {
      "epoch": 0.7504878048780488,
      "grad_norm": 0.07124070078134537,
      "learning_rate": 4.267202426359215e-05,
      "loss": 0.0129,
      "step": 3077
    },
    {
      "epoch": 0.7507317073170732,
      "grad_norm": 0.08840268850326538,
      "learning_rate": 4.266750675353111e-05,
      "loss": 0.028,
      "step": 3078
    },
    {
      "epoch": 0.7509756097560976,
      "grad_norm": 0.11188516765832901,
      "learning_rate": 4.2662988090719345e-05,
      "loss": 0.0166,
      "step": 3079
    },
    {
      "epoch": 0.751219512195122,
      "grad_norm": 0.16329024732112885,
      "learning_rate": 4.2658468275451676e-05,
      "loss": 0.0299,
      "step": 3080
    },
    {
      "epoch": 0.7514634146341463,
      "grad_norm": 0.0729074478149414,
      "learning_rate": 4.265394730802301e-05,
      "loss": 0.0169,
      "step": 3081
    },
    {
      "epoch": 0.7517073170731707,
      "grad_norm": 0.06634499877691269,
      "learning_rate": 4.264942518872833e-05,
      "loss": 0.0204,
      "step": 3082
    },
    {
      "epoch": 0.7519512195121951,
      "grad_norm": 0.12264125049114227,
      "learning_rate": 4.264490191786268e-05,
      "loss": 0.0365,
      "step": 3083
    },
    {
      "epoch": 0.7521951219512195,
      "grad_norm": 0.23394857347011566,
      "learning_rate": 4.2640377495721204e-05,
      "loss": 0.0303,
      "step": 3084
    },
    {
      "epoch": 0.7524390243902439,
      "grad_norm": 0.0839354619383812,
      "learning_rate": 4.2635851922599096e-05,
      "loss": 0.0294,
      "step": 3085
    },
    {
      "epoch": 0.7526829268292683,
      "grad_norm": 0.12740503251552582,
      "learning_rate": 4.263132519879164e-05,
      "loss": 0.0122,
      "step": 3086
    },
    {
      "epoch": 0.7529268292682927,
      "grad_norm": 0.1297937035560608,
      "learning_rate": 4.262679732459418e-05,
      "loss": 0.0226,
      "step": 3087
    },
    {
      "epoch": 0.7531707317073171,
      "grad_norm": 0.15945719182491302,
      "learning_rate": 4.262226830030217e-05,
      "loss": 0.0225,
      "step": 3088
    },
    {
      "epoch": 0.7534146341463415,
      "grad_norm": 0.1570431888103485,
      "learning_rate": 4.261773812621109e-05,
      "loss": 0.0244,
      "step": 3089
    },
    {
      "epoch": 0.7536585365853659,
      "grad_norm": 0.12970812618732452,
      "learning_rate": 4.261320680261653e-05,
      "loss": 0.0413,
      "step": 3090
    },
    {
      "epoch": 0.7539024390243902,
      "grad_norm": 0.08455844223499298,
      "learning_rate": 4.2608674329814145e-05,
      "loss": 0.0111,
      "step": 3091
    },
    {
      "epoch": 0.7541463414634146,
      "grad_norm": 0.12583744525909424,
      "learning_rate": 4.260414070809967e-05,
      "loss": 0.0255,
      "step": 3092
    },
    {
      "epoch": 0.754390243902439,
      "grad_norm": 0.151413232088089,
      "learning_rate": 4.2599605937768904e-05,
      "loss": 0.012,
      "step": 3093
    },
    {
      "epoch": 0.7546341463414634,
      "grad_norm": 0.1968509405851364,
      "learning_rate": 4.2595070019117736e-05,
      "loss": 0.023,
      "step": 3094
    },
    {
      "epoch": 0.7548780487804878,
      "grad_norm": 0.13462667167186737,
      "learning_rate": 4.2590532952442104e-05,
      "loss": 0.0206,
      "step": 3095
    },
    {
      "epoch": 0.7551219512195122,
      "grad_norm": 0.16120874881744385,
      "learning_rate": 4.258599473803805e-05,
      "loss": 0.0356,
      "step": 3096
    },
    {
      "epoch": 0.7553658536585366,
      "grad_norm": 0.10360302031040192,
      "learning_rate": 4.258145537620167e-05,
      "loss": 0.0245,
      "step": 3097
    },
    {
      "epoch": 0.755609756097561,
      "grad_norm": 0.0876089334487915,
      "learning_rate": 4.257691486722915e-05,
      "loss": 0.0194,
      "step": 3098
    },
    {
      "epoch": 0.7558536585365854,
      "grad_norm": 0.07244672626256943,
      "learning_rate": 4.2572373211416746e-05,
      "loss": 0.0249,
      "step": 3099
    },
    {
      "epoch": 0.7560975609756098,
      "grad_norm": 0.15542450547218323,
      "learning_rate": 4.2567830409060784e-05,
      "loss": 0.0139,
      "step": 3100
    },
    {
      "epoch": 0.7563414634146342,
      "grad_norm": 0.08338116109371185,
      "learning_rate": 4.256328646045766e-05,
      "loss": 0.0304,
      "step": 3101
    },
    {
      "epoch": 0.7565853658536585,
      "grad_norm": 0.13320447504520416,
      "learning_rate": 4.2558741365903876e-05,
      "loss": 0.0138,
      "step": 3102
    },
    {
      "epoch": 0.7568292682926829,
      "grad_norm": 0.08525467664003372,
      "learning_rate": 4.255419512569596e-05,
      "loss": 0.0272,
      "step": 3103
    },
    {
      "epoch": 0.7570731707317073,
      "grad_norm": 0.20131826400756836,
      "learning_rate": 4.254964774013055e-05,
      "loss": 0.0225,
      "step": 3104
    },
    {
      "epoch": 0.7573170731707317,
      "grad_norm": 0.09073399752378464,
      "learning_rate": 4.2545099209504354e-05,
      "loss": 0.022,
      "step": 3105
    },
    {
      "epoch": 0.7575609756097561,
      "grad_norm": 0.12350718677043915,
      "learning_rate": 4.2540549534114146e-05,
      "loss": 0.0277,
      "step": 3106
    },
    {
      "epoch": 0.7578048780487805,
      "grad_norm": 0.20351089537143707,
      "learning_rate": 4.2535998714256774e-05,
      "loss": 0.032,
      "step": 3107
    },
    {
      "epoch": 0.7580487804878049,
      "grad_norm": 0.08087749779224396,
      "learning_rate": 4.253144675022917e-05,
      "loss": 0.0221,
      "step": 3108
    },
    {
      "epoch": 0.7582926829268293,
      "grad_norm": 0.09573379158973694,
      "learning_rate": 4.2526893642328335e-05,
      "loss": 0.0232,
      "step": 3109
    },
    {
      "epoch": 0.7585365853658537,
      "grad_norm": 0.14702050387859344,
      "learning_rate": 4.2522339390851344e-05,
      "loss": 0.0171,
      "step": 3110
    },
    {
      "epoch": 0.7587804878048781,
      "grad_norm": 0.07543434947729111,
      "learning_rate": 4.251778399609535e-05,
      "loss": 0.0192,
      "step": 3111
    },
    {
      "epoch": 0.7590243902439024,
      "grad_norm": 0.18316730856895447,
      "learning_rate": 4.251322745835757e-05,
      "loss": 0.0213,
      "step": 3112
    },
    {
      "epoch": 0.7592682926829268,
      "grad_norm": 0.11876357346773148,
      "learning_rate": 4.2508669777935323e-05,
      "loss": 0.0269,
      "step": 3113
    },
    {
      "epoch": 0.7595121951219512,
      "grad_norm": 0.14135031402111053,
      "learning_rate": 4.2504110955125965e-05,
      "loss": 0.0379,
      "step": 3114
    },
    {
      "epoch": 0.7597560975609756,
      "grad_norm": 0.06537829339504242,
      "learning_rate": 4.2499550990226956e-05,
      "loss": 0.0124,
      "step": 3115
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.06675441563129425,
      "learning_rate": 4.249498988353582e-05,
      "loss": 0.0269,
      "step": 3116
    },
    {
      "epoch": 0.7602439024390244,
      "grad_norm": 0.12869219481945038,
      "learning_rate": 4.249042763535014e-05,
      "loss": 0.0226,
      "step": 3117
    },
    {
      "epoch": 0.7604878048780488,
      "grad_norm": 0.09017571061849594,
      "learning_rate": 4.248586424596761e-05,
      "loss": 0.0278,
      "step": 3118
    },
    {
      "epoch": 0.7607317073170732,
      "grad_norm": 0.059740159660577774,
      "learning_rate": 4.248129971568596e-05,
      "loss": 0.0206,
      "step": 3119
    },
    {
      "epoch": 0.7609756097560976,
      "grad_norm": 0.24511095881462097,
      "learning_rate": 4.247673404480302e-05,
      "loss": 0.0507,
      "step": 3120
    },
    {
      "epoch": 0.761219512195122,
      "grad_norm": 0.22882775962352753,
      "learning_rate": 4.247216723361669e-05,
      "loss": 0.0218,
      "step": 3121
    },
    {
      "epoch": 0.7614634146341464,
      "grad_norm": 0.5134267807006836,
      "learning_rate": 4.246759928242493e-05,
      "loss": 0.037,
      "step": 3122
    },
    {
      "epoch": 0.7617073170731707,
      "grad_norm": 0.21126872301101685,
      "learning_rate": 4.2463030191525796e-05,
      "loss": 0.0298,
      "step": 3123
    },
    {
      "epoch": 0.7619512195121951,
      "grad_norm": 0.09113392978906631,
      "learning_rate": 4.24584599612174e-05,
      "loss": 0.0212,
      "step": 3124
    },
    {
      "epoch": 0.7621951219512195,
      "grad_norm": 0.10228291153907776,
      "learning_rate": 4.245388859179793e-05,
      "loss": 0.0445,
      "step": 3125
    },
    {
      "epoch": 0.7624390243902439,
      "grad_norm": 0.32136011123657227,
      "learning_rate": 4.244931608356567e-05,
      "loss": 0.0219,
      "step": 3126
    },
    {
      "epoch": 0.7626829268292683,
      "grad_norm": 0.1326887309551239,
      "learning_rate": 4.244474243681895e-05,
      "loss": 0.0174,
      "step": 3127
    },
    {
      "epoch": 0.7629268292682927,
      "grad_norm": 0.14763952791690826,
      "learning_rate": 4.244016765185619e-05,
      "loss": 0.0197,
      "step": 3128
    },
    {
      "epoch": 0.7631707317073171,
      "grad_norm": 0.10653954744338989,
      "learning_rate": 4.243559172897588e-05,
      "loss": 0.0213,
      "step": 3129
    },
    {
      "epoch": 0.7634146341463415,
      "grad_norm": 0.13483160734176636,
      "learning_rate": 4.2431014668476585e-05,
      "loss": 0.0429,
      "step": 3130
    },
    {
      "epoch": 0.7636585365853659,
      "grad_norm": 0.082186758518219,
      "learning_rate": 4.2426436470656936e-05,
      "loss": 0.0267,
      "step": 3131
    },
    {
      "epoch": 0.7639024390243903,
      "grad_norm": 0.09192296862602234,
      "learning_rate": 4.2421857135815665e-05,
      "loss": 0.0158,
      "step": 3132
    },
    {
      "epoch": 0.7641463414634146,
      "grad_norm": 0.09907299280166626,
      "learning_rate": 4.241727666425155e-05,
      "loss": 0.0244,
      "step": 3133
    },
    {
      "epoch": 0.764390243902439,
      "grad_norm": 0.09943549335002899,
      "learning_rate": 4.241269505626345e-05,
      "loss": 0.0261,
      "step": 3134
    },
    {
      "epoch": 0.7646341463414634,
      "grad_norm": 0.2239135205745697,
      "learning_rate": 4.24081123121503e-05,
      "loss": 0.0231,
      "step": 3135
    },
    {
      "epoch": 0.7648780487804878,
      "grad_norm": 0.1491081714630127,
      "learning_rate": 4.2403528432211116e-05,
      "loss": 0.0205,
      "step": 3136
    },
    {
      "epoch": 0.7651219512195122,
      "grad_norm": 0.07684172689914703,
      "learning_rate": 4.2398943416744976e-05,
      "loss": 0.028,
      "step": 3137
    },
    {
      "epoch": 0.7653658536585366,
      "grad_norm": 0.15334832668304443,
      "learning_rate": 4.239435726605104e-05,
      "loss": 0.0269,
      "step": 3138
    },
    {
      "epoch": 0.765609756097561,
      "grad_norm": 0.10479649901390076,
      "learning_rate": 4.238976998042855e-05,
      "loss": 0.0118,
      "step": 3139
    },
    {
      "epoch": 0.7658536585365854,
      "grad_norm": 0.09025759249925613,
      "learning_rate": 4.23851815601768e-05,
      "loss": 0.0198,
      "step": 3140
    },
    {
      "epoch": 0.7660975609756098,
      "grad_norm": 0.09521385282278061,
      "learning_rate": 4.238059200559517e-05,
      "loss": 0.0173,
      "step": 3141
    },
    {
      "epoch": 0.7663414634146342,
      "grad_norm": 0.1301809698343277,
      "learning_rate": 4.2376001316983126e-05,
      "loss": 0.0233,
      "step": 3142
    },
    {
      "epoch": 0.7665853658536586,
      "grad_norm": 0.05945517122745514,
      "learning_rate": 4.237140949464018e-05,
      "loss": 0.0154,
      "step": 3143
    },
    {
      "epoch": 0.7668292682926829,
      "grad_norm": 0.08203817158937454,
      "learning_rate": 4.236681653886595e-05,
      "loss": 0.0258,
      "step": 3144
    },
    {
      "epoch": 0.7670731707317073,
      "grad_norm": 0.060600124299526215,
      "learning_rate": 4.23622224499601e-05,
      "loss": 0.0215,
      "step": 3145
    },
    {
      "epoch": 0.7673170731707317,
      "grad_norm": 0.12898774445056915,
      "learning_rate": 4.23576272282224e-05,
      "loss": 0.0259,
      "step": 3146
    },
    {
      "epoch": 0.7675609756097561,
      "grad_norm": 0.12586575746536255,
      "learning_rate": 4.2353030873952645e-05,
      "loss": 0.0136,
      "step": 3147
    },
    {
      "epoch": 0.7678048780487805,
      "grad_norm": 0.118012435734272,
      "learning_rate": 4.234843338745075e-05,
      "loss": 0.0279,
      "step": 3148
    },
    {
      "epoch": 0.7680487804878049,
      "grad_norm": 0.09362250566482544,
      "learning_rate": 4.2343834769016685e-05,
      "loss": 0.021,
      "step": 3149
    },
    {
      "epoch": 0.7682926829268293,
      "grad_norm": 0.19155383110046387,
      "learning_rate": 4.2339235018950504e-05,
      "loss": 0.0285,
      "step": 3150
    },
    {
      "epoch": 0.7685365853658537,
      "grad_norm": 0.09972109645605087,
      "learning_rate": 4.233463413755232e-05,
      "loss": 0.0215,
      "step": 3151
    },
    {
      "epoch": 0.7687804878048781,
      "grad_norm": 0.13404840230941772,
      "learning_rate": 4.233003212512232e-05,
      "loss": 0.0392,
      "step": 3152
    },
    {
      "epoch": 0.7690243902439025,
      "grad_norm": 0.1617957353591919,
      "learning_rate": 4.2325428981960765e-05,
      "loss": 0.0159,
      "step": 3153
    },
    {
      "epoch": 0.7692682926829268,
      "grad_norm": 0.09421637654304504,
      "learning_rate": 4.2320824708368025e-05,
      "loss": 0.0168,
      "step": 3154
    },
    {
      "epoch": 0.7695121951219512,
      "grad_norm": 0.09454677253961563,
      "learning_rate": 4.231621930464449e-05,
      "loss": 0.0363,
      "step": 3155
    },
    {
      "epoch": 0.7697560975609756,
      "grad_norm": 0.08620458096265793,
      "learning_rate": 4.231161277109065e-05,
      "loss": 0.016,
      "step": 3156
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.09273076057434082,
      "learning_rate": 4.230700510800708e-05,
      "loss": 0.022,
      "step": 3157
    },
    {
      "epoch": 0.7702439024390244,
      "grad_norm": 0.05414343625307083,
      "learning_rate": 4.23023963156944e-05,
      "loss": 0.0121,
      "step": 3158
    },
    {
      "epoch": 0.7704878048780488,
      "grad_norm": 0.11650671064853668,
      "learning_rate": 4.2297786394453345e-05,
      "loss": 0.0245,
      "step": 3159
    },
    {
      "epoch": 0.7707317073170732,
      "grad_norm": 0.08824659138917923,
      "learning_rate": 4.229317534458467e-05,
      "loss": 0.0253,
      "step": 3160
    },
    {
      "epoch": 0.7709756097560976,
      "grad_norm": 0.09972025454044342,
      "learning_rate": 4.228856316638924e-05,
      "loss": 0.021,
      "step": 3161
    },
    {
      "epoch": 0.771219512195122,
      "grad_norm": 0.09544279426336288,
      "learning_rate": 4.228394986016799e-05,
      "loss": 0.0236,
      "step": 3162
    },
    {
      "epoch": 0.7714634146341464,
      "grad_norm": 0.16971302032470703,
      "learning_rate": 4.227933542622192e-05,
      "loss": 0.021,
      "step": 3163
    },
    {
      "epoch": 0.7717073170731708,
      "grad_norm": 0.219865083694458,
      "learning_rate": 4.227471986485212e-05,
      "loss": 0.0111,
      "step": 3164
    },
    {
      "epoch": 0.7719512195121951,
      "grad_norm": 0.21812710165977478,
      "learning_rate": 4.227010317635973e-05,
      "loss": 0.0224,
      "step": 3165
    },
    {
      "epoch": 0.7721951219512195,
      "grad_norm": 0.16721823811531067,
      "learning_rate": 4.226548536104597e-05,
      "loss": 0.0235,
      "step": 3166
    },
    {
      "epoch": 0.7724390243902439,
      "grad_norm": 0.06858710944652557,
      "learning_rate": 4.226086641921215e-05,
      "loss": 0.019,
      "step": 3167
    },
    {
      "epoch": 0.7726829268292683,
      "grad_norm": 0.09210890531539917,
      "learning_rate": 4.225624635115964e-05,
      "loss": 0.0331,
      "step": 3168
    },
    {
      "epoch": 0.7729268292682927,
      "grad_norm": 0.15317262709140778,
      "learning_rate": 4.2251625157189876e-05,
      "loss": 0.0222,
      "step": 3169
    },
    {
      "epoch": 0.7731707317073171,
      "grad_norm": 0.14247383177280426,
      "learning_rate": 4.224700283760438e-05,
      "loss": 0.0211,
      "step": 3170
    },
    {
      "epoch": 0.7734146341463415,
      "grad_norm": 0.09539858996868134,
      "learning_rate": 4.224237939270476e-05,
      "loss": 0.021,
      "step": 3171
    },
    {
      "epoch": 0.7736585365853659,
      "grad_norm": 0.1316262185573578,
      "learning_rate": 4.223775482279265e-05,
      "loss": 0.0263,
      "step": 3172
    },
    {
      "epoch": 0.7739024390243903,
      "grad_norm": 0.18118681013584137,
      "learning_rate": 4.223312912816983e-05,
      "loss": 0.0158,
      "step": 3173
    },
    {
      "epoch": 0.7741463414634147,
      "grad_norm": 0.0675584226846695,
      "learning_rate": 4.222850230913807e-05,
      "loss": 0.016,
      "step": 3174
    },
    {
      "epoch": 0.774390243902439,
      "grad_norm": 0.14967191219329834,
      "learning_rate": 4.222387436599928e-05,
      "loss": 0.0324,
      "step": 3175
    },
    {
      "epoch": 0.7746341463414634,
      "grad_norm": 0.2027927190065384,
      "learning_rate": 4.221924529905542e-05,
      "loss": 0.031,
      "step": 3176
    },
    {
      "epoch": 0.7748780487804878,
      "grad_norm": 0.07717448472976685,
      "learning_rate": 4.221461510860852e-05,
      "loss": 0.0292,
      "step": 3177
    },
    {
      "epoch": 0.7751219512195122,
      "grad_norm": 0.12062611430883408,
      "learning_rate": 4.220998379496068e-05,
      "loss": 0.056,
      "step": 3178
    },
    {
      "epoch": 0.7753658536585366,
      "grad_norm": 0.10176835209131241,
      "learning_rate": 4.220535135841408e-05,
      "loss": 0.0193,
      "step": 3179
    },
    {
      "epoch": 0.775609756097561,
      "grad_norm": 0.16094784438610077,
      "learning_rate": 4.220071779927097e-05,
      "loss": 0.0337,
      "step": 3180
    },
    {
      "epoch": 0.7758536585365854,
      "grad_norm": 0.11252641677856445,
      "learning_rate": 4.219608311783369e-05,
      "loss": 0.0276,
      "step": 3181
    },
    {
      "epoch": 0.7760975609756098,
      "grad_norm": 0.08494982123374939,
      "learning_rate": 4.219144731440462e-05,
      "loss": 0.0136,
      "step": 3182
    },
    {
      "epoch": 0.7763414634146342,
      "grad_norm": 0.033918775618076324,
      "learning_rate": 4.2186810389286254e-05,
      "loss": 0.0118,
      "step": 3183
    },
    {
      "epoch": 0.7765853658536586,
      "grad_norm": 0.1893921196460724,
      "learning_rate": 4.218217234278111e-05,
      "loss": 0.0312,
      "step": 3184
    },
    {
      "epoch": 0.776829268292683,
      "grad_norm": 0.058140795677900314,
      "learning_rate": 4.217753317519183e-05,
      "loss": 0.0133,
      "step": 3185
    },
    {
      "epoch": 0.7770731707317073,
      "grad_norm": 0.19463957846164703,
      "learning_rate": 4.217289288682109e-05,
      "loss": 0.0225,
      "step": 3186
    },
    {
      "epoch": 0.7773170731707317,
      "grad_norm": 0.10685286670923233,
      "learning_rate": 4.2168251477971666e-05,
      "loss": 0.0419,
      "step": 3187
    },
    {
      "epoch": 0.7775609756097561,
      "grad_norm": 0.29165327548980713,
      "learning_rate": 4.2163608948946385e-05,
      "loss": 0.0313,
      "step": 3188
    },
    {
      "epoch": 0.7778048780487805,
      "grad_norm": 0.13294941186904907,
      "learning_rate": 4.215896530004816e-05,
      "loss": 0.0198,
      "step": 3189
    },
    {
      "epoch": 0.7780487804878049,
      "grad_norm": 0.1391998678445816,
      "learning_rate": 4.215432053157998e-05,
      "loss": 0.0203,
      "step": 3190
    },
    {
      "epoch": 0.7782926829268293,
      "grad_norm": 0.12256895005702972,
      "learning_rate": 4.2149674643844905e-05,
      "loss": 0.0172,
      "step": 3191
    },
    {
      "epoch": 0.7785365853658537,
      "grad_norm": 0.1139097735285759,
      "learning_rate": 4.214502763714606e-05,
      "loss": 0.0291,
      "step": 3192
    },
    {
      "epoch": 0.7787804878048781,
      "grad_norm": 0.09691411256790161,
      "learning_rate": 4.214037951178664e-05,
      "loss": 0.0259,
      "step": 3193
    },
    {
      "epoch": 0.7790243902439025,
      "grad_norm": 0.10904345661401749,
      "learning_rate": 4.2135730268069935e-05,
      "loss": 0.0228,
      "step": 3194
    },
    {
      "epoch": 0.7792682926829269,
      "grad_norm": 0.05799032747745514,
      "learning_rate": 4.213107990629929e-05,
      "loss": 0.0148,
      "step": 3195
    },
    {
      "epoch": 0.7795121951219512,
      "grad_norm": 0.07917032390832901,
      "learning_rate": 4.212642842677811e-05,
      "loss": 0.0236,
      "step": 3196
    },
    {
      "epoch": 0.7797560975609756,
      "grad_norm": 0.16389088332653046,
      "learning_rate": 4.212177582980992e-05,
      "loss": 0.0156,
      "step": 3197
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.09102243185043335,
      "learning_rate": 4.211712211569826e-05,
      "loss": 0.0221,
      "step": 3198
    },
    {
      "epoch": 0.7802439024390244,
      "grad_norm": 0.10581200569868088,
      "learning_rate": 4.211246728474679e-05,
      "loss": 0.0149,
      "step": 3199
    },
    {
      "epoch": 0.7804878048780488,
      "grad_norm": 0.14078757166862488,
      "learning_rate": 4.210781133725922e-05,
      "loss": 0.0251,
      "step": 3200
    },
    {
      "epoch": 0.7807317073170732,
      "grad_norm": 0.07330141216516495,
      "learning_rate": 4.2103154273539324e-05,
      "loss": 0.0246,
      "step": 3201
    },
    {
      "epoch": 0.7809756097560976,
      "grad_norm": 0.11914580315351486,
      "learning_rate": 4.209849609389097e-05,
      "loss": 0.0331,
      "step": 3202
    },
    {
      "epoch": 0.781219512195122,
      "grad_norm": 0.07165692001581192,
      "learning_rate": 4.2093836798618094e-05,
      "loss": 0.0268,
      "step": 3203
    },
    {
      "epoch": 0.7814634146341464,
      "grad_norm": 0.12179771810770035,
      "learning_rate": 4.2089176388024686e-05,
      "loss": 0.0209,
      "step": 3204
    },
    {
      "epoch": 0.7817073170731708,
      "grad_norm": 0.11488685756921768,
      "learning_rate": 4.208451486241485e-05,
      "loss": 0.0374,
      "step": 3205
    },
    {
      "epoch": 0.7819512195121952,
      "grad_norm": 0.06753480434417725,
      "learning_rate": 4.20798522220927e-05,
      "loss": 0.0227,
      "step": 3206
    },
    {
      "epoch": 0.7821951219512195,
      "grad_norm": 0.09711658209562302,
      "learning_rate": 4.207518846736249e-05,
      "loss": 0.0294,
      "step": 3207
    },
    {
      "epoch": 0.7824390243902439,
      "grad_norm": 0.29991403222084045,
      "learning_rate": 4.207052359852851e-05,
      "loss": 0.0248,
      "step": 3208
    },
    {
      "epoch": 0.7826829268292683,
      "grad_norm": 0.10464989393949509,
      "learning_rate": 4.2065857615895114e-05,
      "loss": 0.0184,
      "step": 3209
    },
    {
      "epoch": 0.7829268292682927,
      "grad_norm": 0.09537407010793686,
      "learning_rate": 4.206119051976676e-05,
      "loss": 0.0198,
      "step": 3210
    },
    {
      "epoch": 0.7831707317073171,
      "grad_norm": 0.0874580517411232,
      "learning_rate": 4.205652231044795e-05,
      "loss": 0.0353,
      "step": 3211
    },
    {
      "epoch": 0.7834146341463415,
      "grad_norm": 0.1504373848438263,
      "learning_rate": 4.205185298824327e-05,
      "loss": 0.0358,
      "step": 3212
    },
    {
      "epoch": 0.7836585365853659,
      "grad_norm": 0.10298597812652588,
      "learning_rate": 4.204718255345739e-05,
      "loss": 0.0109,
      "step": 3213
    },
    {
      "epoch": 0.7839024390243903,
      "grad_norm": 0.10072070360183716,
      "learning_rate": 4.204251100639503e-05,
      "loss": 0.0288,
      "step": 3214
    },
    {
      "epoch": 0.7841463414634147,
      "grad_norm": 0.183237686753273,
      "learning_rate": 4.2037838347361005e-05,
      "loss": 0.0239,
      "step": 3215
    },
    {
      "epoch": 0.784390243902439,
      "grad_norm": 0.1304892748594284,
      "learning_rate": 4.203316457666018e-05,
      "loss": 0.0236,
      "step": 3216
    },
    {
      "epoch": 0.7846341463414634,
      "grad_norm": 0.07613738626241684,
      "learning_rate": 4.202848969459751e-05,
      "loss": 0.0177,
      "step": 3217
    },
    {
      "epoch": 0.7848780487804878,
      "grad_norm": 0.10059138387441635,
      "learning_rate": 4.2023813701478024e-05,
      "loss": 0.0142,
      "step": 3218
    },
    {
      "epoch": 0.7851219512195122,
      "grad_norm": 0.13577529788017273,
      "learning_rate": 4.20191365976068e-05,
      "loss": 0.0307,
      "step": 3219
    },
    {
      "epoch": 0.7853658536585366,
      "grad_norm": 0.1338249146938324,
      "learning_rate": 4.201445838328901e-05,
      "loss": 0.0147,
      "step": 3220
    },
    {
      "epoch": 0.785609756097561,
      "grad_norm": 0.08892747014760971,
      "learning_rate": 4.200977905882991e-05,
      "loss": 0.0161,
      "step": 3221
    },
    {
      "epoch": 0.7858536585365854,
      "grad_norm": 0.0721336156129837,
      "learning_rate": 4.200509862453479e-05,
      "loss": 0.0089,
      "step": 3222
    },
    {
      "epoch": 0.7860975609756098,
      "grad_norm": 0.11852507293224335,
      "learning_rate": 4.2000417080709045e-05,
      "loss": 0.0253,
      "step": 3223
    },
    {
      "epoch": 0.7863414634146342,
      "grad_norm": 0.1654616892337799,
      "learning_rate": 4.199573442765812e-05,
      "loss": 0.0206,
      "step": 3224
    },
    {
      "epoch": 0.7865853658536586,
      "grad_norm": 0.09764426946640015,
      "learning_rate": 4.199105066568757e-05,
      "loss": 0.0187,
      "step": 3225
    },
    {
      "epoch": 0.786829268292683,
      "grad_norm": 0.10118076950311661,
      "learning_rate": 4.198636579510297e-05,
      "loss": 0.0309,
      "step": 3226
    },
    {
      "epoch": 0.7870731707317074,
      "grad_norm": 0.1202101781964302,
      "learning_rate": 4.1981679816209995e-05,
      "loss": 0.0353,
      "step": 3227
    },
    {
      "epoch": 0.7873170731707317,
      "grad_norm": 0.16937223076820374,
      "learning_rate": 4.19769927293144e-05,
      "loss": 0.0218,
      "step": 3228
    },
    {
      "epoch": 0.7875609756097561,
      "grad_norm": 0.12164442241191864,
      "learning_rate": 4.197230453472201e-05,
      "loss": 0.0173,
      "step": 3229
    },
    {
      "epoch": 0.7878048780487805,
      "grad_norm": 0.18745875358581543,
      "learning_rate": 4.1967615232738696e-05,
      "loss": 0.019,
      "step": 3230
    },
    {
      "epoch": 0.7880487804878049,
      "grad_norm": 0.10308343172073364,
      "learning_rate": 4.196292482367044e-05,
      "loss": 0.0175,
      "step": 3231
    },
    {
      "epoch": 0.7882926829268293,
      "grad_norm": 0.1161055639386177,
      "learning_rate": 4.195823330782326e-05,
      "loss": 0.0296,
      "step": 3232
    },
    {
      "epoch": 0.7885365853658537,
      "grad_norm": 0.18930573761463165,
      "learning_rate": 4.1953540685503276e-05,
      "loss": 0.0215,
      "step": 3233
    },
    {
      "epoch": 0.7887804878048781,
      "grad_norm": 0.09171254932880402,
      "learning_rate": 4.1948846957016665e-05,
      "loss": 0.0321,
      "step": 3234
    },
    {
      "epoch": 0.7890243902439025,
      "grad_norm": 0.1397002786397934,
      "learning_rate": 4.194415212266967e-05,
      "loss": 0.0148,
      "step": 3235
    },
    {
      "epoch": 0.7892682926829269,
      "grad_norm": 0.08458027988672256,
      "learning_rate": 4.193945618276862e-05,
      "loss": 0.0244,
      "step": 3236
    },
    {
      "epoch": 0.7895121951219513,
      "grad_norm": 0.18471921980381012,
      "learning_rate": 4.1934759137619914e-05,
      "loss": 0.0283,
      "step": 3237
    },
    {
      "epoch": 0.7897560975609756,
      "grad_norm": 0.10982630401849747,
      "learning_rate": 4.1930060987530015e-05,
      "loss": 0.0361,
      "step": 3238
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.0806068554520607,
      "learning_rate": 4.192536173280546e-05,
      "loss": 0.0219,
      "step": 3239
    },
    {
      "epoch": 0.7902439024390244,
      "grad_norm": 0.24468959867954254,
      "learning_rate": 4.192066137375287e-05,
      "loss": 0.0296,
      "step": 3240
    },
    {
      "epoch": 0.7904878048780488,
      "grad_norm": 0.12071063369512558,
      "learning_rate": 4.1915959910678926e-05,
      "loss": 0.0287,
      "step": 3241
    },
    {
      "epoch": 0.7907317073170732,
      "grad_norm": 0.2101086676120758,
      "learning_rate": 4.191125734389038e-05,
      "loss": 0.0244,
      "step": 3242
    },
    {
      "epoch": 0.7909756097560976,
      "grad_norm": 0.14086586236953735,
      "learning_rate": 4.1906553673694066e-05,
      "loss": 0.0436,
      "step": 3243
    },
    {
      "epoch": 0.791219512195122,
      "grad_norm": 0.11227139830589294,
      "learning_rate": 4.190184890039688e-05,
      "loss": 0.0364,
      "step": 3244
    },
    {
      "epoch": 0.7914634146341464,
      "grad_norm": 0.08299446851015091,
      "learning_rate": 4.1897143024305793e-05,
      "loss": 0.0256,
      "step": 3245
    },
    {
      "epoch": 0.7917073170731708,
      "grad_norm": 0.15174297988414764,
      "learning_rate": 4.1892436045727844e-05,
      "loss": 0.0182,
      "step": 3246
    },
    {
      "epoch": 0.7919512195121952,
      "grad_norm": 0.0980062484741211,
      "learning_rate": 4.1887727964970165e-05,
      "loss": 0.0224,
      "step": 3247
    },
    {
      "epoch": 0.7921951219512195,
      "grad_norm": 0.2580992579460144,
      "learning_rate": 4.1883018782339926e-05,
      "loss": 0.0412,
      "step": 3248
    },
    {
      "epoch": 0.7924390243902439,
      "grad_norm": 0.17995478212833405,
      "learning_rate": 4.18783084981444e-05,
      "loss": 0.0396,
      "step": 3249
    },
    {
      "epoch": 0.7926829268292683,
      "grad_norm": 0.11128920316696167,
      "learning_rate": 4.187359711269092e-05,
      "loss": 0.0391,
      "step": 3250
    },
    {
      "epoch": 0.7929268292682927,
      "grad_norm": 0.05667863413691521,
      "learning_rate": 4.1868884626286875e-05,
      "loss": 0.017,
      "step": 3251
    },
    {
      "epoch": 0.7931707317073171,
      "grad_norm": 0.1349099725484848,
      "learning_rate": 4.1864171039239755e-05,
      "loss": 0.0305,
      "step": 3252
    },
    {
      "epoch": 0.7934146341463415,
      "grad_norm": 0.14813269674777985,
      "learning_rate": 4.1859456351857094e-05,
      "loss": 0.0277,
      "step": 3253
    },
    {
      "epoch": 0.7936585365853659,
      "grad_norm": 0.28351449966430664,
      "learning_rate": 4.185474056444653e-05,
      "loss": 0.0376,
      "step": 3254
    },
    {
      "epoch": 0.7939024390243903,
      "grad_norm": 0.17131470143795013,
      "learning_rate": 4.185002367731573e-05,
      "loss": 0.0191,
      "step": 3255
    },
    {
      "epoch": 0.7941463414634147,
      "grad_norm": 0.10208038985729218,
      "learning_rate": 4.1845305690772476e-05,
      "loss": 0.0173,
      "step": 3256
    },
    {
      "epoch": 0.7943902439024391,
      "grad_norm": 0.2193797081708908,
      "learning_rate": 4.184058660512459e-05,
      "loss": 0.0237,
      "step": 3257
    },
    {
      "epoch": 0.7946341463414635,
      "grad_norm": 0.08645946532487869,
      "learning_rate": 4.183586642067998e-05,
      "loss": 0.029,
      "step": 3258
    },
    {
      "epoch": 0.7948780487804878,
      "grad_norm": 0.1527782678604126,
      "learning_rate": 4.183114513774662e-05,
      "loss": 0.0206,
      "step": 3259
    },
    {
      "epoch": 0.7951219512195122,
      "grad_norm": 0.06346726417541504,
      "learning_rate": 4.182642275663258e-05,
      "loss": 0.0225,
      "step": 3260
    },
    {
      "epoch": 0.7953658536585366,
      "grad_norm": 0.05876970663666725,
      "learning_rate": 4.182169927764595e-05,
      "loss": 0.0199,
      "step": 3261
    },
    {
      "epoch": 0.795609756097561,
      "grad_norm": 0.05327260121703148,
      "learning_rate": 4.181697470109495e-05,
      "loss": 0.0151,
      "step": 3262
    },
    {
      "epoch": 0.7958536585365854,
      "grad_norm": 0.1400379091501236,
      "learning_rate": 4.1812249027287824e-05,
      "loss": 0.0157,
      "step": 3263
    },
    {
      "epoch": 0.7960975609756098,
      "grad_norm": 0.10286761820316315,
      "learning_rate": 4.180752225653292e-05,
      "loss": 0.0171,
      "step": 3264
    },
    {
      "epoch": 0.7963414634146342,
      "grad_norm": 0.11593420803546906,
      "learning_rate": 4.180279438913864e-05,
      "loss": 0.013,
      "step": 3265
    },
    {
      "epoch": 0.7965853658536586,
      "grad_norm": 0.2711605429649353,
      "learning_rate": 4.179806542541347e-05,
      "loss": 0.0297,
      "step": 3266
    },
    {
      "epoch": 0.796829268292683,
      "grad_norm": 0.1390801966190338,
      "learning_rate": 4.1793335365665954e-05,
      "loss": 0.0153,
      "step": 3267
    },
    {
      "epoch": 0.7970731707317074,
      "grad_norm": 0.11026620119810104,
      "learning_rate": 4.178860421020471e-05,
      "loss": 0.0426,
      "step": 3268
    },
    {
      "epoch": 0.7973170731707317,
      "grad_norm": 0.09953132271766663,
      "learning_rate": 4.1783871959338436e-05,
      "loss": 0.011,
      "step": 3269
    },
    {
      "epoch": 0.7975609756097561,
      "grad_norm": 0.10229448229074478,
      "learning_rate": 4.17791386133759e-05,
      "loss": 0.0227,
      "step": 3270
    },
    {
      "epoch": 0.7978048780487805,
      "grad_norm": 0.1371787190437317,
      "learning_rate": 4.177440417262593e-05,
      "loss": 0.032,
      "step": 3271
    },
    {
      "epoch": 0.7980487804878049,
      "grad_norm": 0.24617420136928558,
      "learning_rate": 4.176966863739745e-05,
      "loss": 0.0375,
      "step": 3272
    },
    {
      "epoch": 0.7982926829268293,
      "grad_norm": 0.21323728561401367,
      "learning_rate": 4.176493200799942e-05,
      "loss": 0.0264,
      "step": 3273
    },
    {
      "epoch": 0.7985365853658537,
      "grad_norm": 0.3502892255783081,
      "learning_rate": 4.1760194284740904e-05,
      "loss": 0.0308,
      "step": 3274
    },
    {
      "epoch": 0.7987804878048781,
      "grad_norm": 0.10283579677343369,
      "learning_rate": 4.175545546793101e-05,
      "loss": 0.022,
      "step": 3275
    },
    {
      "epoch": 0.7990243902439025,
      "grad_norm": 0.10181499272584915,
      "learning_rate": 4.175071555787895e-05,
      "loss": 0.0343,
      "step": 3276
    },
    {
      "epoch": 0.7992682926829269,
      "grad_norm": 0.10681923478841782,
      "learning_rate": 4.174597455489397e-05,
      "loss": 0.0163,
      "step": 3277
    },
    {
      "epoch": 0.7995121951219513,
      "grad_norm": 0.12682698667049408,
      "learning_rate": 4.1741232459285415e-05,
      "loss": 0.0235,
      "step": 3278
    },
    {
      "epoch": 0.7997560975609757,
      "grad_norm": 0.1438053995370865,
      "learning_rate": 4.17364892713627e-05,
      "loss": 0.0323,
      "step": 3279
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.15287701785564423,
      "learning_rate": 4.173174499143529e-05,
      "loss": 0.0186,
      "step": 3280
    },
    {
      "epoch": 0.8002439024390244,
      "grad_norm": 0.07495114952325821,
      "learning_rate": 4.172699961981275e-05,
      "loss": 0.0141,
      "step": 3281
    },
    {
      "epoch": 0.8004878048780488,
      "grad_norm": 0.07934940606355667,
      "learning_rate": 4.1722253156804674e-05,
      "loss": 0.0281,
      "step": 3282
    },
    {
      "epoch": 0.8007317073170732,
      "grad_norm": 0.14314429461956024,
      "learning_rate": 4.171750560272078e-05,
      "loss": 0.0318,
      "step": 3283
    },
    {
      "epoch": 0.8009756097560976,
      "grad_norm": 0.14404742419719696,
      "learning_rate": 4.1712756957870825e-05,
      "loss": 0.0307,
      "step": 3284
    },
    {
      "epoch": 0.801219512195122,
      "grad_norm": 0.15552717447280884,
      "learning_rate": 4.1708007222564636e-05,
      "loss": 0.0244,
      "step": 3285
    },
    {
      "epoch": 0.8014634146341464,
      "grad_norm": 0.1588616818189621,
      "learning_rate": 4.170325639711213e-05,
      "loss": 0.0416,
      "step": 3286
    },
    {
      "epoch": 0.8017073170731708,
      "grad_norm": 0.05999405309557915,
      "learning_rate": 4.169850448182327e-05,
      "loss": 0.0084,
      "step": 3287
    },
    {
      "epoch": 0.8019512195121952,
      "grad_norm": 0.10264904052019119,
      "learning_rate": 4.169375147700811e-05,
      "loss": 0.0192,
      "step": 3288
    },
    {
      "epoch": 0.8021951219512196,
      "grad_norm": 0.04851892217993736,
      "learning_rate": 4.168899738297678e-05,
      "loss": 0.0124,
      "step": 3289
    },
    {
      "epoch": 0.802439024390244,
      "grad_norm": 0.11125393211841583,
      "learning_rate": 4.168424220003946e-05,
      "loss": 0.0374,
      "step": 3290
    },
    {
      "epoch": 0.8026829268292683,
      "grad_norm": 0.08774256706237793,
      "learning_rate": 4.167948592850641e-05,
      "loss": 0.0165,
      "step": 3291
    },
    {
      "epoch": 0.8029268292682927,
      "grad_norm": 0.05819644406437874,
      "learning_rate": 4.167472856868796e-05,
      "loss": 0.0202,
      "step": 3292
    },
    {
      "epoch": 0.8031707317073171,
      "grad_norm": 0.04673400893807411,
      "learning_rate": 4.1669970120894516e-05,
      "loss": 0.013,
      "step": 3293
    },
    {
      "epoch": 0.8034146341463415,
      "grad_norm": 0.14546215534210205,
      "learning_rate": 4.166521058543656e-05,
      "loss": 0.0285,
      "step": 3294
    },
    {
      "epoch": 0.8036585365853659,
      "grad_norm": 0.08976688235998154,
      "learning_rate": 4.1660449962624626e-05,
      "loss": 0.0101,
      "step": 3295
    },
    {
      "epoch": 0.8039024390243903,
      "grad_norm": 0.18886734545230865,
      "learning_rate": 4.165568825276934e-05,
      "loss": 0.0222,
      "step": 3296
    },
    {
      "epoch": 0.8041463414634147,
      "grad_norm": 0.09932033717632294,
      "learning_rate": 4.165092545618138e-05,
      "loss": 0.0251,
      "step": 3297
    },
    {
      "epoch": 0.8043902439024391,
      "grad_norm": 0.15168049931526184,
      "learning_rate": 4.16461615731715e-05,
      "loss": 0.0317,
      "step": 3298
    },
    {
      "epoch": 0.8046341463414635,
      "grad_norm": 0.07885619252920151,
      "learning_rate": 4.164139660405055e-05,
      "loss": 0.0173,
      "step": 3299
    },
    {
      "epoch": 0.8048780487804879,
      "grad_norm": 0.1423613578081131,
      "learning_rate": 4.163663054912941e-05,
      "loss": 0.0303,
      "step": 3300
    },
    {
      "epoch": 0.8051219512195122,
      "grad_norm": 0.18102413415908813,
      "learning_rate": 4.1631863408719054e-05,
      "loss": 0.0378,
      "step": 3301
    },
    {
      "epoch": 0.8053658536585366,
      "grad_norm": 0.12066371738910675,
      "learning_rate": 4.162709518313052e-05,
      "loss": 0.0165,
      "step": 3302
    },
    {
      "epoch": 0.805609756097561,
      "grad_norm": 0.11485078185796738,
      "learning_rate": 4.1622325872674936e-05,
      "loss": 0.0218,
      "step": 3303
    },
    {
      "epoch": 0.8058536585365854,
      "grad_norm": 0.09629638493061066,
      "learning_rate": 4.1617555477663464e-05,
      "loss": 0.0159,
      "step": 3304
    },
    {
      "epoch": 0.8060975609756098,
      "grad_norm": 0.11918347328901291,
      "learning_rate": 4.161278399840738e-05,
      "loss": 0.0228,
      "step": 3305
    },
    {
      "epoch": 0.8063414634146342,
      "grad_norm": 0.06477100402116776,
      "learning_rate": 4.1608011435217985e-05,
      "loss": 0.0228,
      "step": 3306
    },
    {
      "epoch": 0.8065853658536586,
      "grad_norm": 0.052585046738386154,
      "learning_rate": 4.16032377884067e-05,
      "loss": 0.013,
      "step": 3307
    },
    {
      "epoch": 0.806829268292683,
      "grad_norm": 0.1512978971004486,
      "learning_rate": 4.1598463058284964e-05,
      "loss": 0.0155,
      "step": 3308
    },
    {
      "epoch": 0.8070731707317074,
      "grad_norm": 0.2724120616912842,
      "learning_rate": 4.1593687245164334e-05,
      "loss": 0.0137,
      "step": 3309
    },
    {
      "epoch": 0.8073170731707318,
      "grad_norm": 0.07945277541875839,
      "learning_rate": 4.1588910349356406e-05,
      "loss": 0.0303,
      "step": 3310
    },
    {
      "epoch": 0.8075609756097561,
      "grad_norm": 0.06489217281341553,
      "learning_rate": 4.158413237117287e-05,
      "loss": 0.0163,
      "step": 3311
    },
    {
      "epoch": 0.8078048780487805,
      "grad_norm": 0.24355125427246094,
      "learning_rate": 4.157935331092545e-05,
      "loss": 0.0271,
      "step": 3312
    },
    {
      "epoch": 0.8080487804878049,
      "grad_norm": 0.10323359072208405,
      "learning_rate": 4.157457316892599e-05,
      "loss": 0.0173,
      "step": 3313
    },
    {
      "epoch": 0.8082926829268293,
      "grad_norm": 0.22360631823539734,
      "learning_rate": 4.156979194548637e-05,
      "loss": 0.0194,
      "step": 3314
    },
    {
      "epoch": 0.8085365853658537,
      "grad_norm": 0.4874195158481598,
      "learning_rate": 4.156500964091855e-05,
      "loss": 0.0482,
      "step": 3315
    },
    {
      "epoch": 0.8087804878048781,
      "grad_norm": 0.15328121185302734,
      "learning_rate": 4.156022625553456e-05,
      "loss": 0.0106,
      "step": 3316
    },
    {
      "epoch": 0.8090243902439025,
      "grad_norm": 0.07418131828308105,
      "learning_rate": 4.15554417896465e-05,
      "loss": 0.0292,
      "step": 3317
    },
    {
      "epoch": 0.8092682926829269,
      "grad_norm": 0.18258509039878845,
      "learning_rate": 4.155065624356655e-05,
      "loss": 0.021,
      "step": 3318
    },
    {
      "epoch": 0.8095121951219513,
      "grad_norm": 0.07374469935894012,
      "learning_rate": 4.1545869617606945e-05,
      "loss": 0.0144,
      "step": 3319
    },
    {
      "epoch": 0.8097560975609757,
      "grad_norm": 0.07296004146337509,
      "learning_rate": 4.154108191207999e-05,
      "loss": 0.0139,
      "step": 3320
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.12128555774688721,
      "learning_rate": 4.153629312729809e-05,
      "loss": 0.0298,
      "step": 3321
    },
    {
      "epoch": 0.8102439024390244,
      "grad_norm": 0.2857989966869354,
      "learning_rate": 4.1531503263573684e-05,
      "loss": 0.0282,
      "step": 3322
    },
    {
      "epoch": 0.8104878048780488,
      "grad_norm": 0.12629970908164978,
      "learning_rate": 4.15267123212193e-05,
      "loss": 0.0151,
      "step": 3323
    },
    {
      "epoch": 0.8107317073170732,
      "grad_norm": 0.1522292047739029,
      "learning_rate": 4.1521920300547525e-05,
      "loss": 0.0288,
      "step": 3324
    },
    {
      "epoch": 0.8109756097560976,
      "grad_norm": 0.15597781538963318,
      "learning_rate": 4.151712720187103e-05,
      "loss": 0.0232,
      "step": 3325
    },
    {
      "epoch": 0.811219512195122,
      "grad_norm": 0.13192780315876007,
      "learning_rate": 4.1512333025502545e-05,
      "loss": 0.031,
      "step": 3326
    },
    {
      "epoch": 0.8114634146341464,
      "grad_norm": 0.24988891184329987,
      "learning_rate": 4.1507537771754876e-05,
      "loss": 0.0347,
      "step": 3327
    },
    {
      "epoch": 0.8117073170731708,
      "grad_norm": 0.6338774561882019,
      "learning_rate": 4.150274144094092e-05,
      "loss": 0.0411,
      "step": 3328
    },
    {
      "epoch": 0.8119512195121952,
      "grad_norm": 0.08305324614048004,
      "learning_rate": 4.149794403337359e-05,
      "loss": 0.0328,
      "step": 3329
    },
    {
      "epoch": 0.8121951219512196,
      "grad_norm": 0.14199508726596832,
      "learning_rate": 4.149314554936592e-05,
      "loss": 0.0226,
      "step": 3330
    },
    {
      "epoch": 0.812439024390244,
      "grad_norm": 0.08289982378482819,
      "learning_rate": 4.148834598923099e-05,
      "loss": 0.0246,
      "step": 3331
    },
    {
      "epoch": 0.8126829268292682,
      "grad_norm": 0.09723706543445587,
      "learning_rate": 4.1483545353281965e-05,
      "loss": 0.0227,
      "step": 3332
    },
    {
      "epoch": 0.8129268292682926,
      "grad_norm": 0.22053444385528564,
      "learning_rate": 4.1478743641832064e-05,
      "loss": 0.0113,
      "step": 3333
    },
    {
      "epoch": 0.813170731707317,
      "grad_norm": 0.09385522454977036,
      "learning_rate": 4.147394085519459e-05,
      "loss": 0.0255,
      "step": 3334
    },
    {
      "epoch": 0.8134146341463414,
      "grad_norm": 0.08512410521507263,
      "learning_rate": 4.14691369936829e-05,
      "loss": 0.0194,
      "step": 3335
    },
    {
      "epoch": 0.8136585365853658,
      "grad_norm": 0.3028070330619812,
      "learning_rate": 4.146433205761045e-05,
      "loss": 0.0277,
      "step": 3336
    },
    {
      "epoch": 0.8139024390243902,
      "grad_norm": 0.0969076082110405,
      "learning_rate": 4.145952604729072e-05,
      "loss": 0.0299,
      "step": 3337
    },
    {
      "epoch": 0.8141463414634146,
      "grad_norm": 0.23305876553058624,
      "learning_rate": 4.145471896303731e-05,
      "loss": 0.035,
      "step": 3338
    },
    {
      "epoch": 0.814390243902439,
      "grad_norm": 0.08615988492965698,
      "learning_rate": 4.144991080516386e-05,
      "loss": 0.013,
      "step": 3339
    },
    {
      "epoch": 0.8146341463414634,
      "grad_norm": 0.15541478991508484,
      "learning_rate": 4.1445101573984094e-05,
      "loss": 0.0137,
      "step": 3340
    },
    {
      "epoch": 0.8148780487804878,
      "grad_norm": 0.14696954190731049,
      "learning_rate": 4.144029126981178e-05,
      "loss": 0.0277,
      "step": 3341
    },
    {
      "epoch": 0.8151219512195121,
      "grad_norm": 0.12737329304218292,
      "learning_rate": 4.1435479892960804e-05,
      "loss": 0.0183,
      "step": 3342
    },
    {
      "epoch": 0.8153658536585365,
      "grad_norm": 0.1128523051738739,
      "learning_rate": 4.143066744374508e-05,
      "loss": 0.0247,
      "step": 3343
    },
    {
      "epoch": 0.8156097560975609,
      "grad_norm": 0.19846439361572266,
      "learning_rate": 4.142585392247858e-05,
      "loss": 0.0149,
      "step": 3344
    },
    {
      "epoch": 0.8158536585365853,
      "grad_norm": 0.16838060319423676,
      "learning_rate": 4.142103932947542e-05,
      "loss": 0.0315,
      "step": 3345
    },
    {
      "epoch": 0.8160975609756097,
      "grad_norm": 0.2766737937927246,
      "learning_rate": 4.14162236650497e-05,
      "loss": 0.0198,
      "step": 3346
    },
    {
      "epoch": 0.8163414634146341,
      "grad_norm": 0.18763969838619232,
      "learning_rate": 4.141140692951565e-05,
      "loss": 0.0357,
      "step": 3347
    },
    {
      "epoch": 0.8165853658536585,
      "grad_norm": 0.12603141367435455,
      "learning_rate": 4.140658912318753e-05,
      "loss": 0.026,
      "step": 3348
    },
    {
      "epoch": 0.8168292682926829,
      "grad_norm": 0.09696923941373825,
      "learning_rate": 4.140177024637971e-05,
      "loss": 0.0218,
      "step": 3349
    },
    {
      "epoch": 0.8170731707317073,
      "grad_norm": 0.09799016267061234,
      "learning_rate": 4.139695029940658e-05,
      "loss": 0.0208,
      "step": 3350
    },
    {
      "epoch": 0.8173170731707317,
      "grad_norm": 0.08458630740642548,
      "learning_rate": 4.139212928258264e-05,
      "loss": 0.0189,
      "step": 3351
    },
    {
      "epoch": 0.817560975609756,
      "grad_norm": 0.09651351720094681,
      "learning_rate": 4.138730719622245e-05,
      "loss": 0.033,
      "step": 3352
    },
    {
      "epoch": 0.8178048780487804,
      "grad_norm": 0.14853651821613312,
      "learning_rate": 4.138248404064062e-05,
      "loss": 0.0173,
      "step": 3353
    },
    {
      "epoch": 0.8180487804878048,
      "grad_norm": 0.08391445130109787,
      "learning_rate": 4.137765981615187e-05,
      "loss": 0.0296,
      "step": 3354
    },
    {
      "epoch": 0.8182926829268292,
      "grad_norm": 0.12489716708660126,
      "learning_rate": 4.1372834523070944e-05,
      "loss": 0.0378,
      "step": 3355
    },
    {
      "epoch": 0.8185365853658536,
      "grad_norm": 0.11407621949911118,
      "learning_rate": 4.1368008161712694e-05,
      "loss": 0.018,
      "step": 3356
    },
    {
      "epoch": 0.818780487804878,
      "grad_norm": 0.14851385354995728,
      "learning_rate": 4.136318073239203e-05,
      "loss": 0.0306,
      "step": 3357
    },
    {
      "epoch": 0.8190243902439024,
      "grad_norm": 0.05680026113986969,
      "learning_rate": 4.13583522354239e-05,
      "loss": 0.0146,
      "step": 3358
    },
    {
      "epoch": 0.8192682926829268,
      "grad_norm": 0.151833176612854,
      "learning_rate": 4.135352267112337e-05,
      "loss": 0.0196,
      "step": 3359
    },
    {
      "epoch": 0.8195121951219512,
      "grad_norm": 0.09497719258069992,
      "learning_rate": 4.1348692039805545e-05,
      "loss": 0.0226,
      "step": 3360
    },
    {
      "epoch": 0.8197560975609756,
      "grad_norm": 0.16805092990398407,
      "learning_rate": 4.1343860341785614e-05,
      "loss": 0.0171,
      "step": 3361
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.13475631177425385,
      "learning_rate": 4.133902757737882e-05,
      "loss": 0.036,
      "step": 3362
    },
    {
      "epoch": 0.8202439024390243,
      "grad_norm": 0.06346780061721802,
      "learning_rate": 4.133419374690051e-05,
      "loss": 0.0181,
      "step": 3363
    },
    {
      "epoch": 0.8204878048780487,
      "grad_norm": 0.13482841849327087,
      "learning_rate": 4.132935885066606e-05,
      "loss": 0.0189,
      "step": 3364
    },
    {
      "epoch": 0.8207317073170731,
      "grad_norm": 0.12089792639017105,
      "learning_rate": 4.132452288899092e-05,
      "loss": 0.0205,
      "step": 3365
    },
    {
      "epoch": 0.8209756097560975,
      "grad_norm": 0.0770159587264061,
      "learning_rate": 4.131968586219065e-05,
      "loss": 0.0179,
      "step": 3366
    },
    {
      "epoch": 0.8212195121951219,
      "grad_norm": 0.1219995766878128,
      "learning_rate": 4.131484777058083e-05,
      "loss": 0.0215,
      "step": 3367
    },
    {
      "epoch": 0.8214634146341463,
      "grad_norm": 0.16182556748390198,
      "learning_rate": 4.131000861447715e-05,
      "loss": 0.0331,
      "step": 3368
    },
    {
      "epoch": 0.8217073170731707,
      "grad_norm": 0.18886017799377441,
      "learning_rate": 4.130516839419533e-05,
      "loss": 0.0265,
      "step": 3369
    },
    {
      "epoch": 0.8219512195121951,
      "grad_norm": 0.05715832859277725,
      "learning_rate": 4.130032711005117e-05,
      "loss": 0.0122,
      "step": 3370
    },
    {
      "epoch": 0.8221951219512195,
      "grad_norm": 0.1344272643327713,
      "learning_rate": 4.129548476236059e-05,
      "loss": 0.0119,
      "step": 3371
    },
    {
      "epoch": 0.8224390243902439,
      "grad_norm": 0.28568315505981445,
      "learning_rate": 4.1290641351439506e-05,
      "loss": 0.0273,
      "step": 3372
    },
    {
      "epoch": 0.8226829268292682,
      "grad_norm": 0.09904375672340393,
      "learning_rate": 4.1285796877603945e-05,
      "loss": 0.0294,
      "step": 3373
    },
    {
      "epoch": 0.8229268292682926,
      "grad_norm": 0.08299393206834793,
      "learning_rate": 4.1280951341169997e-05,
      "loss": 0.0291,
      "step": 3374
    },
    {
      "epoch": 0.823170731707317,
      "grad_norm": 0.06548214703798294,
      "learning_rate": 4.12761047424538e-05,
      "loss": 0.0112,
      "step": 3375
    },
    {
      "epoch": 0.8234146341463414,
      "grad_norm": 0.05431462451815605,
      "learning_rate": 4.127125708177161e-05,
      "loss": 0.0173,
      "step": 3376
    },
    {
      "epoch": 0.8236585365853658,
      "grad_norm": 0.10043555498123169,
      "learning_rate": 4.12664083594397e-05,
      "loss": 0.0189,
      "step": 3377
    },
    {
      "epoch": 0.8239024390243902,
      "grad_norm": 0.2298356592655182,
      "learning_rate": 4.126155857577444e-05,
      "loss": 0.0226,
      "step": 3378
    },
    {
      "epoch": 0.8241463414634146,
      "grad_norm": 0.15588799118995667,
      "learning_rate": 4.125670773109226e-05,
      "loss": 0.034,
      "step": 3379
    },
    {
      "epoch": 0.824390243902439,
      "grad_norm": 0.09995362162590027,
      "learning_rate": 4.125185582570968e-05,
      "loss": 0.0142,
      "step": 3380
    },
    {
      "epoch": 0.8246341463414634,
      "grad_norm": 0.08363135904073715,
      "learning_rate": 4.124700285994325e-05,
      "loss": 0.0138,
      "step": 3381
    },
    {
      "epoch": 0.8248780487804878,
      "grad_norm": 0.15447598695755005,
      "learning_rate": 4.124214883410963e-05,
      "loss": 0.014,
      "step": 3382
    },
    {
      "epoch": 0.8251219512195122,
      "grad_norm": 0.1308731734752655,
      "learning_rate": 4.123729374852551e-05,
      "loss": 0.0193,
      "step": 3383
    },
    {
      "epoch": 0.8253658536585365,
      "grad_norm": 0.18060950934886932,
      "learning_rate": 4.123243760350768e-05,
      "loss": 0.0214,
      "step": 3384
    },
    {
      "epoch": 0.8256097560975609,
      "grad_norm": 0.10771951824426651,
      "learning_rate": 4.122758039937299e-05,
      "loss": 0.019,
      "step": 3385
    },
    {
      "epoch": 0.8258536585365853,
      "grad_norm": 0.07050244510173798,
      "learning_rate": 4.122272213643836e-05,
      "loss": 0.0162,
      "step": 3386
    },
    {
      "epoch": 0.8260975609756097,
      "grad_norm": 0.04311415180563927,
      "learning_rate": 4.121786281502077e-05,
      "loss": 0.011,
      "step": 3387
    },
    {
      "epoch": 0.8263414634146341,
      "grad_norm": 0.1335402876138687,
      "learning_rate": 4.121300243543728e-05,
      "loss": 0.0227,
      "step": 3388
    },
    {
      "epoch": 0.8265853658536585,
      "grad_norm": 0.09701530635356903,
      "learning_rate": 4.1208140998005015e-05,
      "loss": 0.0122,
      "step": 3389
    },
    {
      "epoch": 0.8268292682926829,
      "grad_norm": 0.09971588850021362,
      "learning_rate": 4.120327850304117e-05,
      "loss": 0.0162,
      "step": 3390
    },
    {
      "epoch": 0.8270731707317073,
      "grad_norm": 0.11544392257928848,
      "learning_rate": 4.1198414950863e-05,
      "loss": 0.017,
      "step": 3391
    },
    {
      "epoch": 0.8273170731707317,
      "grad_norm": 0.13709677755832672,
      "learning_rate": 4.1193550341787845e-05,
      "loss": 0.0192,
      "step": 3392
    },
    {
      "epoch": 0.827560975609756,
      "grad_norm": 0.20759403705596924,
      "learning_rate": 4.1188684676133104e-05,
      "loss": 0.0417,
      "step": 3393
    },
    {
      "epoch": 0.8278048780487804,
      "grad_norm": 0.2068265825510025,
      "learning_rate": 4.118381795421624e-05,
      "loss": 0.0275,
      "step": 3394
    },
    {
      "epoch": 0.8280487804878048,
      "grad_norm": 0.15668655931949615,
      "learning_rate": 4.117895017635481e-05,
      "loss": 0.0111,
      "step": 3395
    },
    {
      "epoch": 0.8282926829268292,
      "grad_norm": 0.11239691078662872,
      "learning_rate": 4.11740813428664e-05,
      "loss": 0.0212,
      "step": 3396
    },
    {
      "epoch": 0.8285365853658536,
      "grad_norm": 0.14862829446792603,
      "learning_rate": 4.1169211454068704e-05,
      "loss": 0.0267,
      "step": 3397
    },
    {
      "epoch": 0.828780487804878,
      "grad_norm": 0.09563378989696503,
      "learning_rate": 4.116434051027945e-05,
      "loss": 0.0225,
      "step": 3398
    },
    {
      "epoch": 0.8290243902439024,
      "grad_norm": 0.08754928410053253,
      "learning_rate": 4.115946851181647e-05,
      "loss": 0.0209,
      "step": 3399
    },
    {
      "epoch": 0.8292682926829268,
      "grad_norm": 0.08312029391527176,
      "learning_rate": 4.1154595458997634e-05,
      "loss": 0.0331,
      "step": 3400
    },
    {
      "epoch": 0.8295121951219512,
      "grad_norm": 0.07944956421852112,
      "learning_rate": 4.1149721352140904e-05,
      "loss": 0.0261,
      "step": 3401
    },
    {
      "epoch": 0.8297560975609756,
      "grad_norm": 0.15513522922992706,
      "learning_rate": 4.114484619156428e-05,
      "loss": 0.0319,
      "step": 3402
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.09548347443342209,
      "learning_rate": 4.113996997758588e-05,
      "loss": 0.0282,
      "step": 3403
    },
    {
      "epoch": 0.8302439024390244,
      "grad_norm": 0.16998079419136047,
      "learning_rate": 4.113509271052384e-05,
      "loss": 0.0109,
      "step": 3404
    },
    {
      "epoch": 0.8304878048780487,
      "grad_norm": 0.1145850345492363,
      "learning_rate": 4.11302143906964e-05,
      "loss": 0.014,
      "step": 3405
    },
    {
      "epoch": 0.8307317073170731,
      "grad_norm": 0.0735488012433052,
      "learning_rate": 4.112533501842185e-05,
      "loss": 0.0226,
      "step": 3406
    },
    {
      "epoch": 0.8309756097560975,
      "grad_norm": 0.11842967569828033,
      "learning_rate": 4.1120454594018556e-05,
      "loss": 0.0362,
      "step": 3407
    },
    {
      "epoch": 0.8312195121951219,
      "grad_norm": 0.08265028148889542,
      "learning_rate": 4.1115573117804945e-05,
      "loss": 0.0144,
      "step": 3408
    },
    {
      "epoch": 0.8314634146341463,
      "grad_norm": 0.10971041768789291,
      "learning_rate": 4.111069059009952e-05,
      "loss": 0.04,
      "step": 3409
    },
    {
      "epoch": 0.8317073170731707,
      "grad_norm": 0.14043287932872772,
      "learning_rate": 4.110580701122086e-05,
      "loss": 0.024,
      "step": 3410
    },
    {
      "epoch": 0.8319512195121951,
      "grad_norm": 0.10160969942808151,
      "learning_rate": 4.110092238148759e-05,
      "loss": 0.0249,
      "step": 3411
    },
    {
      "epoch": 0.8321951219512195,
      "grad_norm": 0.07882186770439148,
      "learning_rate": 4.1096036701218434e-05,
      "loss": 0.0251,
      "step": 3412
    },
    {
      "epoch": 0.8324390243902439,
      "grad_norm": 0.13970722258090973,
      "learning_rate": 4.109114997073215e-05,
      "loss": 0.0309,
      "step": 3413
    },
    {
      "epoch": 0.8326829268292683,
      "grad_norm": 0.07425494492053986,
      "learning_rate": 4.108626219034759e-05,
      "loss": 0.015,
      "step": 3414
    },
    {
      "epoch": 0.8329268292682926,
      "grad_norm": 0.10019514709711075,
      "learning_rate": 4.108137336038368e-05,
      "loss": 0.0235,
      "step": 3415
    },
    {
      "epoch": 0.833170731707317,
      "grad_norm": 0.05895193666219711,
      "learning_rate": 4.1076483481159375e-05,
      "loss": 0.013,
      "step": 3416
    },
    {
      "epoch": 0.8334146341463414,
      "grad_norm": 0.09852011501789093,
      "learning_rate": 4.107159255299374e-05,
      "loss": 0.0229,
      "step": 3417
    },
    {
      "epoch": 0.8336585365853658,
      "grad_norm": 0.055325280874967575,
      "learning_rate": 4.1066700576205897e-05,
      "loss": 0.0125,
      "step": 3418
    },
    {
      "epoch": 0.8339024390243902,
      "grad_norm": 0.07350235432386398,
      "learning_rate": 4.1061807551115014e-05,
      "loss": 0.0212,
      "step": 3419
    },
    {
      "epoch": 0.8341463414634146,
      "grad_norm": 0.12228765338659286,
      "learning_rate": 4.105691347804036e-05,
      "loss": 0.0124,
      "step": 3420
    },
    {
      "epoch": 0.834390243902439,
      "grad_norm": 0.055241700261831284,
      "learning_rate": 4.105201835730127e-05,
      "loss": 0.0189,
      "step": 3421
    },
    {
      "epoch": 0.8346341463414634,
      "grad_norm": 0.31395018100738525,
      "learning_rate": 4.1047122189217114e-05,
      "loss": 0.0351,
      "step": 3422
    },
    {
      "epoch": 0.8348780487804878,
      "grad_norm": 0.0738452896475792,
      "learning_rate": 4.1042224974107355e-05,
      "loss": 0.0185,
      "step": 3423
    },
    {
      "epoch": 0.8351219512195122,
      "grad_norm": 0.07002012431621552,
      "learning_rate": 4.103732671229153e-05,
      "loss": 0.0138,
      "step": 3424
    },
    {
      "epoch": 0.8353658536585366,
      "grad_norm": 0.27610287070274353,
      "learning_rate": 4.1032427404089235e-05,
      "loss": 0.0215,
      "step": 3425
    },
    {
      "epoch": 0.8356097560975609,
      "grad_norm": 0.0657019391655922,
      "learning_rate": 4.102752704982013e-05,
      "loss": 0.018,
      "step": 3426
    },
    {
      "epoch": 0.8358536585365853,
      "grad_norm": 0.10680121928453445,
      "learning_rate": 4.102262564980395e-05,
      "loss": 0.0248,
      "step": 3427
    },
    {
      "epoch": 0.8360975609756097,
      "grad_norm": 0.2726028859615326,
      "learning_rate": 4.1017723204360497e-05,
      "loss": 0.015,
      "step": 3428
    },
    {
      "epoch": 0.8363414634146341,
      "grad_norm": 0.05917251482605934,
      "learning_rate": 4.1012819713809633e-05,
      "loss": 0.0187,
      "step": 3429
    },
    {
      "epoch": 0.8365853658536585,
      "grad_norm": 0.28664782643318176,
      "learning_rate": 4.100791517847131e-05,
      "loss": 0.0161,
      "step": 3430
    },
    {
      "epoch": 0.8368292682926829,
      "grad_norm": 0.08859971165657043,
      "learning_rate": 4.100300959866553e-05,
      "loss": 0.0189,
      "step": 3431
    },
    {
      "epoch": 0.8370731707317073,
      "grad_norm": 0.15732407569885254,
      "learning_rate": 4.099810297471236e-05,
      "loss": 0.0367,
      "step": 3432
    },
    {
      "epoch": 0.8373170731707317,
      "grad_norm": 0.07659095525741577,
      "learning_rate": 4.099319530693195e-05,
      "loss": 0.0264,
      "step": 3433
    },
    {
      "epoch": 0.8375609756097561,
      "grad_norm": 0.127768412232399,
      "learning_rate": 4.0988286595644496e-05,
      "loss": 0.0195,
      "step": 3434
    },
    {
      "epoch": 0.8378048780487805,
      "grad_norm": 0.08033987134695053,
      "learning_rate": 4.098337684117029e-05,
      "loss": 0.0219,
      "step": 3435
    },
    {
      "epoch": 0.8380487804878048,
      "grad_norm": 0.21648353338241577,
      "learning_rate": 4.097846604382968e-05,
      "loss": 0.0366,
      "step": 3436
    },
    {
      "epoch": 0.8382926829268292,
      "grad_norm": 0.10564076900482178,
      "learning_rate": 4.097355420394307e-05,
      "loss": 0.0222,
      "step": 3437
    },
    {
      "epoch": 0.8385365853658536,
      "grad_norm": 0.08393796533346176,
      "learning_rate": 4.0968641321830946e-05,
      "loss": 0.0172,
      "step": 3438
    },
    {
      "epoch": 0.838780487804878,
      "grad_norm": 0.1471751183271408,
      "learning_rate": 4.0963727397813865e-05,
      "loss": 0.03,
      "step": 3439
    },
    {
      "epoch": 0.8390243902439024,
      "grad_norm": 0.24723906815052032,
      "learning_rate": 4.0958812432212444e-05,
      "loss": 0.0195,
      "step": 3440
    },
    {
      "epoch": 0.8392682926829268,
      "grad_norm": 0.30614736676216125,
      "learning_rate": 4.095389642534736e-05,
      "loss": 0.03,
      "step": 3441
    },
    {
      "epoch": 0.8395121951219512,
      "grad_norm": 0.06052803248167038,
      "learning_rate": 4.0948979377539376e-05,
      "loss": 0.0201,
      "step": 3442
    },
    {
      "epoch": 0.8397560975609756,
      "grad_norm": 0.15995657444000244,
      "learning_rate": 4.094406128910931e-05,
      "loss": 0.0293,
      "step": 3443
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.1880687177181244,
      "learning_rate": 4.0939142160378056e-05,
      "loss": 0.022,
      "step": 3444
    },
    {
      "epoch": 0.8402439024390244,
      "grad_norm": 0.07615633308887482,
      "learning_rate": 4.093422199166658e-05,
      "loss": 0.0238,
      "step": 3445
    },
    {
      "epoch": 0.8404878048780487,
      "grad_norm": 0.26425784826278687,
      "learning_rate": 4.092930078329588e-05,
      "loss": 0.0222,
      "step": 3446
    },
    {
      "epoch": 0.8407317073170731,
      "grad_norm": 0.06276194006204605,
      "learning_rate": 4.092437853558708e-05,
      "loss": 0.0115,
      "step": 3447
    },
    {
      "epoch": 0.8409756097560975,
      "grad_norm": 0.21179546415805817,
      "learning_rate": 4.0919455248861326e-05,
      "loss": 0.0352,
      "step": 3448
    },
    {
      "epoch": 0.8412195121951219,
      "grad_norm": 0.05260230973362923,
      "learning_rate": 4.091453092343985e-05,
      "loss": 0.0215,
      "step": 3449
    },
    {
      "epoch": 0.8414634146341463,
      "grad_norm": 0.08697205036878586,
      "learning_rate": 4.090960555964396e-05,
      "loss": 0.0258,
      "step": 3450
    },
    {
      "epoch": 0.8417073170731707,
      "grad_norm": 0.0898907259106636,
      "learning_rate": 4.0904679157795e-05,
      "loss": 0.0175,
      "step": 3451
    },
    {
      "epoch": 0.8419512195121951,
      "grad_norm": 0.054433975368738174,
      "learning_rate": 4.089975171821442e-05,
      "loss": 0.008,
      "step": 3452
    },
    {
      "epoch": 0.8421951219512195,
      "grad_norm": 0.10933712869882584,
      "learning_rate": 4.089482324122371e-05,
      "loss": 0.0199,
      "step": 3453
    },
    {
      "epoch": 0.8424390243902439,
      "grad_norm": 0.10668206214904785,
      "learning_rate": 4.088989372714444e-05,
      "loss": 0.0178,
      "step": 3454
    },
    {
      "epoch": 0.8426829268292683,
      "grad_norm": 0.10260296612977982,
      "learning_rate": 4.088496317629825e-05,
      "loss": 0.024,
      "step": 3455
    },
    {
      "epoch": 0.8429268292682927,
      "grad_norm": 0.15542857348918915,
      "learning_rate": 4.088003158900685e-05,
      "loss": 0.0202,
      "step": 3456
    },
    {
      "epoch": 0.843170731707317,
      "grad_norm": 0.2278975397348404,
      "learning_rate": 4.0875098965592e-05,
      "loss": 0.0218,
      "step": 3457
    },
    {
      "epoch": 0.8434146341463414,
      "grad_norm": 0.07250085473060608,
      "learning_rate": 4.087016530637553e-05,
      "loss": 0.017,
      "step": 3458
    },
    {
      "epoch": 0.8436585365853658,
      "grad_norm": 0.12652650475502014,
      "learning_rate": 4.0865230611679366e-05,
      "loss": 0.0185,
      "step": 3459
    },
    {
      "epoch": 0.8439024390243902,
      "grad_norm": 0.09898804128170013,
      "learning_rate": 4.086029488182547e-05,
      "loss": 0.0172,
      "step": 3460
    },
    {
      "epoch": 0.8441463414634146,
      "grad_norm": 0.0904155895113945,
      "learning_rate": 4.085535811713589e-05,
      "loss": 0.0226,
      "step": 3461
    },
    {
      "epoch": 0.844390243902439,
      "grad_norm": 0.06801192462444305,
      "learning_rate": 4.0850420317932725e-05,
      "loss": 0.0206,
      "step": 3462
    },
    {
      "epoch": 0.8446341463414634,
      "grad_norm": 0.08180008083581924,
      "learning_rate": 4.084548148453816e-05,
      "loss": 0.02,
      "step": 3463
    },
    {
      "epoch": 0.8448780487804878,
      "grad_norm": 0.10991887003183365,
      "learning_rate": 4.0840541617274436e-05,
      "loss": 0.0239,
      "step": 3464
    },
    {
      "epoch": 0.8451219512195122,
      "grad_norm": 0.09330886602401733,
      "learning_rate": 4.083560071646386e-05,
      "loss": 0.0234,
      "step": 3465
    },
    {
      "epoch": 0.8453658536585366,
      "grad_norm": 0.11179585009813309,
      "learning_rate": 4.083065878242882e-05,
      "loss": 0.0306,
      "step": 3466
    },
    {
      "epoch": 0.845609756097561,
      "grad_norm": 0.09855072945356369,
      "learning_rate": 4.082571581549175e-05,
      "loss": 0.0193,
      "step": 3467
    },
    {
      "epoch": 0.8458536585365853,
      "grad_norm": 0.12445354461669922,
      "learning_rate": 4.082077181597518e-05,
      "loss": 0.0218,
      "step": 3468
    },
    {
      "epoch": 0.8460975609756097,
      "grad_norm": 0.1426047533750534,
      "learning_rate": 4.081582678420167e-05,
      "loss": 0.0206,
      "step": 3469
    },
    {
      "epoch": 0.8463414634146341,
      "grad_norm": 0.1471969336271286,
      "learning_rate": 4.081088072049388e-05,
      "loss": 0.0272,
      "step": 3470
    },
    {
      "epoch": 0.8465853658536585,
      "grad_norm": 0.06079135835170746,
      "learning_rate": 4.080593362517454e-05,
      "loss": 0.0211,
      "step": 3471
    },
    {
      "epoch": 0.8468292682926829,
      "grad_norm": 0.058591246604919434,
      "learning_rate": 4.080098549856641e-05,
      "loss": 0.0089,
      "step": 3472
    },
    {
      "epoch": 0.8470731707317073,
      "grad_norm": 0.04667077213525772,
      "learning_rate": 4.0796036340992336e-05,
      "loss": 0.0189,
      "step": 3473
    },
    {
      "epoch": 0.8473170731707317,
      "grad_norm": 0.11978962272405624,
      "learning_rate": 4.079108615277526e-05,
      "loss": 0.0355,
      "step": 3474
    },
    {
      "epoch": 0.8475609756097561,
      "grad_norm": 0.0948333814740181,
      "learning_rate": 4.078613493423815e-05,
      "loss": 0.014,
      "step": 3475
    },
    {
      "epoch": 0.8478048780487805,
      "grad_norm": 0.08803873509168625,
      "learning_rate": 4.0781182685704056e-05,
      "loss": 0.0349,
      "step": 3476
    },
    {
      "epoch": 0.8480487804878049,
      "grad_norm": 0.08435026556253433,
      "learning_rate": 4.0776229407496116e-05,
      "loss": 0.0235,
      "step": 3477
    },
    {
      "epoch": 0.8482926829268292,
      "grad_norm": 0.1513741910457611,
      "learning_rate": 4.0771275099937486e-05,
      "loss": 0.0219,
      "step": 3478
    },
    {
      "epoch": 0.8485365853658536,
      "grad_norm": 0.08711595088243484,
      "learning_rate": 4.076631976335145e-05,
      "loss": 0.0295,
      "step": 3479
    },
    {
      "epoch": 0.848780487804878,
      "grad_norm": 0.05254179984331131,
      "learning_rate": 4.07613633980613e-05,
      "loss": 0.0166,
      "step": 3480
    },
    {
      "epoch": 0.8490243902439024,
      "grad_norm": 0.11919479072093964,
      "learning_rate": 4.0756406004390454e-05,
      "loss": 0.0172,
      "step": 3481
    },
    {
      "epoch": 0.8492682926829268,
      "grad_norm": 0.29984933137893677,
      "learning_rate": 4.075144758266235e-05,
      "loss": 0.0373,
      "step": 3482
    },
    {
      "epoch": 0.8495121951219512,
      "grad_norm": 0.13042646646499634,
      "learning_rate": 4.07464881332005e-05,
      "loss": 0.0161,
      "step": 3483
    },
    {
      "epoch": 0.8497560975609756,
      "grad_norm": 0.0874234288930893,
      "learning_rate": 4.074152765632852e-05,
      "loss": 0.0205,
      "step": 3484
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.07744727283716202,
      "learning_rate": 4.073656615237003e-05,
      "loss": 0.0337,
      "step": 3485
    },
    {
      "epoch": 0.8502439024390244,
      "grad_norm": 0.14316169917583466,
      "learning_rate": 4.0731603621648786e-05,
      "loss": 0.0255,
      "step": 3486
    },
    {
      "epoch": 0.8504878048780488,
      "grad_norm": 0.1457097977399826,
      "learning_rate": 4.072664006448856e-05,
      "loss": 0.0544,
      "step": 3487
    },
    {
      "epoch": 0.8507317073170731,
      "grad_norm": 0.045594338327646255,
      "learning_rate": 4.072167548121322e-05,
      "loss": 0.0085,
      "step": 3488
    },
    {
      "epoch": 0.8509756097560975,
      "grad_norm": 0.15055638551712036,
      "learning_rate": 4.0716709872146675e-05,
      "loss": 0.0297,
      "step": 3489
    },
    {
      "epoch": 0.8512195121951219,
      "grad_norm": 0.10653951019048691,
      "learning_rate": 4.071174323761293e-05,
      "loss": 0.0477,
      "step": 3490
    },
    {
      "epoch": 0.8514634146341463,
      "grad_norm": 0.18339219689369202,
      "learning_rate": 4.0706775577936034e-05,
      "loss": 0.0315,
      "step": 3491
    },
    {
      "epoch": 0.8517073170731707,
      "grad_norm": 0.32595476508140564,
      "learning_rate": 4.070180689344012e-05,
      "loss": 0.0287,
      "step": 3492
    },
    {
      "epoch": 0.8519512195121951,
      "grad_norm": 0.08963172137737274,
      "learning_rate": 4.069683718444937e-05,
      "loss": 0.0215,
      "step": 3493
    },
    {
      "epoch": 0.8521951219512195,
      "grad_norm": 0.1456991285085678,
      "learning_rate": 4.0691866451288044e-05,
      "loss": 0.032,
      "step": 3494
    },
    {
      "epoch": 0.8524390243902439,
      "grad_norm": 0.12163258343935013,
      "learning_rate": 4.0686894694280464e-05,
      "loss": 0.0239,
      "step": 3495
    },
    {
      "epoch": 0.8526829268292683,
      "grad_norm": 0.12583933770656586,
      "learning_rate": 4.068192191375104e-05,
      "loss": 0.0266,
      "step": 3496
    },
    {
      "epoch": 0.8529268292682927,
      "grad_norm": 0.0783572867512703,
      "learning_rate": 4.0676948110024215e-05,
      "loss": 0.0332,
      "step": 3497
    },
    {
      "epoch": 0.853170731707317,
      "grad_norm": 0.13950739800930023,
      "learning_rate": 4.0671973283424514e-05,
      "loss": 0.041,
      "step": 3498
    },
    {
      "epoch": 0.8534146341463414,
      "grad_norm": 0.09232805669307709,
      "learning_rate": 4.066699743427653e-05,
      "loss": 0.023,
      "step": 3499
    },
    {
      "epoch": 0.8536585365853658,
      "grad_norm": 0.1220240518450737,
      "learning_rate": 4.066202056290493e-05,
      "loss": 0.0192,
      "step": 3500
    },
    {
      "epoch": 0.8539024390243902,
      "grad_norm": 0.20925983786582947,
      "learning_rate": 4.0657042669634435e-05,
      "loss": 0.0281,
      "step": 3501
    },
    {
      "epoch": 0.8541463414634146,
      "grad_norm": 0.08338268846273422,
      "learning_rate": 4.0652063754789835e-05,
      "loss": 0.0183,
      "step": 3502
    },
    {
      "epoch": 0.854390243902439,
      "grad_norm": 0.12326321005821228,
      "learning_rate": 4.064708381869599e-05,
      "loss": 0.0302,
      "step": 3503
    },
    {
      "epoch": 0.8546341463414634,
      "grad_norm": 0.11821737885475159,
      "learning_rate": 4.064210286167783e-05,
      "loss": 0.0156,
      "step": 3504
    },
    {
      "epoch": 0.8548780487804878,
      "grad_norm": 0.17278870940208435,
      "learning_rate": 4.063712088406034e-05,
      "loss": 0.0199,
      "step": 3505
    },
    {
      "epoch": 0.8551219512195122,
      "grad_norm": 0.12778261303901672,
      "learning_rate": 4.063213788616858e-05,
      "loss": 0.031,
      "step": 3506
    },
    {
      "epoch": 0.8553658536585366,
      "grad_norm": 0.10069678723812103,
      "learning_rate": 4.062715386832768e-05,
      "loss": 0.0159,
      "step": 3507
    },
    {
      "epoch": 0.855609756097561,
      "grad_norm": 0.284927099943161,
      "learning_rate": 4.062216883086283e-05,
      "loss": 0.036,
      "step": 3508
    },
    {
      "epoch": 0.8558536585365853,
      "grad_norm": 0.11955595761537552,
      "learning_rate": 4.061718277409928e-05,
      "loss": 0.0288,
      "step": 3509
    },
    {
      "epoch": 0.8560975609756097,
      "grad_norm": 0.15375916659832,
      "learning_rate": 4.061219569836238e-05,
      "loss": 0.0214,
      "step": 3510
    },
    {
      "epoch": 0.8563414634146341,
      "grad_norm": 0.25482770800590515,
      "learning_rate": 4.060720760397749e-05,
      "loss": 0.0242,
      "step": 3511
    },
    {
      "epoch": 0.8565853658536585,
      "grad_norm": 0.27753186225891113,
      "learning_rate": 4.060221849127008e-05,
      "loss": 0.0167,
      "step": 3512
    },
    {
      "epoch": 0.8568292682926829,
      "grad_norm": 0.055872511118650436,
      "learning_rate": 4.0597228360565684e-05,
      "loss": 0.0108,
      "step": 3513
    },
    {
      "epoch": 0.8570731707317073,
      "grad_norm": 0.16019552946090698,
      "learning_rate": 4.059223721218989e-05,
      "loss": 0.0276,
      "step": 3514
    },
    {
      "epoch": 0.8573170731707317,
      "grad_norm": 0.10812961310148239,
      "learning_rate": 4.058724504646834e-05,
      "loss": 0.0224,
      "step": 3515
    },
    {
      "epoch": 0.8575609756097561,
      "grad_norm": 0.16437767446041107,
      "learning_rate": 4.058225186372677e-05,
      "loss": 0.025,
      "step": 3516
    },
    {
      "epoch": 0.8578048780487805,
      "grad_norm": 0.11441192030906677,
      "learning_rate": 4.0577257664290974e-05,
      "loss": 0.0154,
      "step": 3517
    },
    {
      "epoch": 0.8580487804878049,
      "grad_norm": 0.05023849755525589,
      "learning_rate": 4.0572262448486795e-05,
      "loss": 0.0122,
      "step": 3518
    },
    {
      "epoch": 0.8582926829268293,
      "grad_norm": 0.244353249669075,
      "learning_rate": 4.056726621664016e-05,
      "loss": 0.0176,
      "step": 3519
    },
    {
      "epoch": 0.8585365853658536,
      "grad_norm": 0.11302942037582397,
      "learning_rate": 4.056226896907707e-05,
      "loss": 0.0142,
      "step": 3520
    },
    {
      "epoch": 0.858780487804878,
      "grad_norm": 0.07359018921852112,
      "learning_rate": 4.0557270706123565e-05,
      "loss": 0.0241,
      "step": 3521
    },
    {
      "epoch": 0.8590243902439024,
      "grad_norm": 0.31999677419662476,
      "learning_rate": 4.0552271428105775e-05,
      "loss": 0.036,
      "step": 3522
    },
    {
      "epoch": 0.8592682926829268,
      "grad_norm": 0.12622056901454926,
      "learning_rate": 4.0547271135349886e-05,
      "loss": 0.0227,
      "step": 3523
    },
    {
      "epoch": 0.8595121951219512,
      "grad_norm": 0.26234012842178345,
      "learning_rate": 4.054226982818214e-05,
      "loss": 0.0192,
      "step": 3524
    },
    {
      "epoch": 0.8597560975609756,
      "grad_norm": 0.13237503170967102,
      "learning_rate": 4.0537267506928875e-05,
      "loss": 0.0314,
      "step": 3525
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.08607696741819382,
      "learning_rate": 4.053226417191647e-05,
      "loss": 0.0166,
      "step": 3526
    },
    {
      "epoch": 0.8602439024390244,
      "grad_norm": 0.25112804770469666,
      "learning_rate": 4.052725982347137e-05,
      "loss": 0.0217,
      "step": 3527
    },
    {
      "epoch": 0.8604878048780488,
      "grad_norm": 0.14397040009498596,
      "learning_rate": 4.0522254461920105e-05,
      "loss": 0.038,
      "step": 3528
    },
    {
      "epoch": 0.8607317073170732,
      "grad_norm": 0.06072253733873367,
      "learning_rate": 4.0517248087589255e-05,
      "loss": 0.0172,
      "step": 3529
    },
    {
      "epoch": 0.8609756097560975,
      "grad_norm": 0.11583221703767776,
      "learning_rate": 4.051224070080547e-05,
      "loss": 0.0205,
      "step": 3530
    },
    {
      "epoch": 0.8612195121951219,
      "grad_norm": 0.11429056525230408,
      "learning_rate": 4.050723230189546e-05,
      "loss": 0.0239,
      "step": 3531
    },
    {
      "epoch": 0.8614634146341463,
      "grad_norm": 0.10873386263847351,
      "learning_rate": 4.0502222891186024e-05,
      "loss": 0.0234,
      "step": 3532
    },
    {
      "epoch": 0.8617073170731707,
      "grad_norm": 0.11319661885499954,
      "learning_rate": 4.0497212469003995e-05,
      "loss": 0.0198,
      "step": 3533
    },
    {
      "epoch": 0.8619512195121951,
      "grad_norm": 0.0949561670422554,
      "learning_rate": 4.04922010356763e-05,
      "loss": 0.0239,
      "step": 3534
    },
    {
      "epoch": 0.8621951219512195,
      "grad_norm": 0.15892773866653442,
      "learning_rate": 4.0487188591529905e-05,
      "loss": 0.0186,
      "step": 3535
    },
    {
      "epoch": 0.8624390243902439,
      "grad_norm": 0.1006411612033844,
      "learning_rate": 4.0482175136891876e-05,
      "loss": 0.0236,
      "step": 3536
    },
    {
      "epoch": 0.8626829268292683,
      "grad_norm": 0.05950683355331421,
      "learning_rate": 4.047716067208931e-05,
      "loss": 0.0195,
      "step": 3537
    },
    {
      "epoch": 0.8629268292682927,
      "grad_norm": 0.13671252131462097,
      "learning_rate": 4.047214519744938e-05,
      "loss": 0.0133,
      "step": 3538
    },
    {
      "epoch": 0.8631707317073171,
      "grad_norm": 0.13269351422786713,
      "learning_rate": 4.0467128713299354e-05,
      "loss": 0.038,
      "step": 3539
    },
    {
      "epoch": 0.8634146341463415,
      "grad_norm": 0.09015849977731705,
      "learning_rate": 4.0462111219966525e-05,
      "loss": 0.0195,
      "step": 3540
    },
    {
      "epoch": 0.8636585365853658,
      "grad_norm": 0.08491251617670059,
      "learning_rate": 4.0457092717778275e-05,
      "loss": 0.0269,
      "step": 3541
    },
    {
      "epoch": 0.8639024390243902,
      "grad_norm": 0.1541576385498047,
      "learning_rate": 4.045207320706203e-05,
      "loss": 0.027,
      "step": 3542
    },
    {
      "epoch": 0.8641463414634146,
      "grad_norm": 0.04774874076247215,
      "learning_rate": 4.044705268814533e-05,
      "loss": 0.0121,
      "step": 3543
    },
    {
      "epoch": 0.864390243902439,
      "grad_norm": 0.11003771424293518,
      "learning_rate": 4.044203116135572e-05,
      "loss": 0.0376,
      "step": 3544
    },
    {
      "epoch": 0.8646341463414634,
      "grad_norm": 0.1915009766817093,
      "learning_rate": 4.043700862702086e-05,
      "loss": 0.0395,
      "step": 3545
    },
    {
      "epoch": 0.8648780487804878,
      "grad_norm": 0.10689054429531097,
      "learning_rate": 4.043198508546843e-05,
      "loss": 0.0109,
      "step": 3546
    },
    {
      "epoch": 0.8651219512195122,
      "grad_norm": 0.07986973226070404,
      "learning_rate": 4.042696053702623e-05,
      "loss": 0.0343,
      "step": 3547
    },
    {
      "epoch": 0.8653658536585366,
      "grad_norm": 0.10749258100986481,
      "learning_rate": 4.042193498202207e-05,
      "loss": 0.0335,
      "step": 3548
    },
    {
      "epoch": 0.865609756097561,
      "grad_norm": 0.08691764622926712,
      "learning_rate": 4.041690842078387e-05,
      "loss": 0.0272,
      "step": 3549
    },
    {
      "epoch": 0.8658536585365854,
      "grad_norm": 0.21648001670837402,
      "learning_rate": 4.0411880853639584e-05,
      "loss": 0.036,
      "step": 3550
    },
    {
      "epoch": 0.8660975609756097,
      "grad_norm": 0.10147550702095032,
      "learning_rate": 4.0406852280917254e-05,
      "loss": 0.0198,
      "step": 3551
    },
    {
      "epoch": 0.8663414634146341,
      "grad_norm": 0.08777827024459839,
      "learning_rate": 4.040182270294498e-05,
      "loss": 0.0248,
      "step": 3552
    },
    {
      "epoch": 0.8665853658536585,
      "grad_norm": 0.22410668432712555,
      "learning_rate": 4.039679212005093e-05,
      "loss": 0.0266,
      "step": 3553
    },
    {
      "epoch": 0.8668292682926829,
      "grad_norm": 0.1082000657916069,
      "learning_rate": 4.039176053256332e-05,
      "loss": 0.0427,
      "step": 3554
    },
    {
      "epoch": 0.8670731707317073,
      "grad_norm": 0.1936393827199936,
      "learning_rate": 4.0386727940810456e-05,
      "loss": 0.0248,
      "step": 3555
    },
    {
      "epoch": 0.8673170731707317,
      "grad_norm": 0.13753622770309448,
      "learning_rate": 4.0381694345120705e-05,
      "loss": 0.0202,
      "step": 3556
    },
    {
      "epoch": 0.8675609756097561,
      "grad_norm": 0.08103497326374054,
      "learning_rate": 4.037665974582247e-05,
      "loss": 0.0289,
      "step": 3557
    },
    {
      "epoch": 0.8678048780487805,
      "grad_norm": 0.06833025813102722,
      "learning_rate": 4.0371624143244266e-05,
      "loss": 0.0121,
      "step": 3558
    },
    {
      "epoch": 0.8680487804878049,
      "grad_norm": 0.11445143818855286,
      "learning_rate": 4.0366587537714653e-05,
      "loss": 0.0276,
      "step": 3559
    },
    {
      "epoch": 0.8682926829268293,
      "grad_norm": 0.24371181428432465,
      "learning_rate": 4.036154992956223e-05,
      "loss": 0.0217,
      "step": 3560
    },
    {
      "epoch": 0.8685365853658537,
      "grad_norm": 0.19538553059101105,
      "learning_rate": 4.035651131911571e-05,
      "loss": 0.0228,
      "step": 3561
    },
    {
      "epoch": 0.868780487804878,
      "grad_norm": 0.17532974481582642,
      "learning_rate": 4.0351471706703836e-05,
      "loss": 0.017,
      "step": 3562
    },
    {
      "epoch": 0.8690243902439024,
      "grad_norm": 0.18137499690055847,
      "learning_rate": 4.0346431092655425e-05,
      "loss": 0.0248,
      "step": 3563
    },
    {
      "epoch": 0.8692682926829268,
      "grad_norm": 0.11581434309482574,
      "learning_rate": 4.034138947729937e-05,
      "loss": 0.0238,
      "step": 3564
    },
    {
      "epoch": 0.8695121951219512,
      "grad_norm": 0.0812147855758667,
      "learning_rate": 4.0336346860964614e-05,
      "loss": 0.022,
      "step": 3565
    },
    {
      "epoch": 0.8697560975609756,
      "grad_norm": 0.06268353760242462,
      "learning_rate": 4.033130324398017e-05,
      "loss": 0.0138,
      "step": 3566
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.10086299479007721,
      "learning_rate": 4.032625862667513e-05,
      "loss": 0.0152,
      "step": 3567
    },
    {
      "epoch": 0.8702439024390244,
      "grad_norm": 0.05843415483832359,
      "learning_rate": 4.0321213009378625e-05,
      "loss": 0.0181,
      "step": 3568
    },
    {
      "epoch": 0.8704878048780488,
      "grad_norm": 0.07483840733766556,
      "learning_rate": 4.031616639241988e-05,
      "loss": 0.0144,
      "step": 3569
    },
    {
      "epoch": 0.8707317073170732,
      "grad_norm": 0.18928629159927368,
      "learning_rate": 4.031111877612816e-05,
      "loss": 0.0256,
      "step": 3570
    },
    {
      "epoch": 0.8709756097560976,
      "grad_norm": 0.04738723859190941,
      "learning_rate": 4.0306070160832815e-05,
      "loss": 0.0148,
      "step": 3571
    },
    {
      "epoch": 0.871219512195122,
      "grad_norm": 0.15490423142910004,
      "learning_rate": 4.0301020546863255e-05,
      "loss": 0.0313,
      "step": 3572
    },
    {
      "epoch": 0.8714634146341463,
      "grad_norm": 0.17013610899448395,
      "learning_rate": 4.029596993454894e-05,
      "loss": 0.0309,
      "step": 3573
    },
    {
      "epoch": 0.8717073170731707,
      "grad_norm": 0.07156480103731155,
      "learning_rate": 4.0290918324219404e-05,
      "loss": 0.018,
      "step": 3574
    },
    {
      "epoch": 0.8719512195121951,
      "grad_norm": 0.29566535353660583,
      "learning_rate": 4.028586571620427e-05,
      "loss": 0.0175,
      "step": 3575
    },
    {
      "epoch": 0.8721951219512195,
      "grad_norm": 0.08730250597000122,
      "learning_rate": 4.0280812110833186e-05,
      "loss": 0.0176,
      "step": 3576
    },
    {
      "epoch": 0.8724390243902439,
      "grad_norm": 0.107317253947258,
      "learning_rate": 4.027575750843589e-05,
      "loss": 0.0179,
      "step": 3577
    },
    {
      "epoch": 0.8726829268292683,
      "grad_norm": 0.10564784705638885,
      "learning_rate": 4.027070190934218e-05,
      "loss": 0.0236,
      "step": 3578
    },
    {
      "epoch": 0.8729268292682927,
      "grad_norm": 0.12498023360967636,
      "learning_rate": 4.0265645313881915e-05,
      "loss": 0.0314,
      "step": 3579
    },
    {
      "epoch": 0.8731707317073171,
      "grad_norm": 0.19799712300300598,
      "learning_rate": 4.026058772238504e-05,
      "loss": 0.0485,
      "step": 3580
    },
    {
      "epoch": 0.8734146341463415,
      "grad_norm": 0.11068946123123169,
      "learning_rate": 4.0255529135181526e-05,
      "loss": 0.0144,
      "step": 3581
    },
    {
      "epoch": 0.8736585365853659,
      "grad_norm": 0.17713043093681335,
      "learning_rate": 4.0250469552601434e-05,
      "loss": 0.0268,
      "step": 3582
    },
    {
      "epoch": 0.8739024390243902,
      "grad_norm": 0.059606339782476425,
      "learning_rate": 4.024540897497489e-05,
      "loss": 0.0185,
      "step": 3583
    },
    {
      "epoch": 0.8741463414634146,
      "grad_norm": 0.12478362768888474,
      "learning_rate": 4.024034740263209e-05,
      "loss": 0.0239,
      "step": 3584
    },
    {
      "epoch": 0.874390243902439,
      "grad_norm": 0.1122201532125473,
      "learning_rate": 4.023528483590327e-05,
      "loss": 0.0279,
      "step": 3585
    },
    {
      "epoch": 0.8746341463414634,
      "grad_norm": 0.11695310473442078,
      "learning_rate": 4.023022127511875e-05,
      "loss": 0.0258,
      "step": 3586
    },
    {
      "epoch": 0.8748780487804878,
      "grad_norm": 0.16916684806346893,
      "learning_rate": 4.022515672060891e-05,
      "loss": 0.0255,
      "step": 3587
    },
    {
      "epoch": 0.8751219512195122,
      "grad_norm": 0.14423184096813202,
      "learning_rate": 4.0220091172704224e-05,
      "loss": 0.0198,
      "step": 3588
    },
    {
      "epoch": 0.8753658536585366,
      "grad_norm": 0.15355433523654938,
      "learning_rate": 4.021502463173517e-05,
      "loss": 0.0161,
      "step": 3589
    },
    {
      "epoch": 0.875609756097561,
      "grad_norm": 0.1362130343914032,
      "learning_rate": 4.020995709803233e-05,
      "loss": 0.0262,
      "step": 3590
    },
    {
      "epoch": 0.8758536585365854,
      "grad_norm": 0.16165944933891296,
      "learning_rate": 4.020488857192637e-05,
      "loss": 0.0178,
      "step": 3591
    },
    {
      "epoch": 0.8760975609756098,
      "grad_norm": 0.07882384955883026,
      "learning_rate": 4.019981905374796e-05,
      "loss": 0.0179,
      "step": 3592
    },
    {
      "epoch": 0.8763414634146341,
      "grad_norm": 0.16069430112838745,
      "learning_rate": 4.0194748543827896e-05,
      "loss": 0.0236,
      "step": 3593
    },
    {
      "epoch": 0.8765853658536585,
      "grad_norm": 0.08969201892614365,
      "learning_rate": 4.0189677042497e-05,
      "loss": 0.0276,
      "step": 3594
    },
    {
      "epoch": 0.8768292682926829,
      "grad_norm": 0.1077282503247261,
      "learning_rate": 4.018460455008618e-05,
      "loss": 0.0266,
      "step": 3595
    },
    {
      "epoch": 0.8770731707317073,
      "grad_norm": 0.15565037727355957,
      "learning_rate": 4.01795310669264e-05,
      "loss": 0.0313,
      "step": 3596
    },
    {
      "epoch": 0.8773170731707317,
      "grad_norm": 0.09190423041582108,
      "learning_rate": 4.0174456593348686e-05,
      "loss": 0.0404,
      "step": 3597
    },
    {
      "epoch": 0.8775609756097561,
      "grad_norm": 0.07944359630346298,
      "learning_rate": 4.0169381129684136e-05,
      "loss": 0.0109,
      "step": 3598
    },
    {
      "epoch": 0.8778048780487805,
      "grad_norm": 0.139512836933136,
      "learning_rate": 4.01643046762639e-05,
      "loss": 0.0342,
      "step": 3599
    },
    {
      "epoch": 0.8780487804878049,
      "grad_norm": 0.08433850109577179,
      "learning_rate": 4.015922723341921e-05,
      "loss": 0.0247,
      "step": 3600
    },
    {
      "epoch": 0.8782926829268293,
      "grad_norm": 0.1350412368774414,
      "learning_rate": 4.015414880148134e-05,
      "loss": 0.0316,
      "step": 3601
    },
    {
      "epoch": 0.8785365853658537,
      "grad_norm": 0.09897672384977341,
      "learning_rate": 4.014906938078166e-05,
      "loss": 0.0251,
      "step": 3602
    },
    {
      "epoch": 0.878780487804878,
      "grad_norm": 0.06552883237600327,
      "learning_rate": 4.014398897165158e-05,
      "loss": 0.0158,
      "step": 3603
    },
    {
      "epoch": 0.8790243902439024,
      "grad_norm": 0.07864505052566528,
      "learning_rate": 4.013890757442257e-05,
      "loss": 0.0357,
      "step": 3604
    },
    {
      "epoch": 0.8792682926829268,
      "grad_norm": 0.07728612422943115,
      "learning_rate": 4.01338251894262e-05,
      "loss": 0.0237,
      "step": 3605
    },
    {
      "epoch": 0.8795121951219512,
      "grad_norm": 0.35909774899482727,
      "learning_rate": 4.012874181699406e-05,
      "loss": 0.0218,
      "step": 3606
    },
    {
      "epoch": 0.8797560975609756,
      "grad_norm": 0.17002353072166443,
      "learning_rate": 4.012365745745783e-05,
      "loss": 0.0237,
      "step": 3607
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.10616826266050339,
      "learning_rate": 4.0118572111149245e-05,
      "loss": 0.0227,
      "step": 3608
    },
    {
      "epoch": 0.8802439024390244,
      "grad_norm": 0.1453993022441864,
      "learning_rate": 4.011348577840011e-05,
      "loss": 0.017,
      "step": 3609
    },
    {
      "epoch": 0.8804878048780488,
      "grad_norm": 0.11086418479681015,
      "learning_rate": 4.01083984595423e-05,
      "loss": 0.0319,
      "step": 3610
    },
    {
      "epoch": 0.8807317073170732,
      "grad_norm": 0.0685010626912117,
      "learning_rate": 4.0103310154907744e-05,
      "loss": 0.0108,
      "step": 3611
    },
    {
      "epoch": 0.8809756097560976,
      "grad_norm": 0.14986847341060638,
      "learning_rate": 4.009822086482843e-05,
      "loss": 0.0255,
      "step": 3612
    },
    {
      "epoch": 0.881219512195122,
      "grad_norm": 0.17340323328971863,
      "learning_rate": 4.009313058963643e-05,
      "loss": 0.0334,
      "step": 3613
    },
    {
      "epoch": 0.8814634146341463,
      "grad_norm": 0.058988459408283234,
      "learning_rate": 4.008803932966386e-05,
      "loss": 0.0073,
      "step": 3614
    },
    {
      "epoch": 0.8817073170731707,
      "grad_norm": 0.15209664404392242,
      "learning_rate": 4.008294708524292e-05,
      "loss": 0.0175,
      "step": 3615
    },
    {
      "epoch": 0.8819512195121951,
      "grad_norm": 0.11616609990596771,
      "learning_rate": 4.007785385670584e-05,
      "loss": 0.0273,
      "step": 3616
    },
    {
      "epoch": 0.8821951219512195,
      "grad_norm": 0.28636619448661804,
      "learning_rate": 4.0072759644384965e-05,
      "loss": 0.0369,
      "step": 3617
    },
    {
      "epoch": 0.8824390243902439,
      "grad_norm": 0.2565752863883972,
      "learning_rate": 4.006766444861266e-05,
      "loss": 0.0242,
      "step": 3618
    },
    {
      "epoch": 0.8826829268292683,
      "grad_norm": 0.1669565737247467,
      "learning_rate": 4.006256826972138e-05,
      "loss": 0.0218,
      "step": 3619
    },
    {
      "epoch": 0.8829268292682927,
      "grad_norm": 0.07723619043827057,
      "learning_rate": 4.0057471108043634e-05,
      "loss": 0.0153,
      "step": 3620
    },
    {
      "epoch": 0.8831707317073171,
      "grad_norm": 0.06863376498222351,
      "learning_rate": 4.0052372963912e-05,
      "loss": 0.02,
      "step": 3621
    },
    {
      "epoch": 0.8834146341463415,
      "grad_norm": 0.09511807560920715,
      "learning_rate": 4.0047273837659096e-05,
      "loss": 0.0256,
      "step": 3622
    },
    {
      "epoch": 0.8836585365853659,
      "grad_norm": 0.1906626969575882,
      "learning_rate": 4.004217372961764e-05,
      "loss": 0.0243,
      "step": 3623
    },
    {
      "epoch": 0.8839024390243903,
      "grad_norm": 0.09677279740571976,
      "learning_rate": 4.0037072640120396e-05,
      "loss": 0.0182,
      "step": 3624
    },
    {
      "epoch": 0.8841463414634146,
      "grad_norm": 0.05406389385461807,
      "learning_rate": 4.003197056950021e-05,
      "loss": 0.0119,
      "step": 3625
    },
    {
      "epoch": 0.884390243902439,
      "grad_norm": 0.10016898065805435,
      "learning_rate": 4.002686751808994e-05,
      "loss": 0.019,
      "step": 3626
    },
    {
      "epoch": 0.8846341463414634,
      "grad_norm": 0.07875271141529083,
      "learning_rate": 4.002176348622258e-05,
      "loss": 0.021,
      "step": 3627
    },
    {
      "epoch": 0.8848780487804878,
      "grad_norm": 0.09375863522291183,
      "learning_rate": 4.001665847423113e-05,
      "loss": 0.0109,
      "step": 3628
    },
    {
      "epoch": 0.8851219512195122,
      "grad_norm": 0.11376588046550751,
      "learning_rate": 4.00115524824487e-05,
      "loss": 0.0307,
      "step": 3629
    },
    {
      "epoch": 0.8853658536585366,
      "grad_norm": 0.12766234576702118,
      "learning_rate": 4.000644551120841e-05,
      "loss": 0.0299,
      "step": 3630
    },
    {
      "epoch": 0.885609756097561,
      "grad_norm": 0.19908657670021057,
      "learning_rate": 4.00013375608435e-05,
      "loss": 0.023,
      "step": 3631
    },
    {
      "epoch": 0.8858536585365854,
      "grad_norm": 0.12418775260448456,
      "learning_rate": 3.9996228631687226e-05,
      "loss": 0.0267,
      "step": 3632
    },
    {
      "epoch": 0.8860975609756098,
      "grad_norm": 0.22701959311962128,
      "learning_rate": 3.999111872407295e-05,
      "loss": 0.0198,
      "step": 3633
    },
    {
      "epoch": 0.8863414634146342,
      "grad_norm": 0.07931122928857803,
      "learning_rate": 3.9986007838334074e-05,
      "loss": 0.0194,
      "step": 3634
    },
    {
      "epoch": 0.8865853658536585,
      "grad_norm": 0.5874688625335693,
      "learning_rate": 3.9980895974804054e-05,
      "loss": 0.0238,
      "step": 3635
    },
    {
      "epoch": 0.8868292682926829,
      "grad_norm": 0.1365930438041687,
      "learning_rate": 3.997578313381643e-05,
      "loss": 0.0334,
      "step": 3636
    },
    {
      "epoch": 0.8870731707317073,
      "grad_norm": 0.09097263216972351,
      "learning_rate": 3.9970669315704814e-05,
      "loss": 0.0176,
      "step": 3637
    },
    {
      "epoch": 0.8873170731707317,
      "grad_norm": 0.16402602195739746,
      "learning_rate": 3.9965554520802846e-05,
      "loss": 0.0316,
      "step": 3638
    },
    {
      "epoch": 0.8875609756097561,
      "grad_norm": 0.0755901038646698,
      "learning_rate": 3.9960438749444264e-05,
      "loss": 0.0206,
      "step": 3639
    },
    {
      "epoch": 0.8878048780487805,
      "grad_norm": 0.0692819207906723,
      "learning_rate": 3.995532200196285e-05,
      "loss": 0.0183,
      "step": 3640
    },
    {
      "epoch": 0.8880487804878049,
      "grad_norm": 0.08311443030834198,
      "learning_rate": 3.995020427869245e-05,
      "loss": 0.0163,
      "step": 3641
    },
    {
      "epoch": 0.8882926829268293,
      "grad_norm": 0.1447230726480484,
      "learning_rate": 3.994508557996699e-05,
      "loss": 0.0323,
      "step": 3642
    },
    {
      "epoch": 0.8885365853658537,
      "grad_norm": 0.0807373896241188,
      "learning_rate": 3.993996590612046e-05,
      "loss": 0.0234,
      "step": 3643
    },
    {
      "epoch": 0.8887804878048781,
      "grad_norm": 0.15188434720039368,
      "learning_rate": 3.993484525748688e-05,
      "loss": 0.0143,
      "step": 3644
    },
    {
      "epoch": 0.8890243902439025,
      "grad_norm": 0.25172552466392517,
      "learning_rate": 3.992972363440037e-05,
      "loss": 0.0376,
      "step": 3645
    },
    {
      "epoch": 0.8892682926829268,
      "grad_norm": 0.09309778362512589,
      "learning_rate": 3.99246010371951e-05,
      "loss": 0.0204,
      "step": 3646
    },
    {
      "epoch": 0.8895121951219512,
      "grad_norm": 0.3506941795349121,
      "learning_rate": 3.9919477466205294e-05,
      "loss": 0.0224,
      "step": 3647
    },
    {
      "epoch": 0.8897560975609756,
      "grad_norm": 0.0941234827041626,
      "learning_rate": 3.9914352921765255e-05,
      "loss": 0.0236,
      "step": 3648
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.08325609564781189,
      "learning_rate": 3.9909227404209355e-05,
      "loss": 0.0279,
      "step": 3649
    },
    {
      "epoch": 0.8902439024390244,
      "grad_norm": 0.17481957376003265,
      "learning_rate": 3.9904100913872e-05,
      "loss": 0.0368,
      "step": 3650
    },
    {
      "epoch": 0.8904878048780488,
      "grad_norm": 0.0989336371421814,
      "learning_rate": 3.989897345108769e-05,
      "loss": 0.0275,
      "step": 3651
    },
    {
      "epoch": 0.8907317073170732,
      "grad_norm": 0.13347122073173523,
      "learning_rate": 3.989384501619097e-05,
      "loss": 0.0464,
      "step": 3652
    },
    {
      "epoch": 0.8909756097560976,
      "grad_norm": 0.07959247380495071,
      "learning_rate": 3.9888715609516456e-05,
      "loss": 0.0127,
      "step": 3653
    },
    {
      "epoch": 0.891219512195122,
      "grad_norm": 0.22040346264839172,
      "learning_rate": 3.9883585231398835e-05,
      "loss": 0.0341,
      "step": 3654
    },
    {
      "epoch": 0.8914634146341464,
      "grad_norm": 0.13794788718223572,
      "learning_rate": 3.987845388217284e-05,
      "loss": 0.0166,
      "step": 3655
    },
    {
      "epoch": 0.8917073170731707,
      "grad_norm": 0.2831358015537262,
      "learning_rate": 3.9873321562173274e-05,
      "loss": 0.0189,
      "step": 3656
    },
    {
      "epoch": 0.8919512195121951,
      "grad_norm": 0.1340487152338028,
      "learning_rate": 3.9868188271735e-05,
      "loss": 0.035,
      "step": 3657
    },
    {
      "epoch": 0.8921951219512195,
      "grad_norm": 0.15968938171863556,
      "learning_rate": 3.986305401119296e-05,
      "loss": 0.0293,
      "step": 3658
    },
    {
      "epoch": 0.8924390243902439,
      "grad_norm": 0.13531211018562317,
      "learning_rate": 3.9857918780882153e-05,
      "loss": 0.025,
      "step": 3659
    },
    {
      "epoch": 0.8926829268292683,
      "grad_norm": 0.17392675578594208,
      "learning_rate": 3.9852782581137626e-05,
      "loss": 0.0194,
      "step": 3660
    },
    {
      "epoch": 0.8929268292682927,
      "grad_norm": 0.16664107143878937,
      "learning_rate": 3.9847645412294504e-05,
      "loss": 0.023,
      "step": 3661
    },
    {
      "epoch": 0.8931707317073171,
      "grad_norm": 0.07949301600456238,
      "learning_rate": 3.9842507274687976e-05,
      "loss": 0.018,
      "step": 3662
    },
    {
      "epoch": 0.8934146341463415,
      "grad_norm": 0.05634911358356476,
      "learning_rate": 3.983736816865328e-05,
      "loss": 0.0207,
      "step": 3663
    },
    {
      "epoch": 0.8936585365853659,
      "grad_norm": 0.10578752309083939,
      "learning_rate": 3.983222809452574e-05,
      "loss": 0.015,
      "step": 3664
    },
    {
      "epoch": 0.8939024390243903,
      "grad_norm": 0.104355588555336,
      "learning_rate": 3.982708705264071e-05,
      "loss": 0.0324,
      "step": 3665
    },
    {
      "epoch": 0.8941463414634147,
      "grad_norm": 0.03963429480791092,
      "learning_rate": 3.9821945043333645e-05,
      "loss": 0.0044,
      "step": 3666
    },
    {
      "epoch": 0.894390243902439,
      "grad_norm": 0.1235225573182106,
      "learning_rate": 3.9816802066940045e-05,
      "loss": 0.018,
      "step": 3667
    },
    {
      "epoch": 0.8946341463414634,
      "grad_norm": 0.17083488404750824,
      "learning_rate": 3.9811658123795467e-05,
      "loss": 0.0402,
      "step": 3668
    },
    {
      "epoch": 0.8948780487804878,
      "grad_norm": 0.06769359856843948,
      "learning_rate": 3.980651321423554e-05,
      "loss": 0.0238,
      "step": 3669
    },
    {
      "epoch": 0.8951219512195122,
      "grad_norm": 0.06735482066869736,
      "learning_rate": 3.980136733859594e-05,
      "loss": 0.0211,
      "step": 3670
    },
    {
      "epoch": 0.8953658536585366,
      "grad_norm": 0.10095260292291641,
      "learning_rate": 3.979622049721245e-05,
      "loss": 0.0178,
      "step": 3671
    },
    {
      "epoch": 0.895609756097561,
      "grad_norm": 0.09039568901062012,
      "learning_rate": 3.979107269042086e-05,
      "loss": 0.0269,
      "step": 3672
    },
    {
      "epoch": 0.8958536585365854,
      "grad_norm": 0.1101890504360199,
      "learning_rate": 3.9785923918557064e-05,
      "loss": 0.0205,
      "step": 3673
    },
    {
      "epoch": 0.8960975609756098,
      "grad_norm": 0.34777313470840454,
      "learning_rate": 3.978077418195699e-05,
      "loss": 0.0245,
      "step": 3674
    },
    {
      "epoch": 0.8963414634146342,
      "grad_norm": 0.1600121706724167,
      "learning_rate": 3.9775623480956646e-05,
      "loss": 0.0214,
      "step": 3675
    },
    {
      "epoch": 0.8965853658536586,
      "grad_norm": 0.10740742087364197,
      "learning_rate": 3.977047181589211e-05,
      "loss": 0.0242,
      "step": 3676
    },
    {
      "epoch": 0.896829268292683,
      "grad_norm": 0.16490478813648224,
      "learning_rate": 3.976531918709949e-05,
      "loss": 0.0301,
      "step": 3677
    },
    {
      "epoch": 0.8970731707317073,
      "grad_norm": 0.15519414842128754,
      "learning_rate": 3.976016559491501e-05,
      "loss": 0.031,
      "step": 3678
    },
    {
      "epoch": 0.8973170731707317,
      "grad_norm": 0.11080583930015564,
      "learning_rate": 3.9755011039674906e-05,
      "loss": 0.0308,
      "step": 3679
    },
    {
      "epoch": 0.8975609756097561,
      "grad_norm": 0.1251000016927719,
      "learning_rate": 3.9749855521715504e-05,
      "loss": 0.0262,
      "step": 3680
    },
    {
      "epoch": 0.8978048780487805,
      "grad_norm": 0.12925013899803162,
      "learning_rate": 3.974469904137318e-05,
      "loss": 0.0261,
      "step": 3681
    },
    {
      "epoch": 0.8980487804878049,
      "grad_norm": 0.12065805494785309,
      "learning_rate": 3.973954159898438e-05,
      "loss": 0.02,
      "step": 3682
    },
    {
      "epoch": 0.8982926829268293,
      "grad_norm": 0.14015786349773407,
      "learning_rate": 3.97343831948856e-05,
      "loss": 0.0199,
      "step": 3683
    },
    {
      "epoch": 0.8985365853658537,
      "grad_norm": 0.0781211107969284,
      "learning_rate": 3.9729223829413444e-05,
      "loss": 0.0352,
      "step": 3684
    },
    {
      "epoch": 0.8987804878048781,
      "grad_norm": 0.09434057772159576,
      "learning_rate": 3.972406350290452e-05,
      "loss": 0.0247,
      "step": 3685
    },
    {
      "epoch": 0.8990243902439025,
      "grad_norm": 0.1073860302567482,
      "learning_rate": 3.971890221569552e-05,
      "loss": 0.0219,
      "step": 3686
    },
    {
      "epoch": 0.8992682926829269,
      "grad_norm": 0.10913635790348053,
      "learning_rate": 3.971373996812321e-05,
      "loss": 0.0209,
      "step": 3687
    },
    {
      "epoch": 0.8995121951219512,
      "grad_norm": 0.2590342164039612,
      "learning_rate": 3.970857676052441e-05,
      "loss": 0.0176,
      "step": 3688
    },
    {
      "epoch": 0.8997560975609756,
      "grad_norm": 0.13620248436927795,
      "learning_rate": 3.9703412593236014e-05,
      "loss": 0.0185,
      "step": 3689
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.1534217745065689,
      "learning_rate": 3.969824746659494e-05,
      "loss": 0.0269,
      "step": 3690
    },
    {
      "epoch": 0.9002439024390244,
      "grad_norm": 0.15318267047405243,
      "learning_rate": 3.9693081380938224e-05,
      "loss": 0.0171,
      "step": 3691
    },
    {
      "epoch": 0.9004878048780488,
      "grad_norm": 0.1233515664935112,
      "learning_rate": 3.9687914336602926e-05,
      "loss": 0.0245,
      "step": 3692
    },
    {
      "epoch": 0.9007317073170732,
      "grad_norm": 0.09439560770988464,
      "learning_rate": 3.968274633392619e-05,
      "loss": 0.0207,
      "step": 3693
    },
    {
      "epoch": 0.9009756097560976,
      "grad_norm": 0.11202797293663025,
      "learning_rate": 3.9677577373245195e-05,
      "loss": 0.0273,
      "step": 3694
    },
    {
      "epoch": 0.901219512195122,
      "grad_norm": 0.2369230091571808,
      "learning_rate": 3.967240745489722e-05,
      "loss": 0.0194,
      "step": 3695
    },
    {
      "epoch": 0.9014634146341464,
      "grad_norm": 0.09722013026475906,
      "learning_rate": 3.966723657921956e-05,
      "loss": 0.0243,
      "step": 3696
    },
    {
      "epoch": 0.9017073170731708,
      "grad_norm": 0.12667174637317657,
      "learning_rate": 3.966206474654962e-05,
      "loss": 0.0171,
      "step": 3697
    },
    {
      "epoch": 0.9019512195121951,
      "grad_norm": 0.1823352873325348,
      "learning_rate": 3.965689195722484e-05,
      "loss": 0.0175,
      "step": 3698
    },
    {
      "epoch": 0.9021951219512195,
      "grad_norm": 0.07027942687273026,
      "learning_rate": 3.965171821158272e-05,
      "loss": 0.0151,
      "step": 3699
    },
    {
      "epoch": 0.9024390243902439,
      "grad_norm": 0.11448563635349274,
      "learning_rate": 3.964654350996085e-05,
      "loss": 0.0212,
      "step": 3700
    },
    {
      "epoch": 0.9026829268292683,
      "grad_norm": 0.08843124657869339,
      "learning_rate": 3.964136785269685e-05,
      "loss": 0.0275,
      "step": 3701
    },
    {
      "epoch": 0.9029268292682927,
      "grad_norm": 0.09667134284973145,
      "learning_rate": 3.963619124012843e-05,
      "loss": 0.0151,
      "step": 3702
    },
    {
      "epoch": 0.9031707317073171,
      "grad_norm": 0.2888014018535614,
      "learning_rate": 3.9631013672593325e-05,
      "loss": 0.0214,
      "step": 3703
    },
    {
      "epoch": 0.9034146341463415,
      "grad_norm": 0.09249921143054962,
      "learning_rate": 3.962583515042938e-05,
      "loss": 0.0163,
      "step": 3704
    },
    {
      "epoch": 0.9036585365853659,
      "grad_norm": 0.10165974497795105,
      "learning_rate": 3.962065567397445e-05,
      "loss": 0.0289,
      "step": 3705
    },
    {
      "epoch": 0.9039024390243903,
      "grad_norm": 0.1271624118089676,
      "learning_rate": 3.9615475243566505e-05,
      "loss": 0.024,
      "step": 3706
    },
    {
      "epoch": 0.9041463414634147,
      "grad_norm": 0.11089204251766205,
      "learning_rate": 3.961029385954353e-05,
      "loss": 0.0329,
      "step": 3707
    },
    {
      "epoch": 0.904390243902439,
      "grad_norm": 0.3383224606513977,
      "learning_rate": 3.960511152224362e-05,
      "loss": 0.0335,
      "step": 3708
    },
    {
      "epoch": 0.9046341463414634,
      "grad_norm": 0.12060734629631042,
      "learning_rate": 3.959992823200489e-05,
      "loss": 0.0316,
      "step": 3709
    },
    {
      "epoch": 0.9048780487804878,
      "grad_norm": 0.2101132869720459,
      "learning_rate": 3.9594743989165536e-05,
      "loss": 0.025,
      "step": 3710
    },
    {
      "epoch": 0.9051219512195122,
      "grad_norm": 0.12880879640579224,
      "learning_rate": 3.958955879406382e-05,
      "loss": 0.0147,
      "step": 3711
    },
    {
      "epoch": 0.9053658536585366,
      "grad_norm": 0.23503248393535614,
      "learning_rate": 3.958437264703806e-05,
      "loss": 0.0254,
      "step": 3712
    },
    {
      "epoch": 0.905609756097561,
      "grad_norm": 0.14156481623649597,
      "learning_rate": 3.957918554842662e-05,
      "loss": 0.023,
      "step": 3713
    },
    {
      "epoch": 0.9058536585365854,
      "grad_norm": 0.08465995639562607,
      "learning_rate": 3.957399749856796e-05,
      "loss": 0.0217,
      "step": 3714
    },
    {
      "epoch": 0.9060975609756098,
      "grad_norm": 0.1890091598033905,
      "learning_rate": 3.956880849780058e-05,
      "loss": 0.0264,
      "step": 3715
    },
    {
      "epoch": 0.9063414634146342,
      "grad_norm": 0.06081654876470566,
      "learning_rate": 3.9563618546463046e-05,
      "loss": 0.0191,
      "step": 3716
    },
    {
      "epoch": 0.9065853658536586,
      "grad_norm": 0.08654707670211792,
      "learning_rate": 3.9558427644893973e-05,
      "loss": 0.0334,
      "step": 3717
    },
    {
      "epoch": 0.906829268292683,
      "grad_norm": 0.18251296877861023,
      "learning_rate": 3.9553235793432074e-05,
      "loss": 0.0202,
      "step": 3718
    },
    {
      "epoch": 0.9070731707317073,
      "grad_norm": 0.10646786540746689,
      "learning_rate": 3.954804299241609e-05,
      "loss": 0.0262,
      "step": 3719
    },
    {
      "epoch": 0.9073170731707317,
      "grad_norm": 0.19491900503635406,
      "learning_rate": 3.954284924218484e-05,
      "loss": 0.0198,
      "step": 3720
    },
    {
      "epoch": 0.9075609756097561,
      "grad_norm": 0.10643498599529266,
      "learning_rate": 3.9537654543077194e-05,
      "loss": 0.0245,
      "step": 3721
    },
    {
      "epoch": 0.9078048780487805,
      "grad_norm": 0.10722559690475464,
      "learning_rate": 3.953245889543209e-05,
      "loss": 0.0196,
      "step": 3722
    },
    {
      "epoch": 0.9080487804878049,
      "grad_norm": 0.25549474358558655,
      "learning_rate": 3.952726229958853e-05,
      "loss": 0.0276,
      "step": 3723
    },
    {
      "epoch": 0.9082926829268293,
      "grad_norm": 0.24312667548656464,
      "learning_rate": 3.952206475588558e-05,
      "loss": 0.0226,
      "step": 3724
    },
    {
      "epoch": 0.9085365853658537,
      "grad_norm": 0.11740317195653915,
      "learning_rate": 3.9516866264662364e-05,
      "loss": 0.0361,
      "step": 3725
    },
    {
      "epoch": 0.9087804878048781,
      "grad_norm": 0.07689926028251648,
      "learning_rate": 3.9511666826258065e-05,
      "loss": 0.021,
      "step": 3726
    },
    {
      "epoch": 0.9090243902439025,
      "grad_norm": 0.15488941967487335,
      "learning_rate": 3.9506466441011925e-05,
      "loss": 0.027,
      "step": 3727
    },
    {
      "epoch": 0.9092682926829269,
      "grad_norm": 0.07017063349485397,
      "learning_rate": 3.9501265109263264e-05,
      "loss": 0.0218,
      "step": 3728
    },
    {
      "epoch": 0.9095121951219513,
      "grad_norm": 0.06735885888338089,
      "learning_rate": 3.9496062831351446e-05,
      "loss": 0.0211,
      "step": 3729
    },
    {
      "epoch": 0.9097560975609756,
      "grad_norm": 0.11382927745580673,
      "learning_rate": 3.94908596076159e-05,
      "loss": 0.0187,
      "step": 3730
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.10978320986032486,
      "learning_rate": 3.948565543839613e-05,
      "loss": 0.0266,
      "step": 3731
    },
    {
      "epoch": 0.9102439024390244,
      "grad_norm": 0.11765839904546738,
      "learning_rate": 3.948045032403168e-05,
      "loss": 0.0171,
      "step": 3732
    },
    {
      "epoch": 0.9104878048780488,
      "grad_norm": 0.06992845237255096,
      "learning_rate": 3.947524426486218e-05,
      "loss": 0.023,
      "step": 3733
    },
    {
      "epoch": 0.9107317073170732,
      "grad_norm": 0.10088072717189789,
      "learning_rate": 3.9470037261227313e-05,
      "loss": 0.0262,
      "step": 3734
    },
    {
      "epoch": 0.9109756097560976,
      "grad_norm": 0.0989099070429802,
      "learning_rate": 3.9464829313466804e-05,
      "loss": 0.0153,
      "step": 3735
    },
    {
      "epoch": 0.911219512195122,
      "grad_norm": 0.08536890894174576,
      "learning_rate": 3.945962042192046e-05,
      "loss": 0.0308,
      "step": 3736
    },
    {
      "epoch": 0.9114634146341464,
      "grad_norm": 0.10654376447200775,
      "learning_rate": 3.945441058692816e-05,
      "loss": 0.0316,
      "step": 3737
    },
    {
      "epoch": 0.9117073170731708,
      "grad_norm": 0.1920040100812912,
      "learning_rate": 3.944919980882981e-05,
      "loss": 0.0186,
      "step": 3738
    },
    {
      "epoch": 0.9119512195121952,
      "grad_norm": 0.059579357504844666,
      "learning_rate": 3.94439880879654e-05,
      "loss": 0.0087,
      "step": 3739
    },
    {
      "epoch": 0.9121951219512195,
      "grad_norm": 0.19877955317497253,
      "learning_rate": 3.943877542467499e-05,
      "loss": 0.0194,
      "step": 3740
    },
    {
      "epoch": 0.9124390243902439,
      "grad_norm": 0.15160579979419708,
      "learning_rate": 3.943356181929869e-05,
      "loss": 0.0186,
      "step": 3741
    },
    {
      "epoch": 0.9126829268292683,
      "grad_norm": 0.08283211290836334,
      "learning_rate": 3.9428347272176655e-05,
      "loss": 0.0157,
      "step": 3742
    },
    {
      "epoch": 0.9129268292682927,
      "grad_norm": 0.14151179790496826,
      "learning_rate": 3.942313178364914e-05,
      "loss": 0.0184,
      "step": 3743
    },
    {
      "epoch": 0.9131707317073171,
      "grad_norm": 0.11092370003461838,
      "learning_rate": 3.9417915354056426e-05,
      "loss": 0.0252,
      "step": 3744
    },
    {
      "epoch": 0.9134146341463415,
      "grad_norm": 0.2586178481578827,
      "learning_rate": 3.9412697983738876e-05,
      "loss": 0.0314,
      "step": 3745
    },
    {
      "epoch": 0.9136585365853659,
      "grad_norm": 0.11459264904260635,
      "learning_rate": 3.9407479673036904e-05,
      "loss": 0.0322,
      "step": 3746
    },
    {
      "epoch": 0.9139024390243903,
      "grad_norm": 0.07567060738801956,
      "learning_rate": 3.940226042229099e-05,
      "loss": 0.0202,
      "step": 3747
    },
    {
      "epoch": 0.9141463414634147,
      "grad_norm": 0.1290680319070816,
      "learning_rate": 3.9397040231841664e-05,
      "loss": 0.021,
      "step": 3748
    },
    {
      "epoch": 0.9143902439024391,
      "grad_norm": 0.16473764181137085,
      "learning_rate": 3.939181910202954e-05,
      "loss": 0.0395,
      "step": 3749
    },
    {
      "epoch": 0.9146341463414634,
      "grad_norm": 0.08032030612230301,
      "learning_rate": 3.938659703319528e-05,
      "loss": 0.0121,
      "step": 3750
    },
    {
      "epoch": 0.9148780487804878,
      "grad_norm": 0.19901135563850403,
      "learning_rate": 3.9381374025679604e-05,
      "loss": 0.0467,
      "step": 3751
    },
    {
      "epoch": 0.9151219512195122,
      "grad_norm": 0.15618833899497986,
      "learning_rate": 3.93761500798233e-05,
      "loss": 0.0191,
      "step": 3752
    },
    {
      "epoch": 0.9153658536585366,
      "grad_norm": 0.16337928175926208,
      "learning_rate": 3.937092519596722e-05,
      "loss": 0.0328,
      "step": 3753
    },
    {
      "epoch": 0.915609756097561,
      "grad_norm": 0.1457749754190445,
      "learning_rate": 3.936569937445225e-05,
      "loss": 0.0101,
      "step": 3754
    },
    {
      "epoch": 0.9158536585365854,
      "grad_norm": 0.06894823908805847,
      "learning_rate": 3.936047261561938e-05,
      "loss": 0.0221,
      "step": 3755
    },
    {
      "epoch": 0.9160975609756098,
      "grad_norm": 0.2637437880039215,
      "learning_rate": 3.935524491980964e-05,
      "loss": 0.0319,
      "step": 3756
    },
    {
      "epoch": 0.9163414634146342,
      "grad_norm": 0.14860406517982483,
      "learning_rate": 3.93500162873641e-05,
      "loss": 0.0318,
      "step": 3757
    },
    {
      "epoch": 0.9165853658536586,
      "grad_norm": 0.12526385486125946,
      "learning_rate": 3.9344786718623936e-05,
      "loss": 0.021,
      "step": 3758
    },
    {
      "epoch": 0.916829268292683,
      "grad_norm": 0.13647139072418213,
      "learning_rate": 3.933955621393035e-05,
      "loss": 0.0297,
      "step": 3759
    },
    {
      "epoch": 0.9170731707317074,
      "grad_norm": 0.08501797169446945,
      "learning_rate": 3.933432477362462e-05,
      "loss": 0.018,
      "step": 3760
    },
    {
      "epoch": 0.9173170731707317,
      "grad_norm": 0.11020436137914658,
      "learning_rate": 3.9329092398048084e-05,
      "loss": 0.034,
      "step": 3761
    },
    {
      "epoch": 0.9175609756097561,
      "grad_norm": 0.10625201463699341,
      "learning_rate": 3.932385908754213e-05,
      "loss": 0.0264,
      "step": 3762
    },
    {
      "epoch": 0.9178048780487805,
      "grad_norm": 0.12326838821172714,
      "learning_rate": 3.931862484244822e-05,
      "loss": 0.0381,
      "step": 3763
    },
    {
      "epoch": 0.9180487804878049,
      "grad_norm": 0.10669270902872086,
      "learning_rate": 3.9313389663107866e-05,
      "loss": 0.0445,
      "step": 3764
    },
    {
      "epoch": 0.9182926829268293,
      "grad_norm": 0.2280135303735733,
      "learning_rate": 3.930815354986267e-05,
      "loss": 0.0261,
      "step": 3765
    },
    {
      "epoch": 0.9185365853658537,
      "grad_norm": 0.1814098358154297,
      "learning_rate": 3.9302916503054246e-05,
      "loss": 0.0284,
      "step": 3766
    },
    {
      "epoch": 0.9187804878048781,
      "grad_norm": 0.16976918280124664,
      "learning_rate": 3.9297678523024306e-05,
      "loss": 0.0162,
      "step": 3767
    },
    {
      "epoch": 0.9190243902439025,
      "grad_norm": 0.07962155342102051,
      "learning_rate": 3.929243961011461e-05,
      "loss": 0.0242,
      "step": 3768
    },
    {
      "epoch": 0.9192682926829269,
      "grad_norm": 0.07590322196483612,
      "learning_rate": 3.9287199764666985e-05,
      "loss": 0.0222,
      "step": 3769
    },
    {
      "epoch": 0.9195121951219513,
      "grad_norm": 0.06405948102474213,
      "learning_rate": 3.9281958987023315e-05,
      "loss": 0.0208,
      "step": 3770
    },
    {
      "epoch": 0.9197560975609756,
      "grad_norm": 0.11310238391160965,
      "learning_rate": 3.9276717277525544e-05,
      "loss": 0.0153,
      "step": 3771
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.05894574522972107,
      "learning_rate": 3.927147463651567e-05,
      "loss": 0.0072,
      "step": 3772
    },
    {
      "epoch": 0.9202439024390244,
      "grad_norm": 0.1544725000858307,
      "learning_rate": 3.926623106433577e-05,
      "loss": 0.0184,
      "step": 3773
    },
    {
      "epoch": 0.9204878048780488,
      "grad_norm": 0.06544793397188187,
      "learning_rate": 3.926098656132796e-05,
      "loss": 0.0205,
      "step": 3774
    },
    {
      "epoch": 0.9207317073170732,
      "grad_norm": 0.14863763749599457,
      "learning_rate": 3.925574112783443e-05,
      "loss": 0.0237,
      "step": 3775
    },
    {
      "epoch": 0.9209756097560976,
      "grad_norm": 0.18903139233589172,
      "learning_rate": 3.9250494764197445e-05,
      "loss": 0.0177,
      "step": 3776
    },
    {
      "epoch": 0.921219512195122,
      "grad_norm": 0.20738811790943146,
      "learning_rate": 3.924524747075929e-05,
      "loss": 0.0373,
      "step": 3777
    },
    {
      "epoch": 0.9214634146341464,
      "grad_norm": 0.23448745906352997,
      "learning_rate": 3.923999924786237e-05,
      "loss": 0.0343,
      "step": 3778
    },
    {
      "epoch": 0.9217073170731708,
      "grad_norm": 0.07843741029500961,
      "learning_rate": 3.923475009584907e-05,
      "loss": 0.0148,
      "step": 3779
    },
    {
      "epoch": 0.9219512195121952,
      "grad_norm": 0.14825387299060822,
      "learning_rate": 3.922950001506191e-05,
      "loss": 0.0175,
      "step": 3780
    },
    {
      "epoch": 0.9221951219512196,
      "grad_norm": 0.07898504287004471,
      "learning_rate": 3.9224249005843427e-05,
      "loss": 0.0294,
      "step": 3781
    },
    {
      "epoch": 0.922439024390244,
      "grad_norm": 0.08895832300186157,
      "learning_rate": 3.9218997068536245e-05,
      "loss": 0.0194,
      "step": 3782
    },
    {
      "epoch": 0.9226829268292683,
      "grad_norm": 0.13925623893737793,
      "learning_rate": 3.921374420348304e-05,
      "loss": 0.0129,
      "step": 3783
    },
    {
      "epoch": 0.9229268292682927,
      "grad_norm": 0.22721005976200104,
      "learning_rate": 3.920849041102653e-05,
      "loss": 0.0229,
      "step": 3784
    },
    {
      "epoch": 0.9231707317073171,
      "grad_norm": 0.10404746979475021,
      "learning_rate": 3.920323569150952e-05,
      "loss": 0.0236,
      "step": 3785
    },
    {
      "epoch": 0.9234146341463415,
      "grad_norm": 0.19328473508358002,
      "learning_rate": 3.9197980045274865e-05,
      "loss": 0.0314,
      "step": 3786
    },
    {
      "epoch": 0.9236585365853659,
      "grad_norm": 0.0875646099448204,
      "learning_rate": 3.919272347266547e-05,
      "loss": 0.0182,
      "step": 3787
    },
    {
      "epoch": 0.9239024390243903,
      "grad_norm": 0.06901554018259048,
      "learning_rate": 3.9187465974024316e-05,
      "loss": 0.0268,
      "step": 3788
    },
    {
      "epoch": 0.9241463414634147,
      "grad_norm": 0.10783381015062332,
      "learning_rate": 3.9182207549694437e-05,
      "loss": 0.0228,
      "step": 3789
    },
    {
      "epoch": 0.9243902439024391,
      "grad_norm": 0.05279795825481415,
      "learning_rate": 3.917694820001894e-05,
      "loss": 0.013,
      "step": 3790
    },
    {
      "epoch": 0.9246341463414635,
      "grad_norm": 0.23497821390628815,
      "learning_rate": 3.9171687925340975e-05,
      "loss": 0.018,
      "step": 3791
    },
    {
      "epoch": 0.9248780487804878,
      "grad_norm": 0.1656731814146042,
      "learning_rate": 3.9166426726003746e-05,
      "loss": 0.0392,
      "step": 3792
    },
    {
      "epoch": 0.9251219512195122,
      "grad_norm": 0.08165845274925232,
      "learning_rate": 3.916116460235054e-05,
      "loss": 0.016,
      "step": 3793
    },
    {
      "epoch": 0.9253658536585366,
      "grad_norm": 0.06195079907774925,
      "learning_rate": 3.915590155472471e-05,
      "loss": 0.0154,
      "step": 3794
    },
    {
      "epoch": 0.925609756097561,
      "grad_norm": 0.0772845596075058,
      "learning_rate": 3.915063758346963e-05,
      "loss": 0.0221,
      "step": 3795
    },
    {
      "epoch": 0.9258536585365854,
      "grad_norm": 0.16224637627601624,
      "learning_rate": 3.914537268892876e-05,
      "loss": 0.0213,
      "step": 3796
    },
    {
      "epoch": 0.9260975609756098,
      "grad_norm": 0.14406488835811615,
      "learning_rate": 3.9140106871445634e-05,
      "loss": 0.0246,
      "step": 3797
    },
    {
      "epoch": 0.9263414634146342,
      "grad_norm": 0.11729393154382706,
      "learning_rate": 3.913484013136381e-05,
      "loss": 0.0191,
      "step": 3798
    },
    {
      "epoch": 0.9265853658536586,
      "grad_norm": 0.12971945106983185,
      "learning_rate": 3.912957246902695e-05,
      "loss": 0.0185,
      "step": 3799
    },
    {
      "epoch": 0.926829268292683,
      "grad_norm": 0.10460115969181061,
      "learning_rate": 3.9124303884778744e-05,
      "loss": 0.0255,
      "step": 3800
    },
    {
      "epoch": 0.9270731707317074,
      "grad_norm": 0.05714300274848938,
      "learning_rate": 3.9119034378962936e-05,
      "loss": 0.0148,
      "step": 3801
    },
    {
      "epoch": 0.9273170731707318,
      "grad_norm": 0.11332698166370392,
      "learning_rate": 3.9113763951923365e-05,
      "loss": 0.0108,
      "step": 3802
    },
    {
      "epoch": 0.9275609756097561,
      "grad_norm": 0.11429506540298462,
      "learning_rate": 3.910849260400391e-05,
      "loss": 0.0151,
      "step": 3803
    },
    {
      "epoch": 0.9278048780487805,
      "grad_norm": 0.1355608105659485,
      "learning_rate": 3.9103220335548494e-05,
      "loss": 0.023,
      "step": 3804
    },
    {
      "epoch": 0.9280487804878049,
      "grad_norm": 0.17677700519561768,
      "learning_rate": 3.9097947146901126e-05,
      "loss": 0.0434,
      "step": 3805
    },
    {
      "epoch": 0.9282926829268293,
      "grad_norm": 0.16658025979995728,
      "learning_rate": 3.9092673038405855e-05,
      "loss": 0.0225,
      "step": 3806
    },
    {
      "epoch": 0.9285365853658537,
      "grad_norm": 0.23109304904937744,
      "learning_rate": 3.9087398010406825e-05,
      "loss": 0.0245,
      "step": 3807
    },
    {
      "epoch": 0.9287804878048781,
      "grad_norm": 0.08846081793308258,
      "learning_rate": 3.90821220632482e-05,
      "loss": 0.0257,
      "step": 3808
    },
    {
      "epoch": 0.9290243902439025,
      "grad_norm": 0.08076492697000504,
      "learning_rate": 3.907684519727421e-05,
      "loss": 0.0137,
      "step": 3809
    },
    {
      "epoch": 0.9292682926829269,
      "grad_norm": 0.07729029655456543,
      "learning_rate": 3.907156741282917e-05,
      "loss": 0.017,
      "step": 3810
    },
    {
      "epoch": 0.9295121951219513,
      "grad_norm": 0.08122511208057404,
      "learning_rate": 3.906628871025744e-05,
      "loss": 0.013,
      "step": 3811
    },
    {
      "epoch": 0.9297560975609757,
      "grad_norm": 0.18279610574245453,
      "learning_rate": 3.9061009089903415e-05,
      "loss": 0.0266,
      "step": 3812
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.12193917483091354,
      "learning_rate": 3.90557285521116e-05,
      "loss": 0.0354,
      "step": 3813
    },
    {
      "epoch": 0.9302439024390244,
      "grad_norm": 0.13915632665157318,
      "learning_rate": 3.905044709722653e-05,
      "loss": 0.0217,
      "step": 3814
    },
    {
      "epoch": 0.9304878048780488,
      "grad_norm": 0.1471289098262787,
      "learning_rate": 3.9045164725592795e-05,
      "loss": 0.0327,
      "step": 3815
    },
    {
      "epoch": 0.9307317073170732,
      "grad_norm": 0.08788266032934189,
      "learning_rate": 3.903988143755506e-05,
      "loss": 0.0199,
      "step": 3816
    },
    {
      "epoch": 0.9309756097560976,
      "grad_norm": 0.27153685688972473,
      "learning_rate": 3.903459723345805e-05,
      "loss": 0.0351,
      "step": 3817
    },
    {
      "epoch": 0.931219512195122,
      "grad_norm": 0.13442952930927277,
      "learning_rate": 3.902931211364652e-05,
      "loss": 0.0367,
      "step": 3818
    },
    {
      "epoch": 0.9314634146341464,
      "grad_norm": 0.11974110454320908,
      "learning_rate": 3.902402607846533e-05,
      "loss": 0.0154,
      "step": 3819
    },
    {
      "epoch": 0.9317073170731708,
      "grad_norm": 0.08610626310110092,
      "learning_rate": 3.901873912825937e-05,
      "loss": 0.0158,
      "step": 3820
    },
    {
      "epoch": 0.9319512195121952,
      "grad_norm": 0.1337663233280182,
      "learning_rate": 3.901345126337359e-05,
      "loss": 0.0382,
      "step": 3821
    },
    {
      "epoch": 0.9321951219512196,
      "grad_norm": 0.12507197260856628,
      "learning_rate": 3.900816248415302e-05,
      "loss": 0.0174,
      "step": 3822
    },
    {
      "epoch": 0.932439024390244,
      "grad_norm": 0.22251766920089722,
      "learning_rate": 3.900287279094273e-05,
      "loss": 0.0377,
      "step": 3823
    },
    {
      "epoch": 0.9326829268292683,
      "grad_norm": 0.14761480689048767,
      "learning_rate": 3.8997582184087864e-05,
      "loss": 0.0283,
      "step": 3824
    },
    {
      "epoch": 0.9329268292682927,
      "grad_norm": 0.18470312654972076,
      "learning_rate": 3.8992290663933605e-05,
      "loss": 0.029,
      "step": 3825
    },
    {
      "epoch": 0.9331707317073171,
      "grad_norm": 0.10753437131643295,
      "learning_rate": 3.898699823082521e-05,
      "loss": 0.0186,
      "step": 3826
    },
    {
      "epoch": 0.9334146341463415,
      "grad_norm": 0.22586897015571594,
      "learning_rate": 3.898170488510801e-05,
      "loss": 0.0286,
      "step": 3827
    },
    {
      "epoch": 0.9336585365853659,
      "grad_norm": 0.16675905883312225,
      "learning_rate": 3.897641062712737e-05,
      "loss": 0.0226,
      "step": 3828
    },
    {
      "epoch": 0.9339024390243903,
      "grad_norm": 0.29495397210121155,
      "learning_rate": 3.897111545722871e-05,
      "loss": 0.0284,
      "step": 3829
    },
    {
      "epoch": 0.9341463414634147,
      "grad_norm": 0.09377191960811615,
      "learning_rate": 3.896581937575754e-05,
      "loss": 0.0153,
      "step": 3830
    },
    {
      "epoch": 0.9343902439024391,
      "grad_norm": 0.24559323489665985,
      "learning_rate": 3.89605223830594e-05,
      "loss": 0.0247,
      "step": 3831
    },
    {
      "epoch": 0.9346341463414635,
      "grad_norm": 0.09601590037345886,
      "learning_rate": 3.8955224479479935e-05,
      "loss": 0.0137,
      "step": 3832
    },
    {
      "epoch": 0.9348780487804879,
      "grad_norm": 0.12259406596422195,
      "learning_rate": 3.894992566536477e-05,
      "loss": 0.0284,
      "step": 3833
    },
    {
      "epoch": 0.9351219512195122,
      "grad_norm": 0.1352176070213318,
      "learning_rate": 3.894462594105968e-05,
      "loss": 0.0349,
      "step": 3834
    },
    {
      "epoch": 0.9353658536585366,
      "grad_norm": 0.08234234154224396,
      "learning_rate": 3.8939325306910425e-05,
      "loss": 0.0169,
      "step": 3835
    },
    {
      "epoch": 0.935609756097561,
      "grad_norm": 0.0741184800863266,
      "learning_rate": 3.8934023763262866e-05,
      "loss": 0.0178,
      "step": 3836
    },
    {
      "epoch": 0.9358536585365854,
      "grad_norm": 0.1341986060142517,
      "learning_rate": 3.8928721310462915e-05,
      "loss": 0.0168,
      "step": 3837
    },
    {
      "epoch": 0.9360975609756098,
      "grad_norm": 0.1199384480714798,
      "learning_rate": 3.892341794885655e-05,
      "loss": 0.0231,
      "step": 3838
    },
    {
      "epoch": 0.9363414634146342,
      "grad_norm": 0.2829689681529999,
      "learning_rate": 3.8918113678789766e-05,
      "loss": 0.0301,
      "step": 3839
    },
    {
      "epoch": 0.9365853658536586,
      "grad_norm": 0.09098257124423981,
      "learning_rate": 3.8912808500608676e-05,
      "loss": 0.0125,
      "step": 3840
    },
    {
      "epoch": 0.936829268292683,
      "grad_norm": 0.10668057203292847,
      "learning_rate": 3.890750241465943e-05,
      "loss": 0.0238,
      "step": 3841
    },
    {
      "epoch": 0.9370731707317074,
      "grad_norm": 0.19371077418327332,
      "learning_rate": 3.890219542128822e-05,
      "loss": 0.0262,
      "step": 3842
    },
    {
      "epoch": 0.9373170731707318,
      "grad_norm": 0.2147427648305893,
      "learning_rate": 3.8896887520841326e-05,
      "loss": 0.0207,
      "step": 3843
    },
    {
      "epoch": 0.937560975609756,
      "grad_norm": 0.09869831800460815,
      "learning_rate": 3.889157871366507e-05,
      "loss": 0.0143,
      "step": 3844
    },
    {
      "epoch": 0.9378048780487804,
      "grad_norm": 0.19667746126651764,
      "learning_rate": 3.8886269000105814e-05,
      "loss": 0.0243,
      "step": 3845
    },
    {
      "epoch": 0.9380487804878048,
      "grad_norm": 0.14893664419651031,
      "learning_rate": 3.888095838051001e-05,
      "loss": 0.0389,
      "step": 3846
    },
    {
      "epoch": 0.9382926829268292,
      "grad_norm": 0.04973554611206055,
      "learning_rate": 3.887564685522418e-05,
      "loss": 0.0092,
      "step": 3847
    },
    {
      "epoch": 0.9385365853658536,
      "grad_norm": 0.07881472259759903,
      "learning_rate": 3.887033442459487e-05,
      "loss": 0.0174,
      "step": 3848
    },
    {
      "epoch": 0.938780487804878,
      "grad_norm": 0.08607625216245651,
      "learning_rate": 3.8865021088968694e-05,
      "loss": 0.0178,
      "step": 3849
    },
    {
      "epoch": 0.9390243902439024,
      "grad_norm": 0.1046331599354744,
      "learning_rate": 3.885970684869233e-05,
      "loss": 0.0301,
      "step": 3850
    },
    {
      "epoch": 0.9392682926829268,
      "grad_norm": 0.0649513378739357,
      "learning_rate": 3.8854391704112537e-05,
      "loss": 0.0167,
      "step": 3851
    },
    {
      "epoch": 0.9395121951219512,
      "grad_norm": 0.09314361214637756,
      "learning_rate": 3.88490756555761e-05,
      "loss": 0.0416,
      "step": 3852
    },
    {
      "epoch": 0.9397560975609756,
      "grad_norm": 0.12364093959331512,
      "learning_rate": 3.884375870342986e-05,
      "loss": 0.0288,
      "step": 3853
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.11998830735683441,
      "learning_rate": 3.883844084802075e-05,
      "loss": 0.0284,
      "step": 3854
    },
    {
      "epoch": 0.9402439024390243,
      "grad_norm": 0.13116006553173065,
      "learning_rate": 3.883312208969574e-05,
      "loss": 0.0114,
      "step": 3855
    },
    {
      "epoch": 0.9404878048780487,
      "grad_norm": 0.12492181360721588,
      "learning_rate": 3.882780242880185e-05,
      "loss": 0.0251,
      "step": 3856
    },
    {
      "epoch": 0.9407317073170731,
      "grad_norm": 0.0969604104757309,
      "learning_rate": 3.8822481865686196e-05,
      "loss": 0.0203,
      "step": 3857
    },
    {
      "epoch": 0.9409756097560975,
      "grad_norm": 0.12972521781921387,
      "learning_rate": 3.8817160400695904e-05,
      "loss": 0.0268,
      "step": 3858
    },
    {
      "epoch": 0.9412195121951219,
      "grad_norm": 0.3264077603816986,
      "learning_rate": 3.8811838034178204e-05,
      "loss": 0.0332,
      "step": 3859
    },
    {
      "epoch": 0.9414634146341463,
      "grad_norm": 0.14631225168704987,
      "learning_rate": 3.880651476648035e-05,
      "loss": 0.0226,
      "step": 3860
    },
    {
      "epoch": 0.9417073170731707,
      "grad_norm": 0.08521188795566559,
      "learning_rate": 3.880119059794968e-05,
      "loss": 0.0186,
      "step": 3861
    },
    {
      "epoch": 0.9419512195121951,
      "grad_norm": 0.4121970236301422,
      "learning_rate": 3.879586552893356e-05,
      "loss": 0.0162,
      "step": 3862
    },
    {
      "epoch": 0.9421951219512195,
      "grad_norm": 0.12574975192546844,
      "learning_rate": 3.879053955977946e-05,
      "loss": 0.0115,
      "step": 3863
    },
    {
      "epoch": 0.9424390243902439,
      "grad_norm": 0.20214471220970154,
      "learning_rate": 3.878521269083486e-05,
      "loss": 0.0163,
      "step": 3864
    },
    {
      "epoch": 0.9426829268292682,
      "grad_norm": 0.07893093675374985,
      "learning_rate": 3.8779884922447334e-05,
      "loss": 0.0126,
      "step": 3865
    },
    {
      "epoch": 0.9429268292682926,
      "grad_norm": 0.07589086145162582,
      "learning_rate": 3.8774556254964506e-05,
      "loss": 0.0145,
      "step": 3866
    },
    {
      "epoch": 0.943170731707317,
      "grad_norm": 0.12089613080024719,
      "learning_rate": 3.876922668873404e-05,
      "loss": 0.0204,
      "step": 3867
    },
    {
      "epoch": 0.9434146341463414,
      "grad_norm": 0.12792333960533142,
      "learning_rate": 3.87638962241037e-05,
      "loss": 0.0364,
      "step": 3868
    },
    {
      "epoch": 0.9436585365853658,
      "grad_norm": 0.1181190013885498,
      "learning_rate": 3.8758564861421266e-05,
      "loss": 0.0235,
      "step": 3869
    },
    {
      "epoch": 0.9439024390243902,
      "grad_norm": 0.05761071667075157,
      "learning_rate": 3.8753232601034584e-05,
      "loss": 0.0178,
      "step": 3870
    },
    {
      "epoch": 0.9441463414634146,
      "grad_norm": 0.1021169051527977,
      "learning_rate": 3.874789944329158e-05,
      "loss": 0.0213,
      "step": 3871
    },
    {
      "epoch": 0.944390243902439,
      "grad_norm": 0.24505165219306946,
      "learning_rate": 3.874256538854022e-05,
      "loss": 0.0218,
      "step": 3872
    },
    {
      "epoch": 0.9446341463414634,
      "grad_norm": 0.24797214567661285,
      "learning_rate": 3.8737230437128545e-05,
      "loss": 0.0308,
      "step": 3873
    },
    {
      "epoch": 0.9448780487804878,
      "grad_norm": 0.10029569268226624,
      "learning_rate": 3.8731894589404635e-05,
      "loss": 0.0303,
      "step": 3874
    },
    {
      "epoch": 0.9451219512195121,
      "grad_norm": 0.08908743411302567,
      "learning_rate": 3.8726557845716636e-05,
      "loss": 0.0178,
      "step": 3875
    },
    {
      "epoch": 0.9453658536585365,
      "grad_norm": 0.2738605737686157,
      "learning_rate": 3.872122020641277e-05,
      "loss": 0.0279,
      "step": 3876
    },
    {
      "epoch": 0.9456097560975609,
      "grad_norm": 0.0679912194609642,
      "learning_rate": 3.871588167184128e-05,
      "loss": 0.0177,
      "step": 3877
    },
    {
      "epoch": 0.9458536585365853,
      "grad_norm": 0.2691769003868103,
      "learning_rate": 3.87105422423505e-05,
      "loss": 0.0259,
      "step": 3878
    },
    {
      "epoch": 0.9460975609756097,
      "grad_norm": 0.1644759178161621,
      "learning_rate": 3.870520191828882e-05,
      "loss": 0.0259,
      "step": 3879
    },
    {
      "epoch": 0.9463414634146341,
      "grad_norm": 0.07075212895870209,
      "learning_rate": 3.8699860700004666e-05,
      "loss": 0.0227,
      "step": 3880
    },
    {
      "epoch": 0.9465853658536585,
      "grad_norm": 0.07319433242082596,
      "learning_rate": 3.869451858784653e-05,
      "loss": 0.0129,
      "step": 3881
    },
    {
      "epoch": 0.9468292682926829,
      "grad_norm": 0.10246925055980682,
      "learning_rate": 3.868917558216299e-05,
      "loss": 0.0161,
      "step": 3882
    },
    {
      "epoch": 0.9470731707317073,
      "grad_norm": 0.06294088810682297,
      "learning_rate": 3.868383168330265e-05,
      "loss": 0.0167,
      "step": 3883
    },
    {
      "epoch": 0.9473170731707317,
      "grad_norm": 0.10730636864900589,
      "learning_rate": 3.867848689161418e-05,
      "loss": 0.0291,
      "step": 3884
    },
    {
      "epoch": 0.947560975609756,
      "grad_norm": 0.17230647802352905,
      "learning_rate": 3.867314120744632e-05,
      "loss": 0.0281,
      "step": 3885
    },
    {
      "epoch": 0.9478048780487804,
      "grad_norm": 0.24828410148620605,
      "learning_rate": 3.8667794631147844e-05,
      "loss": 0.0357,
      "step": 3886
    },
    {
      "epoch": 0.9480487804878048,
      "grad_norm": 0.1005435585975647,
      "learning_rate": 3.866244716306762e-05,
      "loss": 0.019,
      "step": 3887
    },
    {
      "epoch": 0.9482926829268292,
      "grad_norm": 0.08238893747329712,
      "learning_rate": 3.865709880355453e-05,
      "loss": 0.0191,
      "step": 3888
    },
    {
      "epoch": 0.9485365853658536,
      "grad_norm": 0.16881515085697174,
      "learning_rate": 3.8651749552957564e-05,
      "loss": 0.0219,
      "step": 3889
    },
    {
      "epoch": 0.948780487804878,
      "grad_norm": 0.15200576186180115,
      "learning_rate": 3.8646399411625724e-05,
      "loss": 0.0234,
      "step": 3890
    },
    {
      "epoch": 0.9490243902439024,
      "grad_norm": 0.1372459977865219,
      "learning_rate": 3.8641048379908097e-05,
      "loss": 0.0266,
      "step": 3891
    },
    {
      "epoch": 0.9492682926829268,
      "grad_norm": 0.0687408521771431,
      "learning_rate": 3.863569645815382e-05,
      "loss": 0.0197,
      "step": 3892
    },
    {
      "epoch": 0.9495121951219512,
      "grad_norm": 0.1650433987379074,
      "learning_rate": 3.863034364671211e-05,
      "loss": 0.0205,
      "step": 3893
    },
    {
      "epoch": 0.9497560975609756,
      "grad_norm": 0.13538242876529694,
      "learning_rate": 3.862498994593219e-05,
      "loss": 0.0362,
      "step": 3894
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.17163874208927155,
      "learning_rate": 3.861963535616338e-05,
      "loss": 0.0354,
      "step": 3895
    },
    {
      "epoch": 0.9502439024390243,
      "grad_norm": 0.1986294686794281,
      "learning_rate": 3.8614279877755066e-05,
      "loss": 0.0226,
      "step": 3896
    },
    {
      "epoch": 0.9504878048780487,
      "grad_norm": 0.11559273302555084,
      "learning_rate": 3.860892351105667e-05,
      "loss": 0.018,
      "step": 3897
    },
    {
      "epoch": 0.9507317073170731,
      "grad_norm": 0.16923722624778748,
      "learning_rate": 3.860356625641767e-05,
      "loss": 0.0341,
      "step": 3898
    },
    {
      "epoch": 0.9509756097560975,
      "grad_norm": 0.10471200197935104,
      "learning_rate": 3.859820811418762e-05,
      "loss": 0.0197,
      "step": 3899
    },
    {
      "epoch": 0.9512195121951219,
      "grad_norm": 0.12599316239356995,
      "learning_rate": 3.859284908471611e-05,
      "loss": 0.0269,
      "step": 3900
    },
    {
      "epoch": 0.9514634146341463,
      "grad_norm": 0.1873592883348465,
      "learning_rate": 3.858748916835282e-05,
      "loss": 0.0251,
      "step": 3901
    },
    {
      "epoch": 0.9517073170731707,
      "grad_norm": 0.11426394432783127,
      "learning_rate": 3.858212836544745e-05,
      "loss": 0.0218,
      "step": 3902
    },
    {
      "epoch": 0.9519512195121951,
      "grad_norm": 0.16075512766838074,
      "learning_rate": 3.8576766676349784e-05,
      "loss": 0.0317,
      "step": 3903
    },
    {
      "epoch": 0.9521951219512195,
      "grad_norm": 0.21572092175483704,
      "learning_rate": 3.8571404101409657e-05,
      "loss": 0.0409,
      "step": 3904
    },
    {
      "epoch": 0.9524390243902439,
      "grad_norm": 0.16940012574195862,
      "learning_rate": 3.8566040640976956e-05,
      "loss": 0.0144,
      "step": 3905
    },
    {
      "epoch": 0.9526829268292683,
      "grad_norm": 0.12148591130971909,
      "learning_rate": 3.8560676295401635e-05,
      "loss": 0.0143,
      "step": 3906
    },
    {
      "epoch": 0.9529268292682926,
      "grad_norm": 0.09070202708244324,
      "learning_rate": 3.8555311065033696e-05,
      "loss": 0.0177,
      "step": 3907
    },
    {
      "epoch": 0.953170731707317,
      "grad_norm": 0.14462000131607056,
      "learning_rate": 3.854994495022321e-05,
      "loss": 0.0212,
      "step": 3908
    },
    {
      "epoch": 0.9534146341463414,
      "grad_norm": 0.26036983728408813,
      "learning_rate": 3.8544577951320295e-05,
      "loss": 0.0174,
      "step": 3909
    },
    {
      "epoch": 0.9536585365853658,
      "grad_norm": 0.1114933118224144,
      "learning_rate": 3.8539210068675133e-05,
      "loss": 0.0348,
      "step": 3910
    },
    {
      "epoch": 0.9539024390243902,
      "grad_norm": 0.16211292147636414,
      "learning_rate": 3.8533841302637966e-05,
      "loss": 0.0288,
      "step": 3911
    },
    {
      "epoch": 0.9541463414634146,
      "grad_norm": 0.19462430477142334,
      "learning_rate": 3.852847165355908e-05,
      "loss": 0.0452,
      "step": 3912
    },
    {
      "epoch": 0.954390243902439,
      "grad_norm": 0.09858786314725876,
      "learning_rate": 3.852310112178883e-05,
      "loss": 0.0096,
      "step": 3913
    },
    {
      "epoch": 0.9546341463414634,
      "grad_norm": 0.11413440853357315,
      "learning_rate": 3.8517729707677635e-05,
      "loss": 0.0325,
      "step": 3914
    },
    {
      "epoch": 0.9548780487804878,
      "grad_norm": 0.16637006402015686,
      "learning_rate": 3.8512357411575955e-05,
      "loss": 0.0256,
      "step": 3915
    },
    {
      "epoch": 0.9551219512195122,
      "grad_norm": 0.08215348422527313,
      "learning_rate": 3.850698423383432e-05,
      "loss": 0.0192,
      "step": 3916
    },
    {
      "epoch": 0.9553658536585365,
      "grad_norm": 0.18530113995075226,
      "learning_rate": 3.850161017480331e-05,
      "loss": 0.0234,
      "step": 3917
    },
    {
      "epoch": 0.9556097560975609,
      "grad_norm": 0.1470474749803543,
      "learning_rate": 3.8496235234833584e-05,
      "loss": 0.0249,
      "step": 3918
    },
    {
      "epoch": 0.9558536585365853,
      "grad_norm": 0.1109040379524231,
      "learning_rate": 3.8490859414275814e-05,
      "loss": 0.0407,
      "step": 3919
    },
    {
      "epoch": 0.9560975609756097,
      "grad_norm": 0.07603243738412857,
      "learning_rate": 3.848548271348076e-05,
      "loss": 0.0183,
      "step": 3920
    },
    {
      "epoch": 0.9563414634146341,
      "grad_norm": 0.09453743696212769,
      "learning_rate": 3.8480105132799247e-05,
      "loss": 0.0277,
      "step": 3921
    },
    {
      "epoch": 0.9565853658536585,
      "grad_norm": 0.07369355857372284,
      "learning_rate": 3.8474726672582145e-05,
      "loss": 0.0158,
      "step": 3922
    },
    {
      "epoch": 0.9568292682926829,
      "grad_norm": 0.10583677887916565,
      "learning_rate": 3.846934733318037e-05,
      "loss": 0.0291,
      "step": 3923
    },
    {
      "epoch": 0.9570731707317073,
      "grad_norm": 0.09923817217350006,
      "learning_rate": 3.846396711494492e-05,
      "loss": 0.0289,
      "step": 3924
    },
    {
      "epoch": 0.9573170731707317,
      "grad_norm": 0.11418124288320541,
      "learning_rate": 3.845858601822683e-05,
      "loss": 0.0315,
      "step": 3925
    },
    {
      "epoch": 0.9575609756097561,
      "grad_norm": 0.14816820621490479,
      "learning_rate": 3.845320404337721e-05,
      "loss": 0.0266,
      "step": 3926
    },
    {
      "epoch": 0.9578048780487805,
      "grad_norm": 0.06927330791950226,
      "learning_rate": 3.844782119074721e-05,
      "loss": 0.0144,
      "step": 3927
    },
    {
      "epoch": 0.9580487804878048,
      "grad_norm": 0.10949699580669403,
      "learning_rate": 3.844243746068804e-05,
      "loss": 0.0149,
      "step": 3928
    },
    {
      "epoch": 0.9582926829268292,
      "grad_norm": 0.10325164347887039,
      "learning_rate": 3.843705285355097e-05,
      "loss": 0.0371,
      "step": 3929
    },
    {
      "epoch": 0.9585365853658536,
      "grad_norm": 0.07285980880260468,
      "learning_rate": 3.843166736968735e-05,
      "loss": 0.012,
      "step": 3930
    },
    {
      "epoch": 0.958780487804878,
      "grad_norm": 0.46836167573928833,
      "learning_rate": 3.842628100944855e-05,
      "loss": 0.0195,
      "step": 3931
    },
    {
      "epoch": 0.9590243902439024,
      "grad_norm": 0.1287126988172531,
      "learning_rate": 3.8420893773186004e-05,
      "loss": 0.0218,
      "step": 3932
    },
    {
      "epoch": 0.9592682926829268,
      "grad_norm": 0.11064600944519043,
      "learning_rate": 3.841550566125123e-05,
      "loss": 0.0396,
      "step": 3933
    },
    {
      "epoch": 0.9595121951219512,
      "grad_norm": 0.062223032116889954,
      "learning_rate": 3.841011667399579e-05,
      "loss": 0.0274,
      "step": 3934
    },
    {
      "epoch": 0.9597560975609756,
      "grad_norm": 0.09487557411193848,
      "learning_rate": 3.8404726811771284e-05,
      "loss": 0.0354,
      "step": 3935
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.13202796876430511,
      "learning_rate": 3.839933607492939e-05,
      "loss": 0.0244,
      "step": 3936
    },
    {
      "epoch": 0.9602439024390244,
      "grad_norm": 0.12558668851852417,
      "learning_rate": 3.839394446382183e-05,
      "loss": 0.0151,
      "step": 3937
    },
    {
      "epoch": 0.9604878048780487,
      "grad_norm": 0.0855087861418724,
      "learning_rate": 3.8388551978800394e-05,
      "loss": 0.0079,
      "step": 3938
    },
    {
      "epoch": 0.9607317073170731,
      "grad_norm": 0.5748891830444336,
      "learning_rate": 3.838315862021694e-05,
      "loss": 0.0291,
      "step": 3939
    },
    {
      "epoch": 0.9609756097560975,
      "grad_norm": 0.08200305700302124,
      "learning_rate": 3.837776438842335e-05,
      "loss": 0.014,
      "step": 3940
    },
    {
      "epoch": 0.9612195121951219,
      "grad_norm": 0.19571353495121002,
      "learning_rate": 3.837236928377158e-05,
      "loss": 0.0176,
      "step": 3941
    },
    {
      "epoch": 0.9614634146341463,
      "grad_norm": 0.11125766485929489,
      "learning_rate": 3.836697330661366e-05,
      "loss": 0.0202,
      "step": 3942
    },
    {
      "epoch": 0.9617073170731707,
      "grad_norm": 0.07676021754741669,
      "learning_rate": 3.836157645730166e-05,
      "loss": 0.0218,
      "step": 3943
    },
    {
      "epoch": 0.9619512195121951,
      "grad_norm": 0.17056602239608765,
      "learning_rate": 3.8356178736187685e-05,
      "loss": 0.0176,
      "step": 3944
    },
    {
      "epoch": 0.9621951219512195,
      "grad_norm": 0.2626233398914337,
      "learning_rate": 3.835078014362394e-05,
      "loss": 0.0145,
      "step": 3945
    },
    {
      "epoch": 0.9624390243902439,
      "grad_norm": 0.11356459558010101,
      "learning_rate": 3.8345380679962664e-05,
      "loss": 0.0178,
      "step": 3946
    },
    {
      "epoch": 0.9626829268292683,
      "grad_norm": 0.2970549762248993,
      "learning_rate": 3.8339980345556145e-05,
      "loss": 0.0151,
      "step": 3947
    },
    {
      "epoch": 0.9629268292682926,
      "grad_norm": 0.053068481385707855,
      "learning_rate": 3.833457914075676e-05,
      "loss": 0.0091,
      "step": 3948
    },
    {
      "epoch": 0.963170731707317,
      "grad_norm": 0.20188382267951965,
      "learning_rate": 3.8329177065916886e-05,
      "loss": 0.0206,
      "step": 3949
    },
    {
      "epoch": 0.9634146341463414,
      "grad_norm": 0.11668121069669724,
      "learning_rate": 3.832377412138902e-05,
      "loss": 0.0176,
      "step": 3950
    },
    {
      "epoch": 0.9636585365853658,
      "grad_norm": 0.14078056812286377,
      "learning_rate": 3.83183703075257e-05,
      "loss": 0.0137,
      "step": 3951
    },
    {
      "epoch": 0.9639024390243902,
      "grad_norm": 0.0981927216053009,
      "learning_rate": 3.8312965624679475e-05,
      "loss": 0.0086,
      "step": 3952
    },
    {
      "epoch": 0.9641463414634146,
      "grad_norm": 0.2298666089773178,
      "learning_rate": 3.830756007320299e-05,
      "loss": 0.025,
      "step": 3953
    },
    {
      "epoch": 0.964390243902439,
      "grad_norm": 0.2985646724700928,
      "learning_rate": 3.830215365344896e-05,
      "loss": 0.0158,
      "step": 3954
    },
    {
      "epoch": 0.9646341463414634,
      "grad_norm": 0.15609297156333923,
      "learning_rate": 3.829674636577012e-05,
      "loss": 0.0294,
      "step": 3955
    },
    {
      "epoch": 0.9648780487804878,
      "grad_norm": 0.3026016056537628,
      "learning_rate": 3.8291338210519286e-05,
      "loss": 0.0334,
      "step": 3956
    },
    {
      "epoch": 0.9651219512195122,
      "grad_norm": 0.11052726954221725,
      "learning_rate": 3.828592918804932e-05,
      "loss": 0.0275,
      "step": 3957
    },
    {
      "epoch": 0.9653658536585366,
      "grad_norm": 0.08312346786260605,
      "learning_rate": 3.828051929871315e-05,
      "loss": 0.0146,
      "step": 3958
    },
    {
      "epoch": 0.965609756097561,
      "grad_norm": 0.08687224984169006,
      "learning_rate": 3.827510854286375e-05,
      "loss": 0.0314,
      "step": 3959
    },
    {
      "epoch": 0.9658536585365853,
      "grad_norm": 0.1502898782491684,
      "learning_rate": 3.826969692085416e-05,
      "loss": 0.0125,
      "step": 3960
    },
    {
      "epoch": 0.9660975609756097,
      "grad_norm": 0.05471980199217796,
      "learning_rate": 3.8264284433037465e-05,
      "loss": 0.0217,
      "step": 3961
    },
    {
      "epoch": 0.9663414634146341,
      "grad_norm": 0.1813306212425232,
      "learning_rate": 3.8258871079766815e-05,
      "loss": 0.0314,
      "step": 3962
    },
    {
      "epoch": 0.9665853658536585,
      "grad_norm": 0.12036347389221191,
      "learning_rate": 3.825345686139542e-05,
      "loss": 0.0236,
      "step": 3963
    },
    {
      "epoch": 0.9668292682926829,
      "grad_norm": 0.10911750793457031,
      "learning_rate": 3.824804177827654e-05,
      "loss": 0.027,
      "step": 3964
    },
    {
      "epoch": 0.9670731707317073,
      "grad_norm": 0.10660959780216217,
      "learning_rate": 3.8242625830763484e-05,
      "loss": 0.0258,
      "step": 3965
    },
    {
      "epoch": 0.9673170731707317,
      "grad_norm": 0.09077561646699905,
      "learning_rate": 3.823720901920963e-05,
      "loss": 0.0104,
      "step": 3966
    },
    {
      "epoch": 0.9675609756097561,
      "grad_norm": 0.0963611751794815,
      "learning_rate": 3.8231791343968415e-05,
      "loss": 0.0315,
      "step": 3967
    },
    {
      "epoch": 0.9678048780487805,
      "grad_norm": 0.08180967718362808,
      "learning_rate": 3.822637280539333e-05,
      "loss": 0.0312,
      "step": 3968
    },
    {
      "epoch": 0.9680487804878048,
      "grad_norm": 0.24124306440353394,
      "learning_rate": 3.82209534038379e-05,
      "loss": 0.0282,
      "step": 3969
    },
    {
      "epoch": 0.9682926829268292,
      "grad_norm": 0.05963665619492531,
      "learning_rate": 3.8215533139655733e-05,
      "loss": 0.0141,
      "step": 3970
    },
    {
      "epoch": 0.9685365853658536,
      "grad_norm": 0.12258946895599365,
      "learning_rate": 3.821011201320048e-05,
      "loss": 0.0262,
      "step": 3971
    },
    {
      "epoch": 0.968780487804878,
      "grad_norm": 0.09885674715042114,
      "learning_rate": 3.8204690024825875e-05,
      "loss": 0.0146,
      "step": 3972
    },
    {
      "epoch": 0.9690243902439024,
      "grad_norm": 0.13613265752792358,
      "learning_rate": 3.8199267174885654e-05,
      "loss": 0.0205,
      "step": 3973
    },
    {
      "epoch": 0.9692682926829268,
      "grad_norm": 0.05708746612071991,
      "learning_rate": 3.819384346373366e-05,
      "loss": 0.0163,
      "step": 3974
    },
    {
      "epoch": 0.9695121951219512,
      "grad_norm": 0.07881659269332886,
      "learning_rate": 3.8188418891723765e-05,
      "loss": 0.0302,
      "step": 3975
    },
    {
      "epoch": 0.9697560975609756,
      "grad_norm": 0.10697933286428452,
      "learning_rate": 3.8182993459209925e-05,
      "loss": 0.0284,
      "step": 3976
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.09133081883192062,
      "learning_rate": 3.817756716654611e-05,
      "loss": 0.0251,
      "step": 3977
    },
    {
      "epoch": 0.9702439024390244,
      "grad_norm": 0.09169860184192657,
      "learning_rate": 3.8172140014086384e-05,
      "loss": 0.0124,
      "step": 3978
    },
    {
      "epoch": 0.9704878048780488,
      "grad_norm": 0.09235616028308868,
      "learning_rate": 3.816671200218483e-05,
      "loss": 0.0244,
      "step": 3979
    },
    {
      "epoch": 0.9707317073170731,
      "grad_norm": 0.07528321444988251,
      "learning_rate": 3.816128313119564e-05,
      "loss": 0.0231,
      "step": 3980
    },
    {
      "epoch": 0.9709756097560975,
      "grad_norm": 0.1258300244808197,
      "learning_rate": 3.815585340147302e-05,
      "loss": 0.0159,
      "step": 3981
    },
    {
      "epoch": 0.9712195121951219,
      "grad_norm": 0.08051399886608124,
      "learning_rate": 3.815042281337123e-05,
      "loss": 0.0228,
      "step": 3982
    },
    {
      "epoch": 0.9714634146341463,
      "grad_norm": 0.10083944350481033,
      "learning_rate": 3.814499136724461e-05,
      "loss": 0.0252,
      "step": 3983
    },
    {
      "epoch": 0.9717073170731707,
      "grad_norm": 0.10576582700014114,
      "learning_rate": 3.813955906344754e-05,
      "loss": 0.0212,
      "step": 3984
    },
    {
      "epoch": 0.9719512195121951,
      "grad_norm": 0.0834619551897049,
      "learning_rate": 3.8134125902334474e-05,
      "loss": 0.0155,
      "step": 3985
    },
    {
      "epoch": 0.9721951219512195,
      "grad_norm": 0.08108597248792648,
      "learning_rate": 3.81286918842599e-05,
      "loss": 0.021,
      "step": 3986
    },
    {
      "epoch": 0.9724390243902439,
      "grad_norm": 0.1156558021903038,
      "learning_rate": 3.812325700957836e-05,
      "loss": 0.0196,
      "step": 3987
    },
    {
      "epoch": 0.9726829268292683,
      "grad_norm": 0.07001170516014099,
      "learning_rate": 3.8117821278644485e-05,
      "loss": 0.017,
      "step": 3988
    },
    {
      "epoch": 0.9729268292682927,
      "grad_norm": 0.09203020483255386,
      "learning_rate": 3.811238469181293e-05,
      "loss": 0.023,
      "step": 3989
    },
    {
      "epoch": 0.973170731707317,
      "grad_norm": 0.07210060954093933,
      "learning_rate": 3.8106947249438404e-05,
      "loss": 0.0157,
      "step": 3990
    },
    {
      "epoch": 0.9734146341463414,
      "grad_norm": 0.07835395634174347,
      "learning_rate": 3.81015089518757e-05,
      "loss": 0.0209,
      "step": 3991
    },
    {
      "epoch": 0.9736585365853658,
      "grad_norm": 0.0906951054930687,
      "learning_rate": 3.809606979947965e-05,
      "loss": 0.0195,
      "step": 3992
    },
    {
      "epoch": 0.9739024390243902,
      "grad_norm": 0.1451340615749359,
      "learning_rate": 3.809062979260513e-05,
      "loss": 0.0268,
      "step": 3993
    },
    {
      "epoch": 0.9741463414634146,
      "grad_norm": 0.1666339784860611,
      "learning_rate": 3.80851889316071e-05,
      "loss": 0.0251,
      "step": 3994
    },
    {
      "epoch": 0.974390243902439,
      "grad_norm": 0.0920816957950592,
      "learning_rate": 3.807974721684053e-05,
      "loss": 0.0117,
      "step": 3995
    },
    {
      "epoch": 0.9746341463414634,
      "grad_norm": 0.19014723598957062,
      "learning_rate": 3.807430464866051e-05,
      "loss": 0.028,
      "step": 3996
    },
    {
      "epoch": 0.9748780487804878,
      "grad_norm": 0.11250154674053192,
      "learning_rate": 3.806886122742212e-05,
      "loss": 0.0306,
      "step": 3997
    },
    {
      "epoch": 0.9751219512195122,
      "grad_norm": 0.34725189208984375,
      "learning_rate": 3.806341695348057e-05,
      "loss": 0.0361,
      "step": 3998
    },
    {
      "epoch": 0.9753658536585366,
      "grad_norm": 0.14131774008274078,
      "learning_rate": 3.805797182719104e-05,
      "loss": 0.024,
      "step": 3999
    },
    {
      "epoch": 0.975609756097561,
      "grad_norm": 0.15005220472812653,
      "learning_rate": 3.805252584890882e-05,
      "loss": 0.0378,
      "step": 4000
    },
    {
      "epoch": 0.9758536585365853,
      "grad_norm": 0.08738410472869873,
      "learning_rate": 3.804707901898925e-05,
      "loss": 0.0189,
      "step": 4001
    },
    {
      "epoch": 0.9760975609756097,
      "grad_norm": 0.11185719072818756,
      "learning_rate": 3.804163133778772e-05,
      "loss": 0.0184,
      "step": 4002
    },
    {
      "epoch": 0.9763414634146341,
      "grad_norm": 0.07844819128513336,
      "learning_rate": 3.803618280565966e-05,
      "loss": 0.0083,
      "step": 4003
    },
    {
      "epoch": 0.9765853658536585,
      "grad_norm": 0.09726876020431519,
      "learning_rate": 3.8030733422960575e-05,
      "loss": 0.0318,
      "step": 4004
    },
    {
      "epoch": 0.9768292682926829,
      "grad_norm": 0.09847856312990189,
      "learning_rate": 3.8025283190046035e-05,
      "loss": 0.0238,
      "step": 4005
    },
    {
      "epoch": 0.9770731707317073,
      "grad_norm": 0.1464179903268814,
      "learning_rate": 3.801983210727164e-05,
      "loss": 0.0538,
      "step": 4006
    },
    {
      "epoch": 0.9773170731707317,
      "grad_norm": 0.15053826570510864,
      "learning_rate": 3.801438017499306e-05,
      "loss": 0.0273,
      "step": 4007
    },
    {
      "epoch": 0.9775609756097561,
      "grad_norm": 0.06576479226350784,
      "learning_rate": 3.800892739356601e-05,
      "loss": 0.0125,
      "step": 4008
    },
    {
      "epoch": 0.9778048780487805,
      "grad_norm": 0.11272896826267242,
      "learning_rate": 3.8003473763346264e-05,
      "loss": 0.0221,
      "step": 4009
    },
    {
      "epoch": 0.9780487804878049,
      "grad_norm": 0.15646041929721832,
      "learning_rate": 3.7998019284689674e-05,
      "loss": 0.0261,
      "step": 4010
    },
    {
      "epoch": 0.9782926829268292,
      "grad_norm": 0.11944612860679626,
      "learning_rate": 3.799256395795211e-05,
      "loss": 0.0256,
      "step": 4011
    },
    {
      "epoch": 0.9785365853658536,
      "grad_norm": 0.08131168782711029,
      "learning_rate": 3.798710778348953e-05,
      "loss": 0.02,
      "step": 4012
    },
    {
      "epoch": 0.978780487804878,
      "grad_norm": 0.1225212812423706,
      "learning_rate": 3.7981650761657914e-05,
      "loss": 0.0218,
      "step": 4013
    },
    {
      "epoch": 0.9790243902439024,
      "grad_norm": 0.07209272682666779,
      "learning_rate": 3.797619289281333e-05,
      "loss": 0.0208,
      "step": 4014
    },
    {
      "epoch": 0.9792682926829268,
      "grad_norm": 0.11635648459196091,
      "learning_rate": 3.797073417731189e-05,
      "loss": 0.0254,
      "step": 4015
    },
    {
      "epoch": 0.9795121951219512,
      "grad_norm": 0.11796913295984268,
      "learning_rate": 3.796527461550975e-05,
      "loss": 0.0286,
      "step": 4016
    },
    {
      "epoch": 0.9797560975609756,
      "grad_norm": 0.1813965141773224,
      "learning_rate": 3.7959814207763135e-05,
      "loss": 0.0151,
      "step": 4017
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.10990197956562042,
      "learning_rate": 3.795435295442831e-05,
      "loss": 0.0133,
      "step": 4018
    },
    {
      "epoch": 0.9802439024390244,
      "grad_norm": 0.13470344245433807,
      "learning_rate": 3.794889085586161e-05,
      "loss": 0.0297,
      "step": 4019
    },
    {
      "epoch": 0.9804878048780488,
      "grad_norm": 0.12381743639707565,
      "learning_rate": 3.7943427912419426e-05,
      "loss": 0.0239,
      "step": 4020
    },
    {
      "epoch": 0.9807317073170732,
      "grad_norm": 0.053387533873319626,
      "learning_rate": 3.793796412445819e-05,
      "loss": 0.0146,
      "step": 4021
    },
    {
      "epoch": 0.9809756097560975,
      "grad_norm": 0.11489440500736237,
      "learning_rate": 3.7932499492334404e-05,
      "loss": 0.0253,
      "step": 4022
    },
    {
      "epoch": 0.9812195121951219,
      "grad_norm": 0.1754356026649475,
      "learning_rate": 3.792703401640461e-05,
      "loss": 0.0284,
      "step": 4023
    },
    {
      "epoch": 0.9814634146341463,
      "grad_norm": 0.11820587515830994,
      "learning_rate": 3.7921567697025434e-05,
      "loss": 0.0168,
      "step": 4024
    },
    {
      "epoch": 0.9817073170731707,
      "grad_norm": 0.09341505169868469,
      "learning_rate": 3.7916100534553516e-05,
      "loss": 0.0201,
      "step": 4025
    },
    {
      "epoch": 0.9819512195121951,
      "grad_norm": 0.05518972873687744,
      "learning_rate": 3.791063252934558e-05,
      "loss": 0.0203,
      "step": 4026
    },
    {
      "epoch": 0.9821951219512195,
      "grad_norm": 0.14830873906612396,
      "learning_rate": 3.790516368175839e-05,
      "loss": 0.0187,
      "step": 4027
    },
    {
      "epoch": 0.9824390243902439,
      "grad_norm": 0.1762286275625229,
      "learning_rate": 3.789969399214878e-05,
      "loss": 0.0181,
      "step": 4028
    },
    {
      "epoch": 0.9826829268292683,
      "grad_norm": 0.19244126975536346,
      "learning_rate": 3.7894223460873625e-05,
      "loss": 0.0304,
      "step": 4029
    },
    {
      "epoch": 0.9829268292682927,
      "grad_norm": 0.11880913376808167,
      "learning_rate": 3.788875208828987e-05,
      "loss": 0.0208,
      "step": 4030
    },
    {
      "epoch": 0.9831707317073171,
      "grad_norm": 0.052611131221055984,
      "learning_rate": 3.7883279874754485e-05,
      "loss": 0.0207,
      "step": 4031
    },
    {
      "epoch": 0.9834146341463414,
      "grad_norm": 0.08417253196239471,
      "learning_rate": 3.7877806820624535e-05,
      "loss": 0.0218,
      "step": 4032
    },
    {
      "epoch": 0.9836585365853658,
      "grad_norm": 0.12995555996894836,
      "learning_rate": 3.7872332926257105e-05,
      "loss": 0.0253,
      "step": 4033
    },
    {
      "epoch": 0.9839024390243902,
      "grad_norm": 0.10533010959625244,
      "learning_rate": 3.786685819200937e-05,
      "loss": 0.0327,
      "step": 4034
    },
    {
      "epoch": 0.9841463414634146,
      "grad_norm": 0.1664271503686905,
      "learning_rate": 3.7861382618238513e-05,
      "loss": 0.0313,
      "step": 4035
    },
    {
      "epoch": 0.984390243902439,
      "grad_norm": 0.2776157557964325,
      "learning_rate": 3.785590620530183e-05,
      "loss": 0.0162,
      "step": 4036
    },
    {
      "epoch": 0.9846341463414634,
      "grad_norm": 0.20991240441799164,
      "learning_rate": 3.785042895355661e-05,
      "loss": 0.0118,
      "step": 4037
    },
    {
      "epoch": 0.9848780487804878,
      "grad_norm": 0.17254777252674103,
      "learning_rate": 3.7844950863360243e-05,
      "loss": 0.0229,
      "step": 4038
    },
    {
      "epoch": 0.9851219512195122,
      "grad_norm": 0.07689790427684784,
      "learning_rate": 3.7839471935070147e-05,
      "loss": 0.0124,
      "step": 4039
    },
    {
      "epoch": 0.9853658536585366,
      "grad_norm": 0.040434375405311584,
      "learning_rate": 3.7833992169043824e-05,
      "loss": 0.0138,
      "step": 4040
    },
    {
      "epoch": 0.985609756097561,
      "grad_norm": 0.09210848063230515,
      "learning_rate": 3.78285115656388e-05,
      "loss": 0.017,
      "step": 4041
    },
    {
      "epoch": 0.9858536585365854,
      "grad_norm": 0.08518483489751816,
      "learning_rate": 3.782303012521267e-05,
      "loss": 0.0237,
      "step": 4042
    },
    {
      "epoch": 0.9860975609756097,
      "grad_norm": 0.12963996827602386,
      "learning_rate": 3.781754784812307e-05,
      "loss": 0.0275,
      "step": 4043
    },
    {
      "epoch": 0.9863414634146341,
      "grad_norm": 0.13360929489135742,
      "learning_rate": 3.781206473472771e-05,
      "loss": 0.0115,
      "step": 4044
    },
    {
      "epoch": 0.9865853658536585,
      "grad_norm": 0.22055718302726746,
      "learning_rate": 3.7806580785384355e-05,
      "loss": 0.0276,
      "step": 4045
    },
    {
      "epoch": 0.9868292682926829,
      "grad_norm": 0.13007673621177673,
      "learning_rate": 3.78010960004508e-05,
      "loss": 0.0236,
      "step": 4046
    },
    {
      "epoch": 0.9870731707317073,
      "grad_norm": 0.12274084985256195,
      "learning_rate": 3.779561038028493e-05,
      "loss": 0.0294,
      "step": 4047
    },
    {
      "epoch": 0.9873170731707317,
      "grad_norm": 0.14346911013126373,
      "learning_rate": 3.779012392524465e-05,
      "loss": 0.0091,
      "step": 4048
    },
    {
      "epoch": 0.9875609756097561,
      "grad_norm": 0.1252882182598114,
      "learning_rate": 3.778463663568793e-05,
      "loss": 0.0277,
      "step": 4049
    },
    {
      "epoch": 0.9878048780487805,
      "grad_norm": 0.24831651151180267,
      "learning_rate": 3.777914851197282e-05,
      "loss": 0.0341,
      "step": 4050
    },
    {
      "epoch": 0.9880487804878049,
      "grad_norm": 0.1399303823709488,
      "learning_rate": 3.777365955445739e-05,
      "loss": 0.0193,
      "step": 4051
    },
    {
      "epoch": 0.9882926829268293,
      "grad_norm": 0.12881922721862793,
      "learning_rate": 3.776816976349976e-05,
      "loss": 0.0183,
      "step": 4052
    },
    {
      "epoch": 0.9885365853658536,
      "grad_norm": 0.16203342378139496,
      "learning_rate": 3.776267913945816e-05,
      "loss": 0.0182,
      "step": 4053
    },
    {
      "epoch": 0.988780487804878,
      "grad_norm": 0.06979135423898697,
      "learning_rate": 3.775718768269081e-05,
      "loss": 0.0157,
      "step": 4054
    },
    {
      "epoch": 0.9890243902439024,
      "grad_norm": 0.2375580221414566,
      "learning_rate": 3.775169539355601e-05,
      "loss": 0.0269,
      "step": 4055
    },
    {
      "epoch": 0.9892682926829268,
      "grad_norm": 0.11548813432455063,
      "learning_rate": 3.7746202272412125e-05,
      "loss": 0.0228,
      "step": 4056
    },
    {
      "epoch": 0.9895121951219512,
      "grad_norm": 0.12035981565713882,
      "learning_rate": 3.774070831961757e-05,
      "loss": 0.0154,
      "step": 4057
    },
    {
      "epoch": 0.9897560975609756,
      "grad_norm": 0.08611635863780975,
      "learning_rate": 3.7735213535530794e-05,
      "loss": 0.0232,
      "step": 4058
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.1771412342786789,
      "learning_rate": 3.772971792051032e-05,
      "loss": 0.0171,
      "step": 4059
    },
    {
      "epoch": 0.9902439024390244,
      "grad_norm": 0.2957897186279297,
      "learning_rate": 3.772422147491472e-05,
      "loss": 0.02,
      "step": 4060
    },
    {
      "epoch": 0.9904878048780488,
      "grad_norm": 0.09517496079206467,
      "learning_rate": 3.771872419910264e-05,
      "loss": 0.0145,
      "step": 4061
    },
    {
      "epoch": 0.9907317073170732,
      "grad_norm": 0.07650648802518845,
      "learning_rate": 3.7713226093432716e-05,
      "loss": 0.0167,
      "step": 4062
    },
    {
      "epoch": 0.9909756097560976,
      "grad_norm": 0.052424106746912,
      "learning_rate": 3.7707727158263714e-05,
      "loss": 0.0194,
      "step": 4063
    },
    {
      "epoch": 0.9912195121951219,
      "grad_norm": 0.17840027809143066,
      "learning_rate": 3.770222739395443e-05,
      "loss": 0.0241,
      "step": 4064
    },
    {
      "epoch": 0.9914634146341463,
      "grad_norm": 0.12046463787555695,
      "learning_rate": 3.769672680086368e-05,
      "loss": 0.0315,
      "step": 4065
    },
    {
      "epoch": 0.9917073170731707,
      "grad_norm": 0.11258596181869507,
      "learning_rate": 3.7691225379350384e-05,
      "loss": 0.0111,
      "step": 4066
    },
    {
      "epoch": 0.9919512195121951,
      "grad_norm": 0.146805539727211,
      "learning_rate": 3.7685723129773484e-05,
      "loss": 0.0149,
      "step": 4067
    },
    {
      "epoch": 0.9921951219512195,
      "grad_norm": 0.08893796801567078,
      "learning_rate": 3.7680220052491974e-05,
      "loss": 0.014,
      "step": 4068
    },
    {
      "epoch": 0.9924390243902439,
      "grad_norm": 0.08026177436113358,
      "learning_rate": 3.767471614786493e-05,
      "loss": 0.0066,
      "step": 4069
    },
    {
      "epoch": 0.9926829268292683,
      "grad_norm": 0.23715627193450928,
      "learning_rate": 3.766921141625146e-05,
      "loss": 0.0161,
      "step": 4070
    },
    {
      "epoch": 0.9929268292682927,
      "grad_norm": 0.21716128289699554,
      "learning_rate": 3.766370585801072e-05,
      "loss": 0.0172,
      "step": 4071
    },
    {
      "epoch": 0.9931707317073171,
      "grad_norm": 0.13294395804405212,
      "learning_rate": 3.765819947350196e-05,
      "loss": 0.0285,
      "step": 4072
    },
    {
      "epoch": 0.9934146341463415,
      "grad_norm": 0.19325533509254456,
      "learning_rate": 3.765269226308441e-05,
      "loss": 0.0329,
      "step": 4073
    },
    {
      "epoch": 0.9936585365853658,
      "grad_norm": 0.10520710796117783,
      "learning_rate": 3.7647184227117435e-05,
      "loss": 0.0315,
      "step": 4074
    },
    {
      "epoch": 0.9939024390243902,
      "grad_norm": 0.09094651788473129,
      "learning_rate": 3.7641675365960416e-05,
      "loss": 0.0243,
      "step": 4075
    },
    {
      "epoch": 0.9941463414634146,
      "grad_norm": 0.17709320783615112,
      "learning_rate": 3.763616567997277e-05,
      "loss": 0.0277,
      "step": 4076
    },
    {
      "epoch": 0.994390243902439,
      "grad_norm": 0.16029193997383118,
      "learning_rate": 3.763065516951399e-05,
      "loss": 0.0417,
      "step": 4077
    },
    {
      "epoch": 0.9946341463414634,
      "grad_norm": 0.09368382394313812,
      "learning_rate": 3.7625143834943637e-05,
      "loss": 0.0151,
      "step": 4078
    },
    {
      "epoch": 0.9948780487804878,
      "grad_norm": 0.1792925000190735,
      "learning_rate": 3.7619631676621295e-05,
      "loss": 0.0183,
      "step": 4079
    },
    {
      "epoch": 0.9951219512195122,
      "grad_norm": 0.09038098901510239,
      "learning_rate": 3.761411869490661e-05,
      "loss": 0.0211,
      "step": 4080
    },
    {
      "epoch": 0.9953658536585366,
      "grad_norm": 0.2973593473434448,
      "learning_rate": 3.7608604890159307e-05,
      "loss": 0.0336,
      "step": 4081
    },
    {
      "epoch": 0.995609756097561,
      "grad_norm": 0.06415583938360214,
      "learning_rate": 3.760309026273913e-05,
      "loss": 0.0291,
      "step": 4082
    },
    {
      "epoch": 0.9958536585365854,
      "grad_norm": 0.13789884746074677,
      "learning_rate": 3.7597574813005895e-05,
      "loss": 0.0307,
      "step": 4083
    },
    {
      "epoch": 0.9960975609756098,
      "grad_norm": 0.08966634422540665,
      "learning_rate": 3.7592058541319465e-05,
      "loss": 0.0141,
      "step": 4084
    },
    {
      "epoch": 0.9963414634146341,
      "grad_norm": 0.24637146294116974,
      "learning_rate": 3.758654144803977e-05,
      "loss": 0.0299,
      "step": 4085
    },
    {
      "epoch": 0.9965853658536585,
      "grad_norm": 0.11739001423120499,
      "learning_rate": 3.758102353352677e-05,
      "loss": 0.018,
      "step": 4086
    },
    {
      "epoch": 0.9968292682926829,
      "grad_norm": 0.3029880225658417,
      "learning_rate": 3.75755047981405e-05,
      "loss": 0.0376,
      "step": 4087
    },
    {
      "epoch": 0.9970731707317073,
      "grad_norm": 0.09528310596942902,
      "learning_rate": 3.756998524224104e-05,
      "loss": 0.0135,
      "step": 4088
    },
    {
      "epoch": 0.9973170731707317,
      "grad_norm": 0.08348072320222855,
      "learning_rate": 3.756446486618852e-05,
      "loss": 0.0272,
      "step": 4089
    },
    {
      "epoch": 0.9975609756097561,
      "grad_norm": 0.12767541408538818,
      "learning_rate": 3.7558943670343136e-05,
      "loss": 0.0239,
      "step": 4090
    },
    {
      "epoch": 0.9978048780487805,
      "grad_norm": 0.0949353575706482,
      "learning_rate": 3.755342165506512e-05,
      "loss": 0.0147,
      "step": 4091
    },
    {
      "epoch": 0.9980487804878049,
      "grad_norm": 0.11952077597379684,
      "learning_rate": 3.7547898820714774e-05,
      "loss": 0.0143,
      "step": 4092
    },
    {
      "epoch": 0.9982926829268293,
      "grad_norm": 0.13963031768798828,
      "learning_rate": 3.7542375167652445e-05,
      "loss": 0.0255,
      "step": 4093
    },
    {
      "epoch": 0.9985365853658537,
      "grad_norm": 0.14293186366558075,
      "learning_rate": 3.753685069623853e-05,
      "loss": 0.0219,
      "step": 4094
    },
    {
      "epoch": 0.998780487804878,
      "grad_norm": 0.10997549444437027,
      "learning_rate": 3.753132540683349e-05,
      "loss": 0.0186,
      "step": 4095
    },
    {
      "epoch": 0.9990243902439024,
      "grad_norm": 0.1811065673828125,
      "learning_rate": 3.752579929979783e-05,
      "loss": 0.0269,
      "step": 4096
    },
    {
      "epoch": 0.9992682926829268,
      "grad_norm": 0.07766659557819366,
      "learning_rate": 3.7520272375492095e-05,
      "loss": 0.0155,
      "step": 4097
    },
    {
      "epoch": 0.9995121951219512,
      "grad_norm": 0.1358134150505066,
      "learning_rate": 3.751474463427693e-05,
      "loss": 0.0216,
      "step": 4098
    },
    {
      "epoch": 0.9997560975609756,
      "grad_norm": 0.09583674371242523,
      "learning_rate": 3.750921607651299e-05,
      "loss": 0.0213,
      "step": 4099
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.06304320693016052,
      "learning_rate": 3.7503686702560995e-05,
      "loss": 0.0164,
      "step": 4100
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.42292682926829267,
      "eval_f1": 0.5927469688314494,
      "eval_loss": 0.02288806438446045,
      "eval_precision": 0.6448113698278596,
      "eval_recall": 0.5643628790836913,
      "eval_roc_auc": NaN,
      "eval_runtime": 20.9406,
      "eval_samples_per_second": 195.792,
      "eval_steps_per_second": 24.498,
      "step": 4100
    },
    {
      "epoch": 1.0002439024390244,
      "grad_norm": 0.31904926896095276,
      "learning_rate": 3.7498156512781704e-05,
      "loss": 0.0262,
      "step": 4101
    },
    {
      "epoch": 1.0004878048780488,
      "grad_norm": 0.0849432721734047,
      "learning_rate": 3.749262550753597e-05,
      "loss": 0.0131,
      "step": 4102
    },
    {
      "epoch": 1.0007317073170732,
      "grad_norm": 0.16567447781562805,
      "learning_rate": 3.7487093687184674e-05,
      "loss": 0.0314,
      "step": 4103
    },
    {
      "epoch": 1.0009756097560976,
      "grad_norm": 0.08888841420412064,
      "learning_rate": 3.7481561052088735e-05,
      "loss": 0.0169,
      "step": 4104
    },
    {
      "epoch": 1.001219512195122,
      "grad_norm": 0.15930350124835968,
      "learning_rate": 3.747602760260915e-05,
      "loss": 0.0423,
      "step": 4105
    },
    {
      "epoch": 1.0014634146341463,
      "grad_norm": 0.07081883400678635,
      "learning_rate": 3.747049333910695e-05,
      "loss": 0.021,
      "step": 4106
    },
    {
      "epoch": 1.0017073170731707,
      "grad_norm": 0.335135281085968,
      "learning_rate": 3.7464958261943245e-05,
      "loss": 0.0309,
      "step": 4107
    },
    {
      "epoch": 1.0019512195121951,
      "grad_norm": 0.0953051820397377,
      "learning_rate": 3.745942237147917e-05,
      "loss": 0.0197,
      "step": 4108
    },
    {
      "epoch": 1.0021951219512195,
      "grad_norm": 0.13285067677497864,
      "learning_rate": 3.745388566807592e-05,
      "loss": 0.0213,
      "step": 4109
    },
    {
      "epoch": 1.002439024390244,
      "grad_norm": 0.09916520118713379,
      "learning_rate": 3.744834815209476e-05,
      "loss": 0.0239,
      "step": 4110
    },
    {
      "epoch": 1.0026829268292683,
      "grad_norm": 0.251796692609787,
      "learning_rate": 3.7442809823896986e-05,
      "loss": 0.0342,
      "step": 4111
    },
    {
      "epoch": 1.0029268292682927,
      "grad_norm": 0.10997740179300308,
      "learning_rate": 3.743727068384397e-05,
      "loss": 0.0076,
      "step": 4112
    },
    {
      "epoch": 1.003170731707317,
      "grad_norm": 0.08606769889593124,
      "learning_rate": 3.743173073229711e-05,
      "loss": 0.0232,
      "step": 4113
    },
    {
      "epoch": 1.0034146341463415,
      "grad_norm": 0.1087510958313942,
      "learning_rate": 3.742618996961788e-05,
      "loss": 0.0157,
      "step": 4114
    },
    {
      "epoch": 1.0036585365853659,
      "grad_norm": 0.0935957282781601,
      "learning_rate": 3.7420648396167796e-05,
      "loss": 0.0369,
      "step": 4115
    },
    {
      "epoch": 1.0039024390243902,
      "grad_norm": 0.11008355766534805,
      "learning_rate": 3.741510601230843e-05,
      "loss": 0.0422,
      "step": 4116
    },
    {
      "epoch": 1.0041463414634146,
      "grad_norm": 0.2923584580421448,
      "learning_rate": 3.74095628184014e-05,
      "loss": 0.027,
      "step": 4117
    },
    {
      "epoch": 1.004390243902439,
      "grad_norm": 0.08251461386680603,
      "learning_rate": 3.7404018814808384e-05,
      "loss": 0.0185,
      "step": 4118
    },
    {
      "epoch": 1.0046341463414634,
      "grad_norm": 0.11190929263830185,
      "learning_rate": 3.739847400189111e-05,
      "loss": 0.0191,
      "step": 4119
    },
    {
      "epoch": 1.0048780487804878,
      "grad_norm": 0.14689099788665771,
      "learning_rate": 3.7392928380011366e-05,
      "loss": 0.0263,
      "step": 4120
    },
    {
      "epoch": 1.0051219512195122,
      "grad_norm": 0.1309269368648529,
      "learning_rate": 3.738738194953098e-05,
      "loss": 0.0295,
      "step": 4121
    },
    {
      "epoch": 1.0053658536585366,
      "grad_norm": 0.1255321204662323,
      "learning_rate": 3.738183471081185e-05,
      "loss": 0.0165,
      "step": 4122
    },
    {
      "epoch": 1.005609756097561,
      "grad_norm": 0.14837384223937988,
      "learning_rate": 3.737628666421591e-05,
      "loss": 0.0273,
      "step": 4123
    },
    {
      "epoch": 1.0058536585365854,
      "grad_norm": 0.0663091391324997,
      "learning_rate": 3.737073781010514e-05,
      "loss": 0.0123,
      "step": 4124
    },
    {
      "epoch": 1.0060975609756098,
      "grad_norm": 0.0875716283917427,
      "learning_rate": 3.736518814884161e-05,
      "loss": 0.0126,
      "step": 4125
    },
    {
      "epoch": 1.0063414634146342,
      "grad_norm": 0.13034600019454956,
      "learning_rate": 3.7359637680787405e-05,
      "loss": 0.0192,
      "step": 4126
    },
    {
      "epoch": 1.0065853658536585,
      "grad_norm": 0.17437589168548584,
      "learning_rate": 3.735408640630468e-05,
      "loss": 0.0206,
      "step": 4127
    },
    {
      "epoch": 1.006829268292683,
      "grad_norm": 0.05551394447684288,
      "learning_rate": 3.734853432575563e-05,
      "loss": 0.0127,
      "step": 4128
    },
    {
      "epoch": 1.0070731707317073,
      "grad_norm": 0.1419374942779541,
      "learning_rate": 3.734298143950252e-05,
      "loss": 0.0148,
      "step": 4129
    },
    {
      "epoch": 1.0073170731707317,
      "grad_norm": 0.10614795237779617,
      "learning_rate": 3.733742774790766e-05,
      "loss": 0.0306,
      "step": 4130
    },
    {
      "epoch": 1.007560975609756,
      "grad_norm": 0.07695459574460983,
      "learning_rate": 3.73318732513334e-05,
      "loss": 0.0172,
      "step": 4131
    },
    {
      "epoch": 1.0078048780487805,
      "grad_norm": 0.2125852257013321,
      "learning_rate": 3.732631795014217e-05,
      "loss": 0.0374,
      "step": 4132
    },
    {
      "epoch": 1.0080487804878049,
      "grad_norm": 0.10424017161130905,
      "learning_rate": 3.732076184469643e-05,
      "loss": 0.0356,
      "step": 4133
    },
    {
      "epoch": 1.0082926829268293,
      "grad_norm": 0.11361437290906906,
      "learning_rate": 3.7315204935358686e-05,
      "loss": 0.0121,
      "step": 4134
    },
    {
      "epoch": 1.0085365853658537,
      "grad_norm": 0.13003429770469666,
      "learning_rate": 3.730964722249153e-05,
      "loss": 0.0196,
      "step": 4135
    },
    {
      "epoch": 1.008780487804878,
      "grad_norm": 0.08974630385637283,
      "learning_rate": 3.730408870645757e-05,
      "loss": 0.0201,
      "step": 4136
    },
    {
      "epoch": 1.0090243902439024,
      "grad_norm": 0.09110274910926819,
      "learning_rate": 3.7298529387619497e-05,
      "loss": 0.027,
      "step": 4137
    },
    {
      "epoch": 1.0092682926829268,
      "grad_norm": 0.11280690878629684,
      "learning_rate": 3.729296926634002e-05,
      "loss": 0.0233,
      "step": 4138
    },
    {
      "epoch": 1.0095121951219512,
      "grad_norm": 0.09756259620189667,
      "learning_rate": 3.7287408342981936e-05,
      "loss": 0.0205,
      "step": 4139
    },
    {
      "epoch": 1.0097560975609756,
      "grad_norm": 0.06710932403802872,
      "learning_rate": 3.7281846617908074e-05,
      "loss": 0.0247,
      "step": 4140
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.2727058529853821,
      "learning_rate": 3.727628409148132e-05,
      "loss": 0.0379,
      "step": 4141
    },
    {
      "epoch": 1.0102439024390244,
      "grad_norm": 0.04771149903535843,
      "learning_rate": 3.727072076406461e-05,
      "loss": 0.0131,
      "step": 4142
    },
    {
      "epoch": 1.0104878048780488,
      "grad_norm": 0.08516287058591843,
      "learning_rate": 3.726515663602093e-05,
      "loss": 0.0228,
      "step": 4143
    },
    {
      "epoch": 1.0107317073170732,
      "grad_norm": 0.056664902716875076,
      "learning_rate": 3.725959170771333e-05,
      "loss": 0.0112,
      "step": 4144
    },
    {
      "epoch": 1.0109756097560976,
      "grad_norm": 0.19123607873916626,
      "learning_rate": 3.72540259795049e-05,
      "loss": 0.031,
      "step": 4145
    },
    {
      "epoch": 1.011219512195122,
      "grad_norm": 0.43348997831344604,
      "learning_rate": 3.724845945175879e-05,
      "loss": 0.0272,
      "step": 4146
    },
    {
      "epoch": 1.0114634146341464,
      "grad_norm": 0.08688026666641235,
      "learning_rate": 3.724289212483819e-05,
      "loss": 0.0304,
      "step": 4147
    },
    {
      "epoch": 1.0117073170731707,
      "grad_norm": 0.12050372362136841,
      "learning_rate": 3.723732399910637e-05,
      "loss": 0.026,
      "step": 4148
    },
    {
      "epoch": 1.0119512195121951,
      "grad_norm": 0.18183383345603943,
      "learning_rate": 3.723175507492661e-05,
      "loss": 0.032,
      "step": 4149
    },
    {
      "epoch": 1.0121951219512195,
      "grad_norm": 0.15052953362464905,
      "learning_rate": 3.722618535266229e-05,
      "loss": 0.025,
      "step": 4150
    },
    {
      "epoch": 1.012439024390244,
      "grad_norm": 0.09889986366033554,
      "learning_rate": 3.722061483267679e-05,
      "loss": 0.0193,
      "step": 4151
    },
    {
      "epoch": 1.0126829268292683,
      "grad_norm": 0.13382010161876678,
      "learning_rate": 3.721504351533358e-05,
      "loss": 0.0268,
      "step": 4152
    },
    {
      "epoch": 1.0129268292682927,
      "grad_norm": 0.1550692617893219,
      "learning_rate": 3.720947140099618e-05,
      "loss": 0.0275,
      "step": 4153
    },
    {
      "epoch": 1.013170731707317,
      "grad_norm": 0.14411410689353943,
      "learning_rate": 3.7203898490028156e-05,
      "loss": 0.0284,
      "step": 4154
    },
    {
      "epoch": 1.0134146341463415,
      "grad_norm": 0.04904652014374733,
      "learning_rate": 3.7198324782793106e-05,
      "loss": 0.0051,
      "step": 4155
    },
    {
      "epoch": 1.0136585365853659,
      "grad_norm": 0.10687442868947983,
      "learning_rate": 3.719275027965471e-05,
      "loss": 0.0303,
      "step": 4156
    },
    {
      "epoch": 1.0139024390243903,
      "grad_norm": 0.12189947813749313,
      "learning_rate": 3.718717498097668e-05,
      "loss": 0.0071,
      "step": 4157
    },
    {
      "epoch": 1.0141463414634146,
      "grad_norm": 0.10958679020404816,
      "learning_rate": 3.71815988871228e-05,
      "loss": 0.0194,
      "step": 4158
    },
    {
      "epoch": 1.014390243902439,
      "grad_norm": 0.06631629168987274,
      "learning_rate": 3.717602199845688e-05,
      "loss": 0.0114,
      "step": 4159
    },
    {
      "epoch": 1.0146341463414634,
      "grad_norm": 0.0920650064945221,
      "learning_rate": 3.7170444315342797e-05,
      "loss": 0.0245,
      "step": 4160
    },
    {
      "epoch": 1.0148780487804878,
      "grad_norm": 0.06894616037607193,
      "learning_rate": 3.716486583814447e-05,
      "loss": 0.0135,
      "step": 4161
    },
    {
      "epoch": 1.0151219512195122,
      "grad_norm": 0.08006130903959274,
      "learning_rate": 3.71592865672259e-05,
      "loss": 0.0188,
      "step": 4162
    },
    {
      "epoch": 1.0153658536585366,
      "grad_norm": 0.13996067643165588,
      "learning_rate": 3.715370650295111e-05,
      "loss": 0.0157,
      "step": 4163
    },
    {
      "epoch": 1.015609756097561,
      "grad_norm": 0.08732764422893524,
      "learning_rate": 3.714812564568416e-05,
      "loss": 0.013,
      "step": 4164
    },
    {
      "epoch": 1.0158536585365854,
      "grad_norm": 0.11984600126743317,
      "learning_rate": 3.7142543995789215e-05,
      "loss": 0.0275,
      "step": 4165
    },
    {
      "epoch": 1.0160975609756098,
      "grad_norm": 0.039255496114492416,
      "learning_rate": 3.713696155363044e-05,
      "loss": 0.009,
      "step": 4166
    },
    {
      "epoch": 1.0163414634146342,
      "grad_norm": 0.11135929077863693,
      "learning_rate": 3.713137831957209e-05,
      "loss": 0.0299,
      "step": 4167
    },
    {
      "epoch": 1.0165853658536586,
      "grad_norm": 0.12361752241849899,
      "learning_rate": 3.7125794293978424e-05,
      "loss": 0.0245,
      "step": 4168
    },
    {
      "epoch": 1.016829268292683,
      "grad_norm": 0.1384885162115097,
      "learning_rate": 3.712020947721381e-05,
      "loss": 0.0238,
      "step": 4169
    },
    {
      "epoch": 1.0170731707317073,
      "grad_norm": 0.1274956464767456,
      "learning_rate": 3.711462386964262e-05,
      "loss": 0.0134,
      "step": 4170
    },
    {
      "epoch": 1.0173170731707317,
      "grad_norm": 0.08766048401594162,
      "learning_rate": 3.710903747162933e-05,
      "loss": 0.0188,
      "step": 4171
    },
    {
      "epoch": 1.0175609756097561,
      "grad_norm": 0.17704135179519653,
      "learning_rate": 3.71034502835384e-05,
      "loss": 0.0279,
      "step": 4172
    },
    {
      "epoch": 1.0178048780487805,
      "grad_norm": 0.0767032727599144,
      "learning_rate": 3.7097862305734386e-05,
      "loss": 0.0176,
      "step": 4173
    },
    {
      "epoch": 1.018048780487805,
      "grad_norm": 0.068266361951828,
      "learning_rate": 3.7092273538581905e-05,
      "loss": 0.0148,
      "step": 4174
    },
    {
      "epoch": 1.0182926829268293,
      "grad_norm": 0.14563287794589996,
      "learning_rate": 3.708668398244559e-05,
      "loss": 0.0262,
      "step": 4175
    },
    {
      "epoch": 1.0185365853658537,
      "grad_norm": 0.13527928292751312,
      "learning_rate": 3.7081093637690144e-05,
      "loss": 0.0168,
      "step": 4176
    },
    {
      "epoch": 1.018780487804878,
      "grad_norm": 0.04653741419315338,
      "learning_rate": 3.707550250468032e-05,
      "loss": 0.0085,
      "step": 4177
    },
    {
      "epoch": 1.0190243902439025,
      "grad_norm": 0.07763682305812836,
      "learning_rate": 3.7069910583780924e-05,
      "loss": 0.0198,
      "step": 4178
    },
    {
      "epoch": 1.0192682926829268,
      "grad_norm": 0.2525882422924042,
      "learning_rate": 3.706431787535682e-05,
      "loss": 0.0146,
      "step": 4179
    },
    {
      "epoch": 1.0195121951219512,
      "grad_norm": 0.12801885604858398,
      "learning_rate": 3.70587243797729e-05,
      "loss": 0.0214,
      "step": 4180
    },
    {
      "epoch": 1.0197560975609756,
      "grad_norm": 0.054932452738285065,
      "learning_rate": 3.7053130097394127e-05,
      "loss": 0.0198,
      "step": 4181
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.47644299268722534,
      "learning_rate": 3.704753502858552e-05,
      "loss": 0.0307,
      "step": 4182
    },
    {
      "epoch": 1.0202439024390244,
      "grad_norm": 0.19107335805892944,
      "learning_rate": 3.7041939173712134e-05,
      "loss": 0.0262,
      "step": 4183
    },
    {
      "epoch": 1.0204878048780488,
      "grad_norm": 0.06846950203180313,
      "learning_rate": 3.7036342533139076e-05,
      "loss": 0.0275,
      "step": 4184
    },
    {
      "epoch": 1.0207317073170732,
      "grad_norm": 0.1901686042547226,
      "learning_rate": 3.703074510723151e-05,
      "loss": 0.02,
      "step": 4185
    },
    {
      "epoch": 1.0209756097560976,
      "grad_norm": 0.03844655305147171,
      "learning_rate": 3.702514689635467e-05,
      "loss": 0.0095,
      "step": 4186
    },
    {
      "epoch": 1.021219512195122,
      "grad_norm": 0.09494061022996902,
      "learning_rate": 3.701954790087379e-05,
      "loss": 0.0254,
      "step": 4187
    },
    {
      "epoch": 1.0214634146341464,
      "grad_norm": 0.10277930647134781,
      "learning_rate": 3.701394812115422e-05,
      "loss": 0.0316,
      "step": 4188
    },
    {
      "epoch": 1.0217073170731708,
      "grad_norm": 0.0847625881433487,
      "learning_rate": 3.7008347557561306e-05,
      "loss": 0.017,
      "step": 4189
    },
    {
      "epoch": 1.0219512195121951,
      "grad_norm": 0.09951600432395935,
      "learning_rate": 3.700274621046047e-05,
      "loss": 0.0121,
      "step": 4190
    },
    {
      "epoch": 1.0221951219512195,
      "grad_norm": 0.11624453216791153,
      "learning_rate": 3.69971440802172e-05,
      "loss": 0.0266,
      "step": 4191
    },
    {
      "epoch": 1.022439024390244,
      "grad_norm": 0.19650091230869293,
      "learning_rate": 3.6991541167197004e-05,
      "loss": 0.0216,
      "step": 4192
    },
    {
      "epoch": 1.0226829268292683,
      "grad_norm": 0.23632770776748657,
      "learning_rate": 3.6985937471765445e-05,
      "loss": 0.0367,
      "step": 4193
    },
    {
      "epoch": 1.0229268292682927,
      "grad_norm": 0.13521499931812286,
      "learning_rate": 3.6980332994288165e-05,
      "loss": 0.0208,
      "step": 4194
    },
    {
      "epoch": 1.023170731707317,
      "grad_norm": 0.16075153648853302,
      "learning_rate": 3.697472773513083e-05,
      "loss": 0.031,
      "step": 4195
    },
    {
      "epoch": 1.0234146341463415,
      "grad_norm": 0.21915221214294434,
      "learning_rate": 3.6969121694659176e-05,
      "loss": 0.0258,
      "step": 4196
    },
    {
      "epoch": 1.0236585365853659,
      "grad_norm": 0.21617059409618378,
      "learning_rate": 3.6963514873238965e-05,
      "loss": 0.0272,
      "step": 4197
    },
    {
      "epoch": 1.0239024390243903,
      "grad_norm": 0.20002242922782898,
      "learning_rate": 3.6957907271236036e-05,
      "loss": 0.04,
      "step": 4198
    },
    {
      "epoch": 1.0241463414634147,
      "grad_norm": 0.16897667944431305,
      "learning_rate": 3.695229888901626e-05,
      "loss": 0.034,
      "step": 4199
    },
    {
      "epoch": 1.024390243902439,
      "grad_norm": 0.19470247626304626,
      "learning_rate": 3.6946689726945576e-05,
      "loss": 0.0267,
      "step": 4200
    },
    {
      "epoch": 1.0246341463414634,
      "grad_norm": 0.3949776291847229,
      "learning_rate": 3.694107978538996e-05,
      "loss": 0.0191,
      "step": 4201
    },
    {
      "epoch": 1.0248780487804878,
      "grad_norm": 0.23605862259864807,
      "learning_rate": 3.693546906471544e-05,
      "loss": 0.0282,
      "step": 4202
    },
    {
      "epoch": 1.0251219512195122,
      "grad_norm": 0.2504749000072479,
      "learning_rate": 3.69298575652881e-05,
      "loss": 0.0319,
      "step": 4203
    },
    {
      "epoch": 1.0253658536585366,
      "grad_norm": 0.07481831312179565,
      "learning_rate": 3.692424528747408e-05,
      "loss": 0.0248,
      "step": 4204
    },
    {
      "epoch": 1.025609756097561,
      "grad_norm": 0.1862768828868866,
      "learning_rate": 3.6918632231639554e-05,
      "loss": 0.0268,
      "step": 4205
    },
    {
      "epoch": 1.0258536585365854,
      "grad_norm": 0.12133336067199707,
      "learning_rate": 3.6913018398150764e-05,
      "loss": 0.0229,
      "step": 4206
    },
    {
      "epoch": 1.0260975609756098,
      "grad_norm": 0.11453677713871002,
      "learning_rate": 3.6907403787374e-05,
      "loss": 0.0314,
      "step": 4207
    },
    {
      "epoch": 1.0263414634146342,
      "grad_norm": 0.11094877868890762,
      "learning_rate": 3.690178839967557e-05,
      "loss": 0.0146,
      "step": 4208
    },
    {
      "epoch": 1.0265853658536586,
      "grad_norm": 0.12470361590385437,
      "learning_rate": 3.68961722354219e-05,
      "loss": 0.0192,
      "step": 4209
    },
    {
      "epoch": 1.026829268292683,
      "grad_norm": 0.19911803305149078,
      "learning_rate": 3.68905552949794e-05,
      "loss": 0.0274,
      "step": 4210
    },
    {
      "epoch": 1.0270731707317073,
      "grad_norm": 0.13296660780906677,
      "learning_rate": 3.688493757871457e-05,
      "loss": 0.0283,
      "step": 4211
    },
    {
      "epoch": 1.0273170731707317,
      "grad_norm": 0.1486729383468628,
      "learning_rate": 3.6879319086993946e-05,
      "loss": 0.0352,
      "step": 4212
    },
    {
      "epoch": 1.0275609756097561,
      "grad_norm": 0.11455798894166946,
      "learning_rate": 3.687369982018413e-05,
      "loss": 0.0324,
      "step": 4213
    },
    {
      "epoch": 1.0278048780487805,
      "grad_norm": 0.15618930757045746,
      "learning_rate": 3.686807977865173e-05,
      "loss": 0.0128,
      "step": 4214
    },
    {
      "epoch": 1.028048780487805,
      "grad_norm": 0.17201615869998932,
      "learning_rate": 3.6862458962763465e-05,
      "loss": 0.0247,
      "step": 4215
    },
    {
      "epoch": 1.0282926829268293,
      "grad_norm": 0.07396356761455536,
      "learning_rate": 3.685683737288607e-05,
      "loss": 0.0116,
      "step": 4216
    },
    {
      "epoch": 1.0285365853658537,
      "grad_norm": 0.1500106155872345,
      "learning_rate": 3.685121500938633e-05,
      "loss": 0.0276,
      "step": 4217
    },
    {
      "epoch": 1.028780487804878,
      "grad_norm": 0.04832705482840538,
      "learning_rate": 3.684559187263109e-05,
      "loss": 0.0053,
      "step": 4218
    },
    {
      "epoch": 1.0290243902439025,
      "grad_norm": 0.18364357948303223,
      "learning_rate": 3.683996796298724e-05,
      "loss": 0.0178,
      "step": 4219
    },
    {
      "epoch": 1.0292682926829269,
      "grad_norm": 0.12086953967809677,
      "learning_rate": 3.683434328082174e-05,
      "loss": 0.019,
      "step": 4220
    },
    {
      "epoch": 1.0295121951219512,
      "grad_norm": 0.08678613603115082,
      "learning_rate": 3.6828717826501555e-05,
      "loss": 0.0122,
      "step": 4221
    },
    {
      "epoch": 1.0297560975609756,
      "grad_norm": 0.2179105132818222,
      "learning_rate": 3.682309160039375e-05,
      "loss": 0.0214,
      "step": 4222
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.1085740178823471,
      "learning_rate": 3.6817464602865406e-05,
      "loss": 0.0156,
      "step": 4223
    },
    {
      "epoch": 1.0302439024390244,
      "grad_norm": 0.23636280000209808,
      "learning_rate": 3.681183683428368e-05,
      "loss": 0.0126,
      "step": 4224
    },
    {
      "epoch": 1.0304878048780488,
      "grad_norm": 0.11774863302707672,
      "learning_rate": 3.680620829501576e-05,
      "loss": 0.025,
      "step": 4225
    },
    {
      "epoch": 1.0307317073170732,
      "grad_norm": 0.14969678223133087,
      "learning_rate": 3.680057898542889e-05,
      "loss": 0.0292,
      "step": 4226
    },
    {
      "epoch": 1.0309756097560976,
      "grad_norm": 0.14273442327976227,
      "learning_rate": 3.679494890589037e-05,
      "loss": 0.0214,
      "step": 4227
    },
    {
      "epoch": 1.031219512195122,
      "grad_norm": 0.05981328710913658,
      "learning_rate": 3.678931805676753e-05,
      "loss": 0.0188,
      "step": 4228
    },
    {
      "epoch": 1.0314634146341464,
      "grad_norm": 0.15451563894748688,
      "learning_rate": 3.678368643842778e-05,
      "loss": 0.0377,
      "step": 4229
    },
    {
      "epoch": 1.0317073170731708,
      "grad_norm": 0.13985559344291687,
      "learning_rate": 3.677805405123857e-05,
      "loss": 0.0154,
      "step": 4230
    },
    {
      "epoch": 1.0319512195121952,
      "grad_norm": 0.18160288035869598,
      "learning_rate": 3.677242089556738e-05,
      "loss": 0.0192,
      "step": 4231
    },
    {
      "epoch": 1.0321951219512195,
      "grad_norm": 0.13082534074783325,
      "learning_rate": 3.6766786971781774e-05,
      "loss": 0.015,
      "step": 4232
    },
    {
      "epoch": 1.032439024390244,
      "grad_norm": 0.10895149409770966,
      "learning_rate": 3.676115228024934e-05,
      "loss": 0.0289,
      "step": 4233
    },
    {
      "epoch": 1.0326829268292683,
      "grad_norm": 0.18511651456356049,
      "learning_rate": 3.675551682133772e-05,
      "loss": 0.0219,
      "step": 4234
    },
    {
      "epoch": 1.0329268292682927,
      "grad_norm": 0.07228504866361618,
      "learning_rate": 3.674988059541462e-05,
      "loss": 0.0188,
      "step": 4235
    },
    {
      "epoch": 1.033170731707317,
      "grad_norm": 0.11516909301280975,
      "learning_rate": 3.674424360284777e-05,
      "loss": 0.0294,
      "step": 4236
    },
    {
      "epoch": 1.0334146341463415,
      "grad_norm": 0.07732431590557098,
      "learning_rate": 3.673860584400499e-05,
      "loss": 0.0172,
      "step": 4237
    },
    {
      "epoch": 1.0336585365853659,
      "grad_norm": 0.42003756761550903,
      "learning_rate": 3.673296731925411e-05,
      "loss": 0.0235,
      "step": 4238
    },
    {
      "epoch": 1.0339024390243903,
      "grad_norm": 0.08561644703149796,
      "learning_rate": 3.6727328028963024e-05,
      "loss": 0.0098,
      "step": 4239
    },
    {
      "epoch": 1.0341463414634147,
      "grad_norm": 0.06866711378097534,
      "learning_rate": 3.67216879734997e-05,
      "loss": 0.0264,
      "step": 4240
    },
    {
      "epoch": 1.034390243902439,
      "grad_norm": 0.050588324666023254,
      "learning_rate": 3.67160471532321e-05,
      "loss": 0.0158,
      "step": 4241
    },
    {
      "epoch": 1.0346341463414634,
      "grad_norm": 0.14792141318321228,
      "learning_rate": 3.671040556852829e-05,
      "loss": 0.0156,
      "step": 4242
    },
    {
      "epoch": 1.0348780487804878,
      "grad_norm": 0.13693022727966309,
      "learning_rate": 3.670476321975638e-05,
      "loss": 0.0157,
      "step": 4243
    },
    {
      "epoch": 1.0351219512195122,
      "grad_norm": 0.05709105730056763,
      "learning_rate": 3.66991201072845e-05,
      "loss": 0.019,
      "step": 4244
    },
    {
      "epoch": 1.0353658536585366,
      "grad_norm": 0.07974988222122192,
      "learning_rate": 3.669347623148084e-05,
      "loss": 0.0189,
      "step": 4245
    },
    {
      "epoch": 1.035609756097561,
      "grad_norm": 0.16100716590881348,
      "learning_rate": 3.668783159271366e-05,
      "loss": 0.0309,
      "step": 4246
    },
    {
      "epoch": 1.0358536585365854,
      "grad_norm": 0.10133814811706543,
      "learning_rate": 3.668218619135124e-05,
      "loss": 0.0179,
      "step": 4247
    },
    {
      "epoch": 1.0360975609756098,
      "grad_norm": 0.09662101417779922,
      "learning_rate": 3.667654002776194e-05,
      "loss": 0.0187,
      "step": 4248
    },
    {
      "epoch": 1.0363414634146342,
      "grad_norm": 0.11749076843261719,
      "learning_rate": 3.667089310231415e-05,
      "loss": 0.0137,
      "step": 4249
    },
    {
      "epoch": 1.0365853658536586,
      "grad_norm": 0.08710625022649765,
      "learning_rate": 3.66652454153763e-05,
      "loss": 0.027,
      "step": 4250
    },
    {
      "epoch": 1.036829268292683,
      "grad_norm": 0.11740504205226898,
      "learning_rate": 3.665959696731692e-05,
      "loss": 0.0147,
      "step": 4251
    },
    {
      "epoch": 1.0370731707317074,
      "grad_norm": 0.2241937816143036,
      "learning_rate": 3.665394775850451e-05,
      "loss": 0.027,
      "step": 4252
    },
    {
      "epoch": 1.0373170731707317,
      "grad_norm": 0.11289282888174057,
      "learning_rate": 3.66482977893077e-05,
      "loss": 0.024,
      "step": 4253
    },
    {
      "epoch": 1.0375609756097561,
      "grad_norm": 0.08244673162698746,
      "learning_rate": 3.6642647060095106e-05,
      "loss": 0.0248,
      "step": 4254
    },
    {
      "epoch": 1.0378048780487805,
      "grad_norm": 0.11632174253463745,
      "learning_rate": 3.663699557123544e-05,
      "loss": 0.035,
      "step": 4255
    },
    {
      "epoch": 1.038048780487805,
      "grad_norm": 0.10081604868173599,
      "learning_rate": 3.663134332309743e-05,
      "loss": 0.0184,
      "step": 4256
    },
    {
      "epoch": 1.0382926829268293,
      "grad_norm": 0.08678502589464188,
      "learning_rate": 3.662569031604989e-05,
      "loss": 0.0185,
      "step": 4257
    },
    {
      "epoch": 1.0385365853658537,
      "grad_norm": 0.10029901564121246,
      "learning_rate": 3.662003655046164e-05,
      "loss": 0.015,
      "step": 4258
    },
    {
      "epoch": 1.038780487804878,
      "grad_norm": 0.12626585364341736,
      "learning_rate": 3.6614382026701565e-05,
      "loss": 0.028,
      "step": 4259
    },
    {
      "epoch": 1.0390243902439025,
      "grad_norm": 0.06372280418872833,
      "learning_rate": 3.6608726745138636e-05,
      "loss": 0.014,
      "step": 4260
    },
    {
      "epoch": 1.0392682926829269,
      "grad_norm": 0.06649292260408401,
      "learning_rate": 3.660307070614181e-05,
      "loss": 0.0156,
      "step": 4261
    },
    {
      "epoch": 1.0395121951219513,
      "grad_norm": 0.0957043245434761,
      "learning_rate": 3.659741391008015e-05,
      "loss": 0.0198,
      "step": 4262
    },
    {
      "epoch": 1.0397560975609756,
      "grad_norm": 0.05804530158638954,
      "learning_rate": 3.6591756357322737e-05,
      "loss": 0.0195,
      "step": 4263
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.14865277707576752,
      "learning_rate": 3.6586098048238696e-05,
      "loss": 0.0188,
      "step": 4264
    },
    {
      "epoch": 1.0402439024390244,
      "grad_norm": 0.19994591176509857,
      "learning_rate": 3.658043898319724e-05,
      "loss": 0.0124,
      "step": 4265
    },
    {
      "epoch": 1.0404878048780488,
      "grad_norm": 0.0615977942943573,
      "learning_rate": 3.657477916256758e-05,
      "loss": 0.0162,
      "step": 4266
    },
    {
      "epoch": 1.0407317073170732,
      "grad_norm": 0.11124362796545029,
      "learning_rate": 3.656911858671902e-05,
      "loss": 0.035,
      "step": 4267
    },
    {
      "epoch": 1.0409756097560976,
      "grad_norm": 0.1170390397310257,
      "learning_rate": 3.656345725602089e-05,
      "loss": 0.0184,
      "step": 4268
    },
    {
      "epoch": 1.041219512195122,
      "grad_norm": 0.20235125720500946,
      "learning_rate": 3.6557795170842565e-05,
      "loss": 0.0246,
      "step": 4269
    },
    {
      "epoch": 1.0414634146341464,
      "grad_norm": 0.1423386037349701,
      "learning_rate": 3.655213233155349e-05,
      "loss": 0.0212,
      "step": 4270
    },
    {
      "epoch": 1.0417073170731708,
      "grad_norm": 0.08906590938568115,
      "learning_rate": 3.654646873852314e-05,
      "loss": 0.0214,
      "step": 4271
    },
    {
      "epoch": 1.0419512195121952,
      "grad_norm": 0.05626426264643669,
      "learning_rate": 3.654080439212107e-05,
      "loss": 0.015,
      "step": 4272
    },
    {
      "epoch": 1.0421951219512195,
      "grad_norm": 0.09080583602190018,
      "learning_rate": 3.653513929271683e-05,
      "loss": 0.0259,
      "step": 4273
    },
    {
      "epoch": 1.042439024390244,
      "grad_norm": 0.06645162403583527,
      "learning_rate": 3.652947344068006e-05,
      "loss": 0.0102,
      "step": 4274
    },
    {
      "epoch": 1.0426829268292683,
      "grad_norm": 0.08120054751634598,
      "learning_rate": 3.6523806836380445e-05,
      "loss": 0.0231,
      "step": 4275
    },
    {
      "epoch": 1.0429268292682927,
      "grad_norm": 0.1315007209777832,
      "learning_rate": 3.6518139480187727e-05,
      "loss": 0.0258,
      "step": 4276
    },
    {
      "epoch": 1.0431707317073171,
      "grad_norm": 0.08644025027751923,
      "learning_rate": 3.651247137247165e-05,
      "loss": 0.0146,
      "step": 4277
    },
    {
      "epoch": 1.0434146341463415,
      "grad_norm": 0.12567155063152313,
      "learning_rate": 3.650680251360206e-05,
      "loss": 0.0332,
      "step": 4278
    },
    {
      "epoch": 1.043658536585366,
      "grad_norm": 0.061072882264852524,
      "learning_rate": 3.650113290394884e-05,
      "loss": 0.012,
      "step": 4279
    },
    {
      "epoch": 1.0439024390243903,
      "grad_norm": 0.1657445728778839,
      "learning_rate": 3.649546254388191e-05,
      "loss": 0.0136,
      "step": 4280
    },
    {
      "epoch": 1.0441463414634147,
      "grad_norm": 0.11920363456010818,
      "learning_rate": 3.648979143377123e-05,
      "loss": 0.0177,
      "step": 4281
    },
    {
      "epoch": 1.044390243902439,
      "grad_norm": 0.14602087438106537,
      "learning_rate": 3.648411957398684e-05,
      "loss": 0.0279,
      "step": 4282
    },
    {
      "epoch": 1.0446341463414635,
      "grad_norm": 0.0771484225988388,
      "learning_rate": 3.6478446964898794e-05,
      "loss": 0.0152,
      "step": 4283
    },
    {
      "epoch": 1.0448780487804878,
      "grad_norm": 0.12671822309494019,
      "learning_rate": 3.6472773606877226e-05,
      "loss": 0.0219,
      "step": 4284
    },
    {
      "epoch": 1.0451219512195122,
      "grad_norm": 0.11200011521577835,
      "learning_rate": 3.6467099500292305e-05,
      "loss": 0.0242,
      "step": 4285
    },
    {
      "epoch": 1.0453658536585366,
      "grad_norm": 0.07034388184547424,
      "learning_rate": 3.646142464551425e-05,
      "loss": 0.0146,
      "step": 4286
    },
    {
      "epoch": 1.045609756097561,
      "grad_norm": 0.06205155700445175,
      "learning_rate": 3.645574904291331e-05,
      "loss": 0.0204,
      "step": 4287
    },
    {
      "epoch": 1.0458536585365854,
      "grad_norm": 0.13515950739383698,
      "learning_rate": 3.645007269285983e-05,
      "loss": 0.0224,
      "step": 4288
    },
    {
      "epoch": 1.0460975609756098,
      "grad_norm": 0.11533035337924957,
      "learning_rate": 3.6444395595724146e-05,
      "loss": 0.0162,
      "step": 4289
    },
    {
      "epoch": 1.0463414634146342,
      "grad_norm": 0.09800779819488525,
      "learning_rate": 3.64387177518767e-05,
      "loss": 0.0232,
      "step": 4290
    },
    {
      "epoch": 1.0465853658536586,
      "grad_norm": 0.10378654301166534,
      "learning_rate": 3.643303916168792e-05,
      "loss": 0.0166,
      "step": 4291
    },
    {
      "epoch": 1.046829268292683,
      "grad_norm": 0.08190073072910309,
      "learning_rate": 3.642735982552834e-05,
      "loss": 0.0207,
      "step": 4292
    },
    {
      "epoch": 1.0470731707317074,
      "grad_norm": 0.1330776959657669,
      "learning_rate": 3.6421679743768525e-05,
      "loss": 0.0102,
      "step": 4293
    },
    {
      "epoch": 1.0473170731707317,
      "grad_norm": 0.1366756558418274,
      "learning_rate": 3.641599891677906e-05,
      "loss": 0.0333,
      "step": 4294
    },
    {
      "epoch": 1.0475609756097561,
      "grad_norm": 0.09119752049446106,
      "learning_rate": 3.641031734493061e-05,
      "loss": 0.021,
      "step": 4295
    },
    {
      "epoch": 1.0478048780487805,
      "grad_norm": 0.05530243366956711,
      "learning_rate": 3.640463502859389e-05,
      "loss": 0.0204,
      "step": 4296
    },
    {
      "epoch": 1.048048780487805,
      "grad_norm": 0.2875942885875702,
      "learning_rate": 3.639895196813965e-05,
      "loss": 0.0304,
      "step": 4297
    },
    {
      "epoch": 1.0482926829268293,
      "grad_norm": 0.07826197147369385,
      "learning_rate": 3.639326816393869e-05,
      "loss": 0.0277,
      "step": 4298
    },
    {
      "epoch": 1.0485365853658537,
      "grad_norm": 0.07674973458051682,
      "learning_rate": 3.638758361636185e-05,
      "loss": 0.0242,
      "step": 4299
    },
    {
      "epoch": 1.048780487804878,
      "grad_norm": 0.13260416686534882,
      "learning_rate": 3.638189832578005e-05,
      "loss": 0.0302,
      "step": 4300
    },
    {
      "epoch": 1.0490243902439025,
      "grad_norm": 0.14088204503059387,
      "learning_rate": 3.637621229256423e-05,
      "loss": 0.0232,
      "step": 4301
    },
    {
      "epoch": 1.0492682926829269,
      "grad_norm": 0.07577205449342728,
      "learning_rate": 3.637052551708537e-05,
      "loss": 0.0211,
      "step": 4302
    },
    {
      "epoch": 1.0495121951219513,
      "grad_norm": 0.14603117108345032,
      "learning_rate": 3.636483799971454e-05,
      "loss": 0.0314,
      "step": 4303
    },
    {
      "epoch": 1.0497560975609757,
      "grad_norm": 0.22434109449386597,
      "learning_rate": 3.635914974082282e-05,
      "loss": 0.011,
      "step": 4304
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.06814858317375183,
      "learning_rate": 3.6353460740781345e-05,
      "loss": 0.0191,
      "step": 4305
    },
    {
      "epoch": 1.0502439024390244,
      "grad_norm": 0.08787251263856888,
      "learning_rate": 3.6347770999961326e-05,
      "loss": 0.0155,
      "step": 4306
    },
    {
      "epoch": 1.0504878048780488,
      "grad_norm": 0.07848913967609406,
      "learning_rate": 3.6342080518733986e-05,
      "loss": 0.0209,
      "step": 4307
    },
    {
      "epoch": 1.0507317073170732,
      "grad_norm": 0.11780431121587753,
      "learning_rate": 3.633638929747061e-05,
      "loss": 0.0226,
      "step": 4308
    },
    {
      "epoch": 1.0509756097560976,
      "grad_norm": 0.11817826330661774,
      "learning_rate": 3.633069733654253e-05,
      "loss": 0.0204,
      "step": 4309
    },
    {
      "epoch": 1.051219512195122,
      "grad_norm": 0.1330229789018631,
      "learning_rate": 3.632500463632115e-05,
      "loss": 0.015,
      "step": 4310
    },
    {
      "epoch": 1.0514634146341464,
      "grad_norm": 0.08553119748830795,
      "learning_rate": 3.631931119717787e-05,
      "loss": 0.0231,
      "step": 4311
    },
    {
      "epoch": 1.0517073170731708,
      "grad_norm": 0.11072975397109985,
      "learning_rate": 3.6313617019484194e-05,
      "loss": 0.0267,
      "step": 4312
    },
    {
      "epoch": 1.0519512195121952,
      "grad_norm": 0.146817147731781,
      "learning_rate": 3.6307922103611644e-05,
      "loss": 0.0188,
      "step": 4313
    },
    {
      "epoch": 1.0521951219512196,
      "grad_norm": 0.06965580582618713,
      "learning_rate": 3.630222644993179e-05,
      "loss": 0.0202,
      "step": 4314
    },
    {
      "epoch": 1.052439024390244,
      "grad_norm": 0.10639248043298721,
      "learning_rate": 3.629653005881626e-05,
      "loss": 0.0194,
      "step": 4315
    },
    {
      "epoch": 1.0526829268292683,
      "grad_norm": 0.1867646425962448,
      "learning_rate": 3.629083293063673e-05,
      "loss": 0.0263,
      "step": 4316
    },
    {
      "epoch": 1.0529268292682927,
      "grad_norm": 0.17580372095108032,
      "learning_rate": 3.628513506576491e-05,
      "loss": 0.0267,
      "step": 4317
    },
    {
      "epoch": 1.0531707317073171,
      "grad_norm": 0.0666513592004776,
      "learning_rate": 3.6279436464572576e-05,
      "loss": 0.012,
      "step": 4318
    },
    {
      "epoch": 1.0534146341463415,
      "grad_norm": 0.07734107971191406,
      "learning_rate": 3.6273737127431546e-05,
      "loss": 0.0135,
      "step": 4319
    },
    {
      "epoch": 1.053658536585366,
      "grad_norm": 0.049784280359745026,
      "learning_rate": 3.626803705471368e-05,
      "loss": 0.0119,
      "step": 4320
    },
    {
      "epoch": 1.0539024390243903,
      "grad_norm": 0.22850477695465088,
      "learning_rate": 3.626233624679088e-05,
      "loss": 0.023,
      "step": 4321
    },
    {
      "epoch": 1.0541463414634147,
      "grad_norm": 0.10028999298810959,
      "learning_rate": 3.625663470403513e-05,
      "loss": 0.0149,
      "step": 4322
    },
    {
      "epoch": 1.054390243902439,
      "grad_norm": 0.19993022084236145,
      "learning_rate": 3.625093242681842e-05,
      "loss": 0.0292,
      "step": 4323
    },
    {
      "epoch": 1.0546341463414635,
      "grad_norm": 0.09809961915016174,
      "learning_rate": 3.6245229415512814e-05,
      "loss": 0.0284,
      "step": 4324
    },
    {
      "epoch": 1.0548780487804879,
      "grad_norm": 0.14829571545124054,
      "learning_rate": 3.623952567049041e-05,
      "loss": 0.0219,
      "step": 4325
    },
    {
      "epoch": 1.0551219512195122,
      "grad_norm": 0.0892903134226799,
      "learning_rate": 3.6233821192123365e-05,
      "loss": 0.0195,
      "step": 4326
    },
    {
      "epoch": 1.0553658536585366,
      "grad_norm": 0.06874842941761017,
      "learning_rate": 3.622811598078388e-05,
      "loss": 0.0187,
      "step": 4327
    },
    {
      "epoch": 1.055609756097561,
      "grad_norm": 0.11003458499908447,
      "learning_rate": 3.622241003684419e-05,
      "loss": 0.0247,
      "step": 4328
    },
    {
      "epoch": 1.0558536585365854,
      "grad_norm": 0.191883847117424,
      "learning_rate": 3.6216703360676614e-05,
      "loss": 0.0292,
      "step": 4329
    },
    {
      "epoch": 1.0560975609756098,
      "grad_norm": 0.1078413724899292,
      "learning_rate": 3.621099595265347e-05,
      "loss": 0.0236,
      "step": 4330
    },
    {
      "epoch": 1.0563414634146342,
      "grad_norm": 0.09083922207355499,
      "learning_rate": 3.620528781314717e-05,
      "loss": 0.0152,
      "step": 4331
    },
    {
      "epoch": 1.0565853658536586,
      "grad_norm": 0.2301017940044403,
      "learning_rate": 3.6199578942530143e-05,
      "loss": 0.018,
      "step": 4332
    },
    {
      "epoch": 1.056829268292683,
      "grad_norm": 0.06092119961977005,
      "learning_rate": 3.619386934117487e-05,
      "loss": 0.0132,
      "step": 4333
    },
    {
      "epoch": 1.0570731707317074,
      "grad_norm": 0.16890500485897064,
      "learning_rate": 3.6188159009453885e-05,
      "loss": 0.0322,
      "step": 4334
    },
    {
      "epoch": 1.0573170731707318,
      "grad_norm": 0.10957059264183044,
      "learning_rate": 3.618244794773978e-05,
      "loss": 0.025,
      "step": 4335
    },
    {
      "epoch": 1.0575609756097561,
      "grad_norm": 0.18587902188301086,
      "learning_rate": 3.617673615640519e-05,
      "loss": 0.0174,
      "step": 4336
    },
    {
      "epoch": 1.0578048780487805,
      "grad_norm": 0.2864915132522583,
      "learning_rate": 3.617102363582277e-05,
      "loss": 0.0164,
      "step": 4337
    },
    {
      "epoch": 1.058048780487805,
      "grad_norm": 0.10875385254621506,
      "learning_rate": 3.616531038636525e-05,
      "loss": 0.015,
      "step": 4338
    },
    {
      "epoch": 1.0582926829268293,
      "grad_norm": 0.11586061865091324,
      "learning_rate": 3.6159596408405414e-05,
      "loss": 0.0297,
      "step": 4339
    },
    {
      "epoch": 1.0585365853658537,
      "grad_norm": 0.09371008723974228,
      "learning_rate": 3.615388170231608e-05,
      "loss": 0.0081,
      "step": 4340
    },
    {
      "epoch": 1.058780487804878,
      "grad_norm": 0.18374764919281006,
      "learning_rate": 3.614816626847011e-05,
      "loss": 0.024,
      "step": 4341
    },
    {
      "epoch": 1.0590243902439025,
      "grad_norm": 0.12127040326595306,
      "learning_rate": 3.614245010724041e-05,
      "loss": 0.0207,
      "step": 4342
    },
    {
      "epoch": 1.0592682926829269,
      "grad_norm": 0.08069221675395966,
      "learning_rate": 3.6136733218999955e-05,
      "loss": 0.0192,
      "step": 4343
    },
    {
      "epoch": 1.0595121951219513,
      "grad_norm": 0.10963801294565201,
      "learning_rate": 3.613101560412175e-05,
      "loss": 0.0191,
      "step": 4344
    },
    {
      "epoch": 1.0597560975609757,
      "grad_norm": 0.1486537754535675,
      "learning_rate": 3.612529726297885e-05,
      "loss": 0.0252,
      "step": 4345
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.12463249266147614,
      "learning_rate": 3.611957819594436e-05,
      "loss": 0.0265,
      "step": 4346
    },
    {
      "epoch": 1.0602439024390244,
      "grad_norm": 0.08162553608417511,
      "learning_rate": 3.6113858403391443e-05,
      "loss": 0.0186,
      "step": 4347
    },
    {
      "epoch": 1.0604878048780488,
      "grad_norm": 0.11308959871530533,
      "learning_rate": 3.610813788569328e-05,
      "loss": 0.0142,
      "step": 4348
    },
    {
      "epoch": 1.0607317073170732,
      "grad_norm": 0.23608042299747467,
      "learning_rate": 3.610241664322313e-05,
      "loss": 0.0301,
      "step": 4349
    },
    {
      "epoch": 1.0609756097560976,
      "grad_norm": 0.12763375043869019,
      "learning_rate": 3.609669467635428e-05,
      "loss": 0.0154,
      "step": 4350
    },
    {
      "epoch": 1.061219512195122,
      "grad_norm": 0.07054681330919266,
      "learning_rate": 3.6090971985460063e-05,
      "loss": 0.0267,
      "step": 4351
    },
    {
      "epoch": 1.0614634146341464,
      "grad_norm": 0.11375368386507034,
      "learning_rate": 3.608524857091389e-05,
      "loss": 0.0224,
      "step": 4352
    },
    {
      "epoch": 1.0617073170731708,
      "grad_norm": 0.18841049075126648,
      "learning_rate": 3.607952443308918e-05,
      "loss": 0.0296,
      "step": 4353
    },
    {
      "epoch": 1.0619512195121952,
      "grad_norm": 0.10002158582210541,
      "learning_rate": 3.607379957235941e-05,
      "loss": 0.0239,
      "step": 4354
    },
    {
      "epoch": 1.0621951219512196,
      "grad_norm": 0.06396886706352234,
      "learning_rate": 3.606807398909813e-05,
      "loss": 0.0127,
      "step": 4355
    },
    {
      "epoch": 1.062439024390244,
      "grad_norm": 0.10715299844741821,
      "learning_rate": 3.60623476836789e-05,
      "loss": 0.0434,
      "step": 4356
    },
    {
      "epoch": 1.0626829268292683,
      "grad_norm": 0.20323748886585236,
      "learning_rate": 3.6056620656475346e-05,
      "loss": 0.0384,
      "step": 4357
    },
    {
      "epoch": 1.0629268292682927,
      "grad_norm": 0.08994907885789871,
      "learning_rate": 3.605089290786114e-05,
      "loss": 0.0245,
      "step": 4358
    },
    {
      "epoch": 1.0631707317073171,
      "grad_norm": 0.08712933957576752,
      "learning_rate": 3.604516443821001e-05,
      "loss": 0.0373,
      "step": 4359
    },
    {
      "epoch": 1.0634146341463415,
      "grad_norm": 0.1634591817855835,
      "learning_rate": 3.60394352478957e-05,
      "loss": 0.0174,
      "step": 4360
    },
    {
      "epoch": 1.063658536585366,
      "grad_norm": 0.08930421620607376,
      "learning_rate": 3.603370533729205e-05,
      "loss": 0.0163,
      "step": 4361
    },
    {
      "epoch": 1.0639024390243903,
      "grad_norm": 0.08840477466583252,
      "learning_rate": 3.60279747067729e-05,
      "loss": 0.0335,
      "step": 4362
    },
    {
      "epoch": 1.0641463414634147,
      "grad_norm": 0.1305270940065384,
      "learning_rate": 3.602224335671216e-05,
      "loss": 0.0309,
      "step": 4363
    },
    {
      "epoch": 1.064390243902439,
      "grad_norm": 0.09117773175239563,
      "learning_rate": 3.601651128748378e-05,
      "loss": 0.0166,
      "step": 4364
    },
    {
      "epoch": 1.0646341463414635,
      "grad_norm": 0.06952612847089767,
      "learning_rate": 3.601077849946177e-05,
      "loss": 0.0084,
      "step": 4365
    },
    {
      "epoch": 1.0648780487804879,
      "grad_norm": 0.08068518340587616,
      "learning_rate": 3.600504499302016e-05,
      "loss": 0.0169,
      "step": 4366
    },
    {
      "epoch": 1.0651219512195123,
      "grad_norm": 0.1187211126089096,
      "learning_rate": 3.599931076853306e-05,
      "loss": 0.0198,
      "step": 4367
    },
    {
      "epoch": 1.0653658536585366,
      "grad_norm": 0.10599347203969955,
      "learning_rate": 3.59935758263746e-05,
      "loss": 0.0319,
      "step": 4368
    },
    {
      "epoch": 1.065609756097561,
      "grad_norm": 0.10773665457963943,
      "learning_rate": 3.598784016691897e-05,
      "loss": 0.0329,
      "step": 4369
    },
    {
      "epoch": 1.0658536585365854,
      "grad_norm": 0.09051130712032318,
      "learning_rate": 3.5982103790540414e-05,
      "loss": 0.0218,
      "step": 4370
    },
    {
      "epoch": 1.0660975609756098,
      "grad_norm": 0.10797100514173508,
      "learning_rate": 3.59763666976132e-05,
      "loss": 0.0153,
      "step": 4371
    },
    {
      "epoch": 1.0663414634146342,
      "grad_norm": 0.10319548845291138,
      "learning_rate": 3.597062888851167e-05,
      "loss": 0.0189,
      "step": 4372
    },
    {
      "epoch": 1.0665853658536586,
      "grad_norm": 0.06881912797689438,
      "learning_rate": 3.5964890363610175e-05,
      "loss": 0.0156,
      "step": 4373
    },
    {
      "epoch": 1.066829268292683,
      "grad_norm": 0.06599359214305878,
      "learning_rate": 3.5959151123283164e-05,
      "loss": 0.0264,
      "step": 4374
    },
    {
      "epoch": 1.0670731707317074,
      "grad_norm": 0.08844994753599167,
      "learning_rate": 3.595341116790508e-05,
      "loss": 0.0132,
      "step": 4375
    },
    {
      "epoch": 1.0673170731707318,
      "grad_norm": 0.0997072160243988,
      "learning_rate": 3.594767049785046e-05,
      "loss": 0.0224,
      "step": 4376
    },
    {
      "epoch": 1.0675609756097562,
      "grad_norm": 0.07660475373268127,
      "learning_rate": 3.5941929113493845e-05,
      "loss": 0.0192,
      "step": 4377
    },
    {
      "epoch": 1.0678048780487805,
      "grad_norm": 0.038575585931539536,
      "learning_rate": 3.593618701520985e-05,
      "loss": 0.0073,
      "step": 4378
    },
    {
      "epoch": 1.068048780487805,
      "grad_norm": 0.24451944231987,
      "learning_rate": 3.5930444203373144e-05,
      "loss": 0.0208,
      "step": 4379
    },
    {
      "epoch": 1.0682926829268293,
      "grad_norm": 0.17437632381916046,
      "learning_rate": 3.592470067835841e-05,
      "loss": 0.0232,
      "step": 4380
    },
    {
      "epoch": 1.0685365853658537,
      "grad_norm": 0.06888426840305328,
      "learning_rate": 3.591895644054039e-05,
      "loss": 0.0112,
      "step": 4381
    },
    {
      "epoch": 1.068780487804878,
      "grad_norm": 0.18357183039188385,
      "learning_rate": 3.5913211490293905e-05,
      "loss": 0.0195,
      "step": 4382
    },
    {
      "epoch": 1.0690243902439025,
      "grad_norm": 0.20836442708969116,
      "learning_rate": 3.5907465827993766e-05,
      "loss": 0.021,
      "step": 4383
    },
    {
      "epoch": 1.069268292682927,
      "grad_norm": 0.21347501873970032,
      "learning_rate": 3.590171945401488e-05,
      "loss": 0.0291,
      "step": 4384
    },
    {
      "epoch": 1.0695121951219513,
      "grad_norm": 0.10263058543205261,
      "learning_rate": 3.5895972368732165e-05,
      "loss": 0.0223,
      "step": 4385
    },
    {
      "epoch": 1.0697560975609757,
      "grad_norm": 0.1198619157075882,
      "learning_rate": 3.589022457252062e-05,
      "loss": 0.0237,
      "step": 4386
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.06721650063991547,
      "learning_rate": 3.588447606575525e-05,
      "loss": 0.0157,
      "step": 4387
    },
    {
      "epoch": 1.0702439024390245,
      "grad_norm": 0.16158735752105713,
      "learning_rate": 3.587872684881113e-05,
      "loss": 0.0225,
      "step": 4388
    },
    {
      "epoch": 1.0704878048780488,
      "grad_norm": 0.07541434466838837,
      "learning_rate": 3.5872976922063405e-05,
      "loss": 0.0173,
      "step": 4389
    },
    {
      "epoch": 1.0707317073170732,
      "grad_norm": 0.10605881363153458,
      "learning_rate": 3.5867226285887205e-05,
      "loss": 0.0239,
      "step": 4390
    },
    {
      "epoch": 1.0709756097560976,
      "grad_norm": 0.21853908896446228,
      "learning_rate": 3.5861474940657767e-05,
      "loss": 0.0313,
      "step": 4391
    },
    {
      "epoch": 1.071219512195122,
      "grad_norm": 0.195272758603096,
      "learning_rate": 3.5855722886750346e-05,
      "loss": 0.0252,
      "step": 4392
    },
    {
      "epoch": 1.0714634146341464,
      "grad_norm": 0.08362891525030136,
      "learning_rate": 3.5849970124540225e-05,
      "loss": 0.0223,
      "step": 4393
    },
    {
      "epoch": 1.0717073170731708,
      "grad_norm": 0.158175989985466,
      "learning_rate": 3.5844216654402776e-05,
      "loss": 0.0248,
      "step": 4394
    },
    {
      "epoch": 1.0719512195121952,
      "grad_norm": 0.16254298388957977,
      "learning_rate": 3.5838462476713384e-05,
      "loss": 0.0179,
      "step": 4395
    },
    {
      "epoch": 1.0721951219512196,
      "grad_norm": 0.10621717572212219,
      "learning_rate": 3.583270759184749e-05,
      "loss": 0.0183,
      "step": 4396
    },
    {
      "epoch": 1.072439024390244,
      "grad_norm": 0.1459505558013916,
      "learning_rate": 3.5826952000180604e-05,
      "loss": 0.0387,
      "step": 4397
    },
    {
      "epoch": 1.0726829268292684,
      "grad_norm": 0.12177781760692596,
      "learning_rate": 3.582119570208824e-05,
      "loss": 0.016,
      "step": 4398
    },
    {
      "epoch": 1.0729268292682927,
      "grad_norm": 0.10728401690721512,
      "learning_rate": 3.5815438697945973e-05,
      "loss": 0.0288,
      "step": 4399
    },
    {
      "epoch": 1.0731707317073171,
      "grad_norm": 0.16288632154464722,
      "learning_rate": 3.580968098812945e-05,
      "loss": 0.0315,
      "step": 4400
    },
    {
      "epoch": 1.0734146341463415,
      "grad_norm": 0.04469728842377663,
      "learning_rate": 3.580392257301432e-05,
      "loss": 0.0092,
      "step": 4401
    },
    {
      "epoch": 1.073658536585366,
      "grad_norm": 0.26466232538223267,
      "learning_rate": 3.5798163452976325e-05,
      "loss": 0.0321,
      "step": 4402
    },
    {
      "epoch": 1.0739024390243903,
      "grad_norm": 0.07671226561069489,
      "learning_rate": 3.579240362839123e-05,
      "loss": 0.0218,
      "step": 4403
    },
    {
      "epoch": 1.0741463414634147,
      "grad_norm": 0.14708848297595978,
      "learning_rate": 3.578664309963482e-05,
      "loss": 0.012,
      "step": 4404
    },
    {
      "epoch": 1.074390243902439,
      "grad_norm": 0.1232571080327034,
      "learning_rate": 3.5780881867082986e-05,
      "loss": 0.0145,
      "step": 4405
    },
    {
      "epoch": 1.0746341463414635,
      "grad_norm": 0.12834709882736206,
      "learning_rate": 3.57751199311116e-05,
      "loss": 0.017,
      "step": 4406
    },
    {
      "epoch": 1.0748780487804879,
      "grad_norm": 0.15901392698287964,
      "learning_rate": 3.5769357292096634e-05,
      "loss": 0.0136,
      "step": 4407
    },
    {
      "epoch": 1.0751219512195123,
      "grad_norm": 0.04384688287973404,
      "learning_rate": 3.576359395041406e-05,
      "loss": 0.0038,
      "step": 4408
    },
    {
      "epoch": 1.0753658536585367,
      "grad_norm": 0.09371583163738251,
      "learning_rate": 3.5757829906439946e-05,
      "loss": 0.0077,
      "step": 4409
    },
    {
      "epoch": 1.075609756097561,
      "grad_norm": 0.07466234266757965,
      "learning_rate": 3.575206516055035e-05,
      "loss": 0.0145,
      "step": 4410
    },
    {
      "epoch": 1.0758536585365854,
      "grad_norm": 0.10675415396690369,
      "learning_rate": 3.5746299713121434e-05,
      "loss": 0.0239,
      "step": 4411
    },
    {
      "epoch": 1.0760975609756098,
      "grad_norm": 0.1053522378206253,
      "learning_rate": 3.5740533564529344e-05,
      "loss": 0.0173,
      "step": 4412
    },
    {
      "epoch": 1.0763414634146342,
      "grad_norm": 0.1351098269224167,
      "learning_rate": 3.573476671515033e-05,
      "loss": 0.0211,
      "step": 4413
    },
    {
      "epoch": 1.0765853658536586,
      "grad_norm": 0.3442160487174988,
      "learning_rate": 3.5728999165360645e-05,
      "loss": 0.0301,
      "step": 4414
    },
    {
      "epoch": 1.076829268292683,
      "grad_norm": 0.18030978739261627,
      "learning_rate": 3.572323091553661e-05,
      "loss": 0.0232,
      "step": 4415
    },
    {
      "epoch": 1.0770731707317074,
      "grad_norm": 0.10299219936132431,
      "learning_rate": 3.571746196605458e-05,
      "loss": 0.0211,
      "step": 4416
    },
    {
      "epoch": 1.0773170731707318,
      "grad_norm": 0.09314247220754623,
      "learning_rate": 3.571169231729097e-05,
      "loss": 0.026,
      "step": 4417
    },
    {
      "epoch": 1.0775609756097562,
      "grad_norm": 0.08015601336956024,
      "learning_rate": 3.570592196962223e-05,
      "loss": 0.0143,
      "step": 4418
    },
    {
      "epoch": 1.0778048780487806,
      "grad_norm": 0.2502277195453644,
      "learning_rate": 3.570015092342485e-05,
      "loss": 0.0228,
      "step": 4419
    },
    {
      "epoch": 1.078048780487805,
      "grad_norm": 0.16558417677879333,
      "learning_rate": 3.569437917907538e-05,
      "loss": 0.0147,
      "step": 4420
    },
    {
      "epoch": 1.0782926829268293,
      "grad_norm": 0.16062647104263306,
      "learning_rate": 3.568860673695041e-05,
      "loss": 0.0177,
      "step": 4421
    },
    {
      "epoch": 1.0785365853658537,
      "grad_norm": 0.10144913196563721,
      "learning_rate": 3.5682833597426575e-05,
      "loss": 0.0286,
      "step": 4422
    },
    {
      "epoch": 1.0787804878048781,
      "grad_norm": 0.1183614730834961,
      "learning_rate": 3.567705976088054e-05,
      "loss": 0.0164,
      "step": 4423
    },
    {
      "epoch": 1.0790243902439025,
      "grad_norm": 0.11838704347610474,
      "learning_rate": 3.5671285227689054e-05,
      "loss": 0.028,
      "step": 4424
    },
    {
      "epoch": 1.079268292682927,
      "grad_norm": 0.0928196907043457,
      "learning_rate": 3.566550999822886e-05,
      "loss": 0.0152,
      "step": 4425
    },
    {
      "epoch": 1.0795121951219513,
      "grad_norm": 0.367958128452301,
      "learning_rate": 3.56597340728768e-05,
      "loss": 0.0362,
      "step": 4426
    },
    {
      "epoch": 1.0797560975609757,
      "grad_norm": 0.10660731047391891,
      "learning_rate": 3.5653957452009724e-05,
      "loss": 0.0194,
      "step": 4427
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.1287926882505417,
      "learning_rate": 3.564818013600454e-05,
      "loss": 0.03,
      "step": 4428
    },
    {
      "epoch": 1.0802439024390245,
      "grad_norm": 0.1136196181178093,
      "learning_rate": 3.564240212523819e-05,
      "loss": 0.0273,
      "step": 4429
    },
    {
      "epoch": 1.0804878048780489,
      "grad_norm": 0.13322441279888153,
      "learning_rate": 3.563662342008769e-05,
      "loss": 0.0164,
      "step": 4430
    },
    {
      "epoch": 1.0807317073170732,
      "grad_norm": 0.18378859758377075,
      "learning_rate": 3.563084402093007e-05,
      "loss": 0.0177,
      "step": 4431
    },
    {
      "epoch": 1.0809756097560976,
      "grad_norm": 0.058216642588377,
      "learning_rate": 3.562506392814242e-05,
      "loss": 0.0164,
      "step": 4432
    },
    {
      "epoch": 1.081219512195122,
      "grad_norm": 0.15353046357631683,
      "learning_rate": 3.561928314210188e-05,
      "loss": 0.028,
      "step": 4433
    },
    {
      "epoch": 1.0814634146341464,
      "grad_norm": 0.09279418736696243,
      "learning_rate": 3.561350166318562e-05,
      "loss": 0.0175,
      "step": 4434
    },
    {
      "epoch": 1.0817073170731708,
      "grad_norm": 0.12135615199804306,
      "learning_rate": 3.560771949177086e-05,
      "loss": 0.029,
      "step": 4435
    },
    {
      "epoch": 1.0819512195121952,
      "grad_norm": 0.06229096278548241,
      "learning_rate": 3.560193662823489e-05,
      "loss": 0.0151,
      "step": 4436
    },
    {
      "epoch": 1.0821951219512196,
      "grad_norm": 0.07898272573947906,
      "learning_rate": 3.559615307295501e-05,
      "loss": 0.0209,
      "step": 4437
    },
    {
      "epoch": 1.082439024390244,
      "grad_norm": 0.13309529423713684,
      "learning_rate": 3.559036882630858e-05,
      "loss": 0.0367,
      "step": 4438
    },
    {
      "epoch": 1.0826829268292684,
      "grad_norm": 0.1177004873752594,
      "learning_rate": 3.5584583888673e-05,
      "loss": 0.0197,
      "step": 4439
    },
    {
      "epoch": 1.0829268292682928,
      "grad_norm": 0.1693057268857956,
      "learning_rate": 3.5578798260425714e-05,
      "loss": 0.0298,
      "step": 4440
    },
    {
      "epoch": 1.0831707317073171,
      "grad_norm": 0.13244232535362244,
      "learning_rate": 3.5573011941944246e-05,
      "loss": 0.0267,
      "step": 4441
    },
    {
      "epoch": 1.0834146341463415,
      "grad_norm": 0.09349320083856583,
      "learning_rate": 3.5567224933606104e-05,
      "loss": 0.0211,
      "step": 4442
    },
    {
      "epoch": 1.083658536585366,
      "grad_norm": 0.11290554702281952,
      "learning_rate": 3.556143723578888e-05,
      "loss": 0.0186,
      "step": 4443
    },
    {
      "epoch": 1.0839024390243903,
      "grad_norm": 0.09923677146434784,
      "learning_rate": 3.5555648848870216e-05,
      "loss": 0.0206,
      "step": 4444
    },
    {
      "epoch": 1.0841463414634147,
      "grad_norm": 0.09532526135444641,
      "learning_rate": 3.554985977322778e-05,
      "loss": 0.0168,
      "step": 4445
    },
    {
      "epoch": 1.084390243902439,
      "grad_norm": 0.13098110258579254,
      "learning_rate": 3.554407000923929e-05,
      "loss": 0.0344,
      "step": 4446
    },
    {
      "epoch": 1.0846341463414635,
      "grad_norm": 0.2508646249771118,
      "learning_rate": 3.5538279557282506e-05,
      "loss": 0.0214,
      "step": 4447
    },
    {
      "epoch": 1.0848780487804879,
      "grad_norm": 0.06971307098865509,
      "learning_rate": 3.5532488417735236e-05,
      "loss": 0.0193,
      "step": 4448
    },
    {
      "epoch": 1.0851219512195123,
      "grad_norm": 0.17460234463214874,
      "learning_rate": 3.552669659097535e-05,
      "loss": 0.0218,
      "step": 4449
    },
    {
      "epoch": 1.0853658536585367,
      "grad_norm": 0.09696171432733536,
      "learning_rate": 3.5520904077380734e-05,
      "loss": 0.0299,
      "step": 4450
    },
    {
      "epoch": 1.085609756097561,
      "grad_norm": 0.10341249406337738,
      "learning_rate": 3.551511087732933e-05,
      "loss": 0.0183,
      "step": 4451
    },
    {
      "epoch": 1.0858536585365854,
      "grad_norm": 0.1169123500585556,
      "learning_rate": 3.550931699119914e-05,
      "loss": 0.027,
      "step": 4452
    },
    {
      "epoch": 1.0860975609756098,
      "grad_norm": 0.07910189777612686,
      "learning_rate": 3.5503522419368184e-05,
      "loss": 0.0164,
      "step": 4453
    },
    {
      "epoch": 1.0863414634146342,
      "grad_norm": 0.12875039875507355,
      "learning_rate": 3.5497727162214546e-05,
      "loss": 0.0327,
      "step": 4454
    },
    {
      "epoch": 1.0865853658536586,
      "grad_norm": 0.07398867607116699,
      "learning_rate": 3.549193122011635e-05,
      "loss": 0.0095,
      "step": 4455
    },
    {
      "epoch": 1.086829268292683,
      "grad_norm": 0.15007174015045166,
      "learning_rate": 3.5486134593451746e-05,
      "loss": 0.0305,
      "step": 4456
    },
    {
      "epoch": 1.0870731707317074,
      "grad_norm": 0.1004987359046936,
      "learning_rate": 3.548033728259897e-05,
      "loss": 0.02,
      "step": 4457
    },
    {
      "epoch": 1.0873170731707318,
      "grad_norm": 0.14189785718917847,
      "learning_rate": 3.5474539287936274e-05,
      "loss": 0.0149,
      "step": 4458
    },
    {
      "epoch": 1.0875609756097562,
      "grad_norm": 0.13898210227489471,
      "learning_rate": 3.5468740609841954e-05,
      "loss": 0.0221,
      "step": 4459
    },
    {
      "epoch": 1.0878048780487806,
      "grad_norm": 0.2282920777797699,
      "learning_rate": 3.5462941248694356e-05,
      "loss": 0.0213,
      "step": 4460
    },
    {
      "epoch": 1.088048780487805,
      "grad_norm": 0.12943507730960846,
      "learning_rate": 3.545714120487187e-05,
      "loss": 0.0176,
      "step": 4461
    },
    {
      "epoch": 1.0882926829268293,
      "grad_norm": 0.1200871542096138,
      "learning_rate": 3.545134047875293e-05,
      "loss": 0.0253,
      "step": 4462
    },
    {
      "epoch": 1.0885365853658537,
      "grad_norm": 0.15310968458652496,
      "learning_rate": 3.544553907071603e-05,
      "loss": 0.0217,
      "step": 4463
    },
    {
      "epoch": 1.0887804878048781,
      "grad_norm": 0.0912371501326561,
      "learning_rate": 3.543973698113967e-05,
      "loss": 0.0212,
      "step": 4464
    },
    {
      "epoch": 1.0890243902439025,
      "grad_norm": 0.08493579924106598,
      "learning_rate": 3.5433934210402435e-05,
      "loss": 0.0211,
      "step": 4465
    },
    {
      "epoch": 1.089268292682927,
      "grad_norm": 0.13694030046463013,
      "learning_rate": 3.5428130758882936e-05,
      "loss": 0.0177,
      "step": 4466
    },
    {
      "epoch": 1.0895121951219513,
      "grad_norm": 0.5299752950668335,
      "learning_rate": 3.542232662695982e-05,
      "loss": 0.0234,
      "step": 4467
    },
    {
      "epoch": 1.0897560975609757,
      "grad_norm": 0.16363079845905304,
      "learning_rate": 3.541652181501181e-05,
      "loss": 0.033,
      "step": 4468
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.07386557757854462,
      "learning_rate": 3.541071632341763e-05,
      "loss": 0.012,
      "step": 4469
    },
    {
      "epoch": 1.0902439024390245,
      "grad_norm": 0.08078346401453018,
      "learning_rate": 3.540491015255609e-05,
      "loss": 0.032,
      "step": 4470
    },
    {
      "epoch": 1.0904878048780489,
      "grad_norm": 0.10282953828573227,
      "learning_rate": 3.539910330280601e-05,
      "loss": 0.0237,
      "step": 4471
    },
    {
      "epoch": 1.0907317073170733,
      "grad_norm": 0.14552654325962067,
      "learning_rate": 3.539329577454628e-05,
      "loss": 0.0245,
      "step": 4472
    },
    {
      "epoch": 1.0909756097560976,
      "grad_norm": 0.10002635419368744,
      "learning_rate": 3.5387487568155803e-05,
      "loss": 0.0274,
      "step": 4473
    },
    {
      "epoch": 1.091219512195122,
      "grad_norm": 0.17272838950157166,
      "learning_rate": 3.538167868401357e-05,
      "loss": 0.0198,
      "step": 4474
    },
    {
      "epoch": 1.0914634146341464,
      "grad_norm": 0.08373113721609116,
      "learning_rate": 3.5375869122498586e-05,
      "loss": 0.0184,
      "step": 4475
    },
    {
      "epoch": 1.0917073170731708,
      "grad_norm": 0.12093440443277359,
      "learning_rate": 3.5370058883989896e-05,
      "loss": 0.0081,
      "step": 4476
    },
    {
      "epoch": 1.0919512195121952,
      "grad_norm": 0.08024418354034424,
      "learning_rate": 3.536424796886662e-05,
      "loss": 0.0134,
      "step": 4477
    },
    {
      "epoch": 1.0921951219512196,
      "grad_norm": 0.16276668012142181,
      "learning_rate": 3.535843637750789e-05,
      "loss": 0.0165,
      "step": 4478
    },
    {
      "epoch": 1.092439024390244,
      "grad_norm": 0.10730911791324615,
      "learning_rate": 3.5352624110292895e-05,
      "loss": 0.0162,
      "step": 4479
    },
    {
      "epoch": 1.0926829268292684,
      "grad_norm": 0.09713073819875717,
      "learning_rate": 3.534681116760087e-05,
      "loss": 0.0273,
      "step": 4480
    },
    {
      "epoch": 1.0929268292682928,
      "grad_norm": 0.09500373154878616,
      "learning_rate": 3.534099754981109e-05,
      "loss": 0.017,
      "step": 4481
    },
    {
      "epoch": 1.0931707317073172,
      "grad_norm": 0.07029960304498672,
      "learning_rate": 3.533518325730287e-05,
      "loss": 0.0166,
      "step": 4482
    },
    {
      "epoch": 1.0934146341463415,
      "grad_norm": 0.10964284837245941,
      "learning_rate": 3.53293682904556e-05,
      "loss": 0.0281,
      "step": 4483
    },
    {
      "epoch": 1.093658536585366,
      "grad_norm": 0.08535465598106384,
      "learning_rate": 3.532355264964865e-05,
      "loss": 0.0143,
      "step": 4484
    },
    {
      "epoch": 1.0939024390243903,
      "grad_norm": 0.09709654003381729,
      "learning_rate": 3.531773633526151e-05,
      "loss": 0.0104,
      "step": 4485
    },
    {
      "epoch": 1.0941463414634147,
      "grad_norm": 0.06895147264003754,
      "learning_rate": 3.531191934767365e-05,
      "loss": 0.017,
      "step": 4486
    },
    {
      "epoch": 1.094390243902439,
      "grad_norm": 0.08432921767234802,
      "learning_rate": 3.530610168726462e-05,
      "loss": 0.02,
      "step": 4487
    },
    {
      "epoch": 1.0946341463414635,
      "grad_norm": 0.1286463588476181,
      "learning_rate": 3.530028335441401e-05,
      "loss": 0.0221,
      "step": 4488
    },
    {
      "epoch": 1.094878048780488,
      "grad_norm": 0.12827014923095703,
      "learning_rate": 3.529446434950144e-05,
      "loss": 0.0266,
      "step": 4489
    },
    {
      "epoch": 1.0951219512195123,
      "grad_norm": 0.10912904888391495,
      "learning_rate": 3.528864467290658e-05,
      "loss": 0.0091,
      "step": 4490
    },
    {
      "epoch": 1.0953658536585367,
      "grad_norm": 0.23503386974334717,
      "learning_rate": 3.528282432500916e-05,
      "loss": 0.0312,
      "step": 4491
    },
    {
      "epoch": 1.095609756097561,
      "grad_norm": 0.17635585367679596,
      "learning_rate": 3.5277003306188935e-05,
      "loss": 0.0177,
      "step": 4492
    },
    {
      "epoch": 1.0958536585365855,
      "grad_norm": 0.07533609122037888,
      "learning_rate": 3.527118161682569e-05,
      "loss": 0.0243,
      "step": 4493
    },
    {
      "epoch": 1.0960975609756098,
      "grad_norm": 0.11390463262796402,
      "learning_rate": 3.52653592572993e-05,
      "loss": 0.0246,
      "step": 4494
    },
    {
      "epoch": 1.0963414634146342,
      "grad_norm": 0.06927188485860825,
      "learning_rate": 3.525953622798964e-05,
      "loss": 0.02,
      "step": 4495
    },
    {
      "epoch": 1.0965853658536586,
      "grad_norm": 0.18668268620967865,
      "learning_rate": 3.525371252927666e-05,
      "loss": 0.0223,
      "step": 4496
    },
    {
      "epoch": 1.096829268292683,
      "grad_norm": 0.1899835765361786,
      "learning_rate": 3.524788816154031e-05,
      "loss": 0.0379,
      "step": 4497
    },
    {
      "epoch": 1.0970731707317074,
      "grad_norm": 0.10495033115148544,
      "learning_rate": 3.5242063125160636e-05,
      "loss": 0.0132,
      "step": 4498
    },
    {
      "epoch": 1.0973170731707318,
      "grad_norm": 0.11273777484893799,
      "learning_rate": 3.5236237420517693e-05,
      "loss": 0.0283,
      "step": 4499
    },
    {
      "epoch": 1.0975609756097562,
      "grad_norm": 0.08212599158287048,
      "learning_rate": 3.5230411047991603e-05,
      "loss": 0.0177,
      "step": 4500
    },
    {
      "epoch": 1.0978048780487806,
      "grad_norm": 0.13141407072544098,
      "learning_rate": 3.52245840079625e-05,
      "loss": 0.0445,
      "step": 4501
    },
    {
      "epoch": 1.098048780487805,
      "grad_norm": 0.19569799304008484,
      "learning_rate": 3.5218756300810594e-05,
      "loss": 0.0236,
      "step": 4502
    },
    {
      "epoch": 1.0982926829268294,
      "grad_norm": 0.17503371834754944,
      "learning_rate": 3.5212927926916127e-05,
      "loss": 0.0278,
      "step": 4503
    },
    {
      "epoch": 1.0985365853658537,
      "grad_norm": 0.26064595580101013,
      "learning_rate": 3.5207098886659376e-05,
      "loss": 0.0188,
      "step": 4504
    },
    {
      "epoch": 1.0987804878048781,
      "grad_norm": 0.1751803159713745,
      "learning_rate": 3.5201269180420674e-05,
      "loss": 0.02,
      "step": 4505
    },
    {
      "epoch": 1.0990243902439025,
      "grad_norm": 0.07985507696866989,
      "learning_rate": 3.519543880858038e-05,
      "loss": 0.0143,
      "step": 4506
    },
    {
      "epoch": 1.099268292682927,
      "grad_norm": 0.16963990032672882,
      "learning_rate": 3.5189607771518915e-05,
      "loss": 0.0186,
      "step": 4507
    },
    {
      "epoch": 1.0995121951219513,
      "grad_norm": 0.13049639761447906,
      "learning_rate": 3.518377606961674e-05,
      "loss": 0.0293,
      "step": 4508
    },
    {
      "epoch": 1.0997560975609757,
      "grad_norm": 0.13701646029949188,
      "learning_rate": 3.517794370325435e-05,
      "loss": 0.0271,
      "step": 4509
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.14565998315811157,
      "learning_rate": 3.5172110672812286e-05,
      "loss": 0.0261,
      "step": 4510
    },
    {
      "epoch": 1.1002439024390245,
      "grad_norm": 0.09734638035297394,
      "learning_rate": 3.516627697867114e-05,
      "loss": 0.0129,
      "step": 4511
    },
    {
      "epoch": 1.1004878048780489,
      "grad_norm": 0.13555486500263214,
      "learning_rate": 3.5160442621211556e-05,
      "loss": 0.0216,
      "step": 4512
    },
    {
      "epoch": 1.1007317073170733,
      "grad_norm": 0.07469961047172546,
      "learning_rate": 3.515460760081419e-05,
      "loss": 0.0254,
      "step": 4513
    },
    {
      "epoch": 1.1009756097560977,
      "grad_norm": 0.0874917283654213,
      "learning_rate": 3.514877191785976e-05,
      "loss": 0.0156,
      "step": 4514
    },
    {
      "epoch": 1.101219512195122,
      "grad_norm": 0.0875491350889206,
      "learning_rate": 3.514293557272903e-05,
      "loss": 0.0277,
      "step": 4515
    },
    {
      "epoch": 1.1014634146341464,
      "grad_norm": 0.06886141747236252,
      "learning_rate": 3.5137098565802803e-05,
      "loss": 0.0205,
      "step": 4516
    },
    {
      "epoch": 1.1017073170731708,
      "grad_norm": 0.09179522842168808,
      "learning_rate": 3.513126089746194e-05,
      "loss": 0.0116,
      "step": 4517
    },
    {
      "epoch": 1.1019512195121952,
      "grad_norm": 0.10270247608423233,
      "learning_rate": 3.5125422568087306e-05,
      "loss": 0.011,
      "step": 4518
    },
    {
      "epoch": 1.1021951219512196,
      "grad_norm": 0.1271308958530426,
      "learning_rate": 3.5119583578059846e-05,
      "loss": 0.0164,
      "step": 4519
    },
    {
      "epoch": 1.102439024390244,
      "grad_norm": 0.07501783967018127,
      "learning_rate": 3.511374392776054e-05,
      "loss": 0.013,
      "step": 4520
    },
    {
      "epoch": 1.1026829268292684,
      "grad_norm": 0.11747199296951294,
      "learning_rate": 3.5107903617570405e-05,
      "loss": 0.0242,
      "step": 4521
    },
    {
      "epoch": 1.1029268292682928,
      "grad_norm": 0.1340295374393463,
      "learning_rate": 3.5102062647870504e-05,
      "loss": 0.0348,
      "step": 4522
    },
    {
      "epoch": 1.1031707317073172,
      "grad_norm": 0.08123718947172165,
      "learning_rate": 3.509622101904193e-05,
      "loss": 0.0124,
      "step": 4523
    },
    {
      "epoch": 1.1034146341463416,
      "grad_norm": 0.09756438434123993,
      "learning_rate": 3.509037873146586e-05,
      "loss": 0.0346,
      "step": 4524
    },
    {
      "epoch": 1.103658536585366,
      "grad_norm": 0.09587538987398148,
      "learning_rate": 3.5084535785523456e-05,
      "loss": 0.0373,
      "step": 4525
    },
    {
      "epoch": 1.1039024390243903,
      "grad_norm": 0.2868571877479553,
      "learning_rate": 3.507869218159596e-05,
      "loss": 0.0279,
      "step": 4526
    },
    {
      "epoch": 1.1041463414634147,
      "grad_norm": 0.1756121814250946,
      "learning_rate": 3.507284792006466e-05,
      "loss": 0.013,
      "step": 4527
    },
    {
      "epoch": 1.1043902439024391,
      "grad_norm": 0.19926536083221436,
      "learning_rate": 3.506700300131086e-05,
      "loss": 0.0313,
      "step": 4528
    },
    {
      "epoch": 1.1046341463414635,
      "grad_norm": 0.07999231666326523,
      "learning_rate": 3.5061157425715945e-05,
      "loss": 0.0183,
      "step": 4529
    },
    {
      "epoch": 1.104878048780488,
      "grad_norm": 0.07041747868061066,
      "learning_rate": 3.50553111936613e-05,
      "loss": 0.0183,
      "step": 4530
    },
    {
      "epoch": 1.1051219512195123,
      "grad_norm": 0.11462243646383286,
      "learning_rate": 3.504946430552839e-05,
      "loss": 0.0155,
      "step": 4531
    },
    {
      "epoch": 1.1053658536585367,
      "grad_norm": 0.14816351234912872,
      "learning_rate": 3.5043616761698696e-05,
      "loss": 0.0149,
      "step": 4532
    },
    {
      "epoch": 1.105609756097561,
      "grad_norm": 0.16971455514431,
      "learning_rate": 3.5037768562553745e-05,
      "loss": 0.0224,
      "step": 4533
    },
    {
      "epoch": 1.1058536585365855,
      "grad_norm": 0.10380755364894867,
      "learning_rate": 3.503191970847513e-05,
      "loss": 0.0328,
      "step": 4534
    },
    {
      "epoch": 1.1060975609756099,
      "grad_norm": 0.17415952682495117,
      "learning_rate": 3.5026070199844466e-05,
      "loss": 0.0129,
      "step": 4535
    },
    {
      "epoch": 1.1063414634146342,
      "grad_norm": 0.11511098593473434,
      "learning_rate": 3.502022003704341e-05,
      "loss": 0.0321,
      "step": 4536
    },
    {
      "epoch": 1.1065853658536586,
      "grad_norm": 0.14937002956867218,
      "learning_rate": 3.501436922045368e-05,
      "loss": 0.0164,
      "step": 4537
    },
    {
      "epoch": 1.106829268292683,
      "grad_norm": 0.04801872372627258,
      "learning_rate": 3.500851775045702e-05,
      "loss": 0.0079,
      "step": 4538
    },
    {
      "epoch": 1.1070731707317074,
      "grad_norm": 0.07341581583023071,
      "learning_rate": 3.500266562743519e-05,
      "loss": 0.0051,
      "step": 4539
    },
    {
      "epoch": 1.1073170731707318,
      "grad_norm": 0.16729386150836945,
      "learning_rate": 3.499681285177007e-05,
      "loss": 0.0181,
      "step": 4540
    },
    {
      "epoch": 1.1075609756097562,
      "grad_norm": 0.17142806947231293,
      "learning_rate": 3.499095942384351e-05,
      "loss": 0.0334,
      "step": 4541
    },
    {
      "epoch": 1.1078048780487806,
      "grad_norm": 0.07954912632703781,
      "learning_rate": 3.498510534403743e-05,
      "loss": 0.0193,
      "step": 4542
    },
    {
      "epoch": 1.108048780487805,
      "grad_norm": 0.08743424713611603,
      "learning_rate": 3.49792506127338e-05,
      "loss": 0.0163,
      "step": 4543
    },
    {
      "epoch": 1.1082926829268294,
      "grad_norm": 0.08049257844686508,
      "learning_rate": 3.497339523031461e-05,
      "loss": 0.0114,
      "step": 4544
    },
    {
      "epoch": 1.1085365853658538,
      "grad_norm": 0.0831376239657402,
      "learning_rate": 3.4967539197161915e-05,
      "loss": 0.0156,
      "step": 4545
    },
    {
      "epoch": 1.1087804878048781,
      "grad_norm": 0.11276640743017197,
      "learning_rate": 3.49616825136578e-05,
      "loss": 0.0215,
      "step": 4546
    },
    {
      "epoch": 1.1090243902439025,
      "grad_norm": 0.09490789473056793,
      "learning_rate": 3.49558251801844e-05,
      "loss": 0.0127,
      "step": 4547
    },
    {
      "epoch": 1.109268292682927,
      "grad_norm": 0.13465367257595062,
      "learning_rate": 3.4949967197123885e-05,
      "loss": 0.0343,
      "step": 4548
    },
    {
      "epoch": 1.1095121951219513,
      "grad_norm": 0.08958450704813004,
      "learning_rate": 3.494410856485846e-05,
      "loss": 0.0164,
      "step": 4549
    },
    {
      "epoch": 1.1097560975609757,
      "grad_norm": 0.10337251424789429,
      "learning_rate": 3.49382492837704e-05,
      "loss": 0.0292,
      "step": 4550
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.13816159963607788,
      "learning_rate": 3.493238935424201e-05,
      "loss": 0.0213,
      "step": 4551
    },
    {
      "epoch": 1.1102439024390245,
      "grad_norm": 0.08525542169809341,
      "learning_rate": 3.4926528776655604e-05,
      "loss": 0.0158,
      "step": 4552
    },
    {
      "epoch": 1.1104878048780489,
      "grad_norm": 0.12493220716714859,
      "learning_rate": 3.492066755139359e-05,
      "loss": 0.0227,
      "step": 4553
    },
    {
      "epoch": 1.1107317073170733,
      "grad_norm": 0.08200808614492416,
      "learning_rate": 3.491480567883839e-05,
      "loss": 0.0308,
      "step": 4554
    },
    {
      "epoch": 1.1109756097560977,
      "grad_norm": 0.1427326202392578,
      "learning_rate": 3.490894315937247e-05,
      "loss": 0.0311,
      "step": 4555
    },
    {
      "epoch": 1.111219512195122,
      "grad_norm": 0.060129426419734955,
      "learning_rate": 3.4903079993378344e-05,
      "loss": 0.0154,
      "step": 4556
    },
    {
      "epoch": 1.1114634146341464,
      "grad_norm": 0.09578804671764374,
      "learning_rate": 3.4897216181238554e-05,
      "loss": 0.0215,
      "step": 4557
    },
    {
      "epoch": 1.1117073170731708,
      "grad_norm": 0.20225100219249725,
      "learning_rate": 3.4891351723335714e-05,
      "loss": 0.0237,
      "step": 4558
    },
    {
      "epoch": 1.1119512195121952,
      "grad_norm": 0.07909896969795227,
      "learning_rate": 3.488548662005246e-05,
      "loss": 0.0129,
      "step": 4559
    },
    {
      "epoch": 1.1121951219512196,
      "grad_norm": 0.14935502409934998,
      "learning_rate": 3.487962087177146e-05,
      "loss": 0.0296,
      "step": 4560
    },
    {
      "epoch": 1.112439024390244,
      "grad_norm": 0.03954773396253586,
      "learning_rate": 3.487375447887544e-05,
      "loss": 0.0065,
      "step": 4561
    },
    {
      "epoch": 1.1126829268292684,
      "grad_norm": 0.07071738690137863,
      "learning_rate": 3.4867887441747174e-05,
      "loss": 0.024,
      "step": 4562
    },
    {
      "epoch": 1.1129268292682928,
      "grad_norm": 0.08965004235506058,
      "learning_rate": 3.486201976076946e-05,
      "loss": 0.0271,
      "step": 4563
    },
    {
      "epoch": 1.1131707317073172,
      "grad_norm": 0.12870076298713684,
      "learning_rate": 3.485615143632515e-05,
      "loss": 0.0366,
      "step": 4564
    },
    {
      "epoch": 1.1134146341463416,
      "grad_norm": 0.1790556013584137,
      "learning_rate": 3.4850282468797125e-05,
      "loss": 0.0211,
      "step": 4565
    },
    {
      "epoch": 1.113658536585366,
      "grad_norm": 0.13939714431762695,
      "learning_rate": 3.4844412858568324e-05,
      "loss": 0.026,
      "step": 4566
    },
    {
      "epoch": 1.1139024390243903,
      "grad_norm": 0.17531771957874298,
      "learning_rate": 3.483854260602172e-05,
      "loss": 0.0271,
      "step": 4567
    },
    {
      "epoch": 1.1141463414634147,
      "grad_norm": 0.22947873175144196,
      "learning_rate": 3.4832671711540334e-05,
      "loss": 0.0237,
      "step": 4568
    },
    {
      "epoch": 1.1143902439024391,
      "grad_norm": 0.09259331226348877,
      "learning_rate": 3.4826800175507216e-05,
      "loss": 0.0302,
      "step": 4569
    },
    {
      "epoch": 1.1146341463414635,
      "grad_norm": 0.06894473731517792,
      "learning_rate": 3.482092799830547e-05,
      "loss": 0.0152,
      "step": 4570
    },
    {
      "epoch": 1.114878048780488,
      "grad_norm": 0.08863884955644608,
      "learning_rate": 3.481505518031823e-05,
      "loss": 0.0199,
      "step": 4571
    },
    {
      "epoch": 1.1151219512195123,
      "grad_norm": 0.07121289521455765,
      "learning_rate": 3.4809181721928695e-05,
      "loss": 0.0109,
      "step": 4572
    },
    {
      "epoch": 1.1153658536585367,
      "grad_norm": 0.14797286689281464,
      "learning_rate": 3.480330762352008e-05,
      "loss": 0.0162,
      "step": 4573
    },
    {
      "epoch": 1.115609756097561,
      "grad_norm": 0.08220354467630386,
      "learning_rate": 3.4797432885475654e-05,
      "loss": 0.0251,
      "step": 4574
    },
    {
      "epoch": 1.1158536585365855,
      "grad_norm": 0.13857537508010864,
      "learning_rate": 3.479155750817872e-05,
      "loss": 0.0232,
      "step": 4575
    },
    {
      "epoch": 1.1160975609756099,
      "grad_norm": 0.09137360006570816,
      "learning_rate": 3.478568149201264e-05,
      "loss": 0.0149,
      "step": 4576
    },
    {
      "epoch": 1.1163414634146342,
      "grad_norm": 0.08798123151063919,
      "learning_rate": 3.47798048373608e-05,
      "loss": 0.0136,
      "step": 4577
    },
    {
      "epoch": 1.1165853658536586,
      "grad_norm": 0.15724308788776398,
      "learning_rate": 3.4773927544606626e-05,
      "loss": 0.0357,
      "step": 4578
    },
    {
      "epoch": 1.116829268292683,
      "grad_norm": 0.12050382792949677,
      "learning_rate": 3.4768049614133605e-05,
      "loss": 0.0189,
      "step": 4579
    },
    {
      "epoch": 1.1170731707317074,
      "grad_norm": 0.10560005903244019,
      "learning_rate": 3.4762171046325246e-05,
      "loss": 0.0268,
      "step": 4580
    },
    {
      "epoch": 1.1173170731707316,
      "grad_norm": 0.10055575519800186,
      "learning_rate": 3.475629184156512e-05,
      "loss": 0.0228,
      "step": 4581
    },
    {
      "epoch": 1.1175609756097562,
      "grad_norm": 0.1521344631910324,
      "learning_rate": 3.475041200023681e-05,
      "loss": 0.0246,
      "step": 4582
    },
    {
      "epoch": 1.1178048780487804,
      "grad_norm": 0.12881125509738922,
      "learning_rate": 3.4744531522723965e-05,
      "loss": 0.0282,
      "step": 4583
    },
    {
      "epoch": 1.118048780487805,
      "grad_norm": 0.0869668647646904,
      "learning_rate": 3.473865040941028e-05,
      "loss": 0.0168,
      "step": 4584
    },
    {
      "epoch": 1.1182926829268292,
      "grad_norm": 0.050738073885440826,
      "learning_rate": 3.473276866067946e-05,
      "loss": 0.0166,
      "step": 4585
    },
    {
      "epoch": 1.1185365853658538,
      "grad_norm": 0.0644717738032341,
      "learning_rate": 3.472688627691528e-05,
      "loss": 0.0176,
      "step": 4586
    },
    {
      "epoch": 1.118780487804878,
      "grad_norm": 0.07831685990095139,
      "learning_rate": 3.472100325850155e-05,
      "loss": 0.0209,
      "step": 4587
    },
    {
      "epoch": 1.1190243902439025,
      "grad_norm": 0.19610129296779633,
      "learning_rate": 3.471511960582211e-05,
      "loss": 0.0232,
      "step": 4588
    },
    {
      "epoch": 1.1192682926829267,
      "grad_norm": 0.07168566435575485,
      "learning_rate": 3.470923531926087e-05,
      "loss": 0.0146,
      "step": 4589
    },
    {
      "epoch": 1.1195121951219513,
      "grad_norm": 0.08781270682811737,
      "learning_rate": 3.4703350399201736e-05,
      "loss": 0.0251,
      "step": 4590
    },
    {
      "epoch": 1.1197560975609755,
      "grad_norm": 0.17003759741783142,
      "learning_rate": 3.469746484602869e-05,
      "loss": 0.0288,
      "step": 4591
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.06116997078061104,
      "learning_rate": 3.4691578660125754e-05,
      "loss": 0.0189,
      "step": 4592
    },
    {
      "epoch": 1.1202439024390243,
      "grad_norm": 0.11562585830688477,
      "learning_rate": 3.468569184187699e-05,
      "loss": 0.0298,
      "step": 4593
    },
    {
      "epoch": 1.1204878048780489,
      "grad_norm": 0.11687450855970383,
      "learning_rate": 3.4679804391666473e-05,
      "loss": 0.0187,
      "step": 4594
    },
    {
      "epoch": 1.120731707317073,
      "grad_norm": 0.1096833199262619,
      "learning_rate": 3.4673916309878354e-05,
      "loss": 0.011,
      "step": 4595
    },
    {
      "epoch": 1.1209756097560977,
      "grad_norm": 0.1496700793504715,
      "learning_rate": 3.4668027596896814e-05,
      "loss": 0.0292,
      "step": 4596
    },
    {
      "epoch": 1.1212195121951218,
      "grad_norm": 0.13480237126350403,
      "learning_rate": 3.466213825310607e-05,
      "loss": 0.016,
      "step": 4597
    },
    {
      "epoch": 1.1214634146341464,
      "grad_norm": 0.15306800603866577,
      "learning_rate": 3.4656248278890375e-05,
      "loss": 0.0331,
      "step": 4598
    },
    {
      "epoch": 1.1217073170731706,
      "grad_norm": 0.13707335293293,
      "learning_rate": 3.4650357674634046e-05,
      "loss": 0.0124,
      "step": 4599
    },
    {
      "epoch": 1.1219512195121952,
      "grad_norm": 0.10418330878019333,
      "learning_rate": 3.464446644072142e-05,
      "loss": 0.015,
      "step": 4600
    },
    {
      "epoch": 1.1221951219512194,
      "grad_norm": 0.077883280813694,
      "learning_rate": 3.4638574577536885e-05,
      "loss": 0.0092,
      "step": 4601
    },
    {
      "epoch": 1.122439024390244,
      "grad_norm": 0.1119232177734375,
      "learning_rate": 3.463268208546486e-05,
      "loss": 0.0206,
      "step": 4602
    },
    {
      "epoch": 1.1226829268292682,
      "grad_norm": 0.04342811182141304,
      "learning_rate": 3.462678896488983e-05,
      "loss": 0.01,
      "step": 4603
    },
    {
      "epoch": 1.1229268292682928,
      "grad_norm": 0.13448300957679749,
      "learning_rate": 3.462089521619628e-05,
      "loss": 0.0254,
      "step": 4604
    },
    {
      "epoch": 1.123170731707317,
      "grad_norm": 0.12409396469593048,
      "learning_rate": 3.4615000839768765e-05,
      "loss": 0.0142,
      "step": 4605
    },
    {
      "epoch": 1.1234146341463416,
      "grad_norm": 0.2521982192993164,
      "learning_rate": 3.4609105835991894e-05,
      "loss": 0.0282,
      "step": 4606
    },
    {
      "epoch": 1.1236585365853657,
      "grad_norm": 0.12965074181556702,
      "learning_rate": 3.4603210205250274e-05,
      "loss": 0.0289,
      "step": 4607
    },
    {
      "epoch": 1.1239024390243904,
      "grad_norm": 0.14463452994823456,
      "learning_rate": 3.459731394792859e-05,
      "loss": 0.011,
      "step": 4608
    },
    {
      "epoch": 1.1241463414634145,
      "grad_norm": 0.12704598903656006,
      "learning_rate": 3.4591417064411545e-05,
      "loss": 0.0493,
      "step": 4609
    },
    {
      "epoch": 1.1243902439024391,
      "grad_norm": 0.09366580098867416,
      "learning_rate": 3.458551955508391e-05,
      "loss": 0.027,
      "step": 4610
    },
    {
      "epoch": 1.1246341463414633,
      "grad_norm": 0.1708822399377823,
      "learning_rate": 3.457962142033045e-05,
      "loss": 0.0245,
      "step": 4611
    },
    {
      "epoch": 1.124878048780488,
      "grad_norm": 0.17936435341835022,
      "learning_rate": 3.457372266053604e-05,
      "loss": 0.0236,
      "step": 4612
    },
    {
      "epoch": 1.125121951219512,
      "grad_norm": 0.07526063919067383,
      "learning_rate": 3.456782327608552e-05,
      "loss": 0.0077,
      "step": 4613
    },
    {
      "epoch": 1.1253658536585367,
      "grad_norm": 0.09193886071443558,
      "learning_rate": 3.456192326736383e-05,
      "loss": 0.0197,
      "step": 4614
    },
    {
      "epoch": 1.1256097560975609,
      "grad_norm": 0.07892517000436783,
      "learning_rate": 3.455602263475591e-05,
      "loss": 0.0177,
      "step": 4615
    },
    {
      "epoch": 1.1258536585365855,
      "grad_norm": 0.18219976127147675,
      "learning_rate": 3.455012137864677e-05,
      "loss": 0.0248,
      "step": 4616
    },
    {
      "epoch": 1.1260975609756096,
      "grad_norm": 0.2493574619293213,
      "learning_rate": 3.4544219499421446e-05,
      "loss": 0.0209,
      "step": 4617
    },
    {
      "epoch": 1.1263414634146343,
      "grad_norm": 0.09505204856395721,
      "learning_rate": 3.453831699746503e-05,
      "loss": 0.0206,
      "step": 4618
    },
    {
      "epoch": 1.1265853658536584,
      "grad_norm": 0.15151700377464294,
      "learning_rate": 3.45324138731626e-05,
      "loss": 0.0275,
      "step": 4619
    },
    {
      "epoch": 1.126829268292683,
      "grad_norm": 0.7852851152420044,
      "learning_rate": 3.4526510126899376e-05,
      "loss": 0.0149,
      "step": 4620
    },
    {
      "epoch": 1.1270731707317072,
      "grad_norm": 0.07638619095087051,
      "learning_rate": 3.4520605759060515e-05,
      "loss": 0.0163,
      "step": 4621
    },
    {
      "epoch": 1.1273170731707318,
      "grad_norm": 0.15281853079795837,
      "learning_rate": 3.451470077003127e-05,
      "loss": 0.0203,
      "step": 4622
    },
    {
      "epoch": 1.127560975609756,
      "grad_norm": 0.21531614661216736,
      "learning_rate": 3.450879516019694e-05,
      "loss": 0.0388,
      "step": 4623
    },
    {
      "epoch": 1.1278048780487806,
      "grad_norm": 0.07632600516080856,
      "learning_rate": 3.450288892994283e-05,
      "loss": 0.0144,
      "step": 4624
    },
    {
      "epoch": 1.1280487804878048,
      "grad_norm": 0.1821611523628235,
      "learning_rate": 3.4496982079654303e-05,
      "loss": 0.0211,
      "step": 4625
    },
    {
      "epoch": 1.1282926829268294,
      "grad_norm": 0.1927311271429062,
      "learning_rate": 3.4491074609716767e-05,
      "loss": 0.0186,
      "step": 4626
    },
    {
      "epoch": 1.1285365853658536,
      "grad_norm": 0.09641996026039124,
      "learning_rate": 3.448516652051568e-05,
      "loss": 0.0179,
      "step": 4627
    },
    {
      "epoch": 1.1287804878048782,
      "grad_norm": 0.08559868484735489,
      "learning_rate": 3.447925781243651e-05,
      "loss": 0.0172,
      "step": 4628
    },
    {
      "epoch": 1.1290243902439023,
      "grad_norm": 0.12828116118907928,
      "learning_rate": 3.447334848586478e-05,
      "loss": 0.0154,
      "step": 4629
    },
    {
      "epoch": 1.129268292682927,
      "grad_norm": 0.11573051661252975,
      "learning_rate": 3.4467438541186066e-05,
      "loss": 0.0294,
      "step": 4630
    },
    {
      "epoch": 1.1295121951219511,
      "grad_norm": 0.20307685434818268,
      "learning_rate": 3.446152797878597e-05,
      "loss": 0.0202,
      "step": 4631
    },
    {
      "epoch": 1.1297560975609757,
      "grad_norm": 0.1621100753545761,
      "learning_rate": 3.4455616799050136e-05,
      "loss": 0.0253,
      "step": 4632
    },
    {
      "epoch": 1.13,
      "grad_norm": 0.1953125298023224,
      "learning_rate": 3.444970500236425e-05,
      "loss": 0.0221,
      "step": 4633
    },
    {
      "epoch": 1.1302439024390245,
      "grad_norm": 0.20352086424827576,
      "learning_rate": 3.4443792589114046e-05,
      "loss": 0.0351,
      "step": 4634
    },
    {
      "epoch": 1.1304878048780487,
      "grad_norm": 0.09520824998617172,
      "learning_rate": 3.443787955968529e-05,
      "loss": 0.0165,
      "step": 4635
    },
    {
      "epoch": 1.1307317073170733,
      "grad_norm": 0.0863245278596878,
      "learning_rate": 3.443196591446378e-05,
      "loss": 0.0078,
      "step": 4636
    },
    {
      "epoch": 1.1309756097560975,
      "grad_norm": 0.07019073516130447,
      "learning_rate": 3.442605165383537e-05,
      "loss": 0.0121,
      "step": 4637
    },
    {
      "epoch": 1.131219512195122,
      "grad_norm": 0.09665681421756744,
      "learning_rate": 3.442013677818594e-05,
      "loss": 0.0141,
      "step": 4638
    },
    {
      "epoch": 1.1314634146341462,
      "grad_norm": 0.13692018389701843,
      "learning_rate": 3.441422128790142e-05,
      "loss": 0.0235,
      "step": 4639
    },
    {
      "epoch": 1.1317073170731708,
      "grad_norm": 0.1176343709230423,
      "learning_rate": 3.440830518336779e-05,
      "loss": 0.0226,
      "step": 4640
    },
    {
      "epoch": 1.131951219512195,
      "grad_norm": 0.14108820259571075,
      "learning_rate": 3.440238846497104e-05,
      "loss": 0.034,
      "step": 4641
    },
    {
      "epoch": 1.1321951219512196,
      "grad_norm": 0.14004957675933838,
      "learning_rate": 3.439647113309723e-05,
      "loss": 0.0215,
      "step": 4642
    },
    {
      "epoch": 1.1324390243902438,
      "grad_norm": 0.10765403509140015,
      "learning_rate": 3.439055318813245e-05,
      "loss": 0.0168,
      "step": 4643
    },
    {
      "epoch": 1.1326829268292684,
      "grad_norm": 0.07157111167907715,
      "learning_rate": 3.4384634630462814e-05,
      "loss": 0.013,
      "step": 4644
    },
    {
      "epoch": 1.1329268292682926,
      "grad_norm": 0.15904422104358673,
      "learning_rate": 3.4378715460474504e-05,
      "loss": 0.0279,
      "step": 4645
    },
    {
      "epoch": 1.1331707317073172,
      "grad_norm": 0.2139298915863037,
      "learning_rate": 3.4372795678553713e-05,
      "loss": 0.0229,
      "step": 4646
    },
    {
      "epoch": 1.1334146341463414,
      "grad_norm": 0.07835482060909271,
      "learning_rate": 3.43668752850867e-05,
      "loss": 0.0104,
      "step": 4647
    },
    {
      "epoch": 1.133658536585366,
      "grad_norm": 0.07363700866699219,
      "learning_rate": 3.436095428045976e-05,
      "loss": 0.0267,
      "step": 4648
    },
    {
      "epoch": 1.1339024390243901,
      "grad_norm": 0.11024761199951172,
      "learning_rate": 3.4355032665059194e-05,
      "loss": 0.0179,
      "step": 4649
    },
    {
      "epoch": 1.1341463414634148,
      "grad_norm": 0.08188612014055252,
      "learning_rate": 3.4349110439271395e-05,
      "loss": 0.0253,
      "step": 4650
    },
    {
      "epoch": 1.134390243902439,
      "grad_norm": 0.11023561656475067,
      "learning_rate": 3.434318760348275e-05,
      "loss": 0.0181,
      "step": 4651
    },
    {
      "epoch": 1.1346341463414635,
      "grad_norm": 0.18631486594676971,
      "learning_rate": 3.433726415807972e-05,
      "loss": 0.0172,
      "step": 4652
    },
    {
      "epoch": 1.1348780487804877,
      "grad_norm": 0.0487246997654438,
      "learning_rate": 3.43313401034488e-05,
      "loss": 0.0085,
      "step": 4653
    },
    {
      "epoch": 1.1351219512195123,
      "grad_norm": 0.07996399700641632,
      "learning_rate": 3.4325415439976495e-05,
      "loss": 0.0174,
      "step": 4654
    },
    {
      "epoch": 1.1353658536585365,
      "grad_norm": 0.08056005090475082,
      "learning_rate": 3.431949016804937e-05,
      "loss": 0.029,
      "step": 4655
    },
    {
      "epoch": 1.135609756097561,
      "grad_norm": 0.08972885459661484,
      "learning_rate": 3.431356428805406e-05,
      "loss": 0.0168,
      "step": 4656
    },
    {
      "epoch": 1.1358536585365853,
      "grad_norm": 0.15324142575263977,
      "learning_rate": 3.430763780037718e-05,
      "loss": 0.0232,
      "step": 4657
    },
    {
      "epoch": 1.1360975609756099,
      "grad_norm": 0.10375463962554932,
      "learning_rate": 3.430171070540543e-05,
      "loss": 0.0208,
      "step": 4658
    },
    {
      "epoch": 1.136341463414634,
      "grad_norm": 0.1850423812866211,
      "learning_rate": 3.4295783003525525e-05,
      "loss": 0.0279,
      "step": 4659
    },
    {
      "epoch": 1.1365853658536587,
      "grad_norm": 0.11773441731929779,
      "learning_rate": 3.428985469512425e-05,
      "loss": 0.0308,
      "step": 4660
    },
    {
      "epoch": 1.1368292682926828,
      "grad_norm": 0.07973496615886688,
      "learning_rate": 3.42839257805884e-05,
      "loss": 0.0135,
      "step": 4661
    },
    {
      "epoch": 1.1370731707317074,
      "grad_norm": 0.07246049493551254,
      "learning_rate": 3.427799626030479e-05,
      "loss": 0.0148,
      "step": 4662
    },
    {
      "epoch": 1.1373170731707316,
      "grad_norm": 0.09639610350131989,
      "learning_rate": 3.427206613466034e-05,
      "loss": 0.0167,
      "step": 4663
    },
    {
      "epoch": 1.1375609756097562,
      "grad_norm": 0.09129960834980011,
      "learning_rate": 3.4266135404041956e-05,
      "loss": 0.0187,
      "step": 4664
    },
    {
      "epoch": 1.1378048780487804,
      "grad_norm": 0.08972885459661484,
      "learning_rate": 3.426020406883661e-05,
      "loss": 0.0295,
      "step": 4665
    },
    {
      "epoch": 1.138048780487805,
      "grad_norm": 0.13622529804706573,
      "learning_rate": 3.425427212943129e-05,
      "loss": 0.0235,
      "step": 4666
    },
    {
      "epoch": 1.1382926829268292,
      "grad_norm": 0.09382688254117966,
      "learning_rate": 3.424833958621304e-05,
      "loss": 0.0174,
      "step": 4667
    },
    {
      "epoch": 1.1385365853658538,
      "grad_norm": 0.07835439592599869,
      "learning_rate": 3.4242406439568946e-05,
      "loss": 0.0158,
      "step": 4668
    },
    {
      "epoch": 1.138780487804878,
      "grad_norm": 0.09395268559455872,
      "learning_rate": 3.4236472689886136e-05,
      "loss": 0.0177,
      "step": 4669
    },
    {
      "epoch": 1.1390243902439026,
      "grad_norm": 0.11172936856746674,
      "learning_rate": 3.423053833755175e-05,
      "loss": 0.0308,
      "step": 4670
    },
    {
      "epoch": 1.1392682926829267,
      "grad_norm": 0.09292076528072357,
      "learning_rate": 3.422460338295299e-05,
      "loss": 0.0284,
      "step": 4671
    },
    {
      "epoch": 1.1395121951219513,
      "grad_norm": 0.2877705991268158,
      "learning_rate": 3.4218667826477106e-05,
      "loss": 0.0255,
      "step": 4672
    },
    {
      "epoch": 1.1397560975609755,
      "grad_norm": 0.16521596908569336,
      "learning_rate": 3.421273166851137e-05,
      "loss": 0.0384,
      "step": 4673
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 0.07892969995737076,
      "learning_rate": 3.420679490944309e-05,
      "loss": 0.0123,
      "step": 4674
    },
    {
      "epoch": 1.1402439024390243,
      "grad_norm": 0.08795387297868729,
      "learning_rate": 3.4200857549659624e-05,
      "loss": 0.0158,
      "step": 4675
    },
    {
      "epoch": 1.140487804878049,
      "grad_norm": 0.1013692319393158,
      "learning_rate": 3.4194919589548374e-05,
      "loss": 0.0192,
      "step": 4676
    },
    {
      "epoch": 1.140731707317073,
      "grad_norm": 0.19318611919879913,
      "learning_rate": 3.418898102949677e-05,
      "loss": 0.0234,
      "step": 4677
    },
    {
      "epoch": 1.1409756097560975,
      "grad_norm": 0.3587459623813629,
      "learning_rate": 3.418304186989229e-05,
      "loss": 0.037,
      "step": 4678
    },
    {
      "epoch": 1.1412195121951219,
      "grad_norm": 0.2818777859210968,
      "learning_rate": 3.4177102111122436e-05,
      "loss": 0.0248,
      "step": 4679
    },
    {
      "epoch": 1.1414634146341462,
      "grad_norm": 0.08149918168783188,
      "learning_rate": 3.4171161753574756e-05,
      "loss": 0.0158,
      "step": 4680
    },
    {
      "epoch": 1.1417073170731706,
      "grad_norm": 0.07359446585178375,
      "learning_rate": 3.416522079763685e-05,
      "loss": 0.0186,
      "step": 4681
    },
    {
      "epoch": 1.141951219512195,
      "grad_norm": 0.10174159705638885,
      "learning_rate": 3.4159279243696345e-05,
      "loss": 0.0148,
      "step": 4682
    },
    {
      "epoch": 1.1421951219512194,
      "grad_norm": 0.08068497478961945,
      "learning_rate": 3.4153337092140906e-05,
      "loss": 0.0201,
      "step": 4683
    },
    {
      "epoch": 1.1424390243902438,
      "grad_norm": 0.054644402116537094,
      "learning_rate": 3.4147394343358244e-05,
      "loss": 0.0091,
      "step": 4684
    },
    {
      "epoch": 1.1426829268292682,
      "grad_norm": 0.11091943830251694,
      "learning_rate": 3.414145099773611e-05,
      "loss": 0.0211,
      "step": 4685
    },
    {
      "epoch": 1.1429268292682926,
      "grad_norm": 0.08003820478916168,
      "learning_rate": 3.413550705566229e-05,
      "loss": 0.0163,
      "step": 4686
    },
    {
      "epoch": 1.143170731707317,
      "grad_norm": 0.09826117008924484,
      "learning_rate": 3.412956251752458e-05,
      "loss": 0.0247,
      "step": 4687
    },
    {
      "epoch": 1.1434146341463414,
      "grad_norm": 0.06660887598991394,
      "learning_rate": 3.412361738371087e-05,
      "loss": 0.0099,
      "step": 4688
    },
    {
      "epoch": 1.1436585365853658,
      "grad_norm": 0.12818261981010437,
      "learning_rate": 3.4117671654609065e-05,
      "loss": 0.0297,
      "step": 4689
    },
    {
      "epoch": 1.1439024390243901,
      "grad_norm": 0.18690790235996246,
      "learning_rate": 3.411172533060709e-05,
      "loss": 0.0355,
      "step": 4690
    },
    {
      "epoch": 1.1441463414634145,
      "grad_norm": 0.08894473314285278,
      "learning_rate": 3.4105778412092934e-05,
      "loss": 0.0159,
      "step": 4691
    },
    {
      "epoch": 1.144390243902439,
      "grad_norm": 0.14678458869457245,
      "learning_rate": 3.409983089945461e-05,
      "loss": 0.017,
      "step": 4692
    },
    {
      "epoch": 1.1446341463414633,
      "grad_norm": 0.11169057339429855,
      "learning_rate": 3.409388279308017e-05,
      "loss": 0.0334,
      "step": 4693
    },
    {
      "epoch": 1.1448780487804877,
      "grad_norm": 0.06911618262529373,
      "learning_rate": 3.4087934093357724e-05,
      "loss": 0.0101,
      "step": 4694
    },
    {
      "epoch": 1.145121951219512,
      "grad_norm": 0.2002909779548645,
      "learning_rate": 3.40819848006754e-05,
      "loss": 0.0128,
      "step": 4695
    },
    {
      "epoch": 1.1453658536585365,
      "grad_norm": 0.0863083004951477,
      "learning_rate": 3.4076034915421374e-05,
      "loss": 0.0192,
      "step": 4696
    },
    {
      "epoch": 1.1456097560975609,
      "grad_norm": 0.10491859912872314,
      "learning_rate": 3.4070084437983844e-05,
      "loss": 0.029,
      "step": 4697
    },
    {
      "epoch": 1.1458536585365853,
      "grad_norm": 0.21227441728115082,
      "learning_rate": 3.406413336875108e-05,
      "loss": 0.0323,
      "step": 4698
    },
    {
      "epoch": 1.1460975609756097,
      "grad_norm": 0.07443507015705109,
      "learning_rate": 3.405818170811136e-05,
      "loss": 0.0221,
      "step": 4699
    },
    {
      "epoch": 1.146341463414634,
      "grad_norm": 0.09062928706407547,
      "learning_rate": 3.405222945645301e-05,
      "loss": 0.0154,
      "step": 4700
    },
    {
      "epoch": 1.1465853658536584,
      "grad_norm": 0.18973997235298157,
      "learning_rate": 3.4046276614164394e-05,
      "loss": 0.0149,
      "step": 4701
    },
    {
      "epoch": 1.1468292682926828,
      "grad_norm": 0.15941165387630463,
      "learning_rate": 3.404032318163394e-05,
      "loss": 0.0387,
      "step": 4702
    },
    {
      "epoch": 1.1470731707317072,
      "grad_norm": 0.09865579009056091,
      "learning_rate": 3.4034369159250064e-05,
      "loss": 0.0395,
      "step": 4703
    },
    {
      "epoch": 1.1473170731707316,
      "grad_norm": 0.060743242502212524,
      "learning_rate": 3.4028414547401255e-05,
      "loss": 0.0168,
      "step": 4704
    },
    {
      "epoch": 1.147560975609756,
      "grad_norm": 0.06609484553337097,
      "learning_rate": 3.402245934647603e-05,
      "loss": 0.0166,
      "step": 4705
    },
    {
      "epoch": 1.1478048780487804,
      "grad_norm": 0.19926856458187103,
      "learning_rate": 3.4016503556862956e-05,
      "loss": 0.0329,
      "step": 4706
    },
    {
      "epoch": 1.1480487804878048,
      "grad_norm": 0.13716882467269897,
      "learning_rate": 3.4010547178950636e-05,
      "loss": 0.0215,
      "step": 4707
    },
    {
      "epoch": 1.1482926829268292,
      "grad_norm": 0.10699879378080368,
      "learning_rate": 3.400459021312769e-05,
      "loss": 0.0242,
      "step": 4708
    },
    {
      "epoch": 1.1485365853658536,
      "grad_norm": 0.05750776827335358,
      "learning_rate": 3.39986326597828e-05,
      "loss": 0.0109,
      "step": 4709
    },
    {
      "epoch": 1.148780487804878,
      "grad_norm": 0.05968991667032242,
      "learning_rate": 3.399267451930467e-05,
      "loss": 0.0215,
      "step": 4710
    },
    {
      "epoch": 1.1490243902439023,
      "grad_norm": 0.12132014334201813,
      "learning_rate": 3.398671579208207e-05,
      "loss": 0.021,
      "step": 4711
    },
    {
      "epoch": 1.1492682926829267,
      "grad_norm": 0.2005496472120285,
      "learning_rate": 3.398075647850377e-05,
      "loss": 0.0321,
      "step": 4712
    },
    {
      "epoch": 1.1495121951219511,
      "grad_norm": 0.16296841204166412,
      "learning_rate": 3.3974796578958594e-05,
      "loss": 0.0412,
      "step": 4713
    },
    {
      "epoch": 1.1497560975609755,
      "grad_norm": 0.06396570801734924,
      "learning_rate": 3.396883609383542e-05,
      "loss": 0.0111,
      "step": 4714
    },
    {
      "epoch": 1.15,
      "grad_norm": 0.13008363544940948,
      "learning_rate": 3.396287502352315e-05,
      "loss": 0.0307,
      "step": 4715
    },
    {
      "epoch": 1.1502439024390243,
      "grad_norm": 0.0755808874964714,
      "learning_rate": 3.395691336841073e-05,
      "loss": 0.0268,
      "step": 4716
    },
    {
      "epoch": 1.1504878048780487,
      "grad_norm": 0.24464456737041473,
      "learning_rate": 3.3950951128887115e-05,
      "loss": 0.0152,
      "step": 4717
    },
    {
      "epoch": 1.150731707317073,
      "grad_norm": 0.10341711342334747,
      "learning_rate": 3.394498830534135e-05,
      "loss": 0.018,
      "step": 4718
    },
    {
      "epoch": 1.1509756097560975,
      "grad_norm": 0.17392075061798096,
      "learning_rate": 3.3939024898162486e-05,
      "loss": 0.0307,
      "step": 4719
    },
    {
      "epoch": 1.1512195121951219,
      "grad_norm": 0.18849289417266846,
      "learning_rate": 3.393306090773961e-05,
      "loss": 0.0437,
      "step": 4720
    },
    {
      "epoch": 1.1514634146341463,
      "grad_norm": 0.13302108645439148,
      "learning_rate": 3.392709633446185e-05,
      "loss": 0.037,
      "step": 4721
    },
    {
      "epoch": 1.1517073170731706,
      "grad_norm": 0.11496183276176453,
      "learning_rate": 3.392113117871838e-05,
      "loss": 0.0196,
      "step": 4722
    },
    {
      "epoch": 1.151951219512195,
      "grad_norm": 0.14288896322250366,
      "learning_rate": 3.3915165440898424e-05,
      "loss": 0.0215,
      "step": 4723
    },
    {
      "epoch": 1.1521951219512194,
      "grad_norm": 0.14840462803840637,
      "learning_rate": 3.390919912139121e-05,
      "loss": 0.026,
      "step": 4724
    },
    {
      "epoch": 1.1524390243902438,
      "grad_norm": 0.12975186109542847,
      "learning_rate": 3.3903232220586026e-05,
      "loss": 0.022,
      "step": 4725
    },
    {
      "epoch": 1.1526829268292682,
      "grad_norm": 0.1933814287185669,
      "learning_rate": 3.3897264738872186e-05,
      "loss": 0.0278,
      "step": 4726
    },
    {
      "epoch": 1.1529268292682926,
      "grad_norm": 0.1063145324587822,
      "learning_rate": 3.389129667663907e-05,
      "loss": 0.0248,
      "step": 4727
    },
    {
      "epoch": 1.153170731707317,
      "grad_norm": 0.1310100108385086,
      "learning_rate": 3.3885328034276065e-05,
      "loss": 0.0208,
      "step": 4728
    },
    {
      "epoch": 1.1534146341463414,
      "grad_norm": 0.05850279703736305,
      "learning_rate": 3.38793588121726e-05,
      "loss": 0.0109,
      "step": 4729
    },
    {
      "epoch": 1.1536585365853658,
      "grad_norm": 0.124418243765831,
      "learning_rate": 3.387338901071816e-05,
      "loss": 0.0132,
      "step": 4730
    },
    {
      "epoch": 1.1539024390243902,
      "grad_norm": 0.15265192091464996,
      "learning_rate": 3.386741863030225e-05,
      "loss": 0.0147,
      "step": 4731
    },
    {
      "epoch": 1.1541463414634145,
      "grad_norm": 0.14255483448505402,
      "learning_rate": 3.386144767131442e-05,
      "loss": 0.0304,
      "step": 4732
    },
    {
      "epoch": 1.154390243902439,
      "grad_norm": 0.07987738400697708,
      "learning_rate": 3.385547613414425e-05,
      "loss": 0.014,
      "step": 4733
    },
    {
      "epoch": 1.1546341463414633,
      "grad_norm": 0.1503051370382309,
      "learning_rate": 3.384950401918138e-05,
      "loss": 0.0241,
      "step": 4734
    },
    {
      "epoch": 1.1548780487804877,
      "grad_norm": 0.15200451016426086,
      "learning_rate": 3.3843531326815466e-05,
      "loss": 0.023,
      "step": 4735
    },
    {
      "epoch": 1.155121951219512,
      "grad_norm": 0.17752216756343842,
      "learning_rate": 3.3837558057436194e-05,
      "loss": 0.0191,
      "step": 4736
    },
    {
      "epoch": 1.1553658536585365,
      "grad_norm": 0.12741468846797943,
      "learning_rate": 3.383158421143333e-05,
      "loss": 0.0215,
      "step": 4737
    },
    {
      "epoch": 1.155609756097561,
      "grad_norm": 0.19256842136383057,
      "learning_rate": 3.3825609789196624e-05,
      "loss": 0.0292,
      "step": 4738
    },
    {
      "epoch": 1.1558536585365853,
      "grad_norm": 0.07707680761814117,
      "learning_rate": 3.381963479111589e-05,
      "loss": 0.0179,
      "step": 4739
    },
    {
      "epoch": 1.1560975609756097,
      "grad_norm": 0.14425045251846313,
      "learning_rate": 3.3813659217580984e-05,
      "loss": 0.0229,
      "step": 4740
    },
    {
      "epoch": 1.156341463414634,
      "grad_norm": 0.1649632602930069,
      "learning_rate": 3.380768306898181e-05,
      "loss": 0.0242,
      "step": 4741
    },
    {
      "epoch": 1.1565853658536585,
      "grad_norm": 0.25670769810676575,
      "learning_rate": 3.3801706345708266e-05,
      "loss": 0.0176,
      "step": 4742
    },
    {
      "epoch": 1.1568292682926828,
      "grad_norm": 0.13386355340480804,
      "learning_rate": 3.379572904815033e-05,
      "loss": 0.0156,
      "step": 4743
    },
    {
      "epoch": 1.1570731707317072,
      "grad_norm": 0.13928982615470886,
      "learning_rate": 3.378975117669801e-05,
      "loss": 0.0131,
      "step": 4744
    },
    {
      "epoch": 1.1573170731707316,
      "grad_norm": 0.11628074944019318,
      "learning_rate": 3.3783772731741324e-05,
      "loss": 0.0196,
      "step": 4745
    },
    {
      "epoch": 1.157560975609756,
      "grad_norm": 0.1573915034532547,
      "learning_rate": 3.3777793713670355e-05,
      "loss": 0.0244,
      "step": 4746
    },
    {
      "epoch": 1.1578048780487804,
      "grad_norm": 0.15860556066036224,
      "learning_rate": 3.377181412287522e-05,
      "loss": 0.0294,
      "step": 4747
    },
    {
      "epoch": 1.1580487804878048,
      "grad_norm": 0.08115390688180923,
      "learning_rate": 3.376583395974607e-05,
      "loss": 0.0073,
      "step": 4748
    },
    {
      "epoch": 1.1582926829268292,
      "grad_norm": 0.14054811000823975,
      "learning_rate": 3.375985322467309e-05,
      "loss": 0.0206,
      "step": 4749
    },
    {
      "epoch": 1.1585365853658536,
      "grad_norm": 0.058782998472452164,
      "learning_rate": 3.37538719180465e-05,
      "loss": 0.0119,
      "step": 4750
    },
    {
      "epoch": 1.158780487804878,
      "grad_norm": 0.12629950046539307,
      "learning_rate": 3.3747890040256564e-05,
      "loss": 0.0127,
      "step": 4751
    },
    {
      "epoch": 1.1590243902439024,
      "grad_norm": 0.11490070074796677,
      "learning_rate": 3.374190759169359e-05,
      "loss": 0.0266,
      "step": 4752
    },
    {
      "epoch": 1.1592682926829267,
      "grad_norm": 0.11519743502140045,
      "learning_rate": 3.373592457274789e-05,
      "loss": 0.0163,
      "step": 4753
    },
    {
      "epoch": 1.1595121951219511,
      "grad_norm": 0.07130839675664902,
      "learning_rate": 3.3729940983809876e-05,
      "loss": 0.0206,
      "step": 4754
    },
    {
      "epoch": 1.1597560975609755,
      "grad_norm": 0.11108828336000443,
      "learning_rate": 3.372395682526993e-05,
      "loss": 0.0228,
      "step": 4755
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.11713391542434692,
      "learning_rate": 3.3717972097518505e-05,
      "loss": 0.0132,
      "step": 4756
    },
    {
      "epoch": 1.1602439024390243,
      "grad_norm": 0.1347004622220993,
      "learning_rate": 3.371198680094608e-05,
      "loss": 0.0172,
      "step": 4757
    },
    {
      "epoch": 1.1604878048780487,
      "grad_norm": 0.30844494700431824,
      "learning_rate": 3.37060009359432e-05,
      "loss": 0.0324,
      "step": 4758
    },
    {
      "epoch": 1.160731707317073,
      "grad_norm": 0.14201359450817108,
      "learning_rate": 3.370001450290041e-05,
      "loss": 0.0223,
      "step": 4759
    },
    {
      "epoch": 1.1609756097560975,
      "grad_norm": 0.2202191948890686,
      "learning_rate": 3.369402750220831e-05,
      "loss": 0.0308,
      "step": 4760
    },
    {
      "epoch": 1.1612195121951219,
      "grad_norm": 0.11770404875278473,
      "learning_rate": 3.368803993425752e-05,
      "loss": 0.0192,
      "step": 4761
    },
    {
      "epoch": 1.1614634146341463,
      "grad_norm": 0.1236095130443573,
      "learning_rate": 3.368205179943873e-05,
      "loss": 0.0294,
      "step": 4762
    },
    {
      "epoch": 1.1617073170731707,
      "grad_norm": 0.33175039291381836,
      "learning_rate": 3.367606309814263e-05,
      "loss": 0.0307,
      "step": 4763
    },
    {
      "epoch": 1.161951219512195,
      "grad_norm": 0.11187206953763962,
      "learning_rate": 3.3670073830759975e-05,
      "loss": 0.0183,
      "step": 4764
    },
    {
      "epoch": 1.1621951219512194,
      "grad_norm": 0.14771109819412231,
      "learning_rate": 3.366408399768155e-05,
      "loss": 0.023,
      "step": 4765
    },
    {
      "epoch": 1.1624390243902438,
      "grad_norm": 0.0909418985247612,
      "learning_rate": 3.365809359929816e-05,
      "loss": 0.0141,
      "step": 4766
    },
    {
      "epoch": 1.1626829268292682,
      "grad_norm": 0.18751399219036102,
      "learning_rate": 3.365210263600067e-05,
      "loss": 0.021,
      "step": 4767
    },
    {
      "epoch": 1.1629268292682926,
      "grad_norm": 0.09124726802110672,
      "learning_rate": 3.364611110817997e-05,
      "loss": 0.0211,
      "step": 4768
    },
    {
      "epoch": 1.163170731707317,
      "grad_norm": 0.0717986673116684,
      "learning_rate": 3.364011901622699e-05,
      "loss": 0.0179,
      "step": 4769
    },
    {
      "epoch": 1.1634146341463414,
      "grad_norm": 0.13189122080802917,
      "learning_rate": 3.363412636053269e-05,
      "loss": 0.026,
      "step": 4770
    },
    {
      "epoch": 1.1636585365853658,
      "grad_norm": 0.08212457597255707,
      "learning_rate": 3.362813314148808e-05,
      "loss": 0.0123,
      "step": 4771
    },
    {
      "epoch": 1.1639024390243902,
      "grad_norm": 0.10662347078323364,
      "learning_rate": 3.36221393594842e-05,
      "loss": 0.0217,
      "step": 4772
    },
    {
      "epoch": 1.1641463414634146,
      "grad_norm": 0.1485774964094162,
      "learning_rate": 3.361614501491212e-05,
      "loss": 0.0166,
      "step": 4773
    },
    {
      "epoch": 1.164390243902439,
      "grad_norm": 0.1165119931101799,
      "learning_rate": 3.3610150108162945e-05,
      "loss": 0.0194,
      "step": 4774
    },
    {
      "epoch": 1.1646341463414633,
      "grad_norm": 0.11517035961151123,
      "learning_rate": 3.3604154639627836e-05,
      "loss": 0.0214,
      "step": 4775
    },
    {
      "epoch": 1.1648780487804877,
      "grad_norm": 0.12181931734085083,
      "learning_rate": 3.359815860969798e-05,
      "loss": 0.0173,
      "step": 4776
    },
    {
      "epoch": 1.1651219512195121,
      "grad_norm": 0.09434300661087036,
      "learning_rate": 3.359216201876461e-05,
      "loss": 0.0113,
      "step": 4777
    },
    {
      "epoch": 1.1653658536585365,
      "grad_norm": 0.14836476743221283,
      "learning_rate": 3.358616486721895e-05,
      "loss": 0.0347,
      "step": 4778
    },
    {
      "epoch": 1.165609756097561,
      "grad_norm": 0.10046789050102234,
      "learning_rate": 3.358016715545233e-05,
      "loss": 0.0203,
      "step": 4779
    },
    {
      "epoch": 1.1658536585365853,
      "grad_norm": 0.13462626934051514,
      "learning_rate": 3.357416888385606e-05,
      "loss": 0.0243,
      "step": 4780
    },
    {
      "epoch": 1.1660975609756097,
      "grad_norm": 0.09814170747995377,
      "learning_rate": 3.356817005282152e-05,
      "loss": 0.0123,
      "step": 4781
    },
    {
      "epoch": 1.166341463414634,
      "grad_norm": 0.09333042055368423,
      "learning_rate": 3.356217066274012e-05,
      "loss": 0.0295,
      "step": 4782
    },
    {
      "epoch": 1.1665853658536585,
      "grad_norm": 0.09700442105531693,
      "learning_rate": 3.35561707140033e-05,
      "loss": 0.0134,
      "step": 4783
    },
    {
      "epoch": 1.1668292682926829,
      "grad_norm": 0.11852730810642242,
      "learning_rate": 3.355017020700252e-05,
      "loss": 0.0232,
      "step": 4784
    },
    {
      "epoch": 1.1670731707317072,
      "grad_norm": 0.11662374436855316,
      "learning_rate": 3.3544169142129315e-05,
      "loss": 0.0244,
      "step": 4785
    },
    {
      "epoch": 1.1673170731707316,
      "grad_norm": 0.2813461422920227,
      "learning_rate": 3.353816751977523e-05,
      "loss": 0.0186,
      "step": 4786
    },
    {
      "epoch": 1.167560975609756,
      "grad_norm": 0.07985343784093857,
      "learning_rate": 3.3532165340331856e-05,
      "loss": 0.0166,
      "step": 4787
    },
    {
      "epoch": 1.1678048780487804,
      "grad_norm": 0.0907323807477951,
      "learning_rate": 3.352616260419081e-05,
      "loss": 0.0271,
      "step": 4788
    },
    {
      "epoch": 1.1680487804878048,
      "grad_norm": 0.1975725293159485,
      "learning_rate": 3.3520159311743747e-05,
      "loss": 0.0137,
      "step": 4789
    },
    {
      "epoch": 1.1682926829268292,
      "grad_norm": 0.060708578675985336,
      "learning_rate": 3.351415546338238e-05,
      "loss": 0.0132,
      "step": 4790
    },
    {
      "epoch": 1.1685365853658536,
      "grad_norm": 0.10275215655565262,
      "learning_rate": 3.350815105949844e-05,
      "loss": 0.0171,
      "step": 4791
    },
    {
      "epoch": 1.168780487804878,
      "grad_norm": 0.10517030209302902,
      "learning_rate": 3.350214610048368e-05,
      "loss": 0.0265,
      "step": 4792
    },
    {
      "epoch": 1.1690243902439024,
      "grad_norm": 0.11938591301441193,
      "learning_rate": 3.349614058672991e-05,
      "loss": 0.0344,
      "step": 4793
    },
    {
      "epoch": 1.1692682926829268,
      "grad_norm": 0.11703953891992569,
      "learning_rate": 3.349013451862898e-05,
      "loss": 0.0127,
      "step": 4794
    },
    {
      "epoch": 1.1695121951219511,
      "grad_norm": 0.08696294575929642,
      "learning_rate": 3.348412789657277e-05,
      "loss": 0.022,
      "step": 4795
    },
    {
      "epoch": 1.1697560975609755,
      "grad_norm": 0.16452614963054657,
      "learning_rate": 3.347812072095318e-05,
      "loss": 0.0322,
      "step": 4796
    },
    {
      "epoch": 1.17,
      "grad_norm": 0.14843176305294037,
      "learning_rate": 3.347211299216218e-05,
      "loss": 0.0289,
      "step": 4797
    },
    {
      "epoch": 1.1702439024390243,
      "grad_norm": 0.40516185760498047,
      "learning_rate": 3.346610471059172e-05,
      "loss": 0.0307,
      "step": 4798
    },
    {
      "epoch": 1.1704878048780487,
      "grad_norm": 0.16213001310825348,
      "learning_rate": 3.346009587663386e-05,
      "loss": 0.0231,
      "step": 4799
    },
    {
      "epoch": 1.170731707317073,
      "grad_norm": 0.1368832290172577,
      "learning_rate": 3.345408649068064e-05,
      "loss": 0.02,
      "step": 4800
    },
    {
      "epoch": 1.1709756097560975,
      "grad_norm": 0.05185279622673988,
      "learning_rate": 3.3448076553124167e-05,
      "loss": 0.014,
      "step": 4801
    },
    {
      "epoch": 1.1712195121951219,
      "grad_norm": 0.09518261253833771,
      "learning_rate": 3.344206606435654e-05,
      "loss": 0.0178,
      "step": 4802
    },
    {
      "epoch": 1.1714634146341463,
      "grad_norm": 0.24929602444171906,
      "learning_rate": 3.343605502476996e-05,
      "loss": 0.0218,
      "step": 4803
    },
    {
      "epoch": 1.1717073170731707,
      "grad_norm": 0.07153580337762833,
      "learning_rate": 3.3430043434756615e-05,
      "loss": 0.0177,
      "step": 4804
    },
    {
      "epoch": 1.171951219512195,
      "grad_norm": 0.08210065215826035,
      "learning_rate": 3.342403129470874e-05,
      "loss": 0.0196,
      "step": 4805
    },
    {
      "epoch": 1.1721951219512194,
      "grad_norm": 0.10390174388885498,
      "learning_rate": 3.3418018605018606e-05,
      "loss": 0.0264,
      "step": 4806
    },
    {
      "epoch": 1.1724390243902438,
      "grad_norm": 0.1252322942018509,
      "learning_rate": 3.341200536607853e-05,
      "loss": 0.0249,
      "step": 4807
    },
    {
      "epoch": 1.1726829268292682,
      "grad_norm": 0.13250233232975006,
      "learning_rate": 3.340599157828086e-05,
      "loss": 0.0227,
      "step": 4808
    },
    {
      "epoch": 1.1729268292682926,
      "grad_norm": 0.23968738317489624,
      "learning_rate": 3.339997724201797e-05,
      "loss": 0.0378,
      "step": 4809
    },
    {
      "epoch": 1.173170731707317,
      "grad_norm": 0.10317408293485641,
      "learning_rate": 3.339396235768228e-05,
      "loss": 0.0221,
      "step": 4810
    },
    {
      "epoch": 1.1734146341463414,
      "grad_norm": 0.09667062014341354,
      "learning_rate": 3.3387946925666246e-05,
      "loss": 0.0267,
      "step": 4811
    },
    {
      "epoch": 1.1736585365853658,
      "grad_norm": 0.092880018055439,
      "learning_rate": 3.338193094636235e-05,
      "loss": 0.0153,
      "step": 4812
    },
    {
      "epoch": 1.1739024390243902,
      "grad_norm": 0.06259121745824814,
      "learning_rate": 3.337591442016312e-05,
      "loss": 0.0206,
      "step": 4813
    },
    {
      "epoch": 1.1741463414634146,
      "grad_norm": 0.19300822913646698,
      "learning_rate": 3.336989734746111e-05,
      "loss": 0.0243,
      "step": 4814
    },
    {
      "epoch": 1.174390243902439,
      "grad_norm": 0.08649499714374542,
      "learning_rate": 3.336387972864893e-05,
      "loss": 0.017,
      "step": 4815
    },
    {
      "epoch": 1.1746341463414633,
      "grad_norm": 0.1371101588010788,
      "learning_rate": 3.3357861564119194e-05,
      "loss": 0.0178,
      "step": 4816
    },
    {
      "epoch": 1.1748780487804877,
      "grad_norm": 0.07483546435832977,
      "learning_rate": 3.3351842854264586e-05,
      "loss": 0.0264,
      "step": 4817
    },
    {
      "epoch": 1.1751219512195121,
      "grad_norm": 0.07854161411523819,
      "learning_rate": 3.33458235994778e-05,
      "loss": 0.0128,
      "step": 4818
    },
    {
      "epoch": 1.1753658536585365,
      "grad_norm": 0.16323290765285492,
      "learning_rate": 3.333980380015156e-05,
      "loss": 0.0384,
      "step": 4819
    },
    {
      "epoch": 1.175609756097561,
      "grad_norm": 0.2773546576499939,
      "learning_rate": 3.3333783456678657e-05,
      "loss": 0.0259,
      "step": 4820
    },
    {
      "epoch": 1.1758536585365853,
      "grad_norm": 0.12892518937587738,
      "learning_rate": 3.33277625694519e-05,
      "loss": 0.0288,
      "step": 4821
    },
    {
      "epoch": 1.1760975609756097,
      "grad_norm": 0.10153713822364807,
      "learning_rate": 3.332174113886413e-05,
      "loss": 0.0189,
      "step": 4822
    },
    {
      "epoch": 1.176341463414634,
      "grad_norm": 0.1301109343767166,
      "learning_rate": 3.3315719165308226e-05,
      "loss": 0.0226,
      "step": 4823
    },
    {
      "epoch": 1.1765853658536585,
      "grad_norm": 0.09012263268232346,
      "learning_rate": 3.330969664917709e-05,
      "loss": 0.0196,
      "step": 4824
    },
    {
      "epoch": 1.1768292682926829,
      "grad_norm": 0.11424625664949417,
      "learning_rate": 3.3303673590863706e-05,
      "loss": 0.0171,
      "step": 4825
    },
    {
      "epoch": 1.1770731707317073,
      "grad_norm": 0.043593451380729675,
      "learning_rate": 3.3297649990761033e-05,
      "loss": 0.0172,
      "step": 4826
    },
    {
      "epoch": 1.1773170731707316,
      "grad_norm": 0.22006617486476898,
      "learning_rate": 3.32916258492621e-05,
      "loss": 0.0247,
      "step": 4827
    },
    {
      "epoch": 1.177560975609756,
      "grad_norm": 0.10503460466861725,
      "learning_rate": 3.328560116675996e-05,
      "loss": 0.025,
      "step": 4828
    },
    {
      "epoch": 1.1778048780487804,
      "grad_norm": 0.08210780471563339,
      "learning_rate": 3.32795759436477e-05,
      "loss": 0.0135,
      "step": 4829
    },
    {
      "epoch": 1.1780487804878048,
      "grad_norm": 0.11124472320079803,
      "learning_rate": 3.327355018031847e-05,
      "loss": 0.0235,
      "step": 4830
    },
    {
      "epoch": 1.1782926829268292,
      "grad_norm": 0.16025593876838684,
      "learning_rate": 3.3267523877165415e-05,
      "loss": 0.0205,
      "step": 4831
    },
    {
      "epoch": 1.1785365853658536,
      "grad_norm": 0.4304712414741516,
      "learning_rate": 3.326149703458173e-05,
      "loss": 0.0329,
      "step": 4832
    },
    {
      "epoch": 1.178780487804878,
      "grad_norm": 0.1381624937057495,
      "learning_rate": 3.3255469652960666e-05,
      "loss": 0.0226,
      "step": 4833
    },
    {
      "epoch": 1.1790243902439024,
      "grad_norm": 0.08622123301029205,
      "learning_rate": 3.324944173269547e-05,
      "loss": 0.0153,
      "step": 4834
    },
    {
      "epoch": 1.1792682926829268,
      "grad_norm": 0.10358551889657974,
      "learning_rate": 3.324341327417946e-05,
      "loss": 0.0143,
      "step": 4835
    },
    {
      "epoch": 1.1795121951219512,
      "grad_norm": 0.08706490695476532,
      "learning_rate": 3.323738427780597e-05,
      "loss": 0.0161,
      "step": 4836
    },
    {
      "epoch": 1.1797560975609755,
      "grad_norm": 0.12138567119836807,
      "learning_rate": 3.323135474396836e-05,
      "loss": 0.0292,
      "step": 4837
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.1277776062488556,
      "learning_rate": 3.322532467306007e-05,
      "loss": 0.0313,
      "step": 4838
    },
    {
      "epoch": 1.1802439024390243,
      "grad_norm": 0.07682451605796814,
      "learning_rate": 3.3219294065474514e-05,
      "loss": 0.0151,
      "step": 4839
    },
    {
      "epoch": 1.1804878048780487,
      "grad_norm": 0.08142733573913574,
      "learning_rate": 3.3213262921605184e-05,
      "loss": 0.016,
      "step": 4840
    },
    {
      "epoch": 1.180731707317073,
      "grad_norm": 0.12875403463840485,
      "learning_rate": 3.3207231241845593e-05,
      "loss": 0.0231,
      "step": 4841
    },
    {
      "epoch": 1.1809756097560975,
      "grad_norm": 0.21351811289787292,
      "learning_rate": 3.320119902658929e-05,
      "loss": 0.0279,
      "step": 4842
    },
    {
      "epoch": 1.181219512195122,
      "grad_norm": 0.07879985868930817,
      "learning_rate": 3.3195166276229855e-05,
      "loss": 0.0202,
      "step": 4843
    },
    {
      "epoch": 1.1814634146341463,
      "grad_norm": 0.10148505866527557,
      "learning_rate": 3.318913299116091e-05,
      "loss": 0.0148,
      "step": 4844
    },
    {
      "epoch": 1.1817073170731707,
      "grad_norm": 0.10673940926790237,
      "learning_rate": 3.31830991717761e-05,
      "loss": 0.0108,
      "step": 4845
    },
    {
      "epoch": 1.181951219512195,
      "grad_norm": 0.1536702662706375,
      "learning_rate": 3.3177064818469125e-05,
      "loss": 0.0146,
      "step": 4846
    },
    {
      "epoch": 1.1821951219512195,
      "grad_norm": 0.14858923852443695,
      "learning_rate": 3.31710299316337e-05,
      "loss": 0.0273,
      "step": 4847
    },
    {
      "epoch": 1.1824390243902438,
      "grad_norm": 0.06992397457361221,
      "learning_rate": 3.3164994511663596e-05,
      "loss": 0.014,
      "step": 4848
    },
    {
      "epoch": 1.1826829268292682,
      "grad_norm": 0.10999491810798645,
      "learning_rate": 3.315895855895258e-05,
      "loss": 0.0227,
      "step": 4849
    },
    {
      "epoch": 1.1829268292682926,
      "grad_norm": 0.08194861561059952,
      "learning_rate": 3.315292207389451e-05,
      "loss": 0.0151,
      "step": 4850
    },
    {
      "epoch": 1.183170731707317,
      "grad_norm": 0.11335431039333344,
      "learning_rate": 3.314688505688323e-05,
      "loss": 0.0175,
      "step": 4851
    },
    {
      "epoch": 1.1834146341463414,
      "grad_norm": 0.23295161128044128,
      "learning_rate": 3.314084750831263e-05,
      "loss": 0.0331,
      "step": 4852
    },
    {
      "epoch": 1.1836585365853658,
      "grad_norm": 0.22804129123687744,
      "learning_rate": 3.313480942857666e-05,
      "loss": 0.0174,
      "step": 4853
    },
    {
      "epoch": 1.1839024390243902,
      "grad_norm": 0.2251048982143402,
      "learning_rate": 3.312877081806927e-05,
      "loss": 0.0248,
      "step": 4854
    },
    {
      "epoch": 1.1841463414634146,
      "grad_norm": 0.044401951134204865,
      "learning_rate": 3.3122731677184476e-05,
      "loss": 0.0056,
      "step": 4855
    },
    {
      "epoch": 1.184390243902439,
      "grad_norm": 0.07858338952064514,
      "learning_rate": 3.311669200631631e-05,
      "loss": 0.0147,
      "step": 4856
    },
    {
      "epoch": 1.1846341463414634,
      "grad_norm": 0.23061655461788177,
      "learning_rate": 3.3110651805858825e-05,
      "loss": 0.0135,
      "step": 4857
    },
    {
      "epoch": 1.1848780487804877,
      "grad_norm": 0.08441885560750961,
      "learning_rate": 3.310461107620614e-05,
      "loss": 0.0159,
      "step": 4858
    },
    {
      "epoch": 1.1851219512195121,
      "grad_norm": 0.20095489919185638,
      "learning_rate": 3.309856981775241e-05,
      "loss": 0.0157,
      "step": 4859
    },
    {
      "epoch": 1.1853658536585365,
      "grad_norm": 0.10358618944883347,
      "learning_rate": 3.309252803089178e-05,
      "loss": 0.0228,
      "step": 4860
    },
    {
      "epoch": 1.185609756097561,
      "grad_norm": 0.33211180567741394,
      "learning_rate": 3.3086485716018466e-05,
      "loss": 0.0299,
      "step": 4861
    },
    {
      "epoch": 1.1858536585365853,
      "grad_norm": 0.060395121574401855,
      "learning_rate": 3.308044287352672e-05,
      "loss": 0.0086,
      "step": 4862
    },
    {
      "epoch": 1.1860975609756097,
      "grad_norm": 0.30005306005477905,
      "learning_rate": 3.307439950381081e-05,
      "loss": 0.0223,
      "step": 4863
    },
    {
      "epoch": 1.186341463414634,
      "grad_norm": 0.14024566113948822,
      "learning_rate": 3.306835560726506e-05,
      "loss": 0.0244,
      "step": 4864
    },
    {
      "epoch": 1.1865853658536585,
      "grad_norm": 0.11389777064323425,
      "learning_rate": 3.306231118428379e-05,
      "loss": 0.0123,
      "step": 4865
    },
    {
      "epoch": 1.1868292682926829,
      "grad_norm": 0.1658923178911209,
      "learning_rate": 3.30562662352614e-05,
      "loss": 0.0185,
      "step": 4866
    },
    {
      "epoch": 1.1870731707317073,
      "grad_norm": 0.2424389272928238,
      "learning_rate": 3.305022076059231e-05,
      "loss": 0.0212,
      "step": 4867
    },
    {
      "epoch": 1.1873170731707317,
      "grad_norm": 0.09661141037940979,
      "learning_rate": 3.304417476067096e-05,
      "loss": 0.0258,
      "step": 4868
    },
    {
      "epoch": 1.187560975609756,
      "grad_norm": 0.1165790930390358,
      "learning_rate": 3.3038128235891826e-05,
      "loss": 0.0194,
      "step": 4869
    },
    {
      "epoch": 1.1878048780487804,
      "grad_norm": 0.07776148617267609,
      "learning_rate": 3.3032081186649436e-05,
      "loss": 0.0137,
      "step": 4870
    },
    {
      "epoch": 1.1880487804878048,
      "grad_norm": 0.13485975563526154,
      "learning_rate": 3.302603361333833e-05,
      "loss": 0.0187,
      "step": 4871
    },
    {
      "epoch": 1.1882926829268292,
      "grad_norm": 0.09190888702869415,
      "learning_rate": 3.3019985516353105e-05,
      "loss": 0.0256,
      "step": 4872
    },
    {
      "epoch": 1.1885365853658536,
      "grad_norm": 0.09258801490068436,
      "learning_rate": 3.301393689608838e-05,
      "loss": 0.0257,
      "step": 4873
    },
    {
      "epoch": 1.188780487804878,
      "grad_norm": 0.17428675293922424,
      "learning_rate": 3.300788775293879e-05,
      "loss": 0.0197,
      "step": 4874
    },
    {
      "epoch": 1.1890243902439024,
      "grad_norm": 0.14812028408050537,
      "learning_rate": 3.300183808729906e-05,
      "loss": 0.0227,
      "step": 4875
    },
    {
      "epoch": 1.1892682926829268,
      "grad_norm": 0.12459435313940048,
      "learning_rate": 3.299578789956388e-05,
      "loss": 0.0149,
      "step": 4876
    },
    {
      "epoch": 1.1895121951219512,
      "grad_norm": 0.11979366838932037,
      "learning_rate": 3.2989737190128026e-05,
      "loss": 0.0244,
      "step": 4877
    },
    {
      "epoch": 1.1897560975609756,
      "grad_norm": 0.10076601803302765,
      "learning_rate": 3.298368595938626e-05,
      "loss": 0.0276,
      "step": 4878
    },
    {
      "epoch": 1.19,
      "grad_norm": 0.07878010720014572,
      "learning_rate": 3.2977634207733435e-05,
      "loss": 0.0103,
      "step": 4879
    },
    {
      "epoch": 1.1902439024390243,
      "grad_norm": 0.13209544122219086,
      "learning_rate": 3.2971581935564413e-05,
      "loss": 0.0371,
      "step": 4880
    },
    {
      "epoch": 1.1904878048780487,
      "grad_norm": 0.2593996226787567,
      "learning_rate": 3.296552914327405e-05,
      "loss": 0.0237,
      "step": 4881
    },
    {
      "epoch": 1.1907317073170731,
      "grad_norm": 0.32089531421661377,
      "learning_rate": 3.295947583125731e-05,
      "loss": 0.0401,
      "step": 4882
    },
    {
      "epoch": 1.1909756097560975,
      "grad_norm": 0.1760278344154358,
      "learning_rate": 3.295342199990913e-05,
      "loss": 0.0188,
      "step": 4883
    },
    {
      "epoch": 1.191219512195122,
      "grad_norm": 0.14225473999977112,
      "learning_rate": 3.2947367649624525e-05,
      "loss": 0.0329,
      "step": 4884
    },
    {
      "epoch": 1.1914634146341463,
      "grad_norm": 0.1321190744638443,
      "learning_rate": 3.29413127807985e-05,
      "loss": 0.0196,
      "step": 4885
    },
    {
      "epoch": 1.1917073170731707,
      "grad_norm": 0.2012867033481598,
      "learning_rate": 3.293525739382614e-05,
      "loss": 0.0174,
      "step": 4886
    },
    {
      "epoch": 1.191951219512195,
      "grad_norm": 0.09802756458520889,
      "learning_rate": 3.292920148910251e-05,
      "loss": 0.0133,
      "step": 4887
    },
    {
      "epoch": 1.1921951219512195,
      "grad_norm": 0.14881956577301025,
      "learning_rate": 3.292314506702276e-05,
      "loss": 0.0155,
      "step": 4888
    },
    {
      "epoch": 1.1924390243902439,
      "grad_norm": 0.13295100629329681,
      "learning_rate": 3.291708812798206e-05,
      "loss": 0.0116,
      "step": 4889
    },
    {
      "epoch": 1.1926829268292682,
      "grad_norm": 0.10501928627490997,
      "learning_rate": 3.2911030672375586e-05,
      "loss": 0.0166,
      "step": 4890
    },
    {
      "epoch": 1.1929268292682926,
      "grad_norm": 0.1224597692489624,
      "learning_rate": 3.290497270059858e-05,
      "loss": 0.0251,
      "step": 4891
    },
    {
      "epoch": 1.193170731707317,
      "grad_norm": 0.08586359024047852,
      "learning_rate": 3.2898914213046306e-05,
      "loss": 0.0172,
      "step": 4892
    },
    {
      "epoch": 1.1934146341463414,
      "grad_norm": 0.2067130208015442,
      "learning_rate": 3.2892855210114066e-05,
      "loss": 0.0401,
      "step": 4893
    },
    {
      "epoch": 1.1936585365853658,
      "grad_norm": 0.13947241008281708,
      "learning_rate": 3.288679569219718e-05,
      "loss": 0.026,
      "step": 4894
    },
    {
      "epoch": 1.1939024390243902,
      "grad_norm": 0.1055910736322403,
      "learning_rate": 3.288073565969102e-05,
      "loss": 0.0166,
      "step": 4895
    },
    {
      "epoch": 1.1941463414634146,
      "grad_norm": 0.30449378490448,
      "learning_rate": 3.2874675112990974e-05,
      "loss": 0.0344,
      "step": 4896
    },
    {
      "epoch": 1.194390243902439,
      "grad_norm": 0.1485692709684372,
      "learning_rate": 3.28686140524925e-05,
      "loss": 0.0283,
      "step": 4897
    },
    {
      "epoch": 1.1946341463414634,
      "grad_norm": 0.1268552988767624,
      "learning_rate": 3.286255247859104e-05,
      "loss": 0.013,
      "step": 4898
    },
    {
      "epoch": 1.1948780487804878,
      "grad_norm": 0.07820779830217361,
      "learning_rate": 3.28564903916821e-05,
      "loss": 0.027,
      "step": 4899
    },
    {
      "epoch": 1.1951219512195121,
      "grad_norm": 0.09674699604511261,
      "learning_rate": 3.285042779216121e-05,
      "loss": 0.0257,
      "step": 4900
    },
    {
      "epoch": 1.1953658536585365,
      "grad_norm": 0.11814060807228088,
      "learning_rate": 3.284436468042395e-05,
      "loss": 0.0236,
      "step": 4901
    },
    {
      "epoch": 1.195609756097561,
      "grad_norm": 0.07995865494012833,
      "learning_rate": 3.283830105686589e-05,
      "loss": 0.0224,
      "step": 4902
    },
    {
      "epoch": 1.1958536585365853,
      "grad_norm": 0.07975884526968002,
      "learning_rate": 3.28322369218827e-05,
      "loss": 0.0199,
      "step": 4903
    },
    {
      "epoch": 1.1960975609756097,
      "grad_norm": 0.09905911982059479,
      "learning_rate": 3.2826172275870016e-05,
      "loss": 0.0228,
      "step": 4904
    },
    {
      "epoch": 1.196341463414634,
      "grad_norm": 0.11520354449748993,
      "learning_rate": 3.2820107119223554e-05,
      "loss": 0.0198,
      "step": 4905
    },
    {
      "epoch": 1.1965853658536585,
      "grad_norm": 0.12695667147636414,
      "learning_rate": 3.281404145233904e-05,
      "loss": 0.0195,
      "step": 4906
    },
    {
      "epoch": 1.1968292682926829,
      "grad_norm": 0.10953638702630997,
      "learning_rate": 3.280797527561224e-05,
      "loss": 0.0147,
      "step": 4907
    },
    {
      "epoch": 1.1970731707317073,
      "grad_norm": 0.1187266930937767,
      "learning_rate": 3.2801908589438956e-05,
      "loss": 0.0246,
      "step": 4908
    },
    {
      "epoch": 1.1973170731707317,
      "grad_norm": 0.17847882211208344,
      "learning_rate": 3.279584139421503e-05,
      "loss": 0.0207,
      "step": 4909
    },
    {
      "epoch": 1.197560975609756,
      "grad_norm": 0.22648818790912628,
      "learning_rate": 3.278977369033632e-05,
      "loss": 0.037,
      "step": 4910
    },
    {
      "epoch": 1.1978048780487804,
      "grad_norm": 0.2021717131137848,
      "learning_rate": 3.2783705478198715e-05,
      "loss": 0.0226,
      "step": 4911
    },
    {
      "epoch": 1.1980487804878048,
      "grad_norm": 0.14880405366420746,
      "learning_rate": 3.277763675819815e-05,
      "loss": 0.0149,
      "step": 4912
    },
    {
      "epoch": 1.1982926829268292,
      "grad_norm": 0.10272262245416641,
      "learning_rate": 3.27715675307306e-05,
      "loss": 0.0276,
      "step": 4913
    },
    {
      "epoch": 1.1985365853658536,
      "grad_norm": 0.15923194587230682,
      "learning_rate": 3.276549779619206e-05,
      "loss": 0.0136,
      "step": 4914
    },
    {
      "epoch": 1.198780487804878,
      "grad_norm": 0.09144797921180725,
      "learning_rate": 3.2759427554978566e-05,
      "loss": 0.0171,
      "step": 4915
    },
    {
      "epoch": 1.1990243902439024,
      "grad_norm": 0.14590516686439514,
      "learning_rate": 3.2753356807486175e-05,
      "loss": 0.0212,
      "step": 4916
    },
    {
      "epoch": 1.1992682926829268,
      "grad_norm": 0.10197718441486359,
      "learning_rate": 3.274728555411099e-05,
      "loss": 0.0237,
      "step": 4917
    },
    {
      "epoch": 1.1995121951219512,
      "grad_norm": 0.19043797254562378,
      "learning_rate": 3.2741213795249134e-05,
      "loss": 0.0367,
      "step": 4918
    },
    {
      "epoch": 1.1997560975609756,
      "grad_norm": 0.16346557438373566,
      "learning_rate": 3.2735141531296784e-05,
      "loss": 0.0225,
      "step": 4919
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.16154488921165466,
      "learning_rate": 3.272906876265012e-05,
      "loss": 0.0407,
      "step": 4920
    },
    {
      "epoch": 1.2002439024390243,
      "grad_norm": 0.07574275881052017,
      "learning_rate": 3.272299548970538e-05,
      "loss": 0.0208,
      "step": 4921
    },
    {
      "epoch": 1.2004878048780487,
      "grad_norm": 0.16329392790794373,
      "learning_rate": 3.271692171285884e-05,
      "loss": 0.0193,
      "step": 4922
    },
    {
      "epoch": 1.2007317073170731,
      "grad_norm": 0.3698316514492035,
      "learning_rate": 3.271084743250677e-05,
      "loss": 0.034,
      "step": 4923
    },
    {
      "epoch": 1.2009756097560975,
      "grad_norm": 0.17631308734416962,
      "learning_rate": 3.270477264904551e-05,
      "loss": 0.0151,
      "step": 4924
    },
    {
      "epoch": 1.201219512195122,
      "grad_norm": 0.18120591342449188,
      "learning_rate": 3.2698697362871426e-05,
      "loss": 0.0262,
      "step": 4925
    },
    {
      "epoch": 1.2014634146341463,
      "grad_norm": 0.1403026282787323,
      "learning_rate": 3.2692621574380906e-05,
      "loss": 0.0233,
      "step": 4926
    },
    {
      "epoch": 1.2017073170731707,
      "grad_norm": 0.10496934503316879,
      "learning_rate": 3.268654528397038e-05,
      "loss": 0.0211,
      "step": 4927
    },
    {
      "epoch": 1.201951219512195,
      "grad_norm": 0.1765400767326355,
      "learning_rate": 3.2680468492036295e-05,
      "loss": 0.0321,
      "step": 4928
    },
    {
      "epoch": 1.2021951219512195,
      "grad_norm": 0.05510948598384857,
      "learning_rate": 3.267439119897516e-05,
      "loss": 0.014,
      "step": 4929
    },
    {
      "epoch": 1.2024390243902439,
      "grad_norm": 0.17454373836517334,
      "learning_rate": 3.266831340518349e-05,
      "loss": 0.0225,
      "step": 4930
    },
    {
      "epoch": 1.2026829268292683,
      "grad_norm": 0.1721116155385971,
      "learning_rate": 3.2662235111057854e-05,
      "loss": 0.0313,
      "step": 4931
    },
    {
      "epoch": 1.2029268292682926,
      "grad_norm": 0.055853500962257385,
      "learning_rate": 3.2656156316994824e-05,
      "loss": 0.0161,
      "step": 4932
    },
    {
      "epoch": 1.203170731707317,
      "grad_norm": 0.06969163566827774,
      "learning_rate": 3.2650077023391037e-05,
      "loss": 0.0123,
      "step": 4933
    },
    {
      "epoch": 1.2034146341463414,
      "grad_norm": 0.1353716254234314,
      "learning_rate": 3.264399723064314e-05,
      "loss": 0.0307,
      "step": 4934
    },
    {
      "epoch": 1.2036585365853658,
      "grad_norm": 1.7477222681045532,
      "learning_rate": 3.2637916939147826e-05,
      "loss": 0.0221,
      "step": 4935
    },
    {
      "epoch": 1.2039024390243902,
      "grad_norm": 0.2272227257490158,
      "learning_rate": 3.2631836149301815e-05,
      "loss": 0.0208,
      "step": 4936
    },
    {
      "epoch": 1.2041463414634146,
      "grad_norm": 0.20930632948875427,
      "learning_rate": 3.262575486150186e-05,
      "loss": 0.0282,
      "step": 4937
    },
    {
      "epoch": 1.204390243902439,
      "grad_norm": 0.07492748647928238,
      "learning_rate": 3.261967307614473e-05,
      "loss": 0.0192,
      "step": 4938
    },
    {
      "epoch": 1.2046341463414634,
      "grad_norm": 0.07617086172103882,
      "learning_rate": 3.261359079362728e-05,
      "loss": 0.0116,
      "step": 4939
    },
    {
      "epoch": 1.2048780487804878,
      "grad_norm": 0.31912386417388916,
      "learning_rate": 3.2607508014346323e-05,
      "loss": 0.0239,
      "step": 4940
    },
    {
      "epoch": 1.2051219512195122,
      "grad_norm": 0.09062185883522034,
      "learning_rate": 3.260142473869876e-05,
      "loss": 0.012,
      "step": 4941
    },
    {
      "epoch": 1.2053658536585365,
      "grad_norm": 0.09808722883462906,
      "learning_rate": 3.259534096708151e-05,
      "loss": 0.0158,
      "step": 4942
    },
    {
      "epoch": 1.205609756097561,
      "grad_norm": 0.14283624291419983,
      "learning_rate": 3.258925669989151e-05,
      "loss": 0.029,
      "step": 4943
    },
    {
      "epoch": 1.2058536585365853,
      "grad_norm": 0.15753239393234253,
      "learning_rate": 3.258317193752574e-05,
      "loss": 0.0208,
      "step": 4944
    },
    {
      "epoch": 1.2060975609756097,
      "grad_norm": 0.15645112097263336,
      "learning_rate": 3.257708668038122e-05,
      "loss": 0.0342,
      "step": 4945
    },
    {
      "epoch": 1.206341463414634,
      "grad_norm": 0.14162485301494598,
      "learning_rate": 3.2571000928854985e-05,
      "loss": 0.0325,
      "step": 4946
    },
    {
      "epoch": 1.2065853658536585,
      "grad_norm": 0.2814432382583618,
      "learning_rate": 3.256491468334412e-05,
      "loss": 0.0209,
      "step": 4947
    },
    {
      "epoch": 1.2068292682926829,
      "grad_norm": 0.09596579521894455,
      "learning_rate": 3.2558827944245724e-05,
      "loss": 0.0172,
      "step": 4948
    },
    {
      "epoch": 1.2070731707317073,
      "grad_norm": 0.09086719155311584,
      "learning_rate": 3.255274071195695e-05,
      "loss": 0.0223,
      "step": 4949
    },
    {
      "epoch": 1.2073170731707317,
      "grad_norm": 0.1046394407749176,
      "learning_rate": 3.254665298687496e-05,
      "loss": 0.0253,
      "step": 4950
    },
    {
      "epoch": 1.207560975609756,
      "grad_norm": 0.08883429318666458,
      "learning_rate": 3.2540564769396977e-05,
      "loss": 0.0251,
      "step": 4951
    },
    {
      "epoch": 1.2078048780487805,
      "grad_norm": 0.18008466064929962,
      "learning_rate": 3.253447605992022e-05,
      "loss": 0.0084,
      "step": 4952
    },
    {
      "epoch": 1.2080487804878048,
      "grad_norm": 0.17867827415466309,
      "learning_rate": 3.252838685884196e-05,
      "loss": 0.0334,
      "step": 4953
    },
    {
      "epoch": 1.2082926829268292,
      "grad_norm": 0.1321331113576889,
      "learning_rate": 3.252229716655951e-05,
      "loss": 0.0152,
      "step": 4954
    },
    {
      "epoch": 1.2085365853658536,
      "grad_norm": 0.15108275413513184,
      "learning_rate": 3.251620698347019e-05,
      "loss": 0.022,
      "step": 4955
    },
    {
      "epoch": 1.208780487804878,
      "grad_norm": 0.0802680179476738,
      "learning_rate": 3.251011630997139e-05,
      "loss": 0.0195,
      "step": 4956
    },
    {
      "epoch": 1.2090243902439024,
      "grad_norm": 0.22997891902923584,
      "learning_rate": 3.250402514646048e-05,
      "loss": 0.0263,
      "step": 4957
    },
    {
      "epoch": 1.2092682926829268,
      "grad_norm": 0.08408810943365097,
      "learning_rate": 3.24979334933349e-05,
      "loss": 0.0177,
      "step": 4958
    },
    {
      "epoch": 1.2095121951219512,
      "grad_norm": 0.09701256453990936,
      "learning_rate": 3.249184135099212e-05,
      "loss": 0.0188,
      "step": 4959
    },
    {
      "epoch": 1.2097560975609756,
      "grad_norm": 0.0885298028588295,
      "learning_rate": 3.248574871982962e-05,
      "loss": 0.0309,
      "step": 4960
    },
    {
      "epoch": 1.21,
      "grad_norm": 0.10154176503419876,
      "learning_rate": 3.247965560024494e-05,
      "loss": 0.0245,
      "step": 4961
    },
    {
      "epoch": 1.2102439024390244,
      "grad_norm": 0.133386492729187,
      "learning_rate": 3.247356199263562e-05,
      "loss": 0.024,
      "step": 4962
    },
    {
      "epoch": 1.2104878048780487,
      "grad_norm": 0.18315313756465912,
      "learning_rate": 3.2467467897399265e-05,
      "loss": 0.0248,
      "step": 4963
    },
    {
      "epoch": 1.2107317073170731,
      "grad_norm": 0.11060255020856857,
      "learning_rate": 3.2461373314933484e-05,
      "loss": 0.0191,
      "step": 4964
    },
    {
      "epoch": 1.2109756097560975,
      "grad_norm": 0.10196711868047714,
      "learning_rate": 3.245527824563594e-05,
      "loss": 0.0201,
      "step": 4965
    },
    {
      "epoch": 1.211219512195122,
      "grad_norm": 0.09566280990839005,
      "learning_rate": 3.2449182689904316e-05,
      "loss": 0.0378,
      "step": 4966
    },
    {
      "epoch": 1.2114634146341463,
      "grad_norm": 0.10697435587644577,
      "learning_rate": 3.2443086648136326e-05,
      "loss": 0.0168,
      "step": 4967
    },
    {
      "epoch": 1.2117073170731707,
      "grad_norm": 0.23029573261737823,
      "learning_rate": 3.243699012072971e-05,
      "loss": 0.0092,
      "step": 4968
    },
    {
      "epoch": 1.211951219512195,
      "grad_norm": 0.2552341818809509,
      "learning_rate": 3.243089310808226e-05,
      "loss": 0.0293,
      "step": 4969
    },
    {
      "epoch": 1.2121951219512195,
      "grad_norm": 0.0979660376906395,
      "learning_rate": 3.242479561059178e-05,
      "loss": 0.0151,
      "step": 4970
    },
    {
      "epoch": 1.2124390243902439,
      "grad_norm": 0.12745779752731323,
      "learning_rate": 3.241869762865612e-05,
      "loss": 0.0274,
      "step": 4971
    },
    {
      "epoch": 1.2126829268292683,
      "grad_norm": 0.12620805203914642,
      "learning_rate": 3.241259916267314e-05,
      "loss": 0.0164,
      "step": 4972
    },
    {
      "epoch": 1.2129268292682926,
      "grad_norm": 0.08860329538583755,
      "learning_rate": 3.2406500213040776e-05,
      "loss": 0.0109,
      "step": 4973
    },
    {
      "epoch": 1.213170731707317,
      "grad_norm": 0.15966147184371948,
      "learning_rate": 3.2400400780156934e-05,
      "loss": 0.0323,
      "step": 4974
    },
    {
      "epoch": 1.2134146341463414,
      "grad_norm": 0.1708473116159439,
      "learning_rate": 3.2394300864419595e-05,
      "loss": 0.0138,
      "step": 4975
    },
    {
      "epoch": 1.2136585365853658,
      "grad_norm": 0.22138823568820953,
      "learning_rate": 3.2388200466226764e-05,
      "loss": 0.0354,
      "step": 4976
    },
    {
      "epoch": 1.2139024390243902,
      "grad_norm": 0.28544071316719055,
      "learning_rate": 3.238209958597648e-05,
      "loss": 0.0307,
      "step": 4977
    },
    {
      "epoch": 1.2141463414634146,
      "grad_norm": 0.10315217822790146,
      "learning_rate": 3.2375998224066775e-05,
      "loss": 0.0229,
      "step": 4978
    },
    {
      "epoch": 1.214390243902439,
      "grad_norm": 0.09686776250600815,
      "learning_rate": 3.236989638089578e-05,
      "loss": 0.0133,
      "step": 4979
    },
    {
      "epoch": 1.2146341463414634,
      "grad_norm": 0.10948430001735687,
      "learning_rate": 3.23637940568616e-05,
      "loss": 0.0249,
      "step": 4980
    },
    {
      "epoch": 1.2148780487804878,
      "grad_norm": 0.15226471424102783,
      "learning_rate": 3.235769125236241e-05,
      "loss": 0.0241,
      "step": 4981
    },
    {
      "epoch": 1.2151219512195122,
      "grad_norm": 0.09613299369812012,
      "learning_rate": 3.235158796779638e-05,
      "loss": 0.0181,
      "step": 4982
    },
    {
      "epoch": 1.2153658536585366,
      "grad_norm": 0.1535775363445282,
      "learning_rate": 3.234548420356176e-05,
      "loss": 0.0108,
      "step": 4983
    },
    {
      "epoch": 1.215609756097561,
      "grad_norm": 0.08154784888029099,
      "learning_rate": 3.233937996005676e-05,
      "loss": 0.0225,
      "step": 4984
    },
    {
      "epoch": 1.2158536585365853,
      "grad_norm": 0.2140934020280838,
      "learning_rate": 3.23332752376797e-05,
      "loss": 0.021,
      "step": 4985
    },
    {
      "epoch": 1.2160975609756097,
      "grad_norm": 0.15314115583896637,
      "learning_rate": 3.2327170036828866e-05,
      "loss": 0.0295,
      "step": 4986
    },
    {
      "epoch": 1.2163414634146341,
      "grad_norm": 0.06093393638730049,
      "learning_rate": 3.232106435790263e-05,
      "loss": 0.0157,
      "step": 4987
    },
    {
      "epoch": 1.2165853658536585,
      "grad_norm": 0.11498850584030151,
      "learning_rate": 3.231495820129935e-05,
      "loss": 0.0232,
      "step": 4988
    },
    {
      "epoch": 1.216829268292683,
      "grad_norm": 0.17171312868595123,
      "learning_rate": 3.230885156741744e-05,
      "loss": 0.0253,
      "step": 4989
    },
    {
      "epoch": 1.2170731707317073,
      "grad_norm": 0.03480246290564537,
      "learning_rate": 3.230274445665534e-05,
      "loss": 0.0053,
      "step": 4990
    },
    {
      "epoch": 1.2173170731707317,
      "grad_norm": 0.059977948665618896,
      "learning_rate": 3.2296636869411534e-05,
      "loss": 0.0164,
      "step": 4991
    },
    {
      "epoch": 1.217560975609756,
      "grad_norm": 0.1094038113951683,
      "learning_rate": 3.229052880608449e-05,
      "loss": 0.0208,
      "step": 4992
    },
    {
      "epoch": 1.2178048780487805,
      "grad_norm": 0.15412065386772156,
      "learning_rate": 3.228442026707277e-05,
      "loss": 0.0223,
      "step": 4993
    },
    {
      "epoch": 1.2180487804878048,
      "grad_norm": 0.09788636863231659,
      "learning_rate": 3.227831125277493e-05,
      "loss": 0.0143,
      "step": 4994
    },
    {
      "epoch": 1.2182926829268292,
      "grad_norm": 0.06712361425161362,
      "learning_rate": 3.227220176358955e-05,
      "loss": 0.012,
      "step": 4995
    },
    {
      "epoch": 1.2185365853658536,
      "grad_norm": 0.10702457278966904,
      "learning_rate": 3.226609179991528e-05,
      "loss": 0.0161,
      "step": 4996
    },
    {
      "epoch": 1.218780487804878,
      "grad_norm": 0.107236847281456,
      "learning_rate": 3.225998136215075e-05,
      "loss": 0.0333,
      "step": 4997
    },
    {
      "epoch": 1.2190243902439024,
      "grad_norm": 0.10594462603330612,
      "learning_rate": 3.2253870450694675e-05,
      "loss": 0.0258,
      "step": 4998
    },
    {
      "epoch": 1.2192682926829268,
      "grad_norm": 0.1211576834321022,
      "learning_rate": 3.224775906594575e-05,
      "loss": 0.0157,
      "step": 4999
    },
    {
      "epoch": 1.2195121951219512,
      "grad_norm": 0.12271862477064133,
      "learning_rate": 3.224164720830274e-05,
      "loss": 0.0119,
      "step": 5000
    },
    {
      "epoch": 1.2197560975609756,
      "grad_norm": 0.10601948201656342,
      "learning_rate": 3.2235534878164416e-05,
      "loss": 0.0164,
      "step": 5001
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.08630621433258057,
      "learning_rate": 3.2229422075929585e-05,
      "loss": 0.0204,
      "step": 5002
    },
    {
      "epoch": 1.2202439024390244,
      "grad_norm": 0.11944837123155594,
      "learning_rate": 3.22233088019971e-05,
      "loss": 0.0301,
      "step": 5003
    },
    {
      "epoch": 1.2204878048780488,
      "grad_norm": 0.06383301317691803,
      "learning_rate": 3.221719505676583e-05,
      "loss": 0.0094,
      "step": 5004
    },
    {
      "epoch": 1.2207317073170731,
      "grad_norm": 0.09958717972040176,
      "learning_rate": 3.2211080840634676e-05,
      "loss": 0.0204,
      "step": 5005
    },
    {
      "epoch": 1.2209756097560975,
      "grad_norm": 0.09280941635370255,
      "learning_rate": 3.220496615400258e-05,
      "loss": 0.0159,
      "step": 5006
    },
    {
      "epoch": 1.221219512195122,
      "grad_norm": 0.08309171348810196,
      "learning_rate": 3.219885099726848e-05,
      "loss": 0.0134,
      "step": 5007
    },
    {
      "epoch": 1.2214634146341463,
      "grad_norm": 0.14430099725723267,
      "learning_rate": 3.219273537083141e-05,
      "loss": 0.0232,
      "step": 5008
    },
    {
      "epoch": 1.2217073170731707,
      "grad_norm": 0.10871490836143494,
      "learning_rate": 3.218661927509036e-05,
      "loss": 0.0213,
      "step": 5009
    },
    {
      "epoch": 1.221951219512195,
      "grad_norm": 0.07257623970508575,
      "learning_rate": 3.218050271044441e-05,
      "loss": 0.0141,
      "step": 5010
    },
    {
      "epoch": 1.2221951219512195,
      "grad_norm": 0.06381924450397491,
      "learning_rate": 3.217438567729265e-05,
      "loss": 0.0071,
      "step": 5011
    },
    {
      "epoch": 1.2224390243902439,
      "grad_norm": 0.11732649058103561,
      "learning_rate": 3.216826817603417e-05,
      "loss": 0.0224,
      "step": 5012
    },
    {
      "epoch": 1.2226829268292683,
      "grad_norm": 0.1712801158428192,
      "learning_rate": 3.216215020706814e-05,
      "loss": 0.0372,
      "step": 5013
    },
    {
      "epoch": 1.2229268292682927,
      "grad_norm": 0.11384154111146927,
      "learning_rate": 3.2156031770793734e-05,
      "loss": 0.0147,
      "step": 5014
    },
    {
      "epoch": 1.223170731707317,
      "grad_norm": 0.12550316751003265,
      "learning_rate": 3.214991286761017e-05,
      "loss": 0.0223,
      "step": 5015
    },
    {
      "epoch": 1.2234146341463414,
      "grad_norm": 0.12707534432411194,
      "learning_rate": 3.214379349791667e-05,
      "loss": 0.0352,
      "step": 5016
    },
    {
      "epoch": 1.2236585365853658,
      "grad_norm": 0.15016517043113708,
      "learning_rate": 3.2137673662112524e-05,
      "loss": 0.0189,
      "step": 5017
    },
    {
      "epoch": 1.2239024390243902,
      "grad_norm": 0.08085640519857407,
      "learning_rate": 3.213155336059701e-05,
      "loss": 0.0157,
      "step": 5018
    },
    {
      "epoch": 1.2241463414634146,
      "grad_norm": 0.10874838382005692,
      "learning_rate": 3.2125432593769476e-05,
      "loss": 0.0151,
      "step": 5019
    },
    {
      "epoch": 1.224390243902439,
      "grad_norm": 0.05310668796300888,
      "learning_rate": 3.211931136202928e-05,
      "loss": 0.0122,
      "step": 5020
    },
    {
      "epoch": 1.2246341463414634,
      "grad_norm": 0.09687981754541397,
      "learning_rate": 3.211318966577581e-05,
      "loss": 0.018,
      "step": 5021
    },
    {
      "epoch": 1.2248780487804878,
      "grad_norm": 0.14833073318004608,
      "learning_rate": 3.210706750540849e-05,
      "loss": 0.0188,
      "step": 5022
    },
    {
      "epoch": 1.2251219512195122,
      "grad_norm": 0.12199302017688751,
      "learning_rate": 3.210094488132678e-05,
      "loss": 0.0198,
      "step": 5023
    },
    {
      "epoch": 1.2253658536585366,
      "grad_norm": 0.10697772353887558,
      "learning_rate": 3.2094821793930155e-05,
      "loss": 0.0187,
      "step": 5024
    },
    {
      "epoch": 1.225609756097561,
      "grad_norm": 0.14260190725326538,
      "learning_rate": 3.208869824361811e-05,
      "loss": 0.0363,
      "step": 5025
    },
    {
      "epoch": 1.2258536585365853,
      "grad_norm": 0.11397074908018112,
      "learning_rate": 3.208257423079021e-05,
      "loss": 0.0277,
      "step": 5026
    },
    {
      "epoch": 1.2260975609756097,
      "grad_norm": 0.07159347832202911,
      "learning_rate": 3.207644975584603e-05,
      "loss": 0.0214,
      "step": 5027
    },
    {
      "epoch": 1.2263414634146341,
      "grad_norm": 0.08578921109437943,
      "learning_rate": 3.207032481918517e-05,
      "loss": 0.0222,
      "step": 5028
    },
    {
      "epoch": 1.2265853658536585,
      "grad_norm": 0.16165930032730103,
      "learning_rate": 3.206419942120725e-05,
      "loss": 0.0224,
      "step": 5029
    },
    {
      "epoch": 1.226829268292683,
      "grad_norm": 0.14554738998413086,
      "learning_rate": 3.2058073562311946e-05,
      "loss": 0.0174,
      "step": 5030
    },
    {
      "epoch": 1.2270731707317073,
      "grad_norm": 0.09249868243932724,
      "learning_rate": 3.205194724289895e-05,
      "loss": 0.0216,
      "step": 5031
    },
    {
      "epoch": 1.2273170731707317,
      "grad_norm": 0.13556507229804993,
      "learning_rate": 3.204582046336799e-05,
      "loss": 0.0245,
      "step": 5032
    },
    {
      "epoch": 1.227560975609756,
      "grad_norm": 0.09771379083395004,
      "learning_rate": 3.203969322411882e-05,
      "loss": 0.0169,
      "step": 5033
    },
    {
      "epoch": 1.2278048780487805,
      "grad_norm": 0.09013833105564117,
      "learning_rate": 3.20335655255512e-05,
      "loss": 0.0207,
      "step": 5034
    },
    {
      "epoch": 1.2280487804878049,
      "grad_norm": 0.20206405222415924,
      "learning_rate": 3.2027437368064966e-05,
      "loss": 0.019,
      "step": 5035
    },
    {
      "epoch": 1.2282926829268292,
      "grad_norm": 0.11544805020093918,
      "learning_rate": 3.202130875205997e-05,
      "loss": 0.0121,
      "step": 5036
    },
    {
      "epoch": 1.2285365853658536,
      "grad_norm": 0.21330761909484863,
      "learning_rate": 3.2015179677936054e-05,
      "loss": 0.024,
      "step": 5037
    },
    {
      "epoch": 1.228780487804878,
      "grad_norm": 0.13078536093235016,
      "learning_rate": 3.200905014609315e-05,
      "loss": 0.0359,
      "step": 5038
    },
    {
      "epoch": 1.2290243902439024,
      "grad_norm": 0.10133245587348938,
      "learning_rate": 3.200292015693117e-05,
      "loss": 0.0214,
      "step": 5039
    },
    {
      "epoch": 1.2292682926829268,
      "grad_norm": 0.23409947752952576,
      "learning_rate": 3.1996789710850095e-05,
      "loss": 0.0335,
      "step": 5040
    },
    {
      "epoch": 1.2295121951219512,
      "grad_norm": 0.08669806271791458,
      "learning_rate": 3.199065880824992e-05,
      "loss": 0.022,
      "step": 5041
    },
    {
      "epoch": 1.2297560975609756,
      "grad_norm": 0.11867916584014893,
      "learning_rate": 3.198452744953064e-05,
      "loss": 0.0185,
      "step": 5042
    },
    {
      "epoch": 1.23,
      "grad_norm": 0.17827405035495758,
      "learning_rate": 3.197839563509234e-05,
      "loss": 0.0164,
      "step": 5043
    },
    {
      "epoch": 1.2302439024390244,
      "grad_norm": 0.15707962214946747,
      "learning_rate": 3.197226336533507e-05,
      "loss": 0.0227,
      "step": 5044
    },
    {
      "epoch": 1.2304878048780488,
      "grad_norm": 0.10427971929311752,
      "learning_rate": 3.196613064065898e-05,
      "loss": 0.018,
      "step": 5045
    },
    {
      "epoch": 1.2307317073170732,
      "grad_norm": 0.23324455320835114,
      "learning_rate": 3.1959997461464184e-05,
      "loss": 0.0411,
      "step": 5046
    },
    {
      "epoch": 1.2309756097560975,
      "grad_norm": 0.23136867582798004,
      "learning_rate": 3.195386382815086e-05,
      "loss": 0.024,
      "step": 5047
    },
    {
      "epoch": 1.231219512195122,
      "grad_norm": 0.17188172042369843,
      "learning_rate": 3.19477297411192e-05,
      "loss": 0.0332,
      "step": 5048
    },
    {
      "epoch": 1.2314634146341463,
      "grad_norm": 0.10252825170755386,
      "learning_rate": 3.194159520076947e-05,
      "loss": 0.0257,
      "step": 5049
    },
    {
      "epoch": 1.2317073170731707,
      "grad_norm": 0.06627476960420609,
      "learning_rate": 3.193546020750188e-05,
      "loss": 0.0219,
      "step": 5050
    },
    {
      "epoch": 1.231951219512195,
      "grad_norm": 0.1090354472398758,
      "learning_rate": 3.1929324761716745e-05,
      "loss": 0.0188,
      "step": 5051
    },
    {
      "epoch": 1.2321951219512195,
      "grad_norm": 0.08595646172761917,
      "learning_rate": 3.1923188863814384e-05,
      "loss": 0.0129,
      "step": 5052
    },
    {
      "epoch": 1.2324390243902439,
      "grad_norm": 0.17141658067703247,
      "learning_rate": 3.191705251419515e-05,
      "loss": 0.0146,
      "step": 5053
    },
    {
      "epoch": 1.2326829268292683,
      "grad_norm": 0.15622578561306,
      "learning_rate": 3.19109157132594e-05,
      "loss": 0.0194,
      "step": 5054
    },
    {
      "epoch": 1.2329268292682927,
      "grad_norm": 0.09926909953355789,
      "learning_rate": 3.190477846140757e-05,
      "loss": 0.0211,
      "step": 5055
    },
    {
      "epoch": 1.233170731707317,
      "grad_norm": 0.0730803832411766,
      "learning_rate": 3.189864075904008e-05,
      "loss": 0.0213,
      "step": 5056
    },
    {
      "epoch": 1.2334146341463414,
      "grad_norm": 0.04542829468846321,
      "learning_rate": 3.189250260655739e-05,
      "loss": 0.006,
      "step": 5057
    },
    {
      "epoch": 1.2336585365853658,
      "grad_norm": 0.07614532113075256,
      "learning_rate": 3.1886364004360026e-05,
      "loss": 0.015,
      "step": 5058
    },
    {
      "epoch": 1.2339024390243902,
      "grad_norm": 0.08044995367527008,
      "learning_rate": 3.188022495284847e-05,
      "loss": 0.0185,
      "step": 5059
    },
    {
      "epoch": 1.2341463414634146,
      "grad_norm": 0.11388576030731201,
      "learning_rate": 3.187408545242331e-05,
      "loss": 0.0291,
      "step": 5060
    },
    {
      "epoch": 1.234390243902439,
      "grad_norm": 0.083684042096138,
      "learning_rate": 3.186794550348511e-05,
      "loss": 0.0252,
      "step": 5061
    },
    {
      "epoch": 1.2346341463414634,
      "grad_norm": 0.11780490726232529,
      "learning_rate": 3.18618051064345e-05,
      "loss": 0.0276,
      "step": 5062
    },
    {
      "epoch": 1.2348780487804878,
      "grad_norm": 0.10042940080165863,
      "learning_rate": 3.18556642616721e-05,
      "loss": 0.0174,
      "step": 5063
    },
    {
      "epoch": 1.2351219512195122,
      "grad_norm": 0.1487700492143631,
      "learning_rate": 3.18495229695986e-05,
      "loss": 0.0248,
      "step": 5064
    },
    {
      "epoch": 1.2353658536585366,
      "grad_norm": 0.06715413182973862,
      "learning_rate": 3.1843381230614703e-05,
      "loss": 0.0163,
      "step": 5065
    },
    {
      "epoch": 1.235609756097561,
      "grad_norm": 0.089057058095932,
      "learning_rate": 3.1837239045121134e-05,
      "loss": 0.0127,
      "step": 5066
    },
    {
      "epoch": 1.2358536585365854,
      "grad_norm": 0.06726572662591934,
      "learning_rate": 3.183109641351863e-05,
      "loss": 0.0072,
      "step": 5067
    },
    {
      "epoch": 1.2360975609756097,
      "grad_norm": 0.11242134869098663,
      "learning_rate": 3.182495333620801e-05,
      "loss": 0.0223,
      "step": 5068
    },
    {
      "epoch": 1.2363414634146341,
      "grad_norm": 0.09073910862207413,
      "learning_rate": 3.1818809813590066e-05,
      "loss": 0.0145,
      "step": 5069
    },
    {
      "epoch": 1.2365853658536585,
      "grad_norm": 0.06020350381731987,
      "learning_rate": 3.181266584606567e-05,
      "loss": 0.0095,
      "step": 5070
    },
    {
      "epoch": 1.236829268292683,
      "grad_norm": 0.07603311538696289,
      "learning_rate": 3.180652143403567e-05,
      "loss": 0.011,
      "step": 5071
    },
    {
      "epoch": 1.2370731707317073,
      "grad_norm": 0.10243891179561615,
      "learning_rate": 3.180037657790099e-05,
      "loss": 0.0174,
      "step": 5072
    },
    {
      "epoch": 1.2373170731707317,
      "grad_norm": 0.07931400835514069,
      "learning_rate": 3.179423127806255e-05,
      "loss": 0.0183,
      "step": 5073
    },
    {
      "epoch": 1.237560975609756,
      "grad_norm": 0.08519840985536575,
      "learning_rate": 3.178808553492133e-05,
      "loss": 0.0283,
      "step": 5074
    },
    {
      "epoch": 1.2378048780487805,
      "grad_norm": 0.11562231183052063,
      "learning_rate": 3.178193934887831e-05,
      "loss": 0.0141,
      "step": 5075
    },
    {
      "epoch": 1.2380487804878049,
      "grad_norm": 0.09937962144613266,
      "learning_rate": 3.17757927203345e-05,
      "loss": 0.0086,
      "step": 5076
    },
    {
      "epoch": 1.2382926829268293,
      "grad_norm": 0.23363380134105682,
      "learning_rate": 3.176964564969095e-05,
      "loss": 0.0321,
      "step": 5077
    },
    {
      "epoch": 1.2385365853658536,
      "grad_norm": 0.19067107141017914,
      "learning_rate": 3.176349813734876e-05,
      "loss": 0.0234,
      "step": 5078
    },
    {
      "epoch": 1.238780487804878,
      "grad_norm": 0.1523745357990265,
      "learning_rate": 3.175735018370902e-05,
      "loss": 0.0321,
      "step": 5079
    },
    {
      "epoch": 1.2390243902439024,
      "grad_norm": 0.10420934110879898,
      "learning_rate": 3.1751201789172856e-05,
      "loss": 0.0207,
      "step": 5080
    },
    {
      "epoch": 1.2392682926829268,
      "grad_norm": 0.08098747581243515,
      "learning_rate": 3.1745052954141453e-05,
      "loss": 0.019,
      "step": 5081
    },
    {
      "epoch": 1.2395121951219512,
      "grad_norm": 0.0863988921046257,
      "learning_rate": 3.173890367901599e-05,
      "loss": 0.0139,
      "step": 5082
    },
    {
      "epoch": 1.2397560975609756,
      "grad_norm": 0.1352730542421341,
      "learning_rate": 3.1732753964197696e-05,
      "loss": 0.0156,
      "step": 5083
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.1636093407869339,
      "learning_rate": 3.172660381008781e-05,
      "loss": 0.0316,
      "step": 5084
    },
    {
      "epoch": 1.2402439024390244,
      "grad_norm": 0.30822426080703735,
      "learning_rate": 3.172045321708762e-05,
      "loss": 0.0395,
      "step": 5085
    },
    {
      "epoch": 1.2404878048780488,
      "grad_norm": 0.2724190354347229,
      "learning_rate": 3.171430218559843e-05,
      "loss": 0.0252,
      "step": 5086
    },
    {
      "epoch": 1.2407317073170732,
      "grad_norm": 0.06218312308192253,
      "learning_rate": 3.170815071602158e-05,
      "loss": 0.0095,
      "step": 5087
    },
    {
      "epoch": 1.2409756097560976,
      "grad_norm": 0.09452247619628906,
      "learning_rate": 3.170199880875843e-05,
      "loss": 0.0163,
      "step": 5088
    },
    {
      "epoch": 1.241219512195122,
      "grad_norm": 0.10834456980228424,
      "learning_rate": 3.169584646421037e-05,
      "loss": 0.0276,
      "step": 5089
    },
    {
      "epoch": 1.2414634146341463,
      "grad_norm": 0.10672852396965027,
      "learning_rate": 3.1689693682778836e-05,
      "loss": 0.0197,
      "step": 5090
    },
    {
      "epoch": 1.2417073170731707,
      "grad_norm": 0.12382820248603821,
      "learning_rate": 3.168354046486526e-05,
      "loss": 0.0176,
      "step": 5091
    },
    {
      "epoch": 1.2419512195121951,
      "grad_norm": 0.10929936170578003,
      "learning_rate": 3.1677386810871136e-05,
      "loss": 0.0126,
      "step": 5092
    },
    {
      "epoch": 1.2421951219512195,
      "grad_norm": 0.10594139993190765,
      "learning_rate": 3.167123272119796e-05,
      "loss": 0.0261,
      "step": 5093
    },
    {
      "epoch": 1.242439024390244,
      "grad_norm": 0.23290705680847168,
      "learning_rate": 3.1665078196247264e-05,
      "loss": 0.0364,
      "step": 5094
    },
    {
      "epoch": 1.2426829268292683,
      "grad_norm": 0.08721087872982025,
      "learning_rate": 3.165892323642064e-05,
      "loss": 0.0172,
      "step": 5095
    },
    {
      "epoch": 1.2429268292682927,
      "grad_norm": 0.09189760684967041,
      "learning_rate": 3.1652767842119634e-05,
      "loss": 0.018,
      "step": 5096
    },
    {
      "epoch": 1.243170731707317,
      "grad_norm": 0.10356268286705017,
      "learning_rate": 3.1646612013745904e-05,
      "loss": 0.0204,
      "step": 5097
    },
    {
      "epoch": 1.2434146341463415,
      "grad_norm": 0.13524681329727173,
      "learning_rate": 3.164045575170109e-05,
      "loss": 0.0391,
      "step": 5098
    },
    {
      "epoch": 1.2436585365853658,
      "grad_norm": 0.09079957008361816,
      "learning_rate": 3.163429905638686e-05,
      "loss": 0.0206,
      "step": 5099
    },
    {
      "epoch": 1.2439024390243902,
      "grad_norm": 0.16837875545024872,
      "learning_rate": 3.162814192820494e-05,
      "loss": 0.0293,
      "step": 5100
    },
    {
      "epoch": 1.2441463414634146,
      "grad_norm": 0.15310564637184143,
      "learning_rate": 3.162198436755704e-05,
      "loss": 0.0305,
      "step": 5101
    },
    {
      "epoch": 1.244390243902439,
      "grad_norm": 0.06764128804206848,
      "learning_rate": 3.1615826374844925e-05,
      "loss": 0.0142,
      "step": 5102
    },
    {
      "epoch": 1.2446341463414634,
      "grad_norm": 0.06618660688400269,
      "learning_rate": 3.16096679504704e-05,
      "loss": 0.01,
      "step": 5103
    },
    {
      "epoch": 1.2448780487804878,
      "grad_norm": 0.07134748250246048,
      "learning_rate": 3.160350909483528e-05,
      "loss": 0.0249,
      "step": 5104
    },
    {
      "epoch": 1.2451219512195122,
      "grad_norm": 0.13074891269207,
      "learning_rate": 3.15973498083414e-05,
      "loss": 0.0133,
      "step": 5105
    },
    {
      "epoch": 1.2453658536585366,
      "grad_norm": 0.19077827036380768,
      "learning_rate": 3.159119009139064e-05,
      "loss": 0.0203,
      "step": 5106
    },
    {
      "epoch": 1.245609756097561,
      "grad_norm": 0.13062377274036407,
      "learning_rate": 3.1585029944384906e-05,
      "loss": 0.0165,
      "step": 5107
    },
    {
      "epoch": 1.2458536585365854,
      "grad_norm": 0.3053484261035919,
      "learning_rate": 3.157886936772614e-05,
      "loss": 0.0219,
      "step": 5108
    },
    {
      "epoch": 1.2460975609756098,
      "grad_norm": 0.13408221304416656,
      "learning_rate": 3.1572708361816275e-05,
      "loss": 0.0175,
      "step": 5109
    },
    {
      "epoch": 1.2463414634146341,
      "grad_norm": 0.24257099628448486,
      "learning_rate": 3.15665469270573e-05,
      "loss": 0.026,
      "step": 5110
    },
    {
      "epoch": 1.2465853658536585,
      "grad_norm": 0.2039182037115097,
      "learning_rate": 3.156038506385125e-05,
      "loss": 0.0339,
      "step": 5111
    },
    {
      "epoch": 1.246829268292683,
      "grad_norm": 0.131747305393219,
      "learning_rate": 3.155422277260017e-05,
      "loss": 0.0141,
      "step": 5112
    },
    {
      "epoch": 1.2470731707317073,
      "grad_norm": 0.07386074960231781,
      "learning_rate": 3.1548060053706105e-05,
      "loss": 0.0092,
      "step": 5113
    },
    {
      "epoch": 1.2473170731707317,
      "grad_norm": 0.17769384384155273,
      "learning_rate": 3.1541896907571164e-05,
      "loss": 0.026,
      "step": 5114
    },
    {
      "epoch": 1.247560975609756,
      "grad_norm": 0.08261342346668243,
      "learning_rate": 3.153573333459748e-05,
      "loss": 0.0227,
      "step": 5115
    },
    {
      "epoch": 1.2478048780487805,
      "grad_norm": 0.29241329431533813,
      "learning_rate": 3.1529569335187206e-05,
      "loss": 0.0303,
      "step": 5116
    },
    {
      "epoch": 1.2480487804878049,
      "grad_norm": 0.12257913500070572,
      "learning_rate": 3.1523404909742525e-05,
      "loss": 0.0207,
      "step": 5117
    },
    {
      "epoch": 1.2482926829268293,
      "grad_norm": 0.10784417390823364,
      "learning_rate": 3.1517240058665644e-05,
      "loss": 0.0231,
      "step": 5118
    },
    {
      "epoch": 1.2485365853658537,
      "grad_norm": 0.24555568397045135,
      "learning_rate": 3.151107478235879e-05,
      "loss": 0.026,
      "step": 5119
    },
    {
      "epoch": 1.248780487804878,
      "grad_norm": 0.27206143736839294,
      "learning_rate": 3.150490908122424e-05,
      "loss": 0.0343,
      "step": 5120
    },
    {
      "epoch": 1.2490243902439024,
      "grad_norm": 0.19943900406360626,
      "learning_rate": 3.14987429556643e-05,
      "loss": 0.0321,
      "step": 5121
    },
    {
      "epoch": 1.2492682926829268,
      "grad_norm": 0.04718639329075813,
      "learning_rate": 3.149257640608127e-05,
      "loss": 0.005,
      "step": 5122
    },
    {
      "epoch": 1.2495121951219512,
      "grad_norm": 0.0665489062666893,
      "learning_rate": 3.14864094328775e-05,
      "loss": 0.0146,
      "step": 5123
    },
    {
      "epoch": 1.2497560975609756,
      "grad_norm": 0.08545202761888504,
      "learning_rate": 3.1480242036455375e-05,
      "loss": 0.0115,
      "step": 5124
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.09030289947986603,
      "learning_rate": 3.1474074217217296e-05,
      "loss": 0.0122,
      "step": 5125
    },
    {
      "epoch": 1.2502439024390244,
      "grad_norm": 0.1458737850189209,
      "learning_rate": 3.1467905975565695e-05,
      "loss": 0.0248,
      "step": 5126
    },
    {
      "epoch": 1.2504878048780488,
      "grad_norm": 0.16609151661396027,
      "learning_rate": 3.1461737311903015e-05,
      "loss": 0.0263,
      "step": 5127
    },
    {
      "epoch": 1.2507317073170732,
      "grad_norm": 0.06251651793718338,
      "learning_rate": 3.145556822663177e-05,
      "loss": 0.0134,
      "step": 5128
    },
    {
      "epoch": 1.2509756097560976,
      "grad_norm": 0.08511541038751602,
      "learning_rate": 3.1449398720154464e-05,
      "loss": 0.0273,
      "step": 5129
    },
    {
      "epoch": 1.251219512195122,
      "grad_norm": 0.07312343269586563,
      "learning_rate": 3.144322879287363e-05,
      "loss": 0.0216,
      "step": 5130
    },
    {
      "epoch": 1.2514634146341463,
      "grad_norm": 0.10851690173149109,
      "learning_rate": 3.143705844519183e-05,
      "loss": 0.0212,
      "step": 5131
    },
    {
      "epoch": 1.2517073170731707,
      "grad_norm": 0.11934848874807358,
      "learning_rate": 3.143088767751169e-05,
      "loss": 0.0242,
      "step": 5132
    },
    {
      "epoch": 1.2519512195121951,
      "grad_norm": 0.04597301036119461,
      "learning_rate": 3.142471649023581e-05,
      "loss": 0.0098,
      "step": 5133
    },
    {
      "epoch": 1.2521951219512195,
      "grad_norm": 0.17193269729614258,
      "learning_rate": 3.1418544883766846e-05,
      "loss": 0.0362,
      "step": 5134
    },
    {
      "epoch": 1.252439024390244,
      "grad_norm": 0.08725694566965103,
      "learning_rate": 3.141237285850748e-05,
      "loss": 0.0157,
      "step": 5135
    },
    {
      "epoch": 1.2526829268292683,
      "grad_norm": 0.12691500782966614,
      "learning_rate": 3.140620041486041e-05,
      "loss": 0.03,
      "step": 5136
    },
    {
      "epoch": 1.2529268292682927,
      "grad_norm": 0.10760103911161423,
      "learning_rate": 3.140002755322838e-05,
      "loss": 0.0242,
      "step": 5137
    },
    {
      "epoch": 1.253170731707317,
      "grad_norm": 0.13612107932567596,
      "learning_rate": 3.139385427401414e-05,
      "loss": 0.0246,
      "step": 5138
    },
    {
      "epoch": 1.2534146341463415,
      "grad_norm": 0.1791965812444687,
      "learning_rate": 3.138768057762048e-05,
      "loss": 0.0329,
      "step": 5139
    },
    {
      "epoch": 1.2536585365853659,
      "grad_norm": 0.1209142729640007,
      "learning_rate": 3.138150646445023e-05,
      "loss": 0.0165,
      "step": 5140
    },
    {
      "epoch": 1.2539024390243902,
      "grad_norm": 0.09148260205984116,
      "learning_rate": 3.137533193490621e-05,
      "loss": 0.0283,
      "step": 5141
    },
    {
      "epoch": 1.2541463414634146,
      "grad_norm": 0.09575679898262024,
      "learning_rate": 3.136915698939129e-05,
      "loss": 0.0342,
      "step": 5142
    },
    {
      "epoch": 1.254390243902439,
      "grad_norm": 0.1445809155702591,
      "learning_rate": 3.1362981628308385e-05,
      "loss": 0.0146,
      "step": 5143
    },
    {
      "epoch": 1.2546341463414634,
      "grad_norm": 0.16179874539375305,
      "learning_rate": 3.13568058520604e-05,
      "loss": 0.0284,
      "step": 5144
    },
    {
      "epoch": 1.2548780487804878,
      "grad_norm": 0.13882943987846375,
      "learning_rate": 3.135062966105029e-05,
      "loss": 0.0228,
      "step": 5145
    },
    {
      "epoch": 1.2551219512195122,
      "grad_norm": 0.3072105348110199,
      "learning_rate": 3.1344453055681055e-05,
      "loss": 0.0243,
      "step": 5146
    },
    {
      "epoch": 1.2553658536585366,
      "grad_norm": 0.09600787609815598,
      "learning_rate": 3.133827603635566e-05,
      "loss": 0.0229,
      "step": 5147
    },
    {
      "epoch": 1.255609756097561,
      "grad_norm": 0.08634263277053833,
      "learning_rate": 3.1332098603477174e-05,
      "loss": 0.0235,
      "step": 5148
    },
    {
      "epoch": 1.2558536585365854,
      "grad_norm": 0.08370471745729446,
      "learning_rate": 3.132592075744863e-05,
      "loss": 0.0159,
      "step": 5149
    },
    {
      "epoch": 1.2560975609756098,
      "grad_norm": 0.17044542729854584,
      "learning_rate": 3.131974249867312e-05,
      "loss": 0.0187,
      "step": 5150
    },
    {
      "epoch": 1.2563414634146342,
      "grad_norm": 0.1208956390619278,
      "learning_rate": 3.131356382755376e-05,
      "loss": 0.0189,
      "step": 5151
    },
    {
      "epoch": 1.2565853658536585,
      "grad_norm": 0.10001958161592484,
      "learning_rate": 3.130738474449369e-05,
      "loss": 0.0189,
      "step": 5152
    },
    {
      "epoch": 1.256829268292683,
      "grad_norm": 0.10735205560922623,
      "learning_rate": 3.130120524989607e-05,
      "loss": 0.0217,
      "step": 5153
    },
    {
      "epoch": 1.2570731707317073,
      "grad_norm": 0.09661184996366501,
      "learning_rate": 3.12950253441641e-05,
      "loss": 0.0174,
      "step": 5154
    },
    {
      "epoch": 1.2573170731707317,
      "grad_norm": 0.07409787178039551,
      "learning_rate": 3.1288845027701e-05,
      "loss": 0.0168,
      "step": 5155
    },
    {
      "epoch": 1.257560975609756,
      "grad_norm": 0.12104193866252899,
      "learning_rate": 3.128266430091001e-05,
      "loss": 0.0235,
      "step": 5156
    },
    {
      "epoch": 1.2578048780487805,
      "grad_norm": 0.11107254028320312,
      "learning_rate": 3.127648316419441e-05,
      "loss": 0.016,
      "step": 5157
    },
    {
      "epoch": 1.2580487804878049,
      "grad_norm": 0.09035738557577133,
      "learning_rate": 3.1270301617957495e-05,
      "loss": 0.0133,
      "step": 5158
    },
    {
      "epoch": 1.2582926829268293,
      "grad_norm": 0.11730312556028366,
      "learning_rate": 3.12641196626026e-05,
      "loss": 0.0245,
      "step": 5159
    },
    {
      "epoch": 1.2585365853658537,
      "grad_norm": 0.10772690176963806,
      "learning_rate": 3.125793729853307e-05,
      "loss": 0.0223,
      "step": 5160
    },
    {
      "epoch": 1.258780487804878,
      "grad_norm": 0.20624063909053802,
      "learning_rate": 3.125175452615228e-05,
      "loss": 0.0285,
      "step": 5161
    },
    {
      "epoch": 1.2590243902439024,
      "grad_norm": 0.08089543133974075,
      "learning_rate": 3.1245571345863656e-05,
      "loss": 0.0167,
      "step": 5162
    },
    {
      "epoch": 1.2592682926829268,
      "grad_norm": 0.09394185245037079,
      "learning_rate": 3.1239387758070624e-05,
      "loss": 0.0158,
      "step": 5163
    },
    {
      "epoch": 1.2595121951219512,
      "grad_norm": 0.1462869942188263,
      "learning_rate": 3.123320376317663e-05,
      "loss": 0.016,
      "step": 5164
    },
    {
      "epoch": 1.2597560975609756,
      "grad_norm": 0.11413168907165527,
      "learning_rate": 3.122701936158519e-05,
      "loss": 0.0185,
      "step": 5165
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.058390382677316666,
      "learning_rate": 3.122083455369978e-05,
      "loss": 0.0152,
      "step": 5166
    },
    {
      "epoch": 1.2602439024390244,
      "grad_norm": 0.22638559341430664,
      "learning_rate": 3.121464933992397e-05,
      "loss": 0.0193,
      "step": 5167
    },
    {
      "epoch": 1.2604878048780488,
      "grad_norm": 0.11003909260034561,
      "learning_rate": 3.1208463720661307e-05,
      "loss": 0.0165,
      "step": 5168
    },
    {
      "epoch": 1.2607317073170732,
      "grad_norm": 0.10551250725984573,
      "learning_rate": 3.1202277696315396e-05,
      "loss": 0.0212,
      "step": 5169
    },
    {
      "epoch": 1.2609756097560976,
      "grad_norm": 0.13489961624145508,
      "learning_rate": 3.119609126728986e-05,
      "loss": 0.013,
      "step": 5170
    },
    {
      "epoch": 1.261219512195122,
      "grad_norm": 0.1599065661430359,
      "learning_rate": 3.1189904433988324e-05,
      "loss": 0.0267,
      "step": 5171
    },
    {
      "epoch": 1.2614634146341464,
      "grad_norm": 0.06662684679031372,
      "learning_rate": 3.1183717196814486e-05,
      "loss": 0.0183,
      "step": 5172
    },
    {
      "epoch": 1.2617073170731707,
      "grad_norm": 0.1603056788444519,
      "learning_rate": 3.117752955617202e-05,
      "loss": 0.0193,
      "step": 5173
    },
    {
      "epoch": 1.2619512195121951,
      "grad_norm": 0.057836685329675674,
      "learning_rate": 3.117134151246467e-05,
      "loss": 0.0191,
      "step": 5174
    },
    {
      "epoch": 1.2621951219512195,
      "grad_norm": 0.14337797462940216,
      "learning_rate": 3.116515306609617e-05,
      "loss": 0.0295,
      "step": 5175
    },
    {
      "epoch": 1.262439024390244,
      "grad_norm": 0.3008018136024475,
      "learning_rate": 3.1158964217470314e-05,
      "loss": 0.0174,
      "step": 5176
    },
    {
      "epoch": 1.2626829268292683,
      "grad_norm": 0.1031789779663086,
      "learning_rate": 3.115277496699089e-05,
      "loss": 0.0235,
      "step": 5177
    },
    {
      "epoch": 1.2629268292682927,
      "grad_norm": 0.058292046189308167,
      "learning_rate": 3.114658531506174e-05,
      "loss": 0.0132,
      "step": 5178
    },
    {
      "epoch": 1.263170731707317,
      "grad_norm": 0.0978986844420433,
      "learning_rate": 3.1140395262086716e-05,
      "loss": 0.0249,
      "step": 5179
    },
    {
      "epoch": 1.2634146341463415,
      "grad_norm": 0.08045083284378052,
      "learning_rate": 3.1134204808469694e-05,
      "loss": 0.0155,
      "step": 5180
    },
    {
      "epoch": 1.2636585365853659,
      "grad_norm": 0.1272813081741333,
      "learning_rate": 3.1128013954614595e-05,
      "loss": 0.0233,
      "step": 5181
    },
    {
      "epoch": 1.2639024390243903,
      "grad_norm": 0.07754414528608322,
      "learning_rate": 3.112182270092534e-05,
      "loss": 0.0204,
      "step": 5182
    },
    {
      "epoch": 1.2641463414634146,
      "grad_norm": 0.10715790092945099,
      "learning_rate": 3.1115631047805884e-05,
      "loss": 0.0167,
      "step": 5183
    },
    {
      "epoch": 1.264390243902439,
      "grad_norm": 0.0997125431895256,
      "learning_rate": 3.110943899566024e-05,
      "loss": 0.0197,
      "step": 5184
    },
    {
      "epoch": 1.2646341463414634,
      "grad_norm": 0.1543925553560257,
      "learning_rate": 3.1103246544892395e-05,
      "loss": 0.0148,
      "step": 5185
    },
    {
      "epoch": 1.2648780487804878,
      "grad_norm": 0.15832075476646423,
      "learning_rate": 3.1097053695906395e-05,
      "loss": 0.0279,
      "step": 5186
    },
    {
      "epoch": 1.2651219512195122,
      "grad_norm": 0.1742778718471527,
      "learning_rate": 3.109086044910631e-05,
      "loss": 0.034,
      "step": 5187
    },
    {
      "epoch": 1.2653658536585366,
      "grad_norm": 0.08018745481967926,
      "learning_rate": 3.1084666804896234e-05,
      "loss": 0.0146,
      "step": 5188
    },
    {
      "epoch": 1.265609756097561,
      "grad_norm": 0.10691888630390167,
      "learning_rate": 3.107847276368027e-05,
      "loss": 0.01,
      "step": 5189
    },
    {
      "epoch": 1.2658536585365854,
      "grad_norm": 0.09794852137565613,
      "learning_rate": 3.107227832586257e-05,
      "loss": 0.0106,
      "step": 5190
    },
    {
      "epoch": 1.2660975609756098,
      "grad_norm": 0.20103073120117188,
      "learning_rate": 3.106608349184729e-05,
      "loss": 0.0251,
      "step": 5191
    },
    {
      "epoch": 1.2663414634146342,
      "grad_norm": 0.272754967212677,
      "learning_rate": 3.105988826203863e-05,
      "loss": 0.035,
      "step": 5192
    },
    {
      "epoch": 1.2665853658536586,
      "grad_norm": 0.10819926857948303,
      "learning_rate": 3.105369263684082e-05,
      "loss": 0.0253,
      "step": 5193
    },
    {
      "epoch": 1.266829268292683,
      "grad_norm": 0.12110017240047455,
      "learning_rate": 3.10474966166581e-05,
      "loss": 0.0126,
      "step": 5194
    },
    {
      "epoch": 1.2670731707317073,
      "grad_norm": 0.2606015205383301,
      "learning_rate": 3.104130020189473e-05,
      "loss": 0.0213,
      "step": 5195
    },
    {
      "epoch": 1.2673170731707317,
      "grad_norm": 0.12683598697185516,
      "learning_rate": 3.103510339295502e-05,
      "loss": 0.0211,
      "step": 5196
    },
    {
      "epoch": 1.2675609756097561,
      "grad_norm": 0.1539195030927658,
      "learning_rate": 3.102890619024329e-05,
      "loss": 0.0289,
      "step": 5197
    },
    {
      "epoch": 1.2678048780487805,
      "grad_norm": 0.09446100145578384,
      "learning_rate": 3.102270859416389e-05,
      "loss": 0.0188,
      "step": 5198
    },
    {
      "epoch": 1.268048780487805,
      "grad_norm": 0.2519475817680359,
      "learning_rate": 3.101651060512118e-05,
      "loss": 0.0338,
      "step": 5199
    },
    {
      "epoch": 1.2682926829268293,
      "grad_norm": 0.11554206907749176,
      "learning_rate": 3.1010312223519584e-05,
      "loss": 0.0166,
      "step": 5200
    },
    {
      "epoch": 1.2685365853658537,
      "grad_norm": 0.07156571000814438,
      "learning_rate": 3.1004113449763516e-05,
      "loss": 0.0135,
      "step": 5201
    },
    {
      "epoch": 1.268780487804878,
      "grad_norm": 0.13382543623447418,
      "learning_rate": 3.0997914284257415e-05,
      "loss": 0.0151,
      "step": 5202
    },
    {
      "epoch": 1.2690243902439025,
      "grad_norm": 0.27144595980644226,
      "learning_rate": 3.0991714727405774e-05,
      "loss": 0.027,
      "step": 5203
    },
    {
      "epoch": 1.2692682926829268,
      "grad_norm": 0.2694515883922577,
      "learning_rate": 3.0985514779613084e-05,
      "loss": 0.0153,
      "step": 5204
    },
    {
      "epoch": 1.2695121951219512,
      "grad_norm": 0.0605335496366024,
      "learning_rate": 3.097931444128389e-05,
      "loss": 0.0143,
      "step": 5205
    },
    {
      "epoch": 1.2697560975609756,
      "grad_norm": 0.1260892003774643,
      "learning_rate": 3.0973113712822724e-05,
      "loss": 0.0216,
      "step": 5206
    },
    {
      "epoch": 1.27,
      "grad_norm": 0.0839461013674736,
      "learning_rate": 3.096691259463418e-05,
      "loss": 0.0231,
      "step": 5207
    },
    {
      "epoch": 1.2702439024390244,
      "grad_norm": 0.09926880151033401,
      "learning_rate": 3.096071108712285e-05,
      "loss": 0.0259,
      "step": 5208
    },
    {
      "epoch": 1.2704878048780488,
      "grad_norm": 0.13129574060440063,
      "learning_rate": 3.095450919069336e-05,
      "loss": 0.0251,
      "step": 5209
    },
    {
      "epoch": 1.2707317073170732,
      "grad_norm": 0.09376156330108643,
      "learning_rate": 3.094830690575039e-05,
      "loss": 0.0213,
      "step": 5210
    },
    {
      "epoch": 1.2709756097560976,
      "grad_norm": 0.10426489263772964,
      "learning_rate": 3.09421042326986e-05,
      "loss": 0.0103,
      "step": 5211
    },
    {
      "epoch": 1.271219512195122,
      "grad_norm": 0.07683473825454712,
      "learning_rate": 3.093590117194269e-05,
      "loss": 0.0119,
      "step": 5212
    },
    {
      "epoch": 1.2714634146341464,
      "grad_norm": 0.07144028693437576,
      "learning_rate": 3.0929697723887416e-05,
      "loss": 0.0196,
      "step": 5213
    },
    {
      "epoch": 1.2717073170731708,
      "grad_norm": 0.08574246615171432,
      "learning_rate": 3.0923493888937514e-05,
      "loss": 0.016,
      "step": 5214
    },
    {
      "epoch": 1.2719512195121951,
      "grad_norm": 0.16967935860157013,
      "learning_rate": 3.091728966749776e-05,
      "loss": 0.0125,
      "step": 5215
    },
    {
      "epoch": 1.2721951219512195,
      "grad_norm": 0.11934618651866913,
      "learning_rate": 3.091108505997298e-05,
      "loss": 0.023,
      "step": 5216
    },
    {
      "epoch": 1.272439024390244,
      "grad_norm": 0.060819000005722046,
      "learning_rate": 3.0904880066767983e-05,
      "loss": 0.0164,
      "step": 5217
    },
    {
      "epoch": 1.2726829268292683,
      "grad_norm": 0.11062896996736526,
      "learning_rate": 3.089867468828765e-05,
      "loss": 0.0155,
      "step": 5218
    },
    {
      "epoch": 1.2729268292682927,
      "grad_norm": 0.11032712459564209,
      "learning_rate": 3.089246892493684e-05,
      "loss": 0.0205,
      "step": 5219
    },
    {
      "epoch": 1.273170731707317,
      "grad_norm": 0.049612607806921005,
      "learning_rate": 3.088626277712048e-05,
      "loss": 0.0109,
      "step": 5220
    },
    {
      "epoch": 1.2734146341463415,
      "grad_norm": 0.1217857375741005,
      "learning_rate": 3.088005624524349e-05,
      "loss": 0.0192,
      "step": 5221
    },
    {
      "epoch": 1.2736585365853659,
      "grad_norm": 0.1345323920249939,
      "learning_rate": 3.087384932971084e-05,
      "loss": 0.0301,
      "step": 5222
    },
    {
      "epoch": 1.2739024390243903,
      "grad_norm": 0.11267470568418503,
      "learning_rate": 3.086764203092749e-05,
      "loss": 0.0307,
      "step": 5223
    },
    {
      "epoch": 1.2741463414634147,
      "grad_norm": 0.05537321791052818,
      "learning_rate": 3.086143434929846e-05,
      "loss": 0.0122,
      "step": 5224
    },
    {
      "epoch": 1.274390243902439,
      "grad_norm": 0.21327388286590576,
      "learning_rate": 3.085522628522879e-05,
      "loss": 0.0251,
      "step": 5225
    },
    {
      "epoch": 1.2746341463414634,
      "grad_norm": 0.10828294605016708,
      "learning_rate": 3.0849017839123525e-05,
      "loss": 0.0266,
      "step": 5226
    },
    {
      "epoch": 1.2748780487804878,
      "grad_norm": 0.1299908608198166,
      "learning_rate": 3.0842809011387755e-05,
      "loss": 0.0207,
      "step": 5227
    },
    {
      "epoch": 1.2751219512195122,
      "grad_norm": 0.36192962527275085,
      "learning_rate": 3.083659980242658e-05,
      "loss": 0.019,
      "step": 5228
    },
    {
      "epoch": 1.2753658536585366,
      "grad_norm": 0.12541036307811737,
      "learning_rate": 3.0830390212645136e-05,
      "loss": 0.0175,
      "step": 5229
    },
    {
      "epoch": 1.275609756097561,
      "grad_norm": 0.12313881516456604,
      "learning_rate": 3.082418024244858e-05,
      "loss": 0.0263,
      "step": 5230
    },
    {
      "epoch": 1.2758536585365854,
      "grad_norm": 0.0799301266670227,
      "learning_rate": 3.0817969892242095e-05,
      "loss": 0.0196,
      "step": 5231
    },
    {
      "epoch": 1.2760975609756098,
      "grad_norm": 0.07261207699775696,
      "learning_rate": 3.081175916243088e-05,
      "loss": 0.0114,
      "step": 5232
    },
    {
      "epoch": 1.2763414634146342,
      "grad_norm": 0.11188413202762604,
      "learning_rate": 3.080554805342017e-05,
      "loss": 0.031,
      "step": 5233
    },
    {
      "epoch": 1.2765853658536586,
      "grad_norm": 0.1542530655860901,
      "learning_rate": 3.079933656561523e-05,
      "loss": 0.017,
      "step": 5234
    },
    {
      "epoch": 1.276829268292683,
      "grad_norm": 0.20781226456165314,
      "learning_rate": 3.079312469942133e-05,
      "loss": 0.0289,
      "step": 5235
    },
    {
      "epoch": 1.2770731707317073,
      "grad_norm": 0.09389705210924149,
      "learning_rate": 3.0786912455243774e-05,
      "loss": 0.0256,
      "step": 5236
    },
    {
      "epoch": 1.2773170731707317,
      "grad_norm": 0.06551327556371689,
      "learning_rate": 3.07806998334879e-05,
      "loss": 0.0144,
      "step": 5237
    },
    {
      "epoch": 1.2775609756097561,
      "grad_norm": 0.10297616571187973,
      "learning_rate": 3.077448683455905e-05,
      "loss": 0.0174,
      "step": 5238
    },
    {
      "epoch": 1.2778048780487805,
      "grad_norm": 0.08941560238599777,
      "learning_rate": 3.076827345886263e-05,
      "loss": 0.0163,
      "step": 5239
    },
    {
      "epoch": 1.278048780487805,
      "grad_norm": 0.2462402582168579,
      "learning_rate": 3.076205970680401e-05,
      "loss": 0.0178,
      "step": 5240
    },
    {
      "epoch": 1.2782926829268293,
      "grad_norm": 0.05667432025074959,
      "learning_rate": 3.0755845578788635e-05,
      "loss": 0.0083,
      "step": 5241
    },
    {
      "epoch": 1.2785365853658537,
      "grad_norm": 0.13020211458206177,
      "learning_rate": 3.074963107522195e-05,
      "loss": 0.0126,
      "step": 5242
    },
    {
      "epoch": 1.278780487804878,
      "grad_norm": 0.07818498462438583,
      "learning_rate": 3.074341619650945e-05,
      "loss": 0.0137,
      "step": 5243
    },
    {
      "epoch": 1.2790243902439025,
      "grad_norm": 0.0906396135687828,
      "learning_rate": 3.073720094305662e-05,
      "loss": 0.0157,
      "step": 5244
    },
    {
      "epoch": 1.2792682926829269,
      "grad_norm": 0.23020851612091064,
      "learning_rate": 3.0730985315269e-05,
      "loss": 0.0349,
      "step": 5245
    },
    {
      "epoch": 1.2795121951219512,
      "grad_norm": 0.18666180968284607,
      "learning_rate": 3.072476931355212e-05,
      "loss": 0.0247,
      "step": 5246
    },
    {
      "epoch": 1.2797560975609756,
      "grad_norm": 0.1232009157538414,
      "learning_rate": 3.071855293831157e-05,
      "loss": 0.0234,
      "step": 5247
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.23363184928894043,
      "learning_rate": 3.071233618995296e-05,
      "loss": 0.0278,
      "step": 5248
    },
    {
      "epoch": 1.2802439024390244,
      "grad_norm": 0.16009725630283356,
      "learning_rate": 3.070611906888189e-05,
      "loss": 0.026,
      "step": 5249
    },
    {
      "epoch": 1.2804878048780488,
      "grad_norm": 0.12142381817102432,
      "learning_rate": 3.069990157550402e-05,
      "loss": 0.0161,
      "step": 5250
    },
    {
      "epoch": 1.2807317073170732,
      "grad_norm": 0.1607731580734253,
      "learning_rate": 3.069368371022501e-05,
      "loss": 0.0178,
      "step": 5251
    },
    {
      "epoch": 1.2809756097560976,
      "grad_norm": 0.11643388122320175,
      "learning_rate": 3.068746547345059e-05,
      "loss": 0.0166,
      "step": 5252
    },
    {
      "epoch": 1.281219512195122,
      "grad_norm": 0.07757027447223663,
      "learning_rate": 3.0681246865586444e-05,
      "loss": 0.0106,
      "step": 5253
    },
    {
      "epoch": 1.2814634146341464,
      "grad_norm": 0.09669894725084305,
      "learning_rate": 3.0675027887038334e-05,
      "loss": 0.016,
      "step": 5254
    },
    {
      "epoch": 1.2817073170731708,
      "grad_norm": 0.061057765036821365,
      "learning_rate": 3.0668808538212035e-05,
      "loss": 0.0129,
      "step": 5255
    },
    {
      "epoch": 1.2819512195121952,
      "grad_norm": 0.12099198251962662,
      "learning_rate": 3.066258881951333e-05,
      "loss": 0.0285,
      "step": 5256
    },
    {
      "epoch": 1.2821951219512195,
      "grad_norm": 0.12979735434055328,
      "learning_rate": 3.065636873134803e-05,
      "loss": 0.0315,
      "step": 5257
    },
    {
      "epoch": 1.282439024390244,
      "grad_norm": 0.3805334270000458,
      "learning_rate": 3.065014827412199e-05,
      "loss": 0.0215,
      "step": 5258
    },
    {
      "epoch": 1.2826829268292683,
      "grad_norm": 0.08396485447883606,
      "learning_rate": 3.0643927448241074e-05,
      "loss": 0.0177,
      "step": 5259
    },
    {
      "epoch": 1.2829268292682927,
      "grad_norm": 0.14380621910095215,
      "learning_rate": 3.0637706254111175e-05,
      "loss": 0.016,
      "step": 5260
    },
    {
      "epoch": 1.283170731707317,
      "grad_norm": 0.08358506113290787,
      "learning_rate": 3.0631484692138196e-05,
      "loss": 0.0116,
      "step": 5261
    },
    {
      "epoch": 1.2834146341463415,
      "grad_norm": 0.1640446037054062,
      "learning_rate": 3.062526276272808e-05,
      "loss": 0.0297,
      "step": 5262
    },
    {
      "epoch": 1.2836585365853659,
      "grad_norm": 0.22168506681919098,
      "learning_rate": 3.061904046628679e-05,
      "loss": 0.0208,
      "step": 5263
    },
    {
      "epoch": 1.2839024390243903,
      "grad_norm": 0.10843781381845474,
      "learning_rate": 3.061281780322031e-05,
      "loss": 0.0126,
      "step": 5264
    },
    {
      "epoch": 1.2841463414634147,
      "grad_norm": 0.09647104889154434,
      "learning_rate": 3.060659477393466e-05,
      "loss": 0.0176,
      "step": 5265
    },
    {
      "epoch": 1.284390243902439,
      "grad_norm": 0.11394264549016953,
      "learning_rate": 3.060037137883585e-05,
      "loss": 0.0134,
      "step": 5266
    },
    {
      "epoch": 1.2846341463414634,
      "grad_norm": 0.13608881831169128,
      "learning_rate": 3.059414761832996e-05,
      "loss": 0.0204,
      "step": 5267
    },
    {
      "epoch": 1.2848780487804878,
      "grad_norm": 0.24227657914161682,
      "learning_rate": 3.058792349282306e-05,
      "loss": 0.0176,
      "step": 5268
    },
    {
      "epoch": 1.2851219512195122,
      "grad_norm": 0.10107208043336868,
      "learning_rate": 3.058169900272126e-05,
      "loss": 0.0117,
      "step": 5269
    },
    {
      "epoch": 1.2853658536585366,
      "grad_norm": 0.10441964119672775,
      "learning_rate": 3.057547414843069e-05,
      "loss": 0.0194,
      "step": 5270
    },
    {
      "epoch": 1.285609756097561,
      "grad_norm": 0.09130494296550751,
      "learning_rate": 3.05692489303575e-05,
      "loss": 0.0206,
      "step": 5271
    },
    {
      "epoch": 1.2858536585365854,
      "grad_norm": 0.05793464928865433,
      "learning_rate": 3.056302334890786e-05,
      "loss": 0.0151,
      "step": 5272
    },
    {
      "epoch": 1.2860975609756098,
      "grad_norm": 0.23600950837135315,
      "learning_rate": 3.0556797404487984e-05,
      "loss": 0.0223,
      "step": 5273
    },
    {
      "epoch": 1.2863414634146342,
      "grad_norm": 0.12894058227539062,
      "learning_rate": 3.055057109750409e-05,
      "loss": 0.023,
      "step": 5274
    },
    {
      "epoch": 1.2865853658536586,
      "grad_norm": 0.1614779829978943,
      "learning_rate": 3.0544344428362415e-05,
      "loss": 0.0378,
      "step": 5275
    },
    {
      "epoch": 1.286829268292683,
      "grad_norm": 0.1192714273929596,
      "learning_rate": 3.053811739746924e-05,
      "loss": 0.0303,
      "step": 5276
    },
    {
      "epoch": 1.2870731707317074,
      "grad_norm": 0.08188559859991074,
      "learning_rate": 3.053189000523086e-05,
      "loss": 0.0198,
      "step": 5277
    },
    {
      "epoch": 1.2873170731707317,
      "grad_norm": 0.2180614471435547,
      "learning_rate": 3.0525662252053594e-05,
      "loss": 0.0391,
      "step": 5278
    },
    {
      "epoch": 1.2875609756097561,
      "grad_norm": 0.08359450101852417,
      "learning_rate": 3.0519434138343775e-05,
      "loss": 0.0308,
      "step": 5279
    },
    {
      "epoch": 1.2878048780487805,
      "grad_norm": 0.15534839034080505,
      "learning_rate": 3.0513205664507788e-05,
      "loss": 0.0129,
      "step": 5280
    },
    {
      "epoch": 1.288048780487805,
      "grad_norm": 0.08166816830635071,
      "learning_rate": 3.0506976830952e-05,
      "loss": 0.013,
      "step": 5281
    },
    {
      "epoch": 1.2882926829268293,
      "grad_norm": 0.11938531696796417,
      "learning_rate": 3.0500747638082838e-05,
      "loss": 0.0202,
      "step": 5282
    },
    {
      "epoch": 1.2885365853658537,
      "grad_norm": 0.07865763455629349,
      "learning_rate": 3.0494518086306728e-05,
      "loss": 0.0224,
      "step": 5283
    },
    {
      "epoch": 1.288780487804878,
      "grad_norm": 0.1177007406949997,
      "learning_rate": 3.0488288176030134e-05,
      "loss": 0.017,
      "step": 5284
    },
    {
      "epoch": 1.2890243902439025,
      "grad_norm": 0.10843416303396225,
      "learning_rate": 3.048205790765954e-05,
      "loss": 0.0226,
      "step": 5285
    },
    {
      "epoch": 1.2892682926829269,
      "grad_norm": 0.14899179339408875,
      "learning_rate": 3.0475827281601444e-05,
      "loss": 0.0302,
      "step": 5286
    },
    {
      "epoch": 1.2895121951219513,
      "grad_norm": 0.07612765580415726,
      "learning_rate": 3.046959629826238e-05,
      "loss": 0.0135,
      "step": 5287
    },
    {
      "epoch": 1.2897560975609756,
      "grad_norm": 0.10684942454099655,
      "learning_rate": 3.0463364958048912e-05,
      "loss": 0.0115,
      "step": 5288
    },
    {
      "epoch": 1.29,
      "grad_norm": 0.06697414070367813,
      "learning_rate": 3.045713326136759e-05,
      "loss": 0.0234,
      "step": 5289
    },
    {
      "epoch": 1.2902439024390244,
      "grad_norm": 0.21016249060630798,
      "learning_rate": 3.0450901208625045e-05,
      "loss": 0.0274,
      "step": 5290
    },
    {
      "epoch": 1.2904878048780488,
      "grad_norm": 0.19371064007282257,
      "learning_rate": 3.044466880022787e-05,
      "loss": 0.0362,
      "step": 5291
    },
    {
      "epoch": 1.2907317073170732,
      "grad_norm": 0.14105293154716492,
      "learning_rate": 3.0438436036582728e-05,
      "loss": 0.0333,
      "step": 5292
    },
    {
      "epoch": 1.2909756097560976,
      "grad_norm": 0.16447873413562775,
      "learning_rate": 3.0432202918096286e-05,
      "loss": 0.0148,
      "step": 5293
    },
    {
      "epoch": 1.291219512195122,
      "grad_norm": 0.1769595742225647,
      "learning_rate": 3.0425969445175233e-05,
      "loss": 0.0137,
      "step": 5294
    },
    {
      "epoch": 1.2914634146341464,
      "grad_norm": 0.09895281493663788,
      "learning_rate": 3.041973561822628e-05,
      "loss": 0.0206,
      "step": 5295
    },
    {
      "epoch": 1.2917073170731708,
      "grad_norm": 0.14905788004398346,
      "learning_rate": 3.0413501437656173e-05,
      "loss": 0.0218,
      "step": 5296
    },
    {
      "epoch": 1.2919512195121952,
      "grad_norm": 0.10274378210306168,
      "learning_rate": 3.0407266903871666e-05,
      "loss": 0.0241,
      "step": 5297
    },
    {
      "epoch": 1.2921951219512195,
      "grad_norm": 0.1519930511713028,
      "learning_rate": 3.0401032017279557e-05,
      "loss": 0.0237,
      "step": 5298
    },
    {
      "epoch": 1.292439024390244,
      "grad_norm": 0.10835042595863342,
      "learning_rate": 3.0394796778286633e-05,
      "loss": 0.0205,
      "step": 5299
    },
    {
      "epoch": 1.2926829268292683,
      "grad_norm": 0.12284092605113983,
      "learning_rate": 3.0388561187299736e-05,
      "loss": 0.0182,
      "step": 5300
    },
    {
      "epoch": 1.2929268292682927,
      "grad_norm": 0.07504202425479889,
      "learning_rate": 3.038232524472572e-05,
      "loss": 0.0186,
      "step": 5301
    },
    {
      "epoch": 1.2931707317073171,
      "grad_norm": 0.22219933569431305,
      "learning_rate": 3.0376088950971455e-05,
      "loss": 0.0344,
      "step": 5302
    },
    {
      "epoch": 1.2934146341463415,
      "grad_norm": 0.2744174003601074,
      "learning_rate": 3.0369852306443843e-05,
      "loss": 0.0087,
      "step": 5303
    },
    {
      "epoch": 1.293658536585366,
      "grad_norm": 0.15277723968029022,
      "learning_rate": 3.036361531154981e-05,
      "loss": 0.0247,
      "step": 5304
    },
    {
      "epoch": 1.2939024390243903,
      "grad_norm": 0.14208485186100006,
      "learning_rate": 3.03573779666963e-05,
      "loss": 0.021,
      "step": 5305
    },
    {
      "epoch": 1.2941463414634147,
      "grad_norm": 0.1260342001914978,
      "learning_rate": 3.0351140272290268e-05,
      "loss": 0.0235,
      "step": 5306
    },
    {
      "epoch": 1.294390243902439,
      "grad_norm": 0.15086056292057037,
      "learning_rate": 3.0344902228738727e-05,
      "loss": 0.0174,
      "step": 5307
    },
    {
      "epoch": 1.2946341463414635,
      "grad_norm": 0.14360491931438446,
      "learning_rate": 3.033866383644867e-05,
      "loss": 0.0351,
      "step": 5308
    },
    {
      "epoch": 1.2948780487804878,
      "grad_norm": 0.09834067523479462,
      "learning_rate": 3.0332425095827144e-05,
      "loss": 0.0196,
      "step": 5309
    },
    {
      "epoch": 1.2951219512195122,
      "grad_norm": 0.2415812760591507,
      "learning_rate": 3.03261860072812e-05,
      "loss": 0.0195,
      "step": 5310
    },
    {
      "epoch": 1.2953658536585366,
      "grad_norm": 0.1368889957666397,
      "learning_rate": 3.0319946571217932e-05,
      "loss": 0.0299,
      "step": 5311
    },
    {
      "epoch": 1.295609756097561,
      "grad_norm": 0.16979719698429108,
      "learning_rate": 3.0313706788044428e-05,
      "loss": 0.0231,
      "step": 5312
    },
    {
      "epoch": 1.2958536585365854,
      "grad_norm": 0.09159895032644272,
      "learning_rate": 3.0307466658167834e-05,
      "loss": 0.022,
      "step": 5313
    },
    {
      "epoch": 1.2960975609756098,
      "grad_norm": 0.07176774740219116,
      "learning_rate": 3.030122618199528e-05,
      "loss": 0.0083,
      "step": 5314
    },
    {
      "epoch": 1.2963414634146342,
      "grad_norm": 0.08084741234779358,
      "learning_rate": 3.0294985359933948e-05,
      "loss": 0.0148,
      "step": 5315
    },
    {
      "epoch": 1.2965853658536586,
      "grad_norm": 0.17666412889957428,
      "learning_rate": 3.028874419239103e-05,
      "loss": 0.0241,
      "step": 5316
    },
    {
      "epoch": 1.296829268292683,
      "grad_norm": 0.09570835530757904,
      "learning_rate": 3.0282502679773738e-05,
      "loss": 0.0123,
      "step": 5317
    },
    {
      "epoch": 1.2970731707317074,
      "grad_norm": 0.10806066542863846,
      "learning_rate": 3.027626082248932e-05,
      "loss": 0.0181,
      "step": 5318
    },
    {
      "epoch": 1.2973170731707317,
      "grad_norm": 0.0629645586013794,
      "learning_rate": 3.0270018620945047e-05,
      "loss": 0.0171,
      "step": 5319
    },
    {
      "epoch": 1.2975609756097561,
      "grad_norm": 0.19306430220603943,
      "learning_rate": 3.0263776075548177e-05,
      "loss": 0.0412,
      "step": 5320
    },
    {
      "epoch": 1.2978048780487805,
      "grad_norm": 0.12129087746143341,
      "learning_rate": 3.0257533186706045e-05,
      "loss": 0.0254,
      "step": 5321
    },
    {
      "epoch": 1.298048780487805,
      "grad_norm": 0.09482661634683609,
      "learning_rate": 3.0251289954825963e-05,
      "loss": 0.0176,
      "step": 5322
    },
    {
      "epoch": 1.2982926829268293,
      "grad_norm": 0.10582365840673447,
      "learning_rate": 3.0245046380315285e-05,
      "loss": 0.0239,
      "step": 5323
    },
    {
      "epoch": 1.2985365853658537,
      "grad_norm": 0.7674252390861511,
      "learning_rate": 3.023880246358139e-05,
      "loss": 0.033,
      "step": 5324
    },
    {
      "epoch": 1.298780487804878,
      "grad_norm": 0.06428422778844833,
      "learning_rate": 3.0232558205031668e-05,
      "loss": 0.0187,
      "step": 5325
    },
    {
      "epoch": 1.2990243902439025,
      "grad_norm": 0.05882513150572777,
      "learning_rate": 3.0226313605073542e-05,
      "loss": 0.0129,
      "step": 5326
    },
    {
      "epoch": 1.2992682926829269,
      "grad_norm": 0.13793110847473145,
      "learning_rate": 3.0220068664114452e-05,
      "loss": 0.023,
      "step": 5327
    },
    {
      "epoch": 1.2995121951219513,
      "grad_norm": 0.10111448913812637,
      "learning_rate": 3.0213823382561863e-05,
      "loss": 0.0176,
      "step": 5328
    },
    {
      "epoch": 1.2997560975609757,
      "grad_norm": 0.08046851307153702,
      "learning_rate": 3.0207577760823258e-05,
      "loss": 0.0182,
      "step": 5329
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.13977323472499847,
      "learning_rate": 3.0201331799306154e-05,
      "loss": 0.0251,
      "step": 5330
    },
    {
      "epoch": 1.3002439024390244,
      "grad_norm": 0.08566810190677643,
      "learning_rate": 3.0195085498418063e-05,
      "loss": 0.0205,
      "step": 5331
    },
    {
      "epoch": 1.3004878048780488,
      "grad_norm": 0.2786334156990051,
      "learning_rate": 3.0188838858566553e-05,
      "loss": 0.0366,
      "step": 5332
    },
    {
      "epoch": 1.3007317073170732,
      "grad_norm": 0.2047230452299118,
      "learning_rate": 3.018259188015919e-05,
      "loss": 0.0148,
      "step": 5333
    },
    {
      "epoch": 1.3009756097560976,
      "grad_norm": 0.20317737758159637,
      "learning_rate": 3.017634456360357e-05,
      "loss": 0.0277,
      "step": 5334
    },
    {
      "epoch": 1.301219512195122,
      "grad_norm": 0.11841483414173126,
      "learning_rate": 3.0170096909307314e-05,
      "loss": 0.0219,
      "step": 5335
    },
    {
      "epoch": 1.3014634146341464,
      "grad_norm": 0.13944323360919952,
      "learning_rate": 3.0163848917678065e-05,
      "loss": 0.0094,
      "step": 5336
    },
    {
      "epoch": 1.3017073170731708,
      "grad_norm": 0.055114954710006714,
      "learning_rate": 3.015760058912348e-05,
      "loss": 0.0109,
      "step": 5337
    },
    {
      "epoch": 1.3019512195121952,
      "grad_norm": 0.12245185673236847,
      "learning_rate": 3.015135192405125e-05,
      "loss": 0.0296,
      "step": 5338
    },
    {
      "epoch": 1.3021951219512196,
      "grad_norm": 0.2079605907201767,
      "learning_rate": 3.0145102922869073e-05,
      "loss": 0.0188,
      "step": 5339
    },
    {
      "epoch": 1.302439024390244,
      "grad_norm": 0.4015468657016754,
      "learning_rate": 3.013885358598468e-05,
      "loss": 0.0125,
      "step": 5340
    },
    {
      "epoch": 1.3026829268292683,
      "grad_norm": 0.16381235420703888,
      "learning_rate": 3.0132603913805823e-05,
      "loss": 0.0144,
      "step": 5341
    },
    {
      "epoch": 1.3029268292682927,
      "grad_norm": 0.11321339756250381,
      "learning_rate": 3.0126353906740274e-05,
      "loss": 0.0155,
      "step": 5342
    },
    {
      "epoch": 1.3031707317073171,
      "grad_norm": 0.11789771914482117,
      "learning_rate": 3.0120103565195824e-05,
      "loss": 0.0163,
      "step": 5343
    },
    {
      "epoch": 1.3034146341463415,
      "grad_norm": 0.2658296525478363,
      "learning_rate": 3.011385288958029e-05,
      "loss": 0.0259,
      "step": 5344
    },
    {
      "epoch": 1.303658536585366,
      "grad_norm": 0.08288519084453583,
      "learning_rate": 3.010760188030151e-05,
      "loss": 0.0157,
      "step": 5345
    },
    {
      "epoch": 1.3039024390243903,
      "grad_norm": 0.06863513588905334,
      "learning_rate": 3.0101350537767347e-05,
      "loss": 0.0161,
      "step": 5346
    },
    {
      "epoch": 1.3041463414634147,
      "grad_norm": 0.4234957993030548,
      "learning_rate": 3.0095098862385674e-05,
      "loss": 0.0108,
      "step": 5347
    },
    {
      "epoch": 1.304390243902439,
      "grad_norm": 0.25080257654190063,
      "learning_rate": 3.00888468545644e-05,
      "loss": 0.0133,
      "step": 5348
    },
    {
      "epoch": 1.3046341463414635,
      "grad_norm": 0.20605842769145966,
      "learning_rate": 3.008259451471145e-05,
      "loss": 0.0112,
      "step": 5349
    },
    {
      "epoch": 1.3048780487804879,
      "grad_norm": 0.19415053725242615,
      "learning_rate": 3.007634184323476e-05,
      "loss": 0.0334,
      "step": 5350
    },
    {
      "epoch": 1.3051219512195122,
      "grad_norm": 0.09866522252559662,
      "learning_rate": 3.007008884054231e-05,
      "loss": 0.0193,
      "step": 5351
    },
    {
      "epoch": 1.3053658536585366,
      "grad_norm": 0.11062048375606537,
      "learning_rate": 3.006383550704208e-05,
      "loss": 0.0277,
      "step": 5352
    },
    {
      "epoch": 1.305609756097561,
      "grad_norm": 0.21378351747989655,
      "learning_rate": 3.005758184314209e-05,
      "loss": 0.02,
      "step": 5353
    },
    {
      "epoch": 1.3058536585365854,
      "grad_norm": 0.1597462147474289,
      "learning_rate": 3.0051327849250373e-05,
      "loss": 0.018,
      "step": 5354
    },
    {
      "epoch": 1.3060975609756098,
      "grad_norm": 0.22485075891017914,
      "learning_rate": 3.0045073525774964e-05,
      "loss": 0.0241,
      "step": 5355
    },
    {
      "epoch": 1.3063414634146342,
      "grad_norm": 0.23802632093429565,
      "learning_rate": 3.0038818873123958e-05,
      "loss": 0.0262,
      "step": 5356
    },
    {
      "epoch": 1.3065853658536586,
      "grad_norm": 0.22913455963134766,
      "learning_rate": 3.0032563891705444e-05,
      "loss": 0.0207,
      "step": 5357
    },
    {
      "epoch": 1.306829268292683,
      "grad_norm": 0.0669931098818779,
      "learning_rate": 3.002630858192754e-05,
      "loss": 0.0145,
      "step": 5358
    },
    {
      "epoch": 1.3070731707317074,
      "grad_norm": 0.1810532957315445,
      "learning_rate": 3.0020052944198397e-05,
      "loss": 0.0265,
      "step": 5359
    },
    {
      "epoch": 1.3073170731707318,
      "grad_norm": 0.15548832714557648,
      "learning_rate": 3.0013796978926158e-05,
      "loss": 0.0288,
      "step": 5360
    },
    {
      "epoch": 1.3075609756097561,
      "grad_norm": 0.14172178506851196,
      "learning_rate": 3.0007540686519024e-05,
      "loss": 0.0214,
      "step": 5361
    },
    {
      "epoch": 1.3078048780487805,
      "grad_norm": 0.3686050772666931,
      "learning_rate": 3.0001284067385184e-05,
      "loss": 0.0295,
      "step": 5362
    },
    {
      "epoch": 1.308048780487805,
      "grad_norm": 0.27825218439102173,
      "learning_rate": 2.9995027121932873e-05,
      "loss": 0.0181,
      "step": 5363
    },
    {
      "epoch": 1.3082926829268293,
      "grad_norm": 0.195823073387146,
      "learning_rate": 2.9988769850570325e-05,
      "loss": 0.0143,
      "step": 5364
    },
    {
      "epoch": 1.3085365853658537,
      "grad_norm": 0.15801584720611572,
      "learning_rate": 2.9982512253705818e-05,
      "loss": 0.0172,
      "step": 5365
    },
    {
      "epoch": 1.308780487804878,
      "grad_norm": 0.25582754611968994,
      "learning_rate": 2.997625433174765e-05,
      "loss": 0.0402,
      "step": 5366
    },
    {
      "epoch": 1.3090243902439025,
      "grad_norm": 0.07177972793579102,
      "learning_rate": 2.9969996085104112e-05,
      "loss": 0.0157,
      "step": 5367
    },
    {
      "epoch": 1.3092682926829269,
      "grad_norm": 0.09488769620656967,
      "learning_rate": 2.9963737514183545e-05,
      "loss": 0.0197,
      "step": 5368
    },
    {
      "epoch": 1.3095121951219513,
      "grad_norm": 0.1472037136554718,
      "learning_rate": 2.99574786193943e-05,
      "loss": 0.0382,
      "step": 5369
    },
    {
      "epoch": 1.3097560975609757,
      "grad_norm": 0.16863146424293518,
      "learning_rate": 2.9951219401144752e-05,
      "loss": 0.0398,
      "step": 5370
    },
    {
      "epoch": 1.31,
      "grad_norm": 0.09880875796079636,
      "learning_rate": 2.99449598598433e-05,
      "loss": 0.0195,
      "step": 5371
    },
    {
      "epoch": 1.3102439024390244,
      "grad_norm": 0.14014287292957306,
      "learning_rate": 2.993869999589835e-05,
      "loss": 0.02,
      "step": 5372
    },
    {
      "epoch": 1.3104878048780488,
      "grad_norm": 0.24644534289836884,
      "learning_rate": 2.9932439809718342e-05,
      "loss": 0.0279,
      "step": 5373
    },
    {
      "epoch": 1.3107317073170732,
      "grad_norm": 0.10365701466798782,
      "learning_rate": 2.9926179301711744e-05,
      "loss": 0.0288,
      "step": 5374
    },
    {
      "epoch": 1.3109756097560976,
      "grad_norm": 0.10024278610944748,
      "learning_rate": 2.9919918472287022e-05,
      "loss": 0.0235,
      "step": 5375
    },
    {
      "epoch": 1.311219512195122,
      "grad_norm": 0.07585645467042923,
      "learning_rate": 2.991365732185268e-05,
      "loss": 0.0143,
      "step": 5376
    },
    {
      "epoch": 1.3114634146341464,
      "grad_norm": 0.08525200933218002,
      "learning_rate": 2.9907395850817237e-05,
      "loss": 0.0204,
      "step": 5377
    },
    {
      "epoch": 1.3117073170731708,
      "grad_norm": 0.11037442088127136,
      "learning_rate": 2.9901134059589246e-05,
      "loss": 0.0256,
      "step": 5378
    },
    {
      "epoch": 1.3119512195121952,
      "grad_norm": 0.09070511162281036,
      "learning_rate": 2.9894871948577264e-05,
      "loss": 0.0197,
      "step": 5379
    },
    {
      "epoch": 1.3121951219512196,
      "grad_norm": 0.10432128608226776,
      "learning_rate": 2.988860951818986e-05,
      "loss": 0.0217,
      "step": 5380
    },
    {
      "epoch": 1.312439024390244,
      "grad_norm": 0.10632898658514023,
      "learning_rate": 2.9882346768835658e-05,
      "loss": 0.0241,
      "step": 5381
    },
    {
      "epoch": 1.3126829268292683,
      "grad_norm": 0.06281966716051102,
      "learning_rate": 2.9876083700923267e-05,
      "loss": 0.0108,
      "step": 5382
    },
    {
      "epoch": 1.3129268292682927,
      "grad_norm": 0.11956355720758438,
      "learning_rate": 2.9869820314861352e-05,
      "loss": 0.0217,
      "step": 5383
    },
    {
      "epoch": 1.3131707317073171,
      "grad_norm": 0.3119162321090698,
      "learning_rate": 2.9863556611058563e-05,
      "loss": 0.0302,
      "step": 5384
    },
    {
      "epoch": 1.3134146341463415,
      "grad_norm": 0.13125234842300415,
      "learning_rate": 2.9857292589923595e-05,
      "loss": 0.0267,
      "step": 5385
    },
    {
      "epoch": 1.313658536585366,
      "grad_norm": 0.09446993470191956,
      "learning_rate": 2.9851028251865158e-05,
      "loss": 0.0095,
      "step": 5386
    },
    {
      "epoch": 1.3139024390243903,
      "grad_norm": 0.19531716406345367,
      "learning_rate": 2.984476359729198e-05,
      "loss": 0.0132,
      "step": 5387
    },
    {
      "epoch": 1.3141463414634147,
      "grad_norm": 0.19579465687274933,
      "learning_rate": 2.9838498626612803e-05,
      "loss": 0.0247,
      "step": 5388
    },
    {
      "epoch": 1.314390243902439,
      "grad_norm": 0.08070454001426697,
      "learning_rate": 2.9832233340236403e-05,
      "loss": 0.0243,
      "step": 5389
    },
    {
      "epoch": 1.3146341463414635,
      "grad_norm": 0.21620962023735046,
      "learning_rate": 2.982596773857157e-05,
      "loss": 0.0233,
      "step": 5390
    },
    {
      "epoch": 1.3148780487804879,
      "grad_norm": 0.11836186051368713,
      "learning_rate": 2.9819701822027123e-05,
      "loss": 0.027,
      "step": 5391
    },
    {
      "epoch": 1.3151219512195123,
      "grad_norm": 0.09821191430091858,
      "learning_rate": 2.981343559101188e-05,
      "loss": 0.0134,
      "step": 5392
    },
    {
      "epoch": 1.3153658536585366,
      "grad_norm": 0.07359792292118073,
      "learning_rate": 2.9807169045934695e-05,
      "loss": 0.0074,
      "step": 5393
    },
    {
      "epoch": 1.315609756097561,
      "grad_norm": 0.11017750203609467,
      "learning_rate": 2.9800902187204456e-05,
      "loss": 0.0106,
      "step": 5394
    },
    {
      "epoch": 1.3158536585365854,
      "grad_norm": 0.20391568541526794,
      "learning_rate": 2.9794635015230042e-05,
      "loss": 0.0123,
      "step": 5395
    },
    {
      "epoch": 1.3160975609756098,
      "grad_norm": 0.15930432081222534,
      "learning_rate": 2.9788367530420374e-05,
      "loss": 0.0225,
      "step": 5396
    },
    {
      "epoch": 1.3163414634146342,
      "grad_norm": 0.12133165448904037,
      "learning_rate": 2.9782099733184382e-05,
      "loss": 0.0205,
      "step": 5397
    },
    {
      "epoch": 1.3165853658536586,
      "grad_norm": 0.16260351240634918,
      "learning_rate": 2.9775831623931017e-05,
      "loss": 0.0177,
      "step": 5398
    },
    {
      "epoch": 1.316829268292683,
      "grad_norm": 0.09996568411588669,
      "learning_rate": 2.976956320306926e-05,
      "loss": 0.0152,
      "step": 5399
    },
    {
      "epoch": 1.3170731707317074,
      "grad_norm": 0.1664661169052124,
      "learning_rate": 2.9763294471008114e-05,
      "loss": 0.0266,
      "step": 5400
    },
    {
      "epoch": 1.3173170731707318,
      "grad_norm": 0.11739157885313034,
      "learning_rate": 2.9757025428156576e-05,
      "loss": 0.0241,
      "step": 5401
    },
    {
      "epoch": 1.3175609756097562,
      "grad_norm": 0.13276951014995575,
      "learning_rate": 2.9750756074923692e-05,
      "loss": 0.0335,
      "step": 5402
    },
    {
      "epoch": 1.3178048780487805,
      "grad_norm": 0.1027526780962944,
      "learning_rate": 2.9744486411718526e-05,
      "loss": 0.0231,
      "step": 5403
    },
    {
      "epoch": 1.318048780487805,
      "grad_norm": 0.10415174812078476,
      "learning_rate": 2.973821643895014e-05,
      "loss": 0.0315,
      "step": 5404
    },
    {
      "epoch": 1.3182926829268293,
      "grad_norm": 0.08687075227499008,
      "learning_rate": 2.9731946157027635e-05,
      "loss": 0.0187,
      "step": 5405
    },
    {
      "epoch": 1.3185365853658537,
      "grad_norm": 0.22968967258930206,
      "learning_rate": 2.9725675566360127e-05,
      "loss": 0.0269,
      "step": 5406
    },
    {
      "epoch": 1.318780487804878,
      "grad_norm": 0.11539805680513382,
      "learning_rate": 2.971940466735676e-05,
      "loss": 0.025,
      "step": 5407
    },
    {
      "epoch": 1.3190243902439025,
      "grad_norm": 0.19880232214927673,
      "learning_rate": 2.971313346042669e-05,
      "loss": 0.0121,
      "step": 5408
    },
    {
      "epoch": 1.319268292682927,
      "grad_norm": 0.16246014833450317,
      "learning_rate": 2.9706861945979082e-05,
      "loss": 0.0199,
      "step": 5409
    },
    {
      "epoch": 1.3195121951219513,
      "grad_norm": 0.15016119182109833,
      "learning_rate": 2.970059012442314e-05,
      "loss": 0.0372,
      "step": 5410
    },
    {
      "epoch": 1.3197560975609757,
      "grad_norm": 0.12424802035093307,
      "learning_rate": 2.9694317996168088e-05,
      "loss": 0.0186,
      "step": 5411
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.11537982523441315,
      "learning_rate": 2.9688045561623157e-05,
      "loss": 0.0152,
      "step": 5412
    },
    {
      "epoch": 1.3202439024390245,
      "grad_norm": 0.22206461429595947,
      "learning_rate": 2.9681772821197607e-05,
      "loss": 0.018,
      "step": 5413
    },
    {
      "epoch": 1.3204878048780488,
      "grad_norm": 0.1982380747795105,
      "learning_rate": 2.9675499775300703e-05,
      "loss": 0.0316,
      "step": 5414
    },
    {
      "epoch": 1.3207317073170732,
      "grad_norm": 0.13028556108474731,
      "learning_rate": 2.9669226424341756e-05,
      "loss": 0.0234,
      "step": 5415
    },
    {
      "epoch": 1.3209756097560976,
      "grad_norm": 0.07130202651023865,
      "learning_rate": 2.9662952768730083e-05,
      "loss": 0.0109,
      "step": 5416
    },
    {
      "epoch": 1.321219512195122,
      "grad_norm": 0.08779595047235489,
      "learning_rate": 2.9656678808875014e-05,
      "loss": 0.0157,
      "step": 5417
    },
    {
      "epoch": 1.3214634146341464,
      "grad_norm": 0.10152851790189743,
      "learning_rate": 2.9650404545185907e-05,
      "loss": 0.0122,
      "step": 5418
    },
    {
      "epoch": 1.3217073170731708,
      "grad_norm": 0.2846178412437439,
      "learning_rate": 2.9644129978072138e-05,
      "loss": 0.0302,
      "step": 5419
    },
    {
      "epoch": 1.3219512195121952,
      "grad_norm": 0.22684992849826813,
      "learning_rate": 2.9637855107943107e-05,
      "loss": 0.0132,
      "step": 5420
    },
    {
      "epoch": 1.3221951219512196,
      "grad_norm": 0.11762432008981705,
      "learning_rate": 2.9631579935208232e-05,
      "loss": 0.0131,
      "step": 5421
    },
    {
      "epoch": 1.322439024390244,
      "grad_norm": 0.2111617922782898,
      "learning_rate": 2.962530446027694e-05,
      "loss": 0.0188,
      "step": 5422
    },
    {
      "epoch": 1.3226829268292684,
      "grad_norm": 0.1228855550289154,
      "learning_rate": 2.961902868355869e-05,
      "loss": 0.0207,
      "step": 5423
    },
    {
      "epoch": 1.3229268292682927,
      "grad_norm": 0.15658743679523468,
      "learning_rate": 2.9612752605462963e-05,
      "loss": 0.0257,
      "step": 5424
    },
    {
      "epoch": 1.3231707317073171,
      "grad_norm": 0.13000911474227905,
      "learning_rate": 2.9606476226399248e-05,
      "loss": 0.0451,
      "step": 5425
    },
    {
      "epoch": 1.3234146341463415,
      "grad_norm": 0.14874067902565002,
      "learning_rate": 2.9600199546777063e-05,
      "loss": 0.0279,
      "step": 5426
    },
    {
      "epoch": 1.323658536585366,
      "grad_norm": 0.2260347604751587,
      "learning_rate": 2.9593922567005933e-05,
      "loss": 0.0259,
      "step": 5427
    },
    {
      "epoch": 1.3239024390243903,
      "grad_norm": 0.11107352375984192,
      "learning_rate": 2.9587645287495424e-05,
      "loss": 0.0299,
      "step": 5428
    },
    {
      "epoch": 1.3241463414634147,
      "grad_norm": 0.07044598460197449,
      "learning_rate": 2.958136770865511e-05,
      "loss": 0.0146,
      "step": 5429
    },
    {
      "epoch": 1.324390243902439,
      "grad_norm": 0.17551466822624207,
      "learning_rate": 2.957508983089457e-05,
      "loss": 0.0179,
      "step": 5430
    },
    {
      "epoch": 1.3246341463414635,
      "grad_norm": 0.28993064165115356,
      "learning_rate": 2.9568811654623424e-05,
      "loss": 0.0303,
      "step": 5431
    },
    {
      "epoch": 1.3248780487804879,
      "grad_norm": 0.10464568436145782,
      "learning_rate": 2.9562533180251305e-05,
      "loss": 0.0292,
      "step": 5432
    },
    {
      "epoch": 1.3251219512195123,
      "grad_norm": 0.1658656746149063,
      "learning_rate": 2.9556254408187865e-05,
      "loss": 0.0522,
      "step": 5433
    },
    {
      "epoch": 1.3253658536585367,
      "grad_norm": 0.16976940631866455,
      "learning_rate": 2.9549975338842772e-05,
      "loss": 0.0135,
      "step": 5434
    },
    {
      "epoch": 1.325609756097561,
      "grad_norm": 0.08360390365123749,
      "learning_rate": 2.9543695972625717e-05,
      "loss": 0.0178,
      "step": 5435
    },
    {
      "epoch": 1.3258536585365854,
      "grad_norm": 0.20107272267341614,
      "learning_rate": 2.9537416309946408e-05,
      "loss": 0.0246,
      "step": 5436
    },
    {
      "epoch": 1.3260975609756098,
      "grad_norm": 0.12869931757450104,
      "learning_rate": 2.953113635121458e-05,
      "loss": 0.0187,
      "step": 5437
    },
    {
      "epoch": 1.3263414634146342,
      "grad_norm": 0.08310997486114502,
      "learning_rate": 2.9524856096839975e-05,
      "loss": 0.0179,
      "step": 5438
    },
    {
      "epoch": 1.3265853658536586,
      "grad_norm": 0.14130476117134094,
      "learning_rate": 2.9518575547232358e-05,
      "loss": 0.0171,
      "step": 5439
    },
    {
      "epoch": 1.326829268292683,
      "grad_norm": 0.07765521854162216,
      "learning_rate": 2.9512294702801518e-05,
      "loss": 0.01,
      "step": 5440
    },
    {
      "epoch": 1.3270731707317074,
      "grad_norm": 0.12857431173324585,
      "learning_rate": 2.9506013563957264e-05,
      "loss": 0.0177,
      "step": 5441
    },
    {
      "epoch": 1.3273170731707318,
      "grad_norm": 0.22128413617610931,
      "learning_rate": 2.9499732131109427e-05,
      "loss": 0.0296,
      "step": 5442
    },
    {
      "epoch": 1.3275609756097562,
      "grad_norm": 0.1058763638138771,
      "learning_rate": 2.9493450404667834e-05,
      "loss": 0.0098,
      "step": 5443
    },
    {
      "epoch": 1.3278048780487806,
      "grad_norm": 0.2823699414730072,
      "learning_rate": 2.948716838504235e-05,
      "loss": 0.0184,
      "step": 5444
    },
    {
      "epoch": 1.328048780487805,
      "grad_norm": 0.07129301130771637,
      "learning_rate": 2.9480886072642883e-05,
      "loss": 0.0182,
      "step": 5445
    },
    {
      "epoch": 1.3282926829268293,
      "grad_norm": 0.13367260992527008,
      "learning_rate": 2.9474603467879315e-05,
      "loss": 0.0185,
      "step": 5446
    },
    {
      "epoch": 1.3285365853658537,
      "grad_norm": 0.15545497834682465,
      "learning_rate": 2.9468320571161562e-05,
      "loss": 0.017,
      "step": 5447
    },
    {
      "epoch": 1.3287804878048781,
      "grad_norm": 0.09067241102457047,
      "learning_rate": 2.946203738289957e-05,
      "loss": 0.0224,
      "step": 5448
    },
    {
      "epoch": 1.3290243902439025,
      "grad_norm": 0.11758805066347122,
      "learning_rate": 2.9455753903503297e-05,
      "loss": 0.021,
      "step": 5449
    },
    {
      "epoch": 1.329268292682927,
      "grad_norm": 0.0834648534655571,
      "learning_rate": 2.944947013338273e-05,
      "loss": 0.0121,
      "step": 5450
    },
    {
      "epoch": 1.3295121951219513,
      "grad_norm": 0.15364466607570648,
      "learning_rate": 2.9443186072947854e-05,
      "loss": 0.0241,
      "step": 5451
    },
    {
      "epoch": 1.3297560975609757,
      "grad_norm": 0.10237365961074829,
      "learning_rate": 2.9436901722608685e-05,
      "loss": 0.0202,
      "step": 5452
    },
    {
      "epoch": 1.33,
      "grad_norm": 0.1003076583147049,
      "learning_rate": 2.9430617082775264e-05,
      "loss": 0.0203,
      "step": 5453
    },
    {
      "epoch": 1.3302439024390245,
      "grad_norm": 0.22193022072315216,
      "learning_rate": 2.9424332153857643e-05,
      "loss": 0.026,
      "step": 5454
    },
    {
      "epoch": 1.3304878048780489,
      "grad_norm": 0.11570119857788086,
      "learning_rate": 2.9418046936265897e-05,
      "loss": 0.0255,
      "step": 5455
    },
    {
      "epoch": 1.3307317073170732,
      "grad_norm": 0.08728361129760742,
      "learning_rate": 2.9411761430410107e-05,
      "loss": 0.0152,
      "step": 5456
    },
    {
      "epoch": 1.3309756097560976,
      "grad_norm": 0.09476067870855331,
      "learning_rate": 2.9405475636700385e-05,
      "loss": 0.0134,
      "step": 5457
    },
    {
      "epoch": 1.331219512195122,
      "grad_norm": 0.14419548213481903,
      "learning_rate": 2.939918955554688e-05,
      "loss": 0.0247,
      "step": 5458
    },
    {
      "epoch": 1.3314634146341464,
      "grad_norm": 0.1430935561656952,
      "learning_rate": 2.939290318735971e-05,
      "loss": 0.0322,
      "step": 5459
    },
    {
      "epoch": 1.3317073170731708,
      "grad_norm": 0.164768785238266,
      "learning_rate": 2.9386616532549056e-05,
      "loss": 0.0332,
      "step": 5460
    },
    {
      "epoch": 1.3319512195121952,
      "grad_norm": 0.07344067096710205,
      "learning_rate": 2.9380329591525103e-05,
      "loss": 0.0164,
      "step": 5461
    },
    {
      "epoch": 1.3321951219512196,
      "grad_norm": 0.1917039006948471,
      "learning_rate": 2.9374042364698064e-05,
      "loss": 0.0205,
      "step": 5462
    },
    {
      "epoch": 1.332439024390244,
      "grad_norm": 0.1366974115371704,
      "learning_rate": 2.936775485247814e-05,
      "loss": 0.0169,
      "step": 5463
    },
    {
      "epoch": 1.3326829268292684,
      "grad_norm": 0.0657440721988678,
      "learning_rate": 2.936146705527558e-05,
      "loss": 0.0125,
      "step": 5464
    },
    {
      "epoch": 1.3329268292682928,
      "grad_norm": 0.13735902309417725,
      "learning_rate": 2.935517897350065e-05,
      "loss": 0.0279,
      "step": 5465
    },
    {
      "epoch": 1.3331707317073171,
      "grad_norm": 0.12069583684206009,
      "learning_rate": 2.934889060756363e-05,
      "loss": 0.0211,
      "step": 5466
    },
    {
      "epoch": 1.3334146341463415,
      "grad_norm": 0.08018497377634048,
      "learning_rate": 2.934260195787481e-05,
      "loss": 0.0132,
      "step": 5467
    },
    {
      "epoch": 1.333658536585366,
      "grad_norm": 0.12241549789905548,
      "learning_rate": 2.9336313024844504e-05,
      "loss": 0.0187,
      "step": 5468
    },
    {
      "epoch": 1.3339024390243903,
      "grad_norm": 0.10557139664888382,
      "learning_rate": 2.9330023808883046e-05,
      "loss": 0.0243,
      "step": 5469
    },
    {
      "epoch": 1.3341463414634147,
      "grad_norm": 0.08531559258699417,
      "learning_rate": 2.9323734310400792e-05,
      "loss": 0.0129,
      "step": 5470
    },
    {
      "epoch": 1.334390243902439,
      "grad_norm": 0.09769736975431442,
      "learning_rate": 2.931744452980811e-05,
      "loss": 0.0254,
      "step": 5471
    },
    {
      "epoch": 1.3346341463414635,
      "grad_norm": 0.11364701390266418,
      "learning_rate": 2.931115446751539e-05,
      "loss": 0.0213,
      "step": 5472
    },
    {
      "epoch": 1.3348780487804879,
      "grad_norm": 0.21175381541252136,
      "learning_rate": 2.9304864123933036e-05,
      "loss": 0.0295,
      "step": 5473
    },
    {
      "epoch": 1.3351219512195123,
      "grad_norm": 0.27733734250068665,
      "learning_rate": 2.9298573499471477e-05,
      "loss": 0.0147,
      "step": 5474
    },
    {
      "epoch": 1.3353658536585367,
      "grad_norm": 0.13352833688259125,
      "learning_rate": 2.929228259454116e-05,
      "loss": 0.0221,
      "step": 5475
    },
    {
      "epoch": 1.335609756097561,
      "grad_norm": 0.08092134445905685,
      "learning_rate": 2.928599140955254e-05,
      "loss": 0.019,
      "step": 5476
    },
    {
      "epoch": 1.3358536585365854,
      "grad_norm": 0.17542172968387604,
      "learning_rate": 2.92796999449161e-05,
      "loss": 0.022,
      "step": 5477
    },
    {
      "epoch": 1.3360975609756098,
      "grad_norm": 0.07781306654214859,
      "learning_rate": 2.9273408201042347e-05,
      "loss": 0.0203,
      "step": 5478
    },
    {
      "epoch": 1.3363414634146342,
      "grad_norm": 0.10227962583303452,
      "learning_rate": 2.9267116178341784e-05,
      "loss": 0.0093,
      "step": 5479
    },
    {
      "epoch": 1.3365853658536586,
      "grad_norm": 0.0802575945854187,
      "learning_rate": 2.9260823877224958e-05,
      "loss": 0.0259,
      "step": 5480
    },
    {
      "epoch": 1.336829268292683,
      "grad_norm": 0.2522714138031006,
      "learning_rate": 2.9254531298102418e-05,
      "loss": 0.0283,
      "step": 5481
    },
    {
      "epoch": 1.3370731707317074,
      "grad_norm": 0.27581608295440674,
      "learning_rate": 2.9248238441384728e-05,
      "loss": 0.0483,
      "step": 5482
    },
    {
      "epoch": 1.3373170731707318,
      "grad_norm": 0.1018005907535553,
      "learning_rate": 2.924194530748249e-05,
      "loss": 0.0144,
      "step": 5483
    },
    {
      "epoch": 1.3375609756097562,
      "grad_norm": 0.11515822261571884,
      "learning_rate": 2.9235651896806305e-05,
      "loss": 0.0209,
      "step": 5484
    },
    {
      "epoch": 1.3378048780487806,
      "grad_norm": 0.08952854573726654,
      "learning_rate": 2.9229358209766798e-05,
      "loss": 0.0213,
      "step": 5485
    },
    {
      "epoch": 1.338048780487805,
      "grad_norm": 0.07603655010461807,
      "learning_rate": 2.9223064246774616e-05,
      "loss": 0.0177,
      "step": 5486
    },
    {
      "epoch": 1.3382926829268293,
      "grad_norm": 0.10100327432155609,
      "learning_rate": 2.921677000824043e-05,
      "loss": 0.0179,
      "step": 5487
    },
    {
      "epoch": 1.3385365853658537,
      "grad_norm": 0.14926664531230927,
      "learning_rate": 2.9210475494574907e-05,
      "loss": 0.0332,
      "step": 5488
    },
    {
      "epoch": 1.3387804878048781,
      "grad_norm": 0.0895787701010704,
      "learning_rate": 2.920418070618874e-05,
      "loss": 0.02,
      "step": 5489
    },
    {
      "epoch": 1.3390243902439025,
      "grad_norm": 0.10537811368703842,
      "learning_rate": 2.919788564349265e-05,
      "loss": 0.0154,
      "step": 5490
    },
    {
      "epoch": 1.339268292682927,
      "grad_norm": 0.07412583380937576,
      "learning_rate": 2.9191590306897377e-05,
      "loss": 0.0277,
      "step": 5491
    },
    {
      "epoch": 1.3395121951219513,
      "grad_norm": 0.22928060591220856,
      "learning_rate": 2.918529469681367e-05,
      "loss": 0.0268,
      "step": 5492
    },
    {
      "epoch": 1.3397560975609757,
      "grad_norm": 0.08755125105381012,
      "learning_rate": 2.917899881365229e-05,
      "loss": 0.0155,
      "step": 5493
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.14264103770256042,
      "learning_rate": 2.9172702657824037e-05,
      "loss": 0.0266,
      "step": 5494
    },
    {
      "epoch": 1.3402439024390245,
      "grad_norm": 0.17929993569850922,
      "learning_rate": 2.9166406229739712e-05,
      "loss": 0.0419,
      "step": 5495
    },
    {
      "epoch": 1.3404878048780489,
      "grad_norm": 0.0555320605635643,
      "learning_rate": 2.916010952981013e-05,
      "loss": 0.0157,
      "step": 5496
    },
    {
      "epoch": 1.3407317073170733,
      "grad_norm": 0.14387193322181702,
      "learning_rate": 2.9153812558446144e-05,
      "loss": 0.0151,
      "step": 5497
    },
    {
      "epoch": 1.3409756097560976,
      "grad_norm": 0.09990689903497696,
      "learning_rate": 2.9147515316058594e-05,
      "loss": 0.0169,
      "step": 5498
    },
    {
      "epoch": 1.341219512195122,
      "grad_norm": 0.10196308046579361,
      "learning_rate": 2.914121780305837e-05,
      "loss": 0.0359,
      "step": 5499
    },
    {
      "epoch": 1.3414634146341464,
      "grad_norm": 0.18221601843833923,
      "learning_rate": 2.913492001985636e-05,
      "loss": 0.0196,
      "step": 5500
    },
    {
      "epoch": 1.3417073170731708,
      "grad_norm": 0.09747736901044846,
      "learning_rate": 2.912862196686348e-05,
      "loss": 0.0263,
      "step": 5501
    },
    {
      "epoch": 1.3419512195121952,
      "grad_norm": 0.1297605186700821,
      "learning_rate": 2.9122323644490656e-05,
      "loss": 0.0069,
      "step": 5502
    },
    {
      "epoch": 1.3421951219512196,
      "grad_norm": 0.233828604221344,
      "learning_rate": 2.9116025053148833e-05,
      "loss": 0.0388,
      "step": 5503
    },
    {
      "epoch": 1.342439024390244,
      "grad_norm": 0.19617602229118347,
      "learning_rate": 2.9109726193248976e-05,
      "loss": 0.0277,
      "step": 5504
    },
    {
      "epoch": 1.3426829268292684,
      "grad_norm": 0.06046245992183685,
      "learning_rate": 2.9103427065202066e-05,
      "loss": 0.018,
      "step": 5505
    },
    {
      "epoch": 1.3429268292682928,
      "grad_norm": 0.13964560627937317,
      "learning_rate": 2.90971276694191e-05,
      "loss": 0.0247,
      "step": 5506
    },
    {
      "epoch": 1.3431707317073172,
      "grad_norm": 0.12138471007347107,
      "learning_rate": 2.90908280063111e-05,
      "loss": 0.0265,
      "step": 5507
    },
    {
      "epoch": 1.3434146341463415,
      "grad_norm": 0.12963712215423584,
      "learning_rate": 2.9084528076289085e-05,
      "loss": 0.0153,
      "step": 5508
    },
    {
      "epoch": 1.343658536585366,
      "grad_norm": 0.11515022069215775,
      "learning_rate": 2.907822787976413e-05,
      "loss": 0.0163,
      "step": 5509
    },
    {
      "epoch": 1.34390243902439,
      "grad_norm": 0.16691316664218903,
      "learning_rate": 2.9071927417147283e-05,
      "loss": 0.0227,
      "step": 5510
    },
    {
      "epoch": 1.3441463414634147,
      "grad_norm": 0.31827136874198914,
      "learning_rate": 2.9065626688849645e-05,
      "loss": 0.0168,
      "step": 5511
    },
    {
      "epoch": 1.3443902439024389,
      "grad_norm": 0.08314010500907898,
      "learning_rate": 2.9059325695282302e-05,
      "loss": 0.0195,
      "step": 5512
    },
    {
      "epoch": 1.3446341463414635,
      "grad_norm": 0.07254571467638016,
      "learning_rate": 2.9053024436856392e-05,
      "loss": 0.0176,
      "step": 5513
    },
    {
      "epoch": 1.3448780487804877,
      "grad_norm": 0.19892866909503937,
      "learning_rate": 2.9046722913983042e-05,
      "loss": 0.0516,
      "step": 5514
    },
    {
      "epoch": 1.3451219512195123,
      "grad_norm": 0.1720723956823349,
      "learning_rate": 2.904042112707341e-05,
      "loss": 0.0242,
      "step": 5515
    },
    {
      "epoch": 1.3453658536585364,
      "grad_norm": 0.10843993723392487,
      "learning_rate": 2.9034119076538662e-05,
      "loss": 0.0121,
      "step": 5516
    },
    {
      "epoch": 1.345609756097561,
      "grad_norm": 0.122627854347229,
      "learning_rate": 2.9027816762790005e-05,
      "loss": 0.0162,
      "step": 5517
    },
    {
      "epoch": 1.3458536585365852,
      "grad_norm": 0.08721619844436646,
      "learning_rate": 2.902151418623863e-05,
      "loss": 0.0256,
      "step": 5518
    },
    {
      "epoch": 1.3460975609756098,
      "grad_norm": 0.21558533608913422,
      "learning_rate": 2.9015211347295763e-05,
      "loss": 0.0184,
      "step": 5519
    },
    {
      "epoch": 1.346341463414634,
      "grad_norm": 0.16870157420635223,
      "learning_rate": 2.9008908246372653e-05,
      "loss": 0.0184,
      "step": 5520
    },
    {
      "epoch": 1.3465853658536586,
      "grad_norm": 0.09824997931718826,
      "learning_rate": 2.9002604883880547e-05,
      "loss": 0.0084,
      "step": 5521
    },
    {
      "epoch": 1.3468292682926828,
      "grad_norm": 0.06270650029182434,
      "learning_rate": 2.899630126023073e-05,
      "loss": 0.0073,
      "step": 5522
    },
    {
      "epoch": 1.3470731707317074,
      "grad_norm": 0.08529908955097198,
      "learning_rate": 2.8989997375834482e-05,
      "loss": 0.0149,
      "step": 5523
    },
    {
      "epoch": 1.3473170731707316,
      "grad_norm": 0.11500377207994461,
      "learning_rate": 2.8983693231103127e-05,
      "loss": 0.0178,
      "step": 5524
    },
    {
      "epoch": 1.3475609756097562,
      "grad_norm": 0.1834149956703186,
      "learning_rate": 2.8977388826447975e-05,
      "loss": 0.0272,
      "step": 5525
    },
    {
      "epoch": 1.3478048780487804,
      "grad_norm": 0.17518280446529388,
      "learning_rate": 2.8971084162280394e-05,
      "loss": 0.0236,
      "step": 5526
    },
    {
      "epoch": 1.348048780487805,
      "grad_norm": 0.1437302678823471,
      "learning_rate": 2.8964779239011712e-05,
      "loss": 0.0258,
      "step": 5527
    },
    {
      "epoch": 1.3482926829268291,
      "grad_norm": 0.10196114331483841,
      "learning_rate": 2.895847405705333e-05,
      "loss": 0.0184,
      "step": 5528
    },
    {
      "epoch": 1.3485365853658537,
      "grad_norm": 0.0881299078464508,
      "learning_rate": 2.8952168616816632e-05,
      "loss": 0.0228,
      "step": 5529
    },
    {
      "epoch": 1.348780487804878,
      "grad_norm": 0.11933878809213638,
      "learning_rate": 2.894586291871303e-05,
      "loss": 0.0165,
      "step": 5530
    },
    {
      "epoch": 1.3490243902439025,
      "grad_norm": 0.19832132756710052,
      "learning_rate": 2.8939556963153957e-05,
      "loss": 0.0175,
      "step": 5531
    },
    {
      "epoch": 1.3492682926829267,
      "grad_norm": 0.1413269340991974,
      "learning_rate": 2.8933250750550846e-05,
      "loss": 0.0193,
      "step": 5532
    },
    {
      "epoch": 1.3495121951219513,
      "grad_norm": 0.21213199198246002,
      "learning_rate": 2.8926944281315166e-05,
      "loss": 0.0361,
      "step": 5533
    },
    {
      "epoch": 1.3497560975609755,
      "grad_norm": 0.13641253113746643,
      "learning_rate": 2.89206375558584e-05,
      "loss": 0.0227,
      "step": 5534
    },
    {
      "epoch": 1.35,
      "grad_norm": 0.11195801943540573,
      "learning_rate": 2.8914330574592024e-05,
      "loss": 0.0242,
      "step": 5535
    },
    {
      "epoch": 1.3502439024390243,
      "grad_norm": 0.09014130383729935,
      "learning_rate": 2.890802333792757e-05,
      "loss": 0.0141,
      "step": 5536
    },
    {
      "epoch": 1.3504878048780489,
      "grad_norm": 0.31865063309669495,
      "learning_rate": 2.890171584627655e-05,
      "loss": 0.0358,
      "step": 5537
    },
    {
      "epoch": 1.350731707317073,
      "grad_norm": 0.12843571603298187,
      "learning_rate": 2.889540810005052e-05,
      "loss": 0.0202,
      "step": 5538
    },
    {
      "epoch": 1.3509756097560977,
      "grad_norm": 0.15829341113567352,
      "learning_rate": 2.8889100099661037e-05,
      "loss": 0.0162,
      "step": 5539
    },
    {
      "epoch": 1.3512195121951218,
      "grad_norm": 0.14918018877506256,
      "learning_rate": 2.8882791845519675e-05,
      "loss": 0.0173,
      "step": 5540
    },
    {
      "epoch": 1.3514634146341464,
      "grad_norm": 0.1035013273358345,
      "learning_rate": 2.887648333803803e-05,
      "loss": 0.0234,
      "step": 5541
    },
    {
      "epoch": 1.3517073170731706,
      "grad_norm": 0.13090471923351288,
      "learning_rate": 2.8870174577627714e-05,
      "loss": 0.0255,
      "step": 5542
    },
    {
      "epoch": 1.3519512195121952,
      "grad_norm": 0.117067351937294,
      "learning_rate": 2.886386556470036e-05,
      "loss": 0.0239,
      "step": 5543
    },
    {
      "epoch": 1.3521951219512194,
      "grad_norm": 0.17150020599365234,
      "learning_rate": 2.8857556299667608e-05,
      "loss": 0.0244,
      "step": 5544
    },
    {
      "epoch": 1.352439024390244,
      "grad_norm": 0.12284310907125473,
      "learning_rate": 2.885124678294111e-05,
      "loss": 0.012,
      "step": 5545
    },
    {
      "epoch": 1.3526829268292682,
      "grad_norm": 0.10023977607488632,
      "learning_rate": 2.884493701493255e-05,
      "loss": 0.0204,
      "step": 5546
    },
    {
      "epoch": 1.3529268292682928,
      "grad_norm": 0.21304956078529358,
      "learning_rate": 2.8838626996053625e-05,
      "loss": 0.0298,
      "step": 5547
    },
    {
      "epoch": 1.353170731707317,
      "grad_norm": 0.2916507124900818,
      "learning_rate": 2.8832316726716036e-05,
      "loss": 0.0463,
      "step": 5548
    },
    {
      "epoch": 1.3534146341463416,
      "grad_norm": 0.11608774960041046,
      "learning_rate": 2.882600620733151e-05,
      "loss": 0.017,
      "step": 5549
    },
    {
      "epoch": 1.3536585365853657,
      "grad_norm": 0.15047751367092133,
      "learning_rate": 2.881969543831179e-05,
      "loss": 0.0123,
      "step": 5550
    },
    {
      "epoch": 1.3539024390243903,
      "grad_norm": 0.17194633185863495,
      "learning_rate": 2.8813384420068647e-05,
      "loss": 0.0242,
      "step": 5551
    },
    {
      "epoch": 1.3541463414634145,
      "grad_norm": 0.14449085295200348,
      "learning_rate": 2.8807073153013836e-05,
      "loss": 0.0384,
      "step": 5552
    },
    {
      "epoch": 1.3543902439024391,
      "grad_norm": 0.12855984270572662,
      "learning_rate": 2.880076163755916e-05,
      "loss": 0.0346,
      "step": 5553
    },
    {
      "epoch": 1.3546341463414633,
      "grad_norm": 0.21514484286308289,
      "learning_rate": 2.879444987411642e-05,
      "loss": 0.0194,
      "step": 5554
    },
    {
      "epoch": 1.354878048780488,
      "grad_norm": 0.16070671379566193,
      "learning_rate": 2.8788137863097436e-05,
      "loss": 0.0233,
      "step": 5555
    },
    {
      "epoch": 1.355121951219512,
      "grad_norm": 0.09338057786226273,
      "learning_rate": 2.8781825604914066e-05,
      "loss": 0.0172,
      "step": 5556
    },
    {
      "epoch": 1.3553658536585367,
      "grad_norm": 0.07876972109079361,
      "learning_rate": 2.877551309997814e-05,
      "loss": 0.0166,
      "step": 5557
    },
    {
      "epoch": 1.3556097560975608,
      "grad_norm": 0.4197828769683838,
      "learning_rate": 2.8769200348701545e-05,
      "loss": 0.0143,
      "step": 5558
    },
    {
      "epoch": 1.3558536585365855,
      "grad_norm": 0.1725296974182129,
      "learning_rate": 2.876288735149617e-05,
      "loss": 0.0289,
      "step": 5559
    },
    {
      "epoch": 1.3560975609756096,
      "grad_norm": 0.21632488071918488,
      "learning_rate": 2.8756574108773915e-05,
      "loss": 0.0293,
      "step": 5560
    },
    {
      "epoch": 1.3563414634146342,
      "grad_norm": 0.08582592010498047,
      "learning_rate": 2.87502606209467e-05,
      "loss": 0.0165,
      "step": 5561
    },
    {
      "epoch": 1.3565853658536584,
      "grad_norm": 0.08698760718107224,
      "learning_rate": 2.8743946888426458e-05,
      "loss": 0.0216,
      "step": 5562
    },
    {
      "epoch": 1.356829268292683,
      "grad_norm": 0.08549551665782928,
      "learning_rate": 2.8737632911625144e-05,
      "loss": 0.0076,
      "step": 5563
    },
    {
      "epoch": 1.3570731707317072,
      "grad_norm": 0.06277662515640259,
      "learning_rate": 2.8731318690954723e-05,
      "loss": 0.0147,
      "step": 5564
    },
    {
      "epoch": 1.3573170731707318,
      "grad_norm": 0.13584885001182556,
      "learning_rate": 2.8725004226827178e-05,
      "loss": 0.0203,
      "step": 5565
    },
    {
      "epoch": 1.357560975609756,
      "grad_norm": 0.14219272136688232,
      "learning_rate": 2.8718689519654513e-05,
      "loss": 0.0317,
      "step": 5566
    },
    {
      "epoch": 1.3578048780487806,
      "grad_norm": 0.17741234600543976,
      "learning_rate": 2.871237456984874e-05,
      "loss": 0.017,
      "step": 5567
    },
    {
      "epoch": 1.3580487804878048,
      "grad_norm": 0.19966058433055878,
      "learning_rate": 2.8706059377821893e-05,
      "loss": 0.0249,
      "step": 5568
    },
    {
      "epoch": 1.3582926829268294,
      "grad_norm": 0.17596204578876495,
      "learning_rate": 2.869974394398602e-05,
      "loss": 0.0422,
      "step": 5569
    },
    {
      "epoch": 1.3585365853658535,
      "grad_norm": 0.0851508378982544,
      "learning_rate": 2.8693428268753175e-05,
      "loss": 0.0127,
      "step": 5570
    },
    {
      "epoch": 1.3587804878048781,
      "grad_norm": 0.2706312835216522,
      "learning_rate": 2.868711235253544e-05,
      "loss": 0.0311,
      "step": 5571
    },
    {
      "epoch": 1.3590243902439023,
      "grad_norm": 0.11877859383821487,
      "learning_rate": 2.8680796195744914e-05,
      "loss": 0.0168,
      "step": 5572
    },
    {
      "epoch": 1.359268292682927,
      "grad_norm": 0.2606079876422882,
      "learning_rate": 2.8674479798793707e-05,
      "loss": 0.019,
      "step": 5573
    },
    {
      "epoch": 1.359512195121951,
      "grad_norm": 0.21269215643405914,
      "learning_rate": 2.866816316209394e-05,
      "loss": 0.0174,
      "step": 5574
    },
    {
      "epoch": 1.3597560975609757,
      "grad_norm": 0.14818377792835236,
      "learning_rate": 2.8661846286057754e-05,
      "loss": 0.0203,
      "step": 5575
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.11667145043611526,
      "learning_rate": 2.865552917109731e-05,
      "loss": 0.0162,
      "step": 5576
    },
    {
      "epoch": 1.3602439024390245,
      "grad_norm": 0.049797482788562775,
      "learning_rate": 2.8649211817624783e-05,
      "loss": 0.0098,
      "step": 5577
    },
    {
      "epoch": 1.3604878048780487,
      "grad_norm": 0.17114461958408356,
      "learning_rate": 2.8642894226052348e-05,
      "loss": 0.0219,
      "step": 5578
    },
    {
      "epoch": 1.3607317073170733,
      "grad_norm": 0.13868848979473114,
      "learning_rate": 2.863657639679222e-05,
      "loss": 0.0111,
      "step": 5579
    },
    {
      "epoch": 1.3609756097560974,
      "grad_norm": 0.09316261112689972,
      "learning_rate": 2.863025833025661e-05,
      "loss": 0.0213,
      "step": 5580
    },
    {
      "epoch": 1.361219512195122,
      "grad_norm": 0.11613689363002777,
      "learning_rate": 2.8623940026857766e-05,
      "loss": 0.0208,
      "step": 5581
    },
    {
      "epoch": 1.3614634146341462,
      "grad_norm": 0.09668703377246857,
      "learning_rate": 2.861762148700792e-05,
      "loss": 0.0152,
      "step": 5582
    },
    {
      "epoch": 1.3617073170731708,
      "grad_norm": 0.221297949552536,
      "learning_rate": 2.8611302711119352e-05,
      "loss": 0.0222,
      "step": 5583
    },
    {
      "epoch": 1.361951219512195,
      "grad_norm": 0.15135090053081512,
      "learning_rate": 2.8604983699604337e-05,
      "loss": 0.017,
      "step": 5584
    },
    {
      "epoch": 1.3621951219512196,
      "grad_norm": 0.09396472573280334,
      "learning_rate": 2.8598664452875174e-05,
      "loss": 0.0206,
      "step": 5585
    },
    {
      "epoch": 1.3624390243902438,
      "grad_norm": 0.19389937818050385,
      "learning_rate": 2.859234497134417e-05,
      "loss": 0.0313,
      "step": 5586
    },
    {
      "epoch": 1.3626829268292684,
      "grad_norm": 0.1471119076013565,
      "learning_rate": 2.8586025255423655e-05,
      "loss": 0.0263,
      "step": 5587
    },
    {
      "epoch": 1.3629268292682926,
      "grad_norm": 0.12924312055110931,
      "learning_rate": 2.8579705305525967e-05,
      "loss": 0.0183,
      "step": 5588
    },
    {
      "epoch": 1.3631707317073172,
      "grad_norm": 0.14444907009601593,
      "learning_rate": 2.8573385122063467e-05,
      "loss": 0.0237,
      "step": 5589
    },
    {
      "epoch": 1.3634146341463413,
      "grad_norm": 0.07276938855648041,
      "learning_rate": 2.8567064705448525e-05,
      "loss": 0.0142,
      "step": 5590
    },
    {
      "epoch": 1.363658536585366,
      "grad_norm": 0.23439767956733704,
      "learning_rate": 2.8560744056093537e-05,
      "loss": 0.0132,
      "step": 5591
    },
    {
      "epoch": 1.3639024390243901,
      "grad_norm": 0.14641760289669037,
      "learning_rate": 2.8554423174410892e-05,
      "loss": 0.0215,
      "step": 5592
    },
    {
      "epoch": 1.3641463414634147,
      "grad_norm": 0.09835544973611832,
      "learning_rate": 2.854810206081302e-05,
      "loss": 0.025,
      "step": 5593
    },
    {
      "epoch": 1.364390243902439,
      "grad_norm": 0.12580658495426178,
      "learning_rate": 2.8541780715712358e-05,
      "loss": 0.0079,
      "step": 5594
    },
    {
      "epoch": 1.3646341463414635,
      "grad_norm": 0.1551699936389923,
      "learning_rate": 2.8535459139521335e-05,
      "loss": 0.0285,
      "step": 5595
    },
    {
      "epoch": 1.3648780487804877,
      "grad_norm": 0.20186269283294678,
      "learning_rate": 2.852913733265243e-05,
      "loss": 0.0222,
      "step": 5596
    },
    {
      "epoch": 1.3651219512195123,
      "grad_norm": 0.12779462337493896,
      "learning_rate": 2.852281529551811e-05,
      "loss": 0.0304,
      "step": 5597
    },
    {
      "epoch": 1.3653658536585365,
      "grad_norm": 0.1185188740491867,
      "learning_rate": 2.8516493028530893e-05,
      "loss": 0.0196,
      "step": 5598
    },
    {
      "epoch": 1.365609756097561,
      "grad_norm": 0.07017634809017181,
      "learning_rate": 2.851017053210326e-05,
      "loss": 0.0171,
      "step": 5599
    },
    {
      "epoch": 1.3658536585365852,
      "grad_norm": 0.14705853164196014,
      "learning_rate": 2.850384780664775e-05,
      "loss": 0.021,
      "step": 5600
    },
    {
      "epoch": 1.3660975609756099,
      "grad_norm": 0.07528328895568848,
      "learning_rate": 2.849752485257689e-05,
      "loss": 0.0148,
      "step": 5601
    },
    {
      "epoch": 1.366341463414634,
      "grad_norm": 0.0630260705947876,
      "learning_rate": 2.849120167030325e-05,
      "loss": 0.0116,
      "step": 5602
    },
    {
      "epoch": 1.3665853658536586,
      "grad_norm": 0.15337221324443817,
      "learning_rate": 2.8484878260239383e-05,
      "loss": 0.0206,
      "step": 5603
    },
    {
      "epoch": 1.3668292682926828,
      "grad_norm": 0.10002561658620834,
      "learning_rate": 2.847855462279788e-05,
      "loss": 0.0167,
      "step": 5604
    },
    {
      "epoch": 1.3670731707317074,
      "grad_norm": 0.1139686331152916,
      "learning_rate": 2.8472230758391332e-05,
      "loss": 0.0268,
      "step": 5605
    },
    {
      "epoch": 1.3673170731707316,
      "grad_norm": 0.2717248499393463,
      "learning_rate": 2.846590666743236e-05,
      "loss": 0.0328,
      "step": 5606
    },
    {
      "epoch": 1.3675609756097562,
      "grad_norm": 0.09003401547670364,
      "learning_rate": 2.845958235033359e-05,
      "loss": 0.0179,
      "step": 5607
    },
    {
      "epoch": 1.3678048780487804,
      "grad_norm": 0.20072011649608612,
      "learning_rate": 2.845325780750766e-05,
      "loss": 0.0216,
      "step": 5608
    },
    {
      "epoch": 1.368048780487805,
      "grad_norm": 0.09741342067718506,
      "learning_rate": 2.8446933039367226e-05,
      "loss": 0.0303,
      "step": 5609
    },
    {
      "epoch": 1.3682926829268292,
      "grad_norm": 0.08281946182250977,
      "learning_rate": 2.844060804632497e-05,
      "loss": 0.0257,
      "step": 5610
    },
    {
      "epoch": 1.3685365853658538,
      "grad_norm": 0.05594614893198013,
      "learning_rate": 2.8434282828793574e-05,
      "loss": 0.0156,
      "step": 5611
    },
    {
      "epoch": 1.368780487804878,
      "grad_norm": 0.13074298202991486,
      "learning_rate": 2.8427957387185728e-05,
      "loss": 0.0191,
      "step": 5612
    },
    {
      "epoch": 1.3690243902439025,
      "grad_norm": 0.10269142687320709,
      "learning_rate": 2.8421631721914154e-05,
      "loss": 0.013,
      "step": 5613
    },
    {
      "epoch": 1.3692682926829267,
      "grad_norm": 0.18321414291858673,
      "learning_rate": 2.8415305833391593e-05,
      "loss": 0.0205,
      "step": 5614
    },
    {
      "epoch": 1.3695121951219513,
      "grad_norm": 0.10509061068296432,
      "learning_rate": 2.840897972203078e-05,
      "loss": 0.0196,
      "step": 5615
    },
    {
      "epoch": 1.3697560975609755,
      "grad_norm": 0.11217187345027924,
      "learning_rate": 2.8402653388244476e-05,
      "loss": 0.0247,
      "step": 5616
    },
    {
      "epoch": 1.37,
      "grad_norm": 0.11757931858301163,
      "learning_rate": 2.8396326832445453e-05,
      "loss": 0.0164,
      "step": 5617
    },
    {
      "epoch": 1.3702439024390243,
      "grad_norm": 0.08551278710365295,
      "learning_rate": 2.83900000550465e-05,
      "loss": 0.0119,
      "step": 5618
    },
    {
      "epoch": 1.3704878048780489,
      "grad_norm": 0.14105883240699768,
      "learning_rate": 2.838367305646043e-05,
      "loss": 0.0195,
      "step": 5619
    },
    {
      "epoch": 1.370731707317073,
      "grad_norm": 0.09157693386077881,
      "learning_rate": 2.8377345837100046e-05,
      "loss": 0.0095,
      "step": 5620
    },
    {
      "epoch": 1.3709756097560977,
      "grad_norm": 0.131504625082016,
      "learning_rate": 2.837101839737818e-05,
      "loss": 0.0278,
      "step": 5621
    },
    {
      "epoch": 1.3712195121951218,
      "grad_norm": 0.11508118361234665,
      "learning_rate": 2.8364690737707688e-05,
      "loss": 0.0359,
      "step": 5622
    },
    {
      "epoch": 1.3714634146341464,
      "grad_norm": 0.12021252512931824,
      "learning_rate": 2.835836285850143e-05,
      "loss": 0.013,
      "step": 5623
    },
    {
      "epoch": 1.3717073170731706,
      "grad_norm": 0.07768642157316208,
      "learning_rate": 2.835203476017227e-05,
      "loss": 0.0176,
      "step": 5624
    },
    {
      "epoch": 1.3719512195121952,
      "grad_norm": 0.19925612211227417,
      "learning_rate": 2.83457064431331e-05,
      "loss": 0.0258,
      "step": 5625
    },
    {
      "epoch": 1.3721951219512194,
      "grad_norm": 0.13658776879310608,
      "learning_rate": 2.8339377907796833e-05,
      "loss": 0.0173,
      "step": 5626
    },
    {
      "epoch": 1.372439024390244,
      "grad_norm": 0.07112467288970947,
      "learning_rate": 2.833304915457638e-05,
      "loss": 0.0176,
      "step": 5627
    },
    {
      "epoch": 1.3726829268292682,
      "grad_norm": 0.13074909150600433,
      "learning_rate": 2.8326720183884682e-05,
      "loss": 0.0317,
      "step": 5628
    },
    {
      "epoch": 1.3729268292682928,
      "grad_norm": 0.189424529671669,
      "learning_rate": 2.832039099613466e-05,
      "loss": 0.0164,
      "step": 5629
    },
    {
      "epoch": 1.373170731707317,
      "grad_norm": 0.19557443261146545,
      "learning_rate": 2.8314061591739294e-05,
      "loss": 0.0182,
      "step": 5630
    },
    {
      "epoch": 1.3734146341463416,
      "grad_norm": 0.0996803343296051,
      "learning_rate": 2.8307731971111556e-05,
      "loss": 0.0182,
      "step": 5631
    },
    {
      "epoch": 1.3736585365853657,
      "grad_norm": 0.04671164229512215,
      "learning_rate": 2.830140213466444e-05,
      "loss": 0.0122,
      "step": 5632
    },
    {
      "epoch": 1.3739024390243904,
      "grad_norm": 0.13405179977416992,
      "learning_rate": 2.829507208281093e-05,
      "loss": 0.0184,
      "step": 5633
    },
    {
      "epoch": 1.3741463414634145,
      "grad_norm": 0.15538951754570007,
      "learning_rate": 2.8288741815964058e-05,
      "loss": 0.0244,
      "step": 5634
    },
    {
      "epoch": 1.3743902439024391,
      "grad_norm": 0.1352044939994812,
      "learning_rate": 2.8282411334536857e-05,
      "loss": 0.0242,
      "step": 5635
    },
    {
      "epoch": 1.3746341463414633,
      "grad_norm": 0.3603611886501312,
      "learning_rate": 2.827608063894236e-05,
      "loss": 0.0258,
      "step": 5636
    },
    {
      "epoch": 1.374878048780488,
      "grad_norm": 0.13468211889266968,
      "learning_rate": 2.826974972959363e-05,
      "loss": 0.0249,
      "step": 5637
    },
    {
      "epoch": 1.375121951219512,
      "grad_norm": 0.23356598615646362,
      "learning_rate": 2.8263418606903735e-05,
      "loss": 0.0212,
      "step": 5638
    },
    {
      "epoch": 1.3753658536585367,
      "grad_norm": 0.15560327470302582,
      "learning_rate": 2.825708727128577e-05,
      "loss": 0.017,
      "step": 5639
    },
    {
      "epoch": 1.3756097560975609,
      "grad_norm": 0.09479724615812302,
      "learning_rate": 2.8250755723152835e-05,
      "loss": 0.0317,
      "step": 5640
    },
    {
      "epoch": 1.3758536585365855,
      "grad_norm": 0.12150811403989792,
      "learning_rate": 2.824442396291804e-05,
      "loss": 0.0258,
      "step": 5641
    },
    {
      "epoch": 1.3760975609756096,
      "grad_norm": 0.21238303184509277,
      "learning_rate": 2.823809199099451e-05,
      "loss": 0.0239,
      "step": 5642
    },
    {
      "epoch": 1.3763414634146343,
      "grad_norm": 0.09710013121366501,
      "learning_rate": 2.8231759807795387e-05,
      "loss": 0.0109,
      "step": 5643
    },
    {
      "epoch": 1.3765853658536584,
      "grad_norm": 0.2727982699871063,
      "learning_rate": 2.8225427413733836e-05,
      "loss": 0.0225,
      "step": 5644
    },
    {
      "epoch": 1.376829268292683,
      "grad_norm": 0.09747336059808731,
      "learning_rate": 2.8219094809223022e-05,
      "loss": 0.0097,
      "step": 5645
    },
    {
      "epoch": 1.3770731707317072,
      "grad_norm": 0.1295066922903061,
      "learning_rate": 2.8212761994676125e-05,
      "loss": 0.0324,
      "step": 5646
    },
    {
      "epoch": 1.3773170731707318,
      "grad_norm": 0.07312893122434616,
      "learning_rate": 2.8206428970506337e-05,
      "loss": 0.0158,
      "step": 5647
    },
    {
      "epoch": 1.377560975609756,
      "grad_norm": 0.10436687618494034,
      "learning_rate": 2.820009573712688e-05,
      "loss": 0.0172,
      "step": 5648
    },
    {
      "epoch": 1.3778048780487806,
      "grad_norm": 0.10467217117547989,
      "learning_rate": 2.819376229495097e-05,
      "loss": 0.0254,
      "step": 5649
    },
    {
      "epoch": 1.3780487804878048,
      "grad_norm": 0.11742039769887924,
      "learning_rate": 2.8187428644391845e-05,
      "loss": 0.0399,
      "step": 5650
    },
    {
      "epoch": 1.3782926829268294,
      "grad_norm": 0.1457015573978424,
      "learning_rate": 2.818109478586276e-05,
      "loss": 0.0258,
      "step": 5651
    },
    {
      "epoch": 1.3785365853658536,
      "grad_norm": 0.09547822177410126,
      "learning_rate": 2.817476071977698e-05,
      "loss": 0.0237,
      "step": 5652
    },
    {
      "epoch": 1.3787804878048782,
      "grad_norm": 0.16189055144786835,
      "learning_rate": 2.816842644654779e-05,
      "loss": 0.0214,
      "step": 5653
    },
    {
      "epoch": 1.3790243902439023,
      "grad_norm": 0.15508733689785004,
      "learning_rate": 2.8162091966588468e-05,
      "loss": 0.0118,
      "step": 5654
    },
    {
      "epoch": 1.379268292682927,
      "grad_norm": 0.09854667633771896,
      "learning_rate": 2.8155757280312318e-05,
      "loss": 0.0181,
      "step": 5655
    },
    {
      "epoch": 1.3795121951219511,
      "grad_norm": 0.18026109039783478,
      "learning_rate": 2.8149422388132674e-05,
      "loss": 0.0362,
      "step": 5656
    },
    {
      "epoch": 1.3797560975609757,
      "grad_norm": 0.07122231274843216,
      "learning_rate": 2.8143087290462854e-05,
      "loss": 0.0119,
      "step": 5657
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.20149846374988556,
      "learning_rate": 2.8136751987716213e-05,
      "loss": 0.0238,
      "step": 5658
    },
    {
      "epoch": 1.3802439024390245,
      "grad_norm": 0.1548241823911667,
      "learning_rate": 2.813041648030611e-05,
      "loss": 0.0271,
      "step": 5659
    },
    {
      "epoch": 1.3804878048780487,
      "grad_norm": 0.13779272139072418,
      "learning_rate": 2.8124080768645915e-05,
      "loss": 0.0132,
      "step": 5660
    },
    {
      "epoch": 1.3807317073170733,
      "grad_norm": 0.1463669240474701,
      "learning_rate": 2.8117744853149014e-05,
      "loss": 0.0194,
      "step": 5661
    },
    {
      "epoch": 1.3809756097560975,
      "grad_norm": 0.09602325409650803,
      "learning_rate": 2.8111408734228804e-05,
      "loss": 0.0157,
      "step": 5662
    },
    {
      "epoch": 1.381219512195122,
      "grad_norm": 0.1231573298573494,
      "learning_rate": 2.81050724122987e-05,
      "loss": 0.0121,
      "step": 5663
    },
    {
      "epoch": 1.3814634146341462,
      "grad_norm": 0.08899225294589996,
      "learning_rate": 2.809873588777212e-05,
      "loss": 0.0242,
      "step": 5664
    },
    {
      "epoch": 1.3817073170731708,
      "grad_norm": 0.2720590829849243,
      "learning_rate": 2.809239916106252e-05,
      "loss": 0.0421,
      "step": 5665
    },
    {
      "epoch": 1.381951219512195,
      "grad_norm": 0.15662066638469696,
      "learning_rate": 2.8086062232583333e-05,
      "loss": 0.0477,
      "step": 5666
    },
    {
      "epoch": 1.3821951219512196,
      "grad_norm": 0.08356206864118576,
      "learning_rate": 2.8079725102748034e-05,
      "loss": 0.0151,
      "step": 5667
    },
    {
      "epoch": 1.3824390243902438,
      "grad_norm": 0.14609795808792114,
      "learning_rate": 2.8073387771970106e-05,
      "loss": 0.0168,
      "step": 5668
    },
    {
      "epoch": 1.3826829268292684,
      "grad_norm": 0.2781642973423004,
      "learning_rate": 2.806705024066303e-05,
      "loss": 0.0199,
      "step": 5669
    },
    {
      "epoch": 1.3829268292682926,
      "grad_norm": 0.13413457572460175,
      "learning_rate": 2.8060712509240318e-05,
      "loss": 0.026,
      "step": 5670
    },
    {
      "epoch": 1.3831707317073172,
      "grad_norm": 0.148118793964386,
      "learning_rate": 2.8054374578115476e-05,
      "loss": 0.0154,
      "step": 5671
    },
    {
      "epoch": 1.3834146341463414,
      "grad_norm": 0.08361005783081055,
      "learning_rate": 2.8048036447702054e-05,
      "loss": 0.0187,
      "step": 5672
    },
    {
      "epoch": 1.383658536585366,
      "grad_norm": 0.08128751069307327,
      "learning_rate": 2.8041698118413574e-05,
      "loss": 0.0196,
      "step": 5673
    },
    {
      "epoch": 1.3839024390243901,
      "grad_norm": 0.1370183378458023,
      "learning_rate": 2.8035359590663613e-05,
      "loss": 0.0329,
      "step": 5674
    },
    {
      "epoch": 1.3841463414634148,
      "grad_norm": 0.126257061958313,
      "learning_rate": 2.8029020864865725e-05,
      "loss": 0.0263,
      "step": 5675
    },
    {
      "epoch": 1.384390243902439,
      "grad_norm": 0.1056072935461998,
      "learning_rate": 2.8022681941433505e-05,
      "loss": 0.0126,
      "step": 5676
    },
    {
      "epoch": 1.3846341463414635,
      "grad_norm": 0.11956001073122025,
      "learning_rate": 2.8016342820780533e-05,
      "loss": 0.0189,
      "step": 5677
    },
    {
      "epoch": 1.3848780487804877,
      "grad_norm": 0.2314845323562622,
      "learning_rate": 2.8010003503320438e-05,
      "loss": 0.025,
      "step": 5678
    },
    {
      "epoch": 1.3851219512195123,
      "grad_norm": 0.14323504269123077,
      "learning_rate": 2.8003663989466816e-05,
      "loss": 0.0148,
      "step": 5679
    },
    {
      "epoch": 1.3853658536585365,
      "grad_norm": 0.07738793641328812,
      "learning_rate": 2.7997324279633314e-05,
      "loss": 0.012,
      "step": 5680
    },
    {
      "epoch": 1.385609756097561,
      "grad_norm": 0.11362387239933014,
      "learning_rate": 2.7990984374233582e-05,
      "loss": 0.0145,
      "step": 5681
    },
    {
      "epoch": 1.3858536585365853,
      "grad_norm": 0.1019124761223793,
      "learning_rate": 2.7984644273681282e-05,
      "loss": 0.0121,
      "step": 5682
    },
    {
      "epoch": 1.3860975609756099,
      "grad_norm": 0.07972025871276855,
      "learning_rate": 2.797830397839007e-05,
      "loss": 0.0123,
      "step": 5683
    },
    {
      "epoch": 1.386341463414634,
      "grad_norm": 0.16226740181446075,
      "learning_rate": 2.7971963488773645e-05,
      "loss": 0.0197,
      "step": 5684
    },
    {
      "epoch": 1.3865853658536587,
      "grad_norm": 0.19380399584770203,
      "learning_rate": 2.7965622805245707e-05,
      "loss": 0.0212,
      "step": 5685
    },
    {
      "epoch": 1.3868292682926828,
      "grad_norm": 0.12537573277950287,
      "learning_rate": 2.795928192821995e-05,
      "loss": 0.0295,
      "step": 5686
    },
    {
      "epoch": 1.3870731707317074,
      "grad_norm": 0.07378434389829636,
      "learning_rate": 2.795294085811011e-05,
      "loss": 0.0139,
      "step": 5687
    },
    {
      "epoch": 1.3873170731707316,
      "grad_norm": 0.3852195143699646,
      "learning_rate": 2.794659959532992e-05,
      "loss": 0.0416,
      "step": 5688
    },
    {
      "epoch": 1.3875609756097562,
      "grad_norm": 0.13627490401268005,
      "learning_rate": 2.7940258140293124e-05,
      "loss": 0.0121,
      "step": 5689
    },
    {
      "epoch": 1.3878048780487804,
      "grad_norm": 0.18500421941280365,
      "learning_rate": 2.793391649341348e-05,
      "loss": 0.0351,
      "step": 5690
    },
    {
      "epoch": 1.388048780487805,
      "grad_norm": 0.13874101638793945,
      "learning_rate": 2.792757465510478e-05,
      "loss": 0.0243,
      "step": 5691
    },
    {
      "epoch": 1.3882926829268292,
      "grad_norm": 0.2066366970539093,
      "learning_rate": 2.7921232625780786e-05,
      "loss": 0.0114,
      "step": 5692
    },
    {
      "epoch": 1.3885365853658538,
      "grad_norm": 0.3259156048297882,
      "learning_rate": 2.7914890405855314e-05,
      "loss": 0.0292,
      "step": 5693
    },
    {
      "epoch": 1.388780487804878,
      "grad_norm": 0.13548040390014648,
      "learning_rate": 2.7908547995742155e-05,
      "loss": 0.0216,
      "step": 5694
    },
    {
      "epoch": 1.3890243902439026,
      "grad_norm": 0.12495236843824387,
      "learning_rate": 2.7902205395855158e-05,
      "loss": 0.0206,
      "step": 5695
    },
    {
      "epoch": 1.3892682926829267,
      "grad_norm": 0.05424562841653824,
      "learning_rate": 2.789586260660813e-05,
      "loss": 0.0144,
      "step": 5696
    },
    {
      "epoch": 1.3895121951219513,
      "grad_norm": 0.09219744801521301,
      "learning_rate": 2.788951962841494e-05,
      "loss": 0.0315,
      "step": 5697
    },
    {
      "epoch": 1.3897560975609755,
      "grad_norm": 0.08767138421535492,
      "learning_rate": 2.788317646168943e-05,
      "loss": 0.0189,
      "step": 5698
    },
    {
      "epoch": 1.3900000000000001,
      "grad_norm": 0.07005681097507477,
      "learning_rate": 2.7876833106845496e-05,
      "loss": 0.0192,
      "step": 5699
    },
    {
      "epoch": 1.3902439024390243,
      "grad_norm": 0.12010811269283295,
      "learning_rate": 2.7870489564297e-05,
      "loss": 0.0122,
      "step": 5700
    },
    {
      "epoch": 1.390487804878049,
      "grad_norm": 0.09981590509414673,
      "learning_rate": 2.7864145834457848e-05,
      "loss": 0.0231,
      "step": 5701
    },
    {
      "epoch": 1.390731707317073,
      "grad_norm": 0.0980035588145256,
      "learning_rate": 2.785780191774195e-05,
      "loss": 0.0215,
      "step": 5702
    },
    {
      "epoch": 1.3909756097560977,
      "grad_norm": 0.12109421193599701,
      "learning_rate": 2.7851457814563225e-05,
      "loss": 0.0208,
      "step": 5703
    },
    {
      "epoch": 1.3912195121951219,
      "grad_norm": 0.13021975755691528,
      "learning_rate": 2.7845113525335608e-05,
      "loss": 0.0293,
      "step": 5704
    },
    {
      "epoch": 1.3914634146341465,
      "grad_norm": 0.12241870164871216,
      "learning_rate": 2.783876905047304e-05,
      "loss": 0.0254,
      "step": 5705
    },
    {
      "epoch": 1.3917073170731706,
      "grad_norm": 0.11058304458856583,
      "learning_rate": 2.7832424390389483e-05,
      "loss": 0.0232,
      "step": 5706
    },
    {
      "epoch": 1.3919512195121952,
      "grad_norm": 0.16540654003620148,
      "learning_rate": 2.7826079545498905e-05,
      "loss": 0.0277,
      "step": 5707
    },
    {
      "epoch": 1.3921951219512194,
      "grad_norm": 0.08031974732875824,
      "learning_rate": 2.781973451621529e-05,
      "loss": 0.0204,
      "step": 5708
    },
    {
      "epoch": 1.392439024390244,
      "grad_norm": 0.08941575139760971,
      "learning_rate": 2.7813389302952637e-05,
      "loss": 0.0092,
      "step": 5709
    },
    {
      "epoch": 1.3926829268292682,
      "grad_norm": 0.08715072274208069,
      "learning_rate": 2.780704390612493e-05,
      "loss": 0.0114,
      "step": 5710
    },
    {
      "epoch": 1.3929268292682928,
      "grad_norm": 0.12235905230045319,
      "learning_rate": 2.780069832614621e-05,
      "loss": 0.0322,
      "step": 5711
    },
    {
      "epoch": 1.393170731707317,
      "grad_norm": 0.15830296277999878,
      "learning_rate": 2.77943525634305e-05,
      "loss": 0.0477,
      "step": 5712
    },
    {
      "epoch": 1.3934146341463416,
      "grad_norm": 0.1290620118379593,
      "learning_rate": 2.778800661839183e-05,
      "loss": 0.0295,
      "step": 5713
    },
    {
      "epoch": 1.3936585365853658,
      "grad_norm": 0.211212158203125,
      "learning_rate": 2.7781660491444267e-05,
      "loss": 0.0251,
      "step": 5714
    },
    {
      "epoch": 1.3939024390243904,
      "grad_norm": 0.11964022368192673,
      "learning_rate": 2.777531418300187e-05,
      "loss": 0.0141,
      "step": 5715
    },
    {
      "epoch": 1.3941463414634145,
      "grad_norm": 0.1687553972005844,
      "learning_rate": 2.7768967693478726e-05,
      "loss": 0.0152,
      "step": 5716
    },
    {
      "epoch": 1.3943902439024392,
      "grad_norm": 0.15502716600894928,
      "learning_rate": 2.7762621023288915e-05,
      "loss": 0.0261,
      "step": 5717
    },
    {
      "epoch": 1.3946341463414633,
      "grad_norm": 0.09460314363241196,
      "learning_rate": 2.7756274172846542e-05,
      "loss": 0.0179,
      "step": 5718
    },
    {
      "epoch": 1.394878048780488,
      "grad_norm": 0.13684633374214172,
      "learning_rate": 2.774992714256571e-05,
      "loss": 0.0281,
      "step": 5719
    },
    {
      "epoch": 1.395121951219512,
      "grad_norm": 0.1017489954829216,
      "learning_rate": 2.7743579932860552e-05,
      "loss": 0.0179,
      "step": 5720
    },
    {
      "epoch": 1.3953658536585367,
      "grad_norm": 0.1693924218416214,
      "learning_rate": 2.7737232544145204e-05,
      "loss": 0.0226,
      "step": 5721
    },
    {
      "epoch": 1.3956097560975609,
      "grad_norm": 0.17605377733707428,
      "learning_rate": 2.773088497683381e-05,
      "loss": 0.0161,
      "step": 5722
    },
    {
      "epoch": 1.3958536585365855,
      "grad_norm": 0.12630851566791534,
      "learning_rate": 2.772453723134053e-05,
      "loss": 0.0166,
      "step": 5723
    },
    {
      "epoch": 1.3960975609756097,
      "grad_norm": 0.16232480108737946,
      "learning_rate": 2.771818930807954e-05,
      "loss": 0.0232,
      "step": 5724
    },
    {
      "epoch": 1.3963414634146343,
      "grad_norm": 0.0993121787905693,
      "learning_rate": 2.7711841207465016e-05,
      "loss": 0.0245,
      "step": 5725
    },
    {
      "epoch": 1.3965853658536584,
      "grad_norm": 0.24232101440429688,
      "learning_rate": 2.770549292991116e-05,
      "loss": 0.025,
      "step": 5726
    },
    {
      "epoch": 1.396829268292683,
      "grad_norm": 0.1068124920129776,
      "learning_rate": 2.7699144475832167e-05,
      "loss": 0.0214,
      "step": 5727
    },
    {
      "epoch": 1.3970731707317072,
      "grad_norm": 0.2206602692604065,
      "learning_rate": 2.769279584564226e-05,
      "loss": 0.0236,
      "step": 5728
    },
    {
      "epoch": 1.3973170731707318,
      "grad_norm": 0.12888941168785095,
      "learning_rate": 2.768644703975567e-05,
      "loss": 0.0149,
      "step": 5729
    },
    {
      "epoch": 1.397560975609756,
      "grad_norm": 0.11598452180624008,
      "learning_rate": 2.7680098058586634e-05,
      "loss": 0.0247,
      "step": 5730
    },
    {
      "epoch": 1.3978048780487806,
      "grad_norm": 0.28799769282341003,
      "learning_rate": 2.76737489025494e-05,
      "loss": 0.038,
      "step": 5731
    },
    {
      "epoch": 1.3980487804878048,
      "grad_norm": 0.20197027921676636,
      "learning_rate": 2.766739957205824e-05,
      "loss": 0.0405,
      "step": 5732
    },
    {
      "epoch": 1.3982926829268294,
      "grad_norm": 0.22522877156734467,
      "learning_rate": 2.7661050067527427e-05,
      "loss": 0.0209,
      "step": 5733
    },
    {
      "epoch": 1.3985365853658536,
      "grad_norm": 0.0834021046757698,
      "learning_rate": 2.7654700389371242e-05,
      "loss": 0.0231,
      "step": 5734
    },
    {
      "epoch": 1.3987804878048782,
      "grad_norm": 0.087436243891716,
      "learning_rate": 2.7648350538003982e-05,
      "loss": 0.0153,
      "step": 5735
    },
    {
      "epoch": 1.3990243902439023,
      "grad_norm": 0.15127553045749664,
      "learning_rate": 2.764200051383995e-05,
      "loss": 0.02,
      "step": 5736
    },
    {
      "epoch": 1.399268292682927,
      "grad_norm": 0.08720919489860535,
      "learning_rate": 2.7635650317293484e-05,
      "loss": 0.0197,
      "step": 5737
    },
    {
      "epoch": 1.3995121951219511,
      "grad_norm": 0.16809360682964325,
      "learning_rate": 2.762929994877889e-05,
      "loss": 0.025,
      "step": 5738
    },
    {
      "epoch": 1.3997560975609757,
      "grad_norm": 0.15982165932655334,
      "learning_rate": 2.762294940871053e-05,
      "loss": 0.0285,
      "step": 5739
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.08937080204486847,
      "learning_rate": 2.7616598697502755e-05,
      "loss": 0.0208,
      "step": 5740
    },
    {
      "epoch": 1.4002439024390245,
      "grad_norm": 0.1443118453025818,
      "learning_rate": 2.7610247815569923e-05,
      "loss": 0.0322,
      "step": 5741
    },
    {
      "epoch": 1.4004878048780487,
      "grad_norm": 0.09453979879617691,
      "learning_rate": 2.7603896763326415e-05,
      "loss": 0.0094,
      "step": 5742
    },
    {
      "epoch": 1.4007317073170733,
      "grad_norm": 0.18815721571445465,
      "learning_rate": 2.759754554118661e-05,
      "loss": 0.0224,
      "step": 5743
    },
    {
      "epoch": 1.4009756097560975,
      "grad_norm": 0.16077956557273865,
      "learning_rate": 2.75911941495649e-05,
      "loss": 0.026,
      "step": 5744
    },
    {
      "epoch": 1.401219512195122,
      "grad_norm": 0.08724673092365265,
      "learning_rate": 2.7584842588875715e-05,
      "loss": 0.0161,
      "step": 5745
    },
    {
      "epoch": 1.4014634146341463,
      "grad_norm": 0.1624625027179718,
      "learning_rate": 2.757849085953347e-05,
      "loss": 0.0268,
      "step": 5746
    },
    {
      "epoch": 1.4017073170731709,
      "grad_norm": 0.12653161585330963,
      "learning_rate": 2.757213896195258e-05,
      "loss": 0.0333,
      "step": 5747
    },
    {
      "epoch": 1.401951219512195,
      "grad_norm": 0.14760853350162506,
      "learning_rate": 2.7565786896547496e-05,
      "loss": 0.021,
      "step": 5748
    },
    {
      "epoch": 1.4021951219512196,
      "grad_norm": 0.1521310657262802,
      "learning_rate": 2.7559434663732675e-05,
      "loss": 0.031,
      "step": 5749
    },
    {
      "epoch": 1.4024390243902438,
      "grad_norm": 0.1729138344526291,
      "learning_rate": 2.755308226392258e-05,
      "loss": 0.0255,
      "step": 5750
    },
    {
      "epoch": 1.4026829268292684,
      "grad_norm": 0.18310751020908356,
      "learning_rate": 2.7546729697531687e-05,
      "loss": 0.0386,
      "step": 5751
    },
    {
      "epoch": 1.4029268292682926,
      "grad_norm": 0.16224777698516846,
      "learning_rate": 2.7540376964974468e-05,
      "loss": 0.0248,
      "step": 5752
    },
    {
      "epoch": 1.4031707317073172,
      "grad_norm": 0.13721609115600586,
      "learning_rate": 2.753402406666543e-05,
      "loss": 0.0248,
      "step": 5753
    },
    {
      "epoch": 1.4034146341463414,
      "grad_norm": 0.09955589473247528,
      "learning_rate": 2.752767100301909e-05,
      "loss": 0.0134,
      "step": 5754
    },
    {
      "epoch": 1.403658536585366,
      "grad_norm": 0.1503669023513794,
      "learning_rate": 2.752131777444994e-05,
      "loss": 0.0293,
      "step": 5755
    },
    {
      "epoch": 1.4039024390243902,
      "grad_norm": 0.1309143751859665,
      "learning_rate": 2.7514964381372534e-05,
      "loss": 0.0206,
      "step": 5756
    },
    {
      "epoch": 1.4041463414634148,
      "grad_norm": 0.17433080077171326,
      "learning_rate": 2.750861082420139e-05,
      "loss": 0.0472,
      "step": 5757
    },
    {
      "epoch": 1.404390243902439,
      "grad_norm": 0.11044915020465851,
      "learning_rate": 2.7502257103351082e-05,
      "loss": 0.017,
      "step": 5758
    },
    {
      "epoch": 1.4046341463414636,
      "grad_norm": 0.12845276296138763,
      "learning_rate": 2.749590321923616e-05,
      "loss": 0.0185,
      "step": 5759
    },
    {
      "epoch": 1.4048780487804877,
      "grad_norm": 0.11483132094144821,
      "learning_rate": 2.748954917227118e-05,
      "loss": 0.0212,
      "step": 5760
    },
    {
      "epoch": 1.4051219512195123,
      "grad_norm": 0.1833229959011078,
      "learning_rate": 2.7483194962870745e-05,
      "loss": 0.0196,
      "step": 5761
    },
    {
      "epoch": 1.4053658536585365,
      "grad_norm": 0.08558410406112671,
      "learning_rate": 2.7476840591449436e-05,
      "loss": 0.0138,
      "step": 5762
    },
    {
      "epoch": 1.4056097560975611,
      "grad_norm": 0.11825570464134216,
      "learning_rate": 2.7470486058421867e-05,
      "loss": 0.0235,
      "step": 5763
    },
    {
      "epoch": 1.4058536585365853,
      "grad_norm": 0.07873932272195816,
      "learning_rate": 2.7464131364202644e-05,
      "loss": 0.0191,
      "step": 5764
    },
    {
      "epoch": 1.40609756097561,
      "grad_norm": 0.25189557671546936,
      "learning_rate": 2.745777650920639e-05,
      "loss": 0.0252,
      "step": 5765
    },
    {
      "epoch": 1.406341463414634,
      "grad_norm": 0.09985032677650452,
      "learning_rate": 2.7451421493847744e-05,
      "loss": 0.0249,
      "step": 5766
    },
    {
      "epoch": 1.4065853658536585,
      "grad_norm": 0.22432096302509308,
      "learning_rate": 2.7445066318541356e-05,
      "loss": 0.0348,
      "step": 5767
    },
    {
      "epoch": 1.4068292682926828,
      "grad_norm": 0.14182719588279724,
      "learning_rate": 2.7438710983701872e-05,
      "loss": 0.0225,
      "step": 5768
    },
    {
      "epoch": 1.4070731707317072,
      "grad_norm": 0.11068356037139893,
      "learning_rate": 2.7432355489743956e-05,
      "loss": 0.0232,
      "step": 5769
    },
    {
      "epoch": 1.4073170731707316,
      "grad_norm": 0.07375913113355637,
      "learning_rate": 2.7425999837082294e-05,
      "loss": 0.0167,
      "step": 5770
    },
    {
      "epoch": 1.407560975609756,
      "grad_norm": 0.18663452565670013,
      "learning_rate": 2.7419644026131574e-05,
      "loss": 0.0189,
      "step": 5771
    },
    {
      "epoch": 1.4078048780487804,
      "grad_norm": 0.1238069087266922,
      "learning_rate": 2.741328805730648e-05,
      "loss": 0.0232,
      "step": 5772
    },
    {
      "epoch": 1.4080487804878048,
      "grad_norm": 0.2054029107093811,
      "learning_rate": 2.7406931931021734e-05,
      "loss": 0.0183,
      "step": 5773
    },
    {
      "epoch": 1.4082926829268292,
      "grad_norm": 0.1438744217157364,
      "learning_rate": 2.7400575647692046e-05,
      "loss": 0.0277,
      "step": 5774
    },
    {
      "epoch": 1.4085365853658536,
      "grad_norm": 0.08580974489450455,
      "learning_rate": 2.7394219207732148e-05,
      "loss": 0.0211,
      "step": 5775
    },
    {
      "epoch": 1.408780487804878,
      "grad_norm": 0.15147827565670013,
      "learning_rate": 2.7387862611556773e-05,
      "loss": 0.027,
      "step": 5776
    },
    {
      "epoch": 1.4090243902439024,
      "grad_norm": 0.11588568985462189,
      "learning_rate": 2.7381505859580675e-05,
      "loss": 0.0123,
      "step": 5777
    },
    {
      "epoch": 1.4092682926829267,
      "grad_norm": 0.2032458484172821,
      "learning_rate": 2.737514895221861e-05,
      "loss": 0.0187,
      "step": 5778
    },
    {
      "epoch": 1.4095121951219511,
      "grad_norm": 0.09136778116226196,
      "learning_rate": 2.7368791889885347e-05,
      "loss": 0.017,
      "step": 5779
    },
    {
      "epoch": 1.4097560975609755,
      "grad_norm": 0.1448998749256134,
      "learning_rate": 2.7362434672995663e-05,
      "loss": 0.0245,
      "step": 5780
    },
    {
      "epoch": 1.41,
      "grad_norm": 0.1277676522731781,
      "learning_rate": 2.735607730196435e-05,
      "loss": 0.0166,
      "step": 5781
    },
    {
      "epoch": 1.4102439024390243,
      "grad_norm": 0.1412067413330078,
      "learning_rate": 2.73497197772062e-05,
      "loss": 0.0133,
      "step": 5782
    },
    {
      "epoch": 1.4104878048780487,
      "grad_norm": 0.06485497951507568,
      "learning_rate": 2.7343362099136037e-05,
      "loss": 0.0114,
      "step": 5783
    },
    {
      "epoch": 1.410731707317073,
      "grad_norm": 0.13010305166244507,
      "learning_rate": 2.733700426816867e-05,
      "loss": 0.0284,
      "step": 5784
    },
    {
      "epoch": 1.4109756097560975,
      "grad_norm": 0.06470296531915665,
      "learning_rate": 2.7330646284718925e-05,
      "loss": 0.011,
      "step": 5785
    },
    {
      "epoch": 1.4112195121951219,
      "grad_norm": 0.10395950078964233,
      "learning_rate": 2.7324288149201644e-05,
      "loss": 0.0137,
      "step": 5786
    },
    {
      "epoch": 1.4114634146341463,
      "grad_norm": 0.13629858195781708,
      "learning_rate": 2.7317929862031676e-05,
      "loss": 0.0165,
      "step": 5787
    },
    {
      "epoch": 1.4117073170731707,
      "grad_norm": 0.12281395494937897,
      "learning_rate": 2.7311571423623883e-05,
      "loss": 0.0229,
      "step": 5788
    },
    {
      "epoch": 1.411951219512195,
      "grad_norm": 0.06977690756320953,
      "learning_rate": 2.730521283439313e-05,
      "loss": 0.009,
      "step": 5789
    },
    {
      "epoch": 1.4121951219512194,
      "grad_norm": 0.18475429713726044,
      "learning_rate": 2.7298854094754294e-05,
      "loss": 0.015,
      "step": 5790
    },
    {
      "epoch": 1.4124390243902438,
      "grad_norm": 0.08222909271717072,
      "learning_rate": 2.7292495205122265e-05,
      "loss": 0.0185,
      "step": 5791
    },
    {
      "epoch": 1.4126829268292682,
      "grad_norm": 0.06655680388212204,
      "learning_rate": 2.7286136165911953e-05,
      "loss": 0.0082,
      "step": 5792
    },
    {
      "epoch": 1.4129268292682926,
      "grad_norm": 0.10039068758487701,
      "learning_rate": 2.727977697753825e-05,
      "loss": 0.0183,
      "step": 5793
    },
    {
      "epoch": 1.413170731707317,
      "grad_norm": 0.07448622584342957,
      "learning_rate": 2.727341764041607e-05,
      "loss": 0.0209,
      "step": 5794
    },
    {
      "epoch": 1.4134146341463414,
      "grad_norm": 0.09134410321712494,
      "learning_rate": 2.7267058154960358e-05,
      "loss": 0.0215,
      "step": 5795
    },
    {
      "epoch": 1.4136585365853658,
      "grad_norm": 0.10891121625900269,
      "learning_rate": 2.7260698521586036e-05,
      "loss": 0.0263,
      "step": 5796
    },
    {
      "epoch": 1.4139024390243902,
      "grad_norm": 0.061143528670072556,
      "learning_rate": 2.7254338740708057e-05,
      "loss": 0.0106,
      "step": 5797
    },
    {
      "epoch": 1.4141463414634146,
      "grad_norm": 0.12340301275253296,
      "learning_rate": 2.7247978812741375e-05,
      "loss": 0.0247,
      "step": 5798
    },
    {
      "epoch": 1.414390243902439,
      "grad_norm": 0.07963238656520844,
      "learning_rate": 2.7241618738100956e-05,
      "loss": 0.0132,
      "step": 5799
    },
    {
      "epoch": 1.4146341463414633,
      "grad_norm": 0.07620111852884293,
      "learning_rate": 2.7235258517201785e-05,
      "loss": 0.0169,
      "step": 5800
    },
    {
      "epoch": 1.4148780487804877,
      "grad_norm": 0.2883957624435425,
      "learning_rate": 2.7228898150458836e-05,
      "loss": 0.0391,
      "step": 5801
    },
    {
      "epoch": 1.4151219512195121,
      "grad_norm": 0.1910068839788437,
      "learning_rate": 2.7222537638287105e-05,
      "loss": 0.0249,
      "step": 5802
    },
    {
      "epoch": 1.4153658536585365,
      "grad_norm": 0.07073374092578888,
      "learning_rate": 2.7216176981101586e-05,
      "loss": 0.0262,
      "step": 5803
    },
    {
      "epoch": 1.415609756097561,
      "grad_norm": 0.1548132747411728,
      "learning_rate": 2.7209816179317314e-05,
      "loss": 0.0234,
      "step": 5804
    },
    {
      "epoch": 1.4158536585365853,
      "grad_norm": 0.11920583248138428,
      "learning_rate": 2.7203455233349308e-05,
      "loss": 0.0163,
      "step": 5805
    },
    {
      "epoch": 1.4160975609756097,
      "grad_norm": 0.08453844487667084,
      "learning_rate": 2.7197094143612577e-05,
      "loss": 0.0259,
      "step": 5806
    },
    {
      "epoch": 1.416341463414634,
      "grad_norm": 0.17615360021591187,
      "learning_rate": 2.7190732910522188e-05,
      "loss": 0.0202,
      "step": 5807
    },
    {
      "epoch": 1.4165853658536585,
      "grad_norm": 0.2540341019630432,
      "learning_rate": 2.7184371534493186e-05,
      "loss": 0.0219,
      "step": 5808
    },
    {
      "epoch": 1.4168292682926829,
      "grad_norm": 0.15462267398834229,
      "learning_rate": 2.7178010015940625e-05,
      "loss": 0.0284,
      "step": 5809
    },
    {
      "epoch": 1.4170731707317072,
      "grad_norm": 0.3052791953086853,
      "learning_rate": 2.7171648355279578e-05,
      "loss": 0.0315,
      "step": 5810
    },
    {
      "epoch": 1.4173170731707316,
      "grad_norm": 0.1695147156715393,
      "learning_rate": 2.7165286552925124e-05,
      "loss": 0.0161,
      "step": 5811
    },
    {
      "epoch": 1.417560975609756,
      "grad_norm": 0.09328468143939972,
      "learning_rate": 2.7158924609292348e-05,
      "loss": 0.0124,
      "step": 5812
    },
    {
      "epoch": 1.4178048780487804,
      "grad_norm": 0.08692183345556259,
      "learning_rate": 2.7152562524796353e-05,
      "loss": 0.0117,
      "step": 5813
    },
    {
      "epoch": 1.4180487804878048,
      "grad_norm": 0.09695650637149811,
      "learning_rate": 2.7146200299852243e-05,
      "loss": 0.0192,
      "step": 5814
    },
    {
      "epoch": 1.4182926829268292,
      "grad_norm": 0.10911890864372253,
      "learning_rate": 2.7139837934875128e-05,
      "loss": 0.0215,
      "step": 5815
    },
    {
      "epoch": 1.4185365853658536,
      "grad_norm": 0.1389765441417694,
      "learning_rate": 2.713347543028015e-05,
      "loss": 0.0233,
      "step": 5816
    },
    {
      "epoch": 1.418780487804878,
      "grad_norm": 0.26860952377319336,
      "learning_rate": 2.7127112786482424e-05,
      "loss": 0.0234,
      "step": 5817
    },
    {
      "epoch": 1.4190243902439024,
      "grad_norm": 0.12513130903244019,
      "learning_rate": 2.7120750003897107e-05,
      "loss": 0.0159,
      "step": 5818
    },
    {
      "epoch": 1.4192682926829268,
      "grad_norm": 0.09836982190608978,
      "learning_rate": 2.7114387082939335e-05,
      "loss": 0.0074,
      "step": 5819
    },
    {
      "epoch": 1.4195121951219511,
      "grad_norm": 0.09817361831665039,
      "learning_rate": 2.710802402402428e-05,
      "loss": 0.0209,
      "step": 5820
    },
    {
      "epoch": 1.4197560975609755,
      "grad_norm": 0.1483653336763382,
      "learning_rate": 2.710166082756711e-05,
      "loss": 0.0251,
      "step": 5821
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.297211229801178,
      "learning_rate": 2.7095297493983014e-05,
      "loss": 0.0206,
      "step": 5822
    },
    {
      "epoch": 1.4202439024390243,
      "grad_norm": 0.19877080619335175,
      "learning_rate": 2.708893402368716e-05,
      "loss": 0.0229,
      "step": 5823
    },
    {
      "epoch": 1.4204878048780487,
      "grad_norm": 0.17344041168689728,
      "learning_rate": 2.708257041709476e-05,
      "loss": 0.0262,
      "step": 5824
    },
    {
      "epoch": 1.420731707317073,
      "grad_norm": 0.17152522504329681,
      "learning_rate": 2.7076206674621024e-05,
      "loss": 0.0281,
      "step": 5825
    },
    {
      "epoch": 1.4209756097560975,
      "grad_norm": 0.07610811293125153,
      "learning_rate": 2.706984279668115e-05,
      "loss": 0.0138,
      "step": 5826
    },
    {
      "epoch": 1.4212195121951219,
      "grad_norm": 0.11634852737188339,
      "learning_rate": 2.7063478783690372e-05,
      "loss": 0.021,
      "step": 5827
    },
    {
      "epoch": 1.4214634146341463,
      "grad_norm": 0.09294183552265167,
      "learning_rate": 2.705711463606392e-05,
      "loss": 0.0141,
      "step": 5828
    },
    {
      "epoch": 1.4217073170731707,
      "grad_norm": 0.10891544818878174,
      "learning_rate": 2.705075035421703e-05,
      "loss": 0.0126,
      "step": 5829
    },
    {
      "epoch": 1.421951219512195,
      "grad_norm": 0.09544039517641068,
      "learning_rate": 2.7044385938564964e-05,
      "loss": 0.0181,
      "step": 5830
    },
    {
      "epoch": 1.4221951219512194,
      "grad_norm": 0.11474829912185669,
      "learning_rate": 2.7038021389522973e-05,
      "loss": 0.0228,
      "step": 5831
    },
    {
      "epoch": 1.4224390243902438,
      "grad_norm": 0.1395576149225235,
      "learning_rate": 2.703165670750632e-05,
      "loss": 0.0179,
      "step": 5832
    },
    {
      "epoch": 1.4226829268292682,
      "grad_norm": 0.11327466368675232,
      "learning_rate": 2.7025291892930294e-05,
      "loss": 0.0312,
      "step": 5833
    },
    {
      "epoch": 1.4229268292682926,
      "grad_norm": 0.1717725545167923,
      "learning_rate": 2.7018926946210166e-05,
      "loss": 0.0136,
      "step": 5834
    },
    {
      "epoch": 1.423170731707317,
      "grad_norm": 0.07840884476900101,
      "learning_rate": 2.701256186776124e-05,
      "loss": 0.0112,
      "step": 5835
    },
    {
      "epoch": 1.4234146341463414,
      "grad_norm": 0.13993200659751892,
      "learning_rate": 2.700619665799881e-05,
      "loss": 0.0103,
      "step": 5836
    },
    {
      "epoch": 1.4236585365853658,
      "grad_norm": 0.2615208327770233,
      "learning_rate": 2.6999831317338187e-05,
      "loss": 0.0147,
      "step": 5837
    },
    {
      "epoch": 1.4239024390243902,
      "grad_norm": 0.17320913076400757,
      "learning_rate": 2.69934658461947e-05,
      "loss": 0.0396,
      "step": 5838
    },
    {
      "epoch": 1.4241463414634146,
      "grad_norm": 0.0708196759223938,
      "learning_rate": 2.6987100244983664e-05,
      "loss": 0.0279,
      "step": 5839
    },
    {
      "epoch": 1.424390243902439,
      "grad_norm": 0.09961938112974167,
      "learning_rate": 2.698073451412042e-05,
      "loss": 0.0258,
      "step": 5840
    },
    {
      "epoch": 1.4246341463414633,
      "grad_norm": 0.08080706745386124,
      "learning_rate": 2.6974368654020315e-05,
      "loss": 0.0121,
      "step": 5841
    },
    {
      "epoch": 1.4248780487804877,
      "grad_norm": 0.055831484496593475,
      "learning_rate": 2.6968002665098695e-05,
      "loss": 0.0061,
      "step": 5842
    },
    {
      "epoch": 1.4251219512195121,
      "grad_norm": 0.0938933938741684,
      "learning_rate": 2.6961636547770936e-05,
      "loss": 0.0196,
      "step": 5843
    },
    {
      "epoch": 1.4253658536585365,
      "grad_norm": 0.05765804275870323,
      "learning_rate": 2.695527030245239e-05,
      "loss": 0.0166,
      "step": 5844
    },
    {
      "epoch": 1.425609756097561,
      "grad_norm": 0.08675134927034378,
      "learning_rate": 2.694890392955844e-05,
      "loss": 0.0153,
      "step": 5845
    },
    {
      "epoch": 1.4258536585365853,
      "grad_norm": 0.08980315923690796,
      "learning_rate": 2.694253742950447e-05,
      "loss": 0.0173,
      "step": 5846
    },
    {
      "epoch": 1.4260975609756097,
      "grad_norm": 0.16959451138973236,
      "learning_rate": 2.6936170802705896e-05,
      "loss": 0.0358,
      "step": 5847
    },
    {
      "epoch": 1.426341463414634,
      "grad_norm": 0.10488540679216385,
      "learning_rate": 2.6929804049578094e-05,
      "loss": 0.0122,
      "step": 5848
    },
    {
      "epoch": 1.4265853658536585,
      "grad_norm": 0.09560171514749527,
      "learning_rate": 2.6923437170536482e-05,
      "loss": 0.0107,
      "step": 5849
    },
    {
      "epoch": 1.4268292682926829,
      "grad_norm": 0.11034619808197021,
      "learning_rate": 2.6917070165996494e-05,
      "loss": 0.0116,
      "step": 5850
    },
    {
      "epoch": 1.4270731707317073,
      "grad_norm": 0.08596529811620712,
      "learning_rate": 2.691070303637354e-05,
      "loss": 0.0182,
      "step": 5851
    },
    {
      "epoch": 1.4273170731707316,
      "grad_norm": 0.12751168012619019,
      "learning_rate": 2.6904335782083062e-05,
      "loss": 0.0212,
      "step": 5852
    },
    {
      "epoch": 1.427560975609756,
      "grad_norm": 0.18315915763378143,
      "learning_rate": 2.68979684035405e-05,
      "loss": 0.0165,
      "step": 5853
    },
    {
      "epoch": 1.4278048780487804,
      "grad_norm": 0.11976510286331177,
      "learning_rate": 2.6891600901161312e-05,
      "loss": 0.0184,
      "step": 5854
    },
    {
      "epoch": 1.4280487804878048,
      "grad_norm": 0.23358291387557983,
      "learning_rate": 2.6885233275360967e-05,
      "loss": 0.0257,
      "step": 5855
    },
    {
      "epoch": 1.4282926829268292,
      "grad_norm": 0.11490978300571442,
      "learning_rate": 2.687886552655491e-05,
      "loss": 0.0185,
      "step": 5856
    },
    {
      "epoch": 1.4285365853658536,
      "grad_norm": 0.3160244822502136,
      "learning_rate": 2.687249765515863e-05,
      "loss": 0.0121,
      "step": 5857
    },
    {
      "epoch": 1.428780487804878,
      "grad_norm": 0.13825048506259918,
      "learning_rate": 2.6866129661587612e-05,
      "loss": 0.0194,
      "step": 5858
    },
    {
      "epoch": 1.4290243902439024,
      "grad_norm": 0.13434959948062897,
      "learning_rate": 2.6859761546257345e-05,
      "loss": 0.0136,
      "step": 5859
    },
    {
      "epoch": 1.4292682926829268,
      "grad_norm": 0.11783910542726517,
      "learning_rate": 2.6853393309583336e-05,
      "loss": 0.0271,
      "step": 5860
    },
    {
      "epoch": 1.4295121951219512,
      "grad_norm": 0.06604039669036865,
      "learning_rate": 2.6847024951981083e-05,
      "loss": 0.0136,
      "step": 5861
    },
    {
      "epoch": 1.4297560975609755,
      "grad_norm": 0.1708783656358719,
      "learning_rate": 2.6840656473866106e-05,
      "loss": 0.0261,
      "step": 5862
    },
    {
      "epoch": 1.43,
      "grad_norm": 0.0924144983291626,
      "learning_rate": 2.6834287875653926e-05,
      "loss": 0.0171,
      "step": 5863
    },
    {
      "epoch": 1.4302439024390243,
      "grad_norm": 0.10683030635118484,
      "learning_rate": 2.6827919157760083e-05,
      "loss": 0.0201,
      "step": 5864
    },
    {
      "epoch": 1.4304878048780487,
      "grad_norm": 0.1108463779091835,
      "learning_rate": 2.6821550320600108e-05,
      "loss": 0.0169,
      "step": 5865
    },
    {
      "epoch": 1.430731707317073,
      "grad_norm": 0.0728357806801796,
      "learning_rate": 2.6815181364589552e-05,
      "loss": 0.0153,
      "step": 5866
    },
    {
      "epoch": 1.4309756097560975,
      "grad_norm": 0.09516006708145142,
      "learning_rate": 2.680881229014397e-05,
      "loss": 0.0166,
      "step": 5867
    },
    {
      "epoch": 1.431219512195122,
      "grad_norm": 0.1872127801179886,
      "learning_rate": 2.6802443097678925e-05,
      "loss": 0.0201,
      "step": 5868
    },
    {
      "epoch": 1.4314634146341463,
      "grad_norm": 0.19877468049526215,
      "learning_rate": 2.6796073787609983e-05,
      "loss": 0.0091,
      "step": 5869
    },
    {
      "epoch": 1.4317073170731707,
      "grad_norm": 0.04778488352894783,
      "learning_rate": 2.6789704360352718e-05,
      "loss": 0.0148,
      "step": 5870
    },
    {
      "epoch": 1.431951219512195,
      "grad_norm": 0.10757806152105331,
      "learning_rate": 2.678333481632273e-05,
      "loss": 0.0226,
      "step": 5871
    },
    {
      "epoch": 1.4321951219512195,
      "grad_norm": 0.1277492493391037,
      "learning_rate": 2.6776965155935607e-05,
      "loss": 0.0207,
      "step": 5872
    },
    {
      "epoch": 1.4324390243902438,
      "grad_norm": 0.17973022162914276,
      "learning_rate": 2.677059537960694e-05,
      "loss": 0.0295,
      "step": 5873
    },
    {
      "epoch": 1.4326829268292682,
      "grad_norm": 0.14492562413215637,
      "learning_rate": 2.6764225487752348e-05,
      "loss": 0.0162,
      "step": 5874
    },
    {
      "epoch": 1.4329268292682926,
      "grad_norm": 0.24340665340423584,
      "learning_rate": 2.675785548078744e-05,
      "loss": 0.0159,
      "step": 5875
    },
    {
      "epoch": 1.433170731707317,
      "grad_norm": 0.3198910355567932,
      "learning_rate": 2.6751485359127846e-05,
      "loss": 0.0215,
      "step": 5876
    },
    {
      "epoch": 1.4334146341463414,
      "grad_norm": 0.08715125173330307,
      "learning_rate": 2.6745115123189202e-05,
      "loss": 0.0099,
      "step": 5877
    },
    {
      "epoch": 1.4336585365853658,
      "grad_norm": 0.1746043562889099,
      "learning_rate": 2.673874477338713e-05,
      "loss": 0.0262,
      "step": 5878
    },
    {
      "epoch": 1.4339024390243902,
      "grad_norm": 0.09409739822149277,
      "learning_rate": 2.6732374310137282e-05,
      "loss": 0.026,
      "step": 5879
    },
    {
      "epoch": 1.4341463414634146,
      "grad_norm": 0.07808958739042282,
      "learning_rate": 2.6726003733855315e-05,
      "loss": 0.0247,
      "step": 5880
    },
    {
      "epoch": 1.434390243902439,
      "grad_norm": 0.12404254823923111,
      "learning_rate": 2.6719633044956898e-05,
      "loss": 0.021,
      "step": 5881
    },
    {
      "epoch": 1.4346341463414634,
      "grad_norm": 0.11897122859954834,
      "learning_rate": 2.671326224385768e-05,
      "loss": 0.0299,
      "step": 5882
    },
    {
      "epoch": 1.4348780487804877,
      "grad_norm": 0.14696411788463593,
      "learning_rate": 2.670689133097335e-05,
      "loss": 0.0222,
      "step": 5883
    },
    {
      "epoch": 1.4351219512195121,
      "grad_norm": 0.0667501762509346,
      "learning_rate": 2.670052030671958e-05,
      "loss": 0.0237,
      "step": 5884
    },
    {
      "epoch": 1.4353658536585365,
      "grad_norm": 0.19811472296714783,
      "learning_rate": 2.6694149171512073e-05,
      "loss": 0.0276,
      "step": 5885
    },
    {
      "epoch": 1.435609756097561,
      "grad_norm": 0.08061204105615616,
      "learning_rate": 2.6687777925766512e-05,
      "loss": 0.0185,
      "step": 5886
    },
    {
      "epoch": 1.4358536585365853,
      "grad_norm": 0.125817209482193,
      "learning_rate": 2.668140656989861e-05,
      "loss": 0.0204,
      "step": 5887
    },
    {
      "epoch": 1.4360975609756097,
      "grad_norm": 0.13485978543758392,
      "learning_rate": 2.6675035104324075e-05,
      "loss": 0.0248,
      "step": 5888
    },
    {
      "epoch": 1.436341463414634,
      "grad_norm": 0.18036580085754395,
      "learning_rate": 2.666866352945863e-05,
      "loss": 0.0252,
      "step": 5889
    },
    {
      "epoch": 1.4365853658536585,
      "grad_norm": 0.37830209732055664,
      "learning_rate": 2.6662291845718002e-05,
      "loss": 0.0243,
      "step": 5890
    },
    {
      "epoch": 1.4368292682926829,
      "grad_norm": 0.059883106499910355,
      "learning_rate": 2.6655920053517918e-05,
      "loss": 0.0186,
      "step": 5891
    },
    {
      "epoch": 1.4370731707317073,
      "grad_norm": 0.11561872810125351,
      "learning_rate": 2.6649548153274113e-05,
      "loss": 0.0078,
      "step": 5892
    },
    {
      "epoch": 1.4373170731707317,
      "grad_norm": 0.15298613905906677,
      "learning_rate": 2.6643176145402344e-05,
      "loss": 0.0218,
      "step": 5893
    },
    {
      "epoch": 1.437560975609756,
      "grad_norm": 0.1464536190032959,
      "learning_rate": 2.663680403031837e-05,
      "loss": 0.025,
      "step": 5894
    },
    {
      "epoch": 1.4378048780487804,
      "grad_norm": 0.11845390498638153,
      "learning_rate": 2.6630431808437935e-05,
      "loss": 0.0284,
      "step": 5895
    },
    {
      "epoch": 1.4380487804878048,
      "grad_norm": 0.12801609933376312,
      "learning_rate": 2.662405948017681e-05,
      "loss": 0.0307,
      "step": 5896
    },
    {
      "epoch": 1.4382926829268292,
      "grad_norm": 0.1121554970741272,
      "learning_rate": 2.661768704595079e-05,
      "loss": 0.016,
      "step": 5897
    },
    {
      "epoch": 1.4385365853658536,
      "grad_norm": 0.14488427340984344,
      "learning_rate": 2.6611314506175633e-05,
      "loss": 0.022,
      "step": 5898
    },
    {
      "epoch": 1.438780487804878,
      "grad_norm": 0.1361103504896164,
      "learning_rate": 2.660494186126714e-05,
      "loss": 0.0221,
      "step": 5899
    },
    {
      "epoch": 1.4390243902439024,
      "grad_norm": 0.28729248046875,
      "learning_rate": 2.6598569111641102e-05,
      "loss": 0.0295,
      "step": 5900
    },
    {
      "epoch": 1.4392682926829268,
      "grad_norm": 0.18349702656269073,
      "learning_rate": 2.6592196257713327e-05,
      "loss": 0.0226,
      "step": 5901
    },
    {
      "epoch": 1.4395121951219512,
      "grad_norm": 0.11808204650878906,
      "learning_rate": 2.658582329989962e-05,
      "loss": 0.0337,
      "step": 5902
    },
    {
      "epoch": 1.4397560975609756,
      "grad_norm": 0.15349628031253815,
      "learning_rate": 2.657945023861579e-05,
      "loss": 0.0199,
      "step": 5903
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.11894796788692474,
      "learning_rate": 2.657307707427767e-05,
      "loss": 0.0193,
      "step": 5904
    },
    {
      "epoch": 1.4402439024390243,
      "grad_norm": 0.18520322442054749,
      "learning_rate": 2.656670380730109e-05,
      "loss": 0.0265,
      "step": 5905
    },
    {
      "epoch": 1.4404878048780487,
      "grad_norm": 0.05508715659379959,
      "learning_rate": 2.6560330438101882e-05,
      "loss": 0.013,
      "step": 5906
    },
    {
      "epoch": 1.4407317073170731,
      "grad_norm": 0.15978574752807617,
      "learning_rate": 2.6553956967095893e-05,
      "loss": 0.0178,
      "step": 5907
    },
    {
      "epoch": 1.4409756097560975,
      "grad_norm": 0.07286656647920609,
      "learning_rate": 2.6547583394698966e-05,
      "loss": 0.0088,
      "step": 5908
    },
    {
      "epoch": 1.441219512195122,
      "grad_norm": 0.25712308287620544,
      "learning_rate": 2.6541209721326953e-05,
      "loss": 0.0208,
      "step": 5909
    },
    {
      "epoch": 1.4414634146341463,
      "grad_norm": 0.11459727585315704,
      "learning_rate": 2.6534835947395738e-05,
      "loss": 0.0237,
      "step": 5910
    },
    {
      "epoch": 1.4417073170731707,
      "grad_norm": 0.13732004165649414,
      "learning_rate": 2.6528462073321165e-05,
      "loss": 0.0201,
      "step": 5911
    },
    {
      "epoch": 1.441951219512195,
      "grad_norm": 0.11119216680526733,
      "learning_rate": 2.6522088099519123e-05,
      "loss": 0.0139,
      "step": 5912
    },
    {
      "epoch": 1.4421951219512195,
      "grad_norm": 0.15954387187957764,
      "learning_rate": 2.6515714026405492e-05,
      "loss": 0.0171,
      "step": 5913
    },
    {
      "epoch": 1.4424390243902439,
      "grad_norm": 0.10208078473806381,
      "learning_rate": 2.6509339854396164e-05,
      "loss": 0.0182,
      "step": 5914
    },
    {
      "epoch": 1.4426829268292682,
      "grad_norm": 0.18023140728473663,
      "learning_rate": 2.6502965583907025e-05,
      "loss": 0.0188,
      "step": 5915
    },
    {
      "epoch": 1.4429268292682926,
      "grad_norm": 0.13942410051822662,
      "learning_rate": 2.649659121535399e-05,
      "loss": 0.029,
      "step": 5916
    },
    {
      "epoch": 1.443170731707317,
      "grad_norm": 0.07856997847557068,
      "learning_rate": 2.649021674915295e-05,
      "loss": 0.0208,
      "step": 5917
    },
    {
      "epoch": 1.4434146341463414,
      "grad_norm": 0.06709062308073044,
      "learning_rate": 2.6483842185719838e-05,
      "loss": 0.0187,
      "step": 5918
    },
    {
      "epoch": 1.4436585365853658,
      "grad_norm": 0.1534651815891266,
      "learning_rate": 2.647746752547057e-05,
      "loss": 0.014,
      "step": 5919
    },
    {
      "epoch": 1.4439024390243902,
      "grad_norm": 0.09025733917951584,
      "learning_rate": 2.6471092768821055e-05,
      "loss": 0.0183,
      "step": 5920
    },
    {
      "epoch": 1.4441463414634146,
      "grad_norm": 0.08767637610435486,
      "learning_rate": 2.646471791618725e-05,
      "loss": 0.012,
      "step": 5921
    },
    {
      "epoch": 1.444390243902439,
      "grad_norm": 0.11564010381698608,
      "learning_rate": 2.6458342967985077e-05,
      "loss": 0.0253,
      "step": 5922
    },
    {
      "epoch": 1.4446341463414634,
      "grad_norm": 0.10206709802150726,
      "learning_rate": 2.64519679246305e-05,
      "loss": 0.0173,
      "step": 5923
    },
    {
      "epoch": 1.4448780487804878,
      "grad_norm": 0.07583364844322205,
      "learning_rate": 2.6445592786539458e-05,
      "loss": 0.0127,
      "step": 5924
    },
    {
      "epoch": 1.4451219512195121,
      "grad_norm": 0.258512020111084,
      "learning_rate": 2.643921755412791e-05,
      "loss": 0.0219,
      "step": 5925
    },
    {
      "epoch": 1.4453658536585365,
      "grad_norm": 0.13843539357185364,
      "learning_rate": 2.6432842227811815e-05,
      "loss": 0.0232,
      "step": 5926
    },
    {
      "epoch": 1.445609756097561,
      "grad_norm": 0.1344175934791565,
      "learning_rate": 2.6426466808007167e-05,
      "loss": 0.0282,
      "step": 5927
    },
    {
      "epoch": 1.4458536585365853,
      "grad_norm": 0.22701826691627502,
      "learning_rate": 2.642009129512991e-05,
      "loss": 0.0204,
      "step": 5928
    },
    {
      "epoch": 1.4460975609756097,
      "grad_norm": 0.09851806610822678,
      "learning_rate": 2.6413715689596053e-05,
      "loss": 0.0211,
      "step": 5929
    },
    {
      "epoch": 1.446341463414634,
      "grad_norm": 0.17087572813034058,
      "learning_rate": 2.640733999182157e-05,
      "loss": 0.0278,
      "step": 5930
    },
    {
      "epoch": 1.4465853658536585,
      "grad_norm": 0.06346987932920456,
      "learning_rate": 2.6400964202222467e-05,
      "loss": 0.0206,
      "step": 5931
    },
    {
      "epoch": 1.4468292682926829,
      "grad_norm": 0.0898526981472969,
      "learning_rate": 2.639458832121474e-05,
      "loss": 0.013,
      "step": 5932
    },
    {
      "epoch": 1.4470731707317073,
      "grad_norm": 0.1777738481760025,
      "learning_rate": 2.6388212349214393e-05,
      "loss": 0.0343,
      "step": 5933
    },
    {
      "epoch": 1.4473170731707317,
      "grad_norm": 0.13033242523670197,
      "learning_rate": 2.638183628663743e-05,
      "loss": 0.0225,
      "step": 5934
    },
    {
      "epoch": 1.447560975609756,
      "grad_norm": 0.11694902926683426,
      "learning_rate": 2.637546013389989e-05,
      "loss": 0.0137,
      "step": 5935
    },
    {
      "epoch": 1.4478048780487804,
      "grad_norm": 0.1379072070121765,
      "learning_rate": 2.6369083891417784e-05,
      "loss": 0.018,
      "step": 5936
    },
    {
      "epoch": 1.4480487804878048,
      "grad_norm": 0.09080546349287033,
      "learning_rate": 2.6362707559607143e-05,
      "loss": 0.0126,
      "step": 5937
    },
    {
      "epoch": 1.4482926829268292,
      "grad_norm": 0.11884491890668869,
      "learning_rate": 2.6356331138884006e-05,
      "loss": 0.0261,
      "step": 5938
    },
    {
      "epoch": 1.4485365853658536,
      "grad_norm": 0.11275531351566315,
      "learning_rate": 2.634995462966441e-05,
      "loss": 0.0389,
      "step": 5939
    },
    {
      "epoch": 1.448780487804878,
      "grad_norm": 0.15781503915786743,
      "learning_rate": 2.634357803236442e-05,
      "loss": 0.0224,
      "step": 5940
    },
    {
      "epoch": 1.4490243902439024,
      "grad_norm": 0.16171486675739288,
      "learning_rate": 2.633720134740007e-05,
      "loss": 0.0304,
      "step": 5941
    },
    {
      "epoch": 1.4492682926829268,
      "grad_norm": 0.2760430872440338,
      "learning_rate": 2.6330824575187417e-05,
      "loss": 0.0235,
      "step": 5942
    },
    {
      "epoch": 1.4495121951219512,
      "grad_norm": 0.09675378352403641,
      "learning_rate": 2.6324447716142542e-05,
      "loss": 0.0191,
      "step": 5943
    },
    {
      "epoch": 1.4497560975609756,
      "grad_norm": 0.10567585378885269,
      "learning_rate": 2.6318070770681512e-05,
      "loss": 0.0201,
      "step": 5944
    },
    {
      "epoch": 1.45,
      "grad_norm": 0.10313203185796738,
      "learning_rate": 2.6311693739220393e-05,
      "loss": 0.0198,
      "step": 5945
    },
    {
      "epoch": 1.4502439024390243,
      "grad_norm": 0.10246061533689499,
      "learning_rate": 2.6305316622175276e-05,
      "loss": 0.015,
      "step": 5946
    },
    {
      "epoch": 1.4504878048780487,
      "grad_norm": 0.10644600540399551,
      "learning_rate": 2.6298939419962242e-05,
      "loss": 0.0124,
      "step": 5947
    },
    {
      "epoch": 1.4507317073170731,
      "grad_norm": 0.08588526397943497,
      "learning_rate": 2.6292562132997396e-05,
      "loss": 0.0143,
      "step": 5948
    },
    {
      "epoch": 1.4509756097560975,
      "grad_norm": 0.12041959911584854,
      "learning_rate": 2.628618476169682e-05,
      "loss": 0.0208,
      "step": 5949
    },
    {
      "epoch": 1.451219512195122,
      "grad_norm": 0.10374017059803009,
      "learning_rate": 2.6279807306476627e-05,
      "loss": 0.0185,
      "step": 5950
    },
    {
      "epoch": 1.4514634146341463,
      "grad_norm": 0.15482662618160248,
      "learning_rate": 2.627342976775292e-05,
      "loss": 0.0128,
      "step": 5951
    },
    {
      "epoch": 1.4517073170731707,
      "grad_norm": 0.2590388357639313,
      "learning_rate": 2.626705214594183e-05,
      "loss": 0.0339,
      "step": 5952
    },
    {
      "epoch": 1.451951219512195,
      "grad_norm": 0.09265449643135071,
      "learning_rate": 2.626067444145946e-05,
      "loss": 0.011,
      "step": 5953
    },
    {
      "epoch": 1.4521951219512195,
      "grad_norm": 0.2673344910144806,
      "learning_rate": 2.6254296654721943e-05,
      "loss": 0.0267,
      "step": 5954
    },
    {
      "epoch": 1.4524390243902439,
      "grad_norm": 0.2366141378879547,
      "learning_rate": 2.6247918786145403e-05,
      "loss": 0.034,
      "step": 5955
    },
    {
      "epoch": 1.4526829268292683,
      "grad_norm": 0.08947451412677765,
      "learning_rate": 2.6241540836145994e-05,
      "loss": 0.0203,
      "step": 5956
    },
    {
      "epoch": 1.4529268292682926,
      "grad_norm": 0.2305874228477478,
      "learning_rate": 2.6235162805139845e-05,
      "loss": 0.0275,
      "step": 5957
    },
    {
      "epoch": 1.453170731707317,
      "grad_norm": 0.11326231807470322,
      "learning_rate": 2.6228784693543094e-05,
      "loss": 0.0299,
      "step": 5958
    },
    {
      "epoch": 1.4534146341463414,
      "grad_norm": 0.22268618643283844,
      "learning_rate": 2.6222406501771908e-05,
      "loss": 0.0215,
      "step": 5959
    },
    {
      "epoch": 1.4536585365853658,
      "grad_norm": 0.15222381055355072,
      "learning_rate": 2.6216028230242438e-05,
      "loss": 0.0169,
      "step": 5960
    },
    {
      "epoch": 1.4539024390243902,
      "grad_norm": 0.07437246292829514,
      "learning_rate": 2.6209649879370857e-05,
      "loss": 0.0192,
      "step": 5961
    },
    {
      "epoch": 1.4541463414634146,
      "grad_norm": 0.19501391053199768,
      "learning_rate": 2.620327144957332e-05,
      "loss": 0.0258,
      "step": 5962
    },
    {
      "epoch": 1.454390243902439,
      "grad_norm": 0.119292251765728,
      "learning_rate": 2.6196892941265998e-05,
      "loss": 0.0216,
      "step": 5963
    },
    {
      "epoch": 1.4546341463414634,
      "grad_norm": 0.10476584732532501,
      "learning_rate": 2.619051435486508e-05,
      "loss": 0.0147,
      "step": 5964
    },
    {
      "epoch": 1.4548780487804878,
      "grad_norm": 0.11440747231245041,
      "learning_rate": 2.6184135690786747e-05,
      "loss": 0.0245,
      "step": 5965
    },
    {
      "epoch": 1.4551219512195122,
      "grad_norm": 0.11981531977653503,
      "learning_rate": 2.617775694944719e-05,
      "loss": 0.0226,
      "step": 5966
    },
    {
      "epoch": 1.4553658536585365,
      "grad_norm": 0.1824144870042801,
      "learning_rate": 2.6171378131262586e-05,
      "loss": 0.0329,
      "step": 5967
    },
    {
      "epoch": 1.455609756097561,
      "grad_norm": 0.1701294183731079,
      "learning_rate": 2.6164999236649152e-05,
      "loss": 0.019,
      "step": 5968
    },
    {
      "epoch": 1.4558536585365853,
      "grad_norm": 0.061830785125494,
      "learning_rate": 2.6158620266023086e-05,
      "loss": 0.01,
      "step": 5969
    },
    {
      "epoch": 1.4560975609756097,
      "grad_norm": 0.11304343491792679,
      "learning_rate": 2.615224121980059e-05,
      "loss": 0.0198,
      "step": 5970
    },
    {
      "epoch": 1.456341463414634,
      "grad_norm": 0.2090558558702469,
      "learning_rate": 2.614586209839788e-05,
      "loss": 0.0178,
      "step": 5971
    },
    {
      "epoch": 1.4565853658536585,
      "grad_norm": 0.08784306794404984,
      "learning_rate": 2.6139482902231178e-05,
      "loss": 0.0153,
      "step": 5972
    },
    {
      "epoch": 1.4568292682926829,
      "grad_norm": 0.17495736479759216,
      "learning_rate": 2.6133103631716705e-05,
      "loss": 0.0203,
      "step": 5973
    },
    {
      "epoch": 1.4570731707317073,
      "grad_norm": 0.10615722835063934,
      "learning_rate": 2.6126724287270693e-05,
      "loss": 0.02,
      "step": 5974
    },
    {
      "epoch": 1.4573170731707317,
      "grad_norm": 0.1660795509815216,
      "learning_rate": 2.6120344869309367e-05,
      "loss": 0.0297,
      "step": 5975
    },
    {
      "epoch": 1.457560975609756,
      "grad_norm": 0.056337762624025345,
      "learning_rate": 2.6113965378248968e-05,
      "loss": 0.0072,
      "step": 5976
    },
    {
      "epoch": 1.4578048780487805,
      "grad_norm": 0.09138356894254684,
      "learning_rate": 2.6107585814505736e-05,
      "loss": 0.0218,
      "step": 5977
    },
    {
      "epoch": 1.4580487804878048,
      "grad_norm": 0.08106517791748047,
      "learning_rate": 2.6101206178495924e-05,
      "loss": 0.0167,
      "step": 5978
    },
    {
      "epoch": 1.4582926829268292,
      "grad_norm": 0.07153697311878204,
      "learning_rate": 2.6094826470635775e-05,
      "loss": 0.0083,
      "step": 5979
    },
    {
      "epoch": 1.4585365853658536,
      "grad_norm": 0.14607171714305878,
      "learning_rate": 2.6088446691341558e-05,
      "loss": 0.038,
      "step": 5980
    },
    {
      "epoch": 1.458780487804878,
      "grad_norm": 0.12001007050275803,
      "learning_rate": 2.608206684102952e-05,
      "loss": 0.0131,
      "step": 5981
    },
    {
      "epoch": 1.4590243902439024,
      "grad_norm": 0.1720689833164215,
      "learning_rate": 2.607568692011594e-05,
      "loss": 0.0251,
      "step": 5982
    },
    {
      "epoch": 1.4592682926829268,
      "grad_norm": 0.1502038687467575,
      "learning_rate": 2.6069306929017074e-05,
      "loss": 0.0282,
      "step": 5983
    },
    {
      "epoch": 1.4595121951219512,
      "grad_norm": 0.12313132733106613,
      "learning_rate": 2.6062926868149206e-05,
      "loss": 0.0201,
      "step": 5984
    },
    {
      "epoch": 1.4597560975609756,
      "grad_norm": 0.0906822606921196,
      "learning_rate": 2.605654673792861e-05,
      "loss": 0.0183,
      "step": 5985
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.08023625612258911,
      "learning_rate": 2.6050166538771586e-05,
      "loss": 0.0133,
      "step": 5986
    },
    {
      "epoch": 1.4602439024390244,
      "grad_norm": 0.760686993598938,
      "learning_rate": 2.60437862710944e-05,
      "loss": 0.0308,
      "step": 5987
    },
    {
      "epoch": 1.4604878048780487,
      "grad_norm": 0.10433679819107056,
      "learning_rate": 2.603740593531336e-05,
      "loss": 0.0159,
      "step": 5988
    },
    {
      "epoch": 1.4607317073170731,
      "grad_norm": 0.06810130923986435,
      "learning_rate": 2.603102553184475e-05,
      "loss": 0.0116,
      "step": 5989
    },
    {
      "epoch": 1.4609756097560975,
      "grad_norm": 0.1148371696472168,
      "learning_rate": 2.602464506110489e-05,
      "loss": 0.0165,
      "step": 5990
    },
    {
      "epoch": 1.461219512195122,
      "grad_norm": 0.08616239577531815,
      "learning_rate": 2.6018264523510078e-05,
      "loss": 0.022,
      "step": 5991
    },
    {
      "epoch": 1.4614634146341463,
      "grad_norm": 0.12572117149829865,
      "learning_rate": 2.6011883919476617e-05,
      "loss": 0.0384,
      "step": 5992
    },
    {
      "epoch": 1.4617073170731707,
      "grad_norm": 0.18366028368473053,
      "learning_rate": 2.6005503249420827e-05,
      "loss": 0.016,
      "step": 5993
    },
    {
      "epoch": 1.461951219512195,
      "grad_norm": 0.12425302714109421,
      "learning_rate": 2.599912251375903e-05,
      "loss": 0.0154,
      "step": 5994
    },
    {
      "epoch": 1.4621951219512195,
      "grad_norm": 0.282195508480072,
      "learning_rate": 2.5992741712907555e-05,
      "loss": 0.0458,
      "step": 5995
    },
    {
      "epoch": 1.4624390243902439,
      "grad_norm": 0.11226952075958252,
      "learning_rate": 2.598636084728271e-05,
      "loss": 0.0334,
      "step": 5996
    },
    {
      "epoch": 1.4626829268292683,
      "grad_norm": 0.16635288298130035,
      "learning_rate": 2.597997991730084e-05,
      "loss": 0.0102,
      "step": 5997
    },
    {
      "epoch": 1.4629268292682926,
      "grad_norm": 0.23350737988948822,
      "learning_rate": 2.597359892337829e-05,
      "loss": 0.0275,
      "step": 5998
    },
    {
      "epoch": 1.463170731707317,
      "grad_norm": 0.13160136342048645,
      "learning_rate": 2.596721786593139e-05,
      "loss": 0.0286,
      "step": 5999
    },
    {
      "epoch": 1.4634146341463414,
      "grad_norm": 0.13798968493938446,
      "learning_rate": 2.5960836745376477e-05,
      "loss": 0.0274,
      "step": 6000
    },
    {
      "epoch": 1.4636585365853658,
      "grad_norm": 0.11197357624769211,
      "learning_rate": 2.595445556212991e-05,
      "loss": 0.0214,
      "step": 6001
    },
    {
      "epoch": 1.4639024390243902,
      "grad_norm": 0.21030260622501373,
      "learning_rate": 2.594807431660804e-05,
      "loss": 0.0159,
      "step": 6002
    },
    {
      "epoch": 1.4641463414634146,
      "grad_norm": 0.20468486845493317,
      "learning_rate": 2.594169300922723e-05,
      "loss": 0.0257,
      "step": 6003
    },
    {
      "epoch": 1.464390243902439,
      "grad_norm": 0.1872503012418747,
      "learning_rate": 2.5935311640403826e-05,
      "loss": 0.0273,
      "step": 6004
    },
    {
      "epoch": 1.4646341463414634,
      "grad_norm": 0.17293129861354828,
      "learning_rate": 2.5928930210554197e-05,
      "loss": 0.0289,
      "step": 6005
    },
    {
      "epoch": 1.4648780487804878,
      "grad_norm": 0.0898296907544136,
      "learning_rate": 2.5922548720094726e-05,
      "loss": 0.0215,
      "step": 6006
    },
    {
      "epoch": 1.4651219512195122,
      "grad_norm": 0.19330866634845734,
      "learning_rate": 2.591616716944177e-05,
      "loss": 0.0314,
      "step": 6007
    },
    {
      "epoch": 1.4653658536585366,
      "grad_norm": 0.23267604410648346,
      "learning_rate": 2.590978555901172e-05,
      "loss": 0.0153,
      "step": 6008
    },
    {
      "epoch": 1.465609756097561,
      "grad_norm": 0.19131575524806976,
      "learning_rate": 2.5903403889220933e-05,
      "loss": 0.0147,
      "step": 6009
    },
    {
      "epoch": 1.4658536585365853,
      "grad_norm": 0.13380342721939087,
      "learning_rate": 2.589702216048581e-05,
      "loss": 0.0193,
      "step": 6010
    },
    {
      "epoch": 1.4660975609756097,
      "grad_norm": 0.0928187295794487,
      "learning_rate": 2.5890640373222742e-05,
      "loss": 0.0266,
      "step": 6011
    },
    {
      "epoch": 1.4663414634146341,
      "grad_norm": 0.15738540887832642,
      "learning_rate": 2.5884258527848114e-05,
      "loss": 0.0288,
      "step": 6012
    },
    {
      "epoch": 1.4665853658536585,
      "grad_norm": 0.0811302587389946,
      "learning_rate": 2.5877876624778325e-05,
      "loss": 0.0165,
      "step": 6013
    },
    {
      "epoch": 1.466829268292683,
      "grad_norm": 0.10580161213874817,
      "learning_rate": 2.5871494664429772e-05,
      "loss": 0.0168,
      "step": 6014
    },
    {
      "epoch": 1.4670731707317073,
      "grad_norm": 0.21726883947849274,
      "learning_rate": 2.5865112647218865e-05,
      "loss": 0.0353,
      "step": 6015
    },
    {
      "epoch": 1.4673170731707317,
      "grad_norm": 0.13100561499595642,
      "learning_rate": 2.5858730573562006e-05,
      "loss": 0.0238,
      "step": 6016
    },
    {
      "epoch": 1.467560975609756,
      "grad_norm": 0.1037575975060463,
      "learning_rate": 2.5852348443875603e-05,
      "loss": 0.0149,
      "step": 6017
    },
    {
      "epoch": 1.4678048780487805,
      "grad_norm": 0.27352479100227356,
      "learning_rate": 2.5845966258576072e-05,
      "loss": 0.0295,
      "step": 6018
    },
    {
      "epoch": 1.4680487804878048,
      "grad_norm": 0.10721506178379059,
      "learning_rate": 2.5839584018079832e-05,
      "loss": 0.0212,
      "step": 6019
    },
    {
      "epoch": 1.4682926829268292,
      "grad_norm": 0.16186359524726868,
      "learning_rate": 2.583320172280331e-05,
      "loss": 0.028,
      "step": 6020
    },
    {
      "epoch": 1.4685365853658536,
      "grad_norm": 0.22208204865455627,
      "learning_rate": 2.582681937316292e-05,
      "loss": 0.0143,
      "step": 6021
    },
    {
      "epoch": 1.468780487804878,
      "grad_norm": 0.19924528896808624,
      "learning_rate": 2.5820436969575097e-05,
      "loss": 0.0105,
      "step": 6022
    },
    {
      "epoch": 1.4690243902439024,
      "grad_norm": 0.09850607067346573,
      "learning_rate": 2.5814054512456283e-05,
      "loss": 0.0186,
      "step": 6023
    },
    {
      "epoch": 1.4692682926829268,
      "grad_norm": 0.16439096629619598,
      "learning_rate": 2.5807672002222893e-05,
      "loss": 0.0099,
      "step": 6024
    },
    {
      "epoch": 1.4695121951219512,
      "grad_norm": 0.32079291343688965,
      "learning_rate": 2.5801289439291388e-05,
      "loss": 0.0368,
      "step": 6025
    },
    {
      "epoch": 1.4697560975609756,
      "grad_norm": 0.09348804503679276,
      "learning_rate": 2.579490682407819e-05,
      "loss": 0.0159,
      "step": 6026
    },
    {
      "epoch": 1.47,
      "grad_norm": 0.1522436887025833,
      "learning_rate": 2.5788524156999753e-05,
      "loss": 0.025,
      "step": 6027
    },
    {
      "epoch": 1.4702439024390244,
      "grad_norm": 0.12227167934179306,
      "learning_rate": 2.578214143847254e-05,
      "loss": 0.014,
      "step": 6028
    },
    {
      "epoch": 1.4704878048780488,
      "grad_norm": 0.07888403534889221,
      "learning_rate": 2.5775758668912987e-05,
      "loss": 0.0121,
      "step": 6029
    },
    {
      "epoch": 1.4707317073170731,
      "grad_norm": 0.12055902928113937,
      "learning_rate": 2.5769375848737553e-05,
      "loss": 0.028,
      "step": 6030
    },
    {
      "epoch": 1.4709756097560975,
      "grad_norm": 0.09571188688278198,
      "learning_rate": 2.5762992978362704e-05,
      "loss": 0.0137,
      "step": 6031
    },
    {
      "epoch": 1.471219512195122,
      "grad_norm": 0.0827980786561966,
      "learning_rate": 2.57566100582049e-05,
      "loss": 0.0189,
      "step": 6032
    },
    {
      "epoch": 1.4714634146341463,
      "grad_norm": 0.17162421345710754,
      "learning_rate": 2.5750227088680606e-05,
      "loss": 0.0272,
      "step": 6033
    },
    {
      "epoch": 1.4717073170731707,
      "grad_norm": 0.18952558934688568,
      "learning_rate": 2.5743844070206285e-05,
      "loss": 0.0254,
      "step": 6034
    },
    {
      "epoch": 1.471951219512195,
      "grad_norm": 0.14573903381824493,
      "learning_rate": 2.5737461003198416e-05,
      "loss": 0.0204,
      "step": 6035
    },
    {
      "epoch": 1.4721951219512195,
      "grad_norm": 0.07733587920665741,
      "learning_rate": 2.5731077888073478e-05,
      "loss": 0.0097,
      "step": 6036
    },
    {
      "epoch": 1.4724390243902439,
      "grad_norm": 0.1219390407204628,
      "learning_rate": 2.572469472524795e-05,
      "loss": 0.0125,
      "step": 6037
    },
    {
      "epoch": 1.4726829268292683,
      "grad_norm": 0.168738454580307,
      "learning_rate": 2.57183115151383e-05,
      "loss": 0.0193,
      "step": 6038
    },
    {
      "epoch": 1.4729268292682927,
      "grad_norm": 0.1085638627409935,
      "learning_rate": 2.5711928258161026e-05,
      "loss": 0.0219,
      "step": 6039
    },
    {
      "epoch": 1.473170731707317,
      "grad_norm": 0.0957428514957428,
      "learning_rate": 2.5705544954732617e-05,
      "loss": 0.0176,
      "step": 6040
    },
    {
      "epoch": 1.4734146341463414,
      "grad_norm": 0.0751362144947052,
      "learning_rate": 2.569916160526956e-05,
      "loss": 0.0098,
      "step": 6041
    },
    {
      "epoch": 1.4736585365853658,
      "grad_norm": 0.19694846868515015,
      "learning_rate": 2.5692778210188345e-05,
      "loss": 0.0305,
      "step": 6042
    },
    {
      "epoch": 1.4739024390243902,
      "grad_norm": 0.12965033948421478,
      "learning_rate": 2.568639476990547e-05,
      "loss": 0.0177,
      "step": 6043
    },
    {
      "epoch": 1.4741463414634146,
      "grad_norm": 0.08706895262002945,
      "learning_rate": 2.5680011284837442e-05,
      "loss": 0.0166,
      "step": 6044
    },
    {
      "epoch": 1.474390243902439,
      "grad_norm": 0.11298380047082901,
      "learning_rate": 2.567362775540076e-05,
      "loss": 0.0224,
      "step": 6045
    },
    {
      "epoch": 1.4746341463414634,
      "grad_norm": 0.09818757325410843,
      "learning_rate": 2.5667244182011925e-05,
      "loss": 0.0174,
      "step": 6046
    },
    {
      "epoch": 1.4748780487804878,
      "grad_norm": 0.0874166488647461,
      "learning_rate": 2.5660860565087454e-05,
      "loss": 0.014,
      "step": 6047
    },
    {
      "epoch": 1.4751219512195122,
      "grad_norm": 0.135128915309906,
      "learning_rate": 2.565447690504385e-05,
      "loss": 0.0197,
      "step": 6048
    },
    {
      "epoch": 1.4753658536585366,
      "grad_norm": 0.1806308776140213,
      "learning_rate": 2.5648093202297634e-05,
      "loss": 0.0227,
      "step": 6049
    },
    {
      "epoch": 1.475609756097561,
      "grad_norm": 0.11386708915233612,
      "learning_rate": 2.5641709457265322e-05,
      "loss": 0.0207,
      "step": 6050
    },
    {
      "epoch": 1.4758536585365853,
      "grad_norm": 0.08328643441200256,
      "learning_rate": 2.563532567036343e-05,
      "loss": 0.0217,
      "step": 6051
    },
    {
      "epoch": 1.4760975609756097,
      "grad_norm": 0.09570202976465225,
      "learning_rate": 2.562894184200848e-05,
      "loss": 0.0159,
      "step": 6052
    },
    {
      "epoch": 1.4763414634146341,
      "grad_norm": 0.09421445429325104,
      "learning_rate": 2.5622557972617e-05,
      "loss": 0.02,
      "step": 6053
    },
    {
      "epoch": 1.4765853658536585,
      "grad_norm": 0.10063004493713379,
      "learning_rate": 2.5616174062605526e-05,
      "loss": 0.0076,
      "step": 6054
    },
    {
      "epoch": 1.476829268292683,
      "grad_norm": 0.10007382184267044,
      "learning_rate": 2.5609790112390575e-05,
      "loss": 0.0225,
      "step": 6055
    },
    {
      "epoch": 1.4770731707317073,
      "grad_norm": 0.11029919981956482,
      "learning_rate": 2.560340612238869e-05,
      "loss": 0.0246,
      "step": 6056
    },
    {
      "epoch": 1.4773170731707317,
      "grad_norm": 0.18075530230998993,
      "learning_rate": 2.5597022093016397e-05,
      "loss": 0.0278,
      "step": 6057
    },
    {
      "epoch": 1.477560975609756,
      "grad_norm": 0.1113339439034462,
      "learning_rate": 2.559063802469025e-05,
      "loss": 0.013,
      "step": 6058
    },
    {
      "epoch": 1.4778048780487805,
      "grad_norm": 0.12543678283691406,
      "learning_rate": 2.558425391782677e-05,
      "loss": 0.03,
      "step": 6059
    },
    {
      "epoch": 1.4780487804878049,
      "grad_norm": 0.08751935511827469,
      "learning_rate": 2.557786977284251e-05,
      "loss": 0.0139,
      "step": 6060
    },
    {
      "epoch": 1.4782926829268292,
      "grad_norm": 0.14451861381530762,
      "learning_rate": 2.557148559015402e-05,
      "loss": 0.0332,
      "step": 6061
    },
    {
      "epoch": 1.4785365853658536,
      "grad_norm": 0.13906224071979523,
      "learning_rate": 2.5565101370177846e-05,
      "loss": 0.0314,
      "step": 6062
    },
    {
      "epoch": 1.478780487804878,
      "grad_norm": 0.07475969195365906,
      "learning_rate": 2.5558717113330538e-05,
      "loss": 0.0084,
      "step": 6063
    },
    {
      "epoch": 1.4790243902439024,
      "grad_norm": 0.14890646934509277,
      "learning_rate": 2.5552332820028646e-05,
      "loss": 0.0395,
      "step": 6064
    },
    {
      "epoch": 1.4792682926829268,
      "grad_norm": 0.18278861045837402,
      "learning_rate": 2.554594849068873e-05,
      "loss": 0.0286,
      "step": 6065
    },
    {
      "epoch": 1.4795121951219512,
      "grad_norm": 0.061440229415893555,
      "learning_rate": 2.5539564125727343e-05,
      "loss": 0.0152,
      "step": 6066
    },
    {
      "epoch": 1.4797560975609756,
      "grad_norm": 0.07434002310037613,
      "learning_rate": 2.5533179725561057e-05,
      "loss": 0.0147,
      "step": 6067
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.07676573097705841,
      "learning_rate": 2.5526795290606414e-05,
      "loss": 0.0116,
      "step": 6068
    },
    {
      "epoch": 1.4802439024390244,
      "grad_norm": 0.10696238279342651,
      "learning_rate": 2.552041082128e-05,
      "loss": 0.017,
      "step": 6069
    },
    {
      "epoch": 1.4804878048780488,
      "grad_norm": 0.2888590097427368,
      "learning_rate": 2.5514026317998362e-05,
      "loss": 0.0205,
      "step": 6070
    },
    {
      "epoch": 1.4807317073170732,
      "grad_norm": 0.18236692249774933,
      "learning_rate": 2.5507641781178092e-05,
      "loss": 0.0323,
      "step": 6071
    },
    {
      "epoch": 1.4809756097560975,
      "grad_norm": 0.059891700744628906,
      "learning_rate": 2.5501257211235745e-05,
      "loss": 0.0205,
      "step": 6072
    },
    {
      "epoch": 1.481219512195122,
      "grad_norm": 0.1358380913734436,
      "learning_rate": 2.5494872608587906e-05,
      "loss": 0.0162,
      "step": 6073
    },
    {
      "epoch": 1.4814634146341463,
      "grad_norm": 0.1286631077528,
      "learning_rate": 2.5488487973651138e-05,
      "loss": 0.0169,
      "step": 6074
    },
    {
      "epoch": 1.4817073170731707,
      "grad_norm": 0.2255633920431137,
      "learning_rate": 2.548210330684203e-05,
      "loss": 0.0143,
      "step": 6075
    },
    {
      "epoch": 1.481951219512195,
      "grad_norm": 0.14320194721221924,
      "learning_rate": 2.5475718608577153e-05,
      "loss": 0.0314,
      "step": 6076
    },
    {
      "epoch": 1.4821951219512195,
      "grad_norm": 0.10531974583864212,
      "learning_rate": 2.546933387927309e-05,
      "loss": 0.0166,
      "step": 6077
    },
    {
      "epoch": 1.4824390243902439,
      "grad_norm": 0.11762525141239166,
      "learning_rate": 2.546294911934643e-05,
      "loss": 0.0199,
      "step": 6078
    },
    {
      "epoch": 1.4826829268292683,
      "grad_norm": 0.06563317775726318,
      "learning_rate": 2.5456564329213762e-05,
      "loss": 0.0088,
      "step": 6079
    },
    {
      "epoch": 1.4829268292682927,
      "grad_norm": 0.12222415208816528,
      "learning_rate": 2.5450179509291666e-05,
      "loss": 0.0258,
      "step": 6080
    },
    {
      "epoch": 1.483170731707317,
      "grad_norm": 0.14129236340522766,
      "learning_rate": 2.5443794659996735e-05,
      "loss": 0.0216,
      "step": 6081
    },
    {
      "epoch": 1.4834146341463414,
      "grad_norm": 0.12144789099693298,
      "learning_rate": 2.5437409781745558e-05,
      "loss": 0.027,
      "step": 6082
    },
    {
      "epoch": 1.4836585365853658,
      "grad_norm": 0.08920847624540329,
      "learning_rate": 2.5431024874954734e-05,
      "loss": 0.0079,
      "step": 6083
    },
    {
      "epoch": 1.4839024390243902,
      "grad_norm": 0.12872804701328278,
      "learning_rate": 2.5424639940040862e-05,
      "loss": 0.0232,
      "step": 6084
    },
    {
      "epoch": 1.4841463414634146,
      "grad_norm": 0.17538070678710938,
      "learning_rate": 2.5418254977420525e-05,
      "loss": 0.035,
      "step": 6085
    },
    {
      "epoch": 1.484390243902439,
      "grad_norm": 0.11019596457481384,
      "learning_rate": 2.5411869987510334e-05,
      "loss": 0.0093,
      "step": 6086
    },
    {
      "epoch": 1.4846341463414634,
      "grad_norm": 0.07253721356391907,
      "learning_rate": 2.5405484970726895e-05,
      "loss": 0.0143,
      "step": 6087
    },
    {
      "epoch": 1.4848780487804878,
      "grad_norm": 0.21933133900165558,
      "learning_rate": 2.5399099927486792e-05,
      "loss": 0.0279,
      "step": 6088
    },
    {
      "epoch": 1.4851219512195122,
      "grad_norm": 0.15340861678123474,
      "learning_rate": 2.539271485820665e-05,
      "loss": 0.0281,
      "step": 6089
    },
    {
      "epoch": 1.4853658536585366,
      "grad_norm": 0.14098381996154785,
      "learning_rate": 2.5386329763303057e-05,
      "loss": 0.016,
      "step": 6090
    },
    {
      "epoch": 1.485609756097561,
      "grad_norm": 0.17151014506816864,
      "learning_rate": 2.5379944643192633e-05,
      "loss": 0.0167,
      "step": 6091
    },
    {
      "epoch": 1.4858536585365854,
      "grad_norm": 0.1249629408121109,
      "learning_rate": 2.5373559498291988e-05,
      "loss": 0.0093,
      "step": 6092
    },
    {
      "epoch": 1.4860975609756097,
      "grad_norm": 0.09356581419706345,
      "learning_rate": 2.5367174329017723e-05,
      "loss": 0.0181,
      "step": 6093
    },
    {
      "epoch": 1.4863414634146341,
      "grad_norm": 0.2549377381801605,
      "learning_rate": 2.5360789135786455e-05,
      "loss": 0.0132,
      "step": 6094
    },
    {
      "epoch": 1.4865853658536585,
      "grad_norm": 0.1336865872144699,
      "learning_rate": 2.5354403919014807e-05,
      "loss": 0.0306,
      "step": 6095
    },
    {
      "epoch": 1.486829268292683,
      "grad_norm": 0.06184155121445656,
      "learning_rate": 2.534801867911939e-05,
      "loss": 0.0116,
      "step": 6096
    },
    {
      "epoch": 1.4870731707317073,
      "grad_norm": 0.056466806679964066,
      "learning_rate": 2.534163341651682e-05,
      "loss": 0.0106,
      "step": 6097
    },
    {
      "epoch": 1.4873170731707317,
      "grad_norm": 0.14277224242687225,
      "learning_rate": 2.5335248131623708e-05,
      "loss": 0.0289,
      "step": 6098
    },
    {
      "epoch": 1.487560975609756,
      "grad_norm": 0.16164766252040863,
      "learning_rate": 2.532886282485668e-05,
      "loss": 0.0249,
      "step": 6099
    },
    {
      "epoch": 1.4878048780487805,
      "grad_norm": 0.1534244269132614,
      "learning_rate": 2.532247749663237e-05,
      "loss": 0.0201,
      "step": 6100
    },
    {
      "epoch": 1.4880487804878049,
      "grad_norm": 0.10785163193941116,
      "learning_rate": 2.5316092147367383e-05,
      "loss": 0.0312,
      "step": 6101
    },
    {
      "epoch": 1.4882926829268293,
      "grad_norm": 0.08499322831630707,
      "learning_rate": 2.5309706777478355e-05,
      "loss": 0.0246,
      "step": 6102
    },
    {
      "epoch": 1.4885365853658536,
      "grad_norm": 0.1493631899356842,
      "learning_rate": 2.5303321387381902e-05,
      "loss": 0.0117,
      "step": 6103
    },
    {
      "epoch": 1.488780487804878,
      "grad_norm": 0.0798354223370552,
      "learning_rate": 2.5296935977494663e-05,
      "loss": 0.0102,
      "step": 6104
    },
    {
      "epoch": 1.4890243902439024,
      "grad_norm": 0.09003887325525284,
      "learning_rate": 2.5290550548233254e-05,
      "loss": 0.0167,
      "step": 6105
    },
    {
      "epoch": 1.4892682926829268,
      "grad_norm": 0.0750521868467331,
      "learning_rate": 2.5284165100014322e-05,
      "loss": 0.0146,
      "step": 6106
    },
    {
      "epoch": 1.4895121951219512,
      "grad_norm": 0.08481010049581528,
      "learning_rate": 2.527777963325448e-05,
      "loss": 0.0148,
      "step": 6107
    },
    {
      "epoch": 1.4897560975609756,
      "grad_norm": 0.09995841234922409,
      "learning_rate": 2.5271394148370364e-05,
      "loss": 0.0105,
      "step": 6108
    },
    {
      "epoch": 1.49,
      "grad_norm": 0.351749062538147,
      "learning_rate": 2.5265008645778622e-05,
      "loss": 0.0291,
      "step": 6109
    },
    {
      "epoch": 1.4902439024390244,
      "grad_norm": 0.11583559215068817,
      "learning_rate": 2.5258623125895864e-05,
      "loss": 0.0173,
      "step": 6110
    },
    {
      "epoch": 1.4904878048780488,
      "grad_norm": 0.10071651637554169,
      "learning_rate": 2.5252237589138743e-05,
      "loss": 0.0186,
      "step": 6111
    },
    {
      "epoch": 1.4907317073170732,
      "grad_norm": 0.18861958384513855,
      "learning_rate": 2.524585203592389e-05,
      "loss": 0.028,
      "step": 6112
    },
    {
      "epoch": 1.4909756097560976,
      "grad_norm": 0.2040736824274063,
      "learning_rate": 2.523946646666795e-05,
      "loss": 0.0201,
      "step": 6113
    },
    {
      "epoch": 1.491219512195122,
      "grad_norm": 0.12450852990150452,
      "learning_rate": 2.523308088178756e-05,
      "loss": 0.0164,
      "step": 6114
    },
    {
      "epoch": 1.4914634146341463,
      "grad_norm": 0.1208290234208107,
      "learning_rate": 2.5226695281699346e-05,
      "loss": 0.0399,
      "step": 6115
    },
    {
      "epoch": 1.4917073170731707,
      "grad_norm": 0.2420569658279419,
      "learning_rate": 2.5220309666819963e-05,
      "loss": 0.0204,
      "step": 6116
    },
    {
      "epoch": 1.4919512195121951,
      "grad_norm": 0.18002966046333313,
      "learning_rate": 2.5213924037566052e-05,
      "loss": 0.0168,
      "step": 6117
    },
    {
      "epoch": 1.4921951219512195,
      "grad_norm": 0.10131670534610748,
      "learning_rate": 2.520753839435424e-05,
      "loss": 0.0191,
      "step": 6118
    },
    {
      "epoch": 1.492439024390244,
      "grad_norm": 0.17407067120075226,
      "learning_rate": 2.520115273760119e-05,
      "loss": 0.0296,
      "step": 6119
    },
    {
      "epoch": 1.4926829268292683,
      "grad_norm": 0.07418907433748245,
      "learning_rate": 2.519476706772354e-05,
      "loss": 0.0146,
      "step": 6120
    },
    {
      "epoch": 1.4929268292682927,
      "grad_norm": 0.10801129043102264,
      "learning_rate": 2.5188381385137945e-05,
      "loss": 0.0179,
      "step": 6121
    },
    {
      "epoch": 1.493170731707317,
      "grad_norm": 0.12378095835447311,
      "learning_rate": 2.5181995690261033e-05,
      "loss": 0.0275,
      "step": 6122
    },
    {
      "epoch": 1.4934146341463415,
      "grad_norm": 0.08988921344280243,
      "learning_rate": 2.517560998350946e-05,
      "loss": 0.0157,
      "step": 6123
    },
    {
      "epoch": 1.4936585365853658,
      "grad_norm": 0.08583987504243851,
      "learning_rate": 2.516922426529987e-05,
      "loss": 0.0214,
      "step": 6124
    },
    {
      "epoch": 1.4939024390243902,
      "grad_norm": 0.153898686170578,
      "learning_rate": 2.516283853604891e-05,
      "loss": 0.014,
      "step": 6125
    },
    {
      "epoch": 1.4941463414634146,
      "grad_norm": 0.13063891232013702,
      "learning_rate": 2.5156452796173248e-05,
      "loss": 0.0089,
      "step": 6126
    },
    {
      "epoch": 1.494390243902439,
      "grad_norm": 0.1266855150461197,
      "learning_rate": 2.515006704608951e-05,
      "loss": 0.0259,
      "step": 6127
    },
    {
      "epoch": 1.4946341463414634,
      "grad_norm": 0.5417306423187256,
      "learning_rate": 2.5143681286214353e-05,
      "loss": 0.0209,
      "step": 6128
    },
    {
      "epoch": 1.4948780487804878,
      "grad_norm": 0.10546059906482697,
      "learning_rate": 2.513729551696444e-05,
      "loss": 0.0239,
      "step": 6129
    },
    {
      "epoch": 1.4951219512195122,
      "grad_norm": 0.08490771800279617,
      "learning_rate": 2.5130909738756426e-05,
      "loss": 0.013,
      "step": 6130
    },
    {
      "epoch": 1.4953658536585366,
      "grad_norm": 0.20393678545951843,
      "learning_rate": 2.5124523952006936e-05,
      "loss": 0.0252,
      "step": 6131
    },
    {
      "epoch": 1.495609756097561,
      "grad_norm": 0.11824937909841537,
      "learning_rate": 2.5118138157132638e-05,
      "loss": 0.0139,
      "step": 6132
    },
    {
      "epoch": 1.4958536585365854,
      "grad_norm": 0.11423934996128082,
      "learning_rate": 2.5111752354550188e-05,
      "loss": 0.0181,
      "step": 6133
    },
    {
      "epoch": 1.4960975609756098,
      "grad_norm": 0.10061271488666534,
      "learning_rate": 2.510536654467625e-05,
      "loss": 0.0194,
      "step": 6134
    },
    {
      "epoch": 1.4963414634146341,
      "grad_norm": 0.10526806116104126,
      "learning_rate": 2.5098980727927457e-05,
      "loss": 0.0211,
      "step": 6135
    },
    {
      "epoch": 1.4965853658536585,
      "grad_norm": 0.1193600594997406,
      "learning_rate": 2.5092594904720473e-05,
      "loss": 0.0204,
      "step": 6136
    },
    {
      "epoch": 1.496829268292683,
      "grad_norm": 0.08429580926895142,
      "learning_rate": 2.5086209075471962e-05,
      "loss": 0.0089,
      "step": 6137
    },
    {
      "epoch": 1.4970731707317073,
      "grad_norm": 0.08848942071199417,
      "learning_rate": 2.507982324059857e-05,
      "loss": 0.0127,
      "step": 6138
    },
    {
      "epoch": 1.4973170731707317,
      "grad_norm": 0.048762280493974686,
      "learning_rate": 2.5073437400516965e-05,
      "loss": 0.0075,
      "step": 6139
    },
    {
      "epoch": 1.497560975609756,
      "grad_norm": 0.12280526757240295,
      "learning_rate": 2.5067051555643783e-05,
      "loss": 0.0157,
      "step": 6140
    },
    {
      "epoch": 1.4978048780487805,
      "grad_norm": 0.12192243337631226,
      "learning_rate": 2.5060665706395693e-05,
      "loss": 0.0334,
      "step": 6141
    },
    {
      "epoch": 1.4980487804878049,
      "grad_norm": 0.14803555607795715,
      "learning_rate": 2.505427985318935e-05,
      "loss": 0.0258,
      "step": 6142
    },
    {
      "epoch": 1.4982926829268293,
      "grad_norm": 0.2525339424610138,
      "learning_rate": 2.504789399644142e-05,
      "loss": 0.0259,
      "step": 6143
    },
    {
      "epoch": 1.4985365853658537,
      "grad_norm": 0.18542233109474182,
      "learning_rate": 2.5041508136568548e-05,
      "loss": 0.0173,
      "step": 6144
    },
    {
      "epoch": 1.498780487804878,
      "grad_norm": 0.053790636360645294,
      "learning_rate": 2.5035122273987393e-05,
      "loss": 0.0125,
      "step": 6145
    },
    {
      "epoch": 1.4990243902439024,
      "grad_norm": 0.11821290105581284,
      "learning_rate": 2.5028736409114628e-05,
      "loss": 0.0185,
      "step": 6146
    },
    {
      "epoch": 1.4992682926829268,
      "grad_norm": 0.09996061772108078,
      "learning_rate": 2.50223505423669e-05,
      "loss": 0.0138,
      "step": 6147
    },
    {
      "epoch": 1.4995121951219512,
      "grad_norm": 0.12236156314611435,
      "learning_rate": 2.501596467416086e-05,
      "loss": 0.0331,
      "step": 6148
    },
    {
      "epoch": 1.4997560975609756,
      "grad_norm": 0.11859027296304703,
      "learning_rate": 2.5009578804913175e-05,
      "loss": 0.0197,
      "step": 6149
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.11079540103673935,
      "learning_rate": 2.5003192935040503e-05,
      "loss": 0.0232,
      "step": 6150
    },
    {
      "epoch": 1.5002439024390244,
      "grad_norm": 0.057890135794878006,
      "learning_rate": 2.4996807064959506e-05,
      "loss": 0.0124,
      "step": 6151
    },
    {
      "epoch": 1.5004878048780488,
      "grad_norm": 0.22242583334445953,
      "learning_rate": 2.4990421195086834e-05,
      "loss": 0.0271,
      "step": 6152
    },
    {
      "epoch": 1.5007317073170732,
      "grad_norm": 0.17283809185028076,
      "learning_rate": 2.498403532583915e-05,
      "loss": 0.0245,
      "step": 6153
    },
    {
      "epoch": 1.5009756097560976,
      "grad_norm": 0.12849216163158417,
      "learning_rate": 2.497764945763311e-05,
      "loss": 0.0258,
      "step": 6154
    },
    {
      "epoch": 1.501219512195122,
      "grad_norm": 0.19233249127864838,
      "learning_rate": 2.4971263590885378e-05,
      "loss": 0.0183,
      "step": 6155
    },
    {
      "epoch": 1.5014634146341463,
      "grad_norm": 0.11770809441804886,
      "learning_rate": 2.496487772601261e-05,
      "loss": 0.0321,
      "step": 6156
    },
    {
      "epoch": 1.5017073170731707,
      "grad_norm": 0.14763925969600677,
      "learning_rate": 2.4958491863431458e-05,
      "loss": 0.0223,
      "step": 6157
    },
    {
      "epoch": 1.5019512195121951,
      "grad_norm": 0.16039352118968964,
      "learning_rate": 2.4952106003558586e-05,
      "loss": 0.038,
      "step": 6158
    },
    {
      "epoch": 1.5021951219512195,
      "grad_norm": 0.09149544686079025,
      "learning_rate": 2.4945720146810656e-05,
      "loss": 0.0139,
      "step": 6159
    },
    {
      "epoch": 1.502439024390244,
      "grad_norm": 0.1970052421092987,
      "learning_rate": 2.4939334293604313e-05,
      "loss": 0.0144,
      "step": 6160
    },
    {
      "epoch": 1.5026829268292683,
      "grad_norm": 0.13788360357284546,
      "learning_rate": 2.4932948444356223e-05,
      "loss": 0.0198,
      "step": 6161
    },
    {
      "epoch": 1.5029268292682927,
      "grad_norm": 0.11731418967247009,
      "learning_rate": 2.4926562599483044e-05,
      "loss": 0.014,
      "step": 6162
    },
    {
      "epoch": 1.503170731707317,
      "grad_norm": 0.14182479679584503,
      "learning_rate": 2.4920176759401433e-05,
      "loss": 0.0209,
      "step": 6163
    },
    {
      "epoch": 1.5034146341463415,
      "grad_norm": 0.35746338963508606,
      "learning_rate": 2.4913790924528037e-05,
      "loss": 0.0267,
      "step": 6164
    },
    {
      "epoch": 1.5036585365853659,
      "grad_norm": 0.13588804006576538,
      "learning_rate": 2.4907405095279523e-05,
      "loss": 0.0381,
      "step": 6165
    },
    {
      "epoch": 1.5039024390243902,
      "grad_norm": 0.13020563125610352,
      "learning_rate": 2.4901019272072542e-05,
      "loss": 0.0203,
      "step": 6166
    },
    {
      "epoch": 1.5041463414634146,
      "grad_norm": 0.13002488017082214,
      "learning_rate": 2.4894633455323758e-05,
      "loss": 0.0115,
      "step": 6167
    },
    {
      "epoch": 1.504390243902439,
      "grad_norm": 0.09534181654453278,
      "learning_rate": 2.4888247645449814e-05,
      "loss": 0.0239,
      "step": 6168
    },
    {
      "epoch": 1.5046341463414634,
      "grad_norm": 0.09021292626857758,
      "learning_rate": 2.488186184286737e-05,
      "loss": 0.0176,
      "step": 6169
    },
    {
      "epoch": 1.5048780487804878,
      "grad_norm": 0.07770559936761856,
      "learning_rate": 2.4875476047993077e-05,
      "loss": 0.0104,
      "step": 6170
    },
    {
      "epoch": 1.5051219512195122,
      "grad_norm": 0.16168846189975739,
      "learning_rate": 2.4869090261243587e-05,
      "loss": 0.0246,
      "step": 6171
    },
    {
      "epoch": 1.5053658536585366,
      "grad_norm": 0.10113976895809174,
      "learning_rate": 2.4862704483035564e-05,
      "loss": 0.0138,
      "step": 6172
    },
    {
      "epoch": 1.505609756097561,
      "grad_norm": 0.09399942308664322,
      "learning_rate": 2.485631871378565e-05,
      "loss": 0.0198,
      "step": 6173
    },
    {
      "epoch": 1.5058536585365854,
      "grad_norm": 0.1397029310464859,
      "learning_rate": 2.4849932953910495e-05,
      "loss": 0.0245,
      "step": 6174
    },
    {
      "epoch": 1.5060975609756098,
      "grad_norm": 0.0849790945649147,
      "learning_rate": 2.4843547203826758e-05,
      "loss": 0.0137,
      "step": 6175
    },
    {
      "epoch": 1.5063414634146342,
      "grad_norm": 0.16675052046775818,
      "learning_rate": 2.4837161463951096e-05,
      "loss": 0.0184,
      "step": 6176
    },
    {
      "epoch": 1.5065853658536585,
      "grad_norm": 0.09996720403432846,
      "learning_rate": 2.4830775734700137e-05,
      "loss": 0.02,
      "step": 6177
    },
    {
      "epoch": 1.506829268292683,
      "grad_norm": 0.1496647596359253,
      "learning_rate": 2.4824390016490547e-05,
      "loss": 0.0326,
      "step": 6178
    },
    {
      "epoch": 1.5070731707317073,
      "grad_norm": 0.11139127612113953,
      "learning_rate": 2.4818004309738972e-05,
      "loss": 0.0167,
      "step": 6179
    },
    {
      "epoch": 1.5073170731707317,
      "grad_norm": 0.2203870415687561,
      "learning_rate": 2.4811618614862064e-05,
      "loss": 0.0105,
      "step": 6180
    },
    {
      "epoch": 1.507560975609756,
      "grad_norm": 0.07694123685359955,
      "learning_rate": 2.4805232932276458e-05,
      "loss": 0.0165,
      "step": 6181
    },
    {
      "epoch": 1.5078048780487805,
      "grad_norm": 0.08416766673326492,
      "learning_rate": 2.4798847262398805e-05,
      "loss": 0.0144,
      "step": 6182
    },
    {
      "epoch": 1.5080487804878049,
      "grad_norm": 0.09478230774402618,
      "learning_rate": 2.4792461605645756e-05,
      "loss": 0.0116,
      "step": 6183
    },
    {
      "epoch": 1.5082926829268293,
      "grad_norm": 0.11910068988800049,
      "learning_rate": 2.478607596243396e-05,
      "loss": 0.0214,
      "step": 6184
    },
    {
      "epoch": 1.5085365853658537,
      "grad_norm": 0.17551885545253754,
      "learning_rate": 2.4779690333180046e-05,
      "loss": 0.0213,
      "step": 6185
    },
    {
      "epoch": 1.508780487804878,
      "grad_norm": 0.18654467165470123,
      "learning_rate": 2.4773304718300663e-05,
      "loss": 0.0202,
      "step": 6186
    },
    {
      "epoch": 1.5090243902439024,
      "grad_norm": 0.07045482099056244,
      "learning_rate": 2.4766919118212454e-05,
      "loss": 0.018,
      "step": 6187
    },
    {
      "epoch": 1.5092682926829268,
      "grad_norm": 0.1175074651837349,
      "learning_rate": 2.4760533533332054e-05,
      "loss": 0.0221,
      "step": 6188
    },
    {
      "epoch": 1.5095121951219512,
      "grad_norm": 0.11527298390865326,
      "learning_rate": 2.475414796407611e-05,
      "loss": 0.0416,
      "step": 6189
    },
    {
      "epoch": 1.5097560975609756,
      "grad_norm": 0.12423402816057205,
      "learning_rate": 2.4747762410861263e-05,
      "loss": 0.019,
      "step": 6190
    },
    {
      "epoch": 1.51,
      "grad_norm": 0.17722271382808685,
      "learning_rate": 2.474137687410414e-05,
      "loss": 0.0295,
      "step": 6191
    },
    {
      "epoch": 1.5102439024390244,
      "grad_norm": 0.12740325927734375,
      "learning_rate": 2.4734991354221387e-05,
      "loss": 0.0169,
      "step": 6192
    },
    {
      "epoch": 1.5104878048780488,
      "grad_norm": 0.08631962537765503,
      "learning_rate": 2.4728605851629642e-05,
      "loss": 0.0157,
      "step": 6193
    },
    {
      "epoch": 1.5107317073170732,
      "grad_norm": 0.09324823319911957,
      "learning_rate": 2.4722220366745527e-05,
      "loss": 0.0201,
      "step": 6194
    },
    {
      "epoch": 1.5109756097560976,
      "grad_norm": 0.10212285071611404,
      "learning_rate": 2.4715834899985684e-05,
      "loss": 0.0164,
      "step": 6195
    },
    {
      "epoch": 1.511219512195122,
      "grad_norm": 0.12396568059921265,
      "learning_rate": 2.4709449451766745e-05,
      "loss": 0.018,
      "step": 6196
    },
    {
      "epoch": 1.5114634146341464,
      "grad_norm": 0.2602997124195099,
      "learning_rate": 2.4703064022505342e-05,
      "loss": 0.0249,
      "step": 6197
    },
    {
      "epoch": 1.5117073170731707,
      "grad_norm": 0.1368250697851181,
      "learning_rate": 2.46966786126181e-05,
      "loss": 0.0415,
      "step": 6198
    },
    {
      "epoch": 1.5119512195121951,
      "grad_norm": 0.20875225961208344,
      "learning_rate": 2.4690293222521648e-05,
      "loss": 0.0181,
      "step": 6199
    },
    {
      "epoch": 1.5121951219512195,
      "grad_norm": 0.07004432380199432,
      "learning_rate": 2.4683907852632616e-05,
      "loss": 0.0164,
      "step": 6200
    },
    {
      "epoch": 1.512439024390244,
      "grad_norm": 0.11756378412246704,
      "learning_rate": 2.467752250336764e-05,
      "loss": 0.0246,
      "step": 6201
    },
    {
      "epoch": 1.5126829268292683,
      "grad_norm": 0.09039431065320969,
      "learning_rate": 2.4671137175143326e-05,
      "loss": 0.0211,
      "step": 6202
    },
    {
      "epoch": 1.5129268292682927,
      "grad_norm": 0.16617396473884583,
      "learning_rate": 2.4664751868376305e-05,
      "loss": 0.024,
      "step": 6203
    },
    {
      "epoch": 1.513170731707317,
      "grad_norm": 0.1086110770702362,
      "learning_rate": 2.465836658348319e-05,
      "loss": 0.0165,
      "step": 6204
    },
    {
      "epoch": 1.5134146341463415,
      "grad_norm": 0.08995144069194794,
      "learning_rate": 2.4651981320880616e-05,
      "loss": 0.0165,
      "step": 6205
    },
    {
      "epoch": 1.5136585365853659,
      "grad_norm": 0.09274275600910187,
      "learning_rate": 2.4645596080985202e-05,
      "loss": 0.0098,
      "step": 6206
    },
    {
      "epoch": 1.5139024390243903,
      "grad_norm": 0.13141246140003204,
      "learning_rate": 2.4639210864213547e-05,
      "loss": 0.0227,
      "step": 6207
    },
    {
      "epoch": 1.5141463414634146,
      "grad_norm": 0.1982211172580719,
      "learning_rate": 2.4632825670982283e-05,
      "loss": 0.0257,
      "step": 6208
    },
    {
      "epoch": 1.514390243902439,
      "grad_norm": 0.11477808654308319,
      "learning_rate": 2.4626440501708018e-05,
      "loss": 0.0247,
      "step": 6209
    },
    {
      "epoch": 1.5146341463414634,
      "grad_norm": 0.16853290796279907,
      "learning_rate": 2.4620055356807373e-05,
      "loss": 0.0214,
      "step": 6210
    },
    {
      "epoch": 1.5148780487804878,
      "grad_norm": 0.17234376072883606,
      "learning_rate": 2.461367023669695e-05,
      "loss": 0.0132,
      "step": 6211
    },
    {
      "epoch": 1.5151219512195122,
      "grad_norm": 0.14349372684955597,
      "learning_rate": 2.4607285141793356e-05,
      "loss": 0.0184,
      "step": 6212
    },
    {
      "epoch": 1.5153658536585366,
      "grad_norm": 0.16637277603149414,
      "learning_rate": 2.4600900072513207e-05,
      "loss": 0.0209,
      "step": 6213
    },
    {
      "epoch": 1.515609756097561,
      "grad_norm": 0.1093980148434639,
      "learning_rate": 2.4594515029273114e-05,
      "loss": 0.0152,
      "step": 6214
    },
    {
      "epoch": 1.5158536585365854,
      "grad_norm": 0.13066411018371582,
      "learning_rate": 2.4588130012489665e-05,
      "loss": 0.008,
      "step": 6215
    },
    {
      "epoch": 1.5160975609756098,
      "grad_norm": 0.0791168063879013,
      "learning_rate": 2.458174502257947e-05,
      "loss": 0.0143,
      "step": 6216
    },
    {
      "epoch": 1.5163414634146342,
      "grad_norm": 0.12661761045455933,
      "learning_rate": 2.4575360059959147e-05,
      "loss": 0.0194,
      "step": 6217
    },
    {
      "epoch": 1.5165853658536586,
      "grad_norm": 0.13516439497470856,
      "learning_rate": 2.456897512504527e-05,
      "loss": 0.008,
      "step": 6218
    },
    {
      "epoch": 1.516829268292683,
      "grad_norm": 0.13270176947116852,
      "learning_rate": 2.456259021825445e-05,
      "loss": 0.0141,
      "step": 6219
    },
    {
      "epoch": 1.5170731707317073,
      "grad_norm": 0.17291821539402008,
      "learning_rate": 2.4556205340003275e-05,
      "loss": 0.029,
      "step": 6220
    },
    {
      "epoch": 1.5173170731707317,
      "grad_norm": 0.1575143188238144,
      "learning_rate": 2.4549820490708343e-05,
      "loss": 0.0281,
      "step": 6221
    },
    {
      "epoch": 1.5175609756097561,
      "grad_norm": 0.0802115797996521,
      "learning_rate": 2.4543435670786243e-05,
      "loss": 0.0249,
      "step": 6222
    },
    {
      "epoch": 1.5178048780487805,
      "grad_norm": 0.11154048889875412,
      "learning_rate": 2.4537050880653575e-05,
      "loss": 0.0176,
      "step": 6223
    },
    {
      "epoch": 1.518048780487805,
      "grad_norm": 0.04767550155520439,
      "learning_rate": 2.4530666120726917e-05,
      "loss": 0.0072,
      "step": 6224
    },
    {
      "epoch": 1.5182926829268293,
      "grad_norm": 0.13750267028808594,
      "learning_rate": 2.4524281391422856e-05,
      "loss": 0.0263,
      "step": 6225
    },
    {
      "epoch": 1.5185365853658537,
      "grad_norm": 0.11438353359699249,
      "learning_rate": 2.4517896693157978e-05,
      "loss": 0.0216,
      "step": 6226
    },
    {
      "epoch": 1.518780487804878,
      "grad_norm": 0.07937634736299515,
      "learning_rate": 2.4511512026348868e-05,
      "loss": 0.0179,
      "step": 6227
    },
    {
      "epoch": 1.5190243902439025,
      "grad_norm": 0.05682794749736786,
      "learning_rate": 2.4505127391412096e-05,
      "loss": 0.01,
      "step": 6228
    },
    {
      "epoch": 1.5192682926829268,
      "grad_norm": 0.05935859680175781,
      "learning_rate": 2.4498742788764254e-05,
      "loss": 0.0115,
      "step": 6229
    },
    {
      "epoch": 1.5195121951219512,
      "grad_norm": 0.05116139352321625,
      "learning_rate": 2.4492358218821903e-05,
      "loss": 0.0134,
      "step": 6230
    },
    {
      "epoch": 1.5197560975609756,
      "grad_norm": 0.09386535733938217,
      "learning_rate": 2.4485973682001634e-05,
      "loss": 0.0226,
      "step": 6231
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.059218473732471466,
      "learning_rate": 2.4479589178720003e-05,
      "loss": 0.0095,
      "step": 6232
    },
    {
      "epoch": 1.5202439024390244,
      "grad_norm": 0.3115822374820709,
      "learning_rate": 2.4473204709393585e-05,
      "loss": 0.0212,
      "step": 6233
    },
    {
      "epoch": 1.5204878048780488,
      "grad_norm": 0.29282471537590027,
      "learning_rate": 2.4466820274438955e-05,
      "loss": 0.0444,
      "step": 6234
    },
    {
      "epoch": 1.5207317073170732,
      "grad_norm": 0.11485585570335388,
      "learning_rate": 2.4460435874272662e-05,
      "loss": 0.0309,
      "step": 6235
    },
    {
      "epoch": 1.5209756097560976,
      "grad_norm": 0.10512781143188477,
      "learning_rate": 2.445405150931128e-05,
      "loss": 0.0248,
      "step": 6236
    },
    {
      "epoch": 1.521219512195122,
      "grad_norm": 0.1167156994342804,
      "learning_rate": 2.4447667179971363e-05,
      "loss": 0.0281,
      "step": 6237
    },
    {
      "epoch": 1.5214634146341464,
      "grad_norm": 0.10577241331338882,
      "learning_rate": 2.444128288666947e-05,
      "loss": 0.0216,
      "step": 6238
    },
    {
      "epoch": 1.5217073170731708,
      "grad_norm": 0.18864159286022186,
      "learning_rate": 2.443489862982216e-05,
      "loss": 0.0174,
      "step": 6239
    },
    {
      "epoch": 1.5219512195121951,
      "grad_norm": 0.08762580901384354,
      "learning_rate": 2.442851440984599e-05,
      "loss": 0.0096,
      "step": 6240
    },
    {
      "epoch": 1.5221951219512195,
      "grad_norm": 0.054571837186813354,
      "learning_rate": 2.4422130227157496e-05,
      "loss": 0.0077,
      "step": 6241
    },
    {
      "epoch": 1.522439024390244,
      "grad_norm": 0.08385215699672699,
      "learning_rate": 2.4415746082173234e-05,
      "loss": 0.0184,
      "step": 6242
    },
    {
      "epoch": 1.5226829268292683,
      "grad_norm": 0.18340422213077545,
      "learning_rate": 2.4409361975309755e-05,
      "loss": 0.0216,
      "step": 6243
    },
    {
      "epoch": 1.5229268292682927,
      "grad_norm": 0.219629168510437,
      "learning_rate": 2.4402977906983605e-05,
      "loss": 0.0117,
      "step": 6244
    },
    {
      "epoch": 1.523170731707317,
      "grad_norm": 0.13313348591327667,
      "learning_rate": 2.4396593877611315e-05,
      "loss": 0.0168,
      "step": 6245
    },
    {
      "epoch": 1.5234146341463415,
      "grad_norm": 0.09929933398962021,
      "learning_rate": 2.4390209887609424e-05,
      "loss": 0.0186,
      "step": 6246
    },
    {
      "epoch": 1.5236585365853659,
      "grad_norm": 0.12628310918807983,
      "learning_rate": 2.4383825937394477e-05,
      "loss": 0.0174,
      "step": 6247
    },
    {
      "epoch": 1.5239024390243903,
      "grad_norm": 0.09278740733861923,
      "learning_rate": 2.4377442027383e-05,
      "loss": 0.0193,
      "step": 6248
    },
    {
      "epoch": 1.5241463414634147,
      "grad_norm": 0.09113045781850815,
      "learning_rate": 2.437105815799152e-05,
      "loss": 0.0089,
      "step": 6249
    },
    {
      "epoch": 1.524390243902439,
      "grad_norm": 0.17732420563697815,
      "learning_rate": 2.4364674329636583e-05,
      "loss": 0.0386,
      "step": 6250
    },
    {
      "epoch": 1.5246341463414634,
      "grad_norm": 0.11824344098567963,
      "learning_rate": 2.4358290542734687e-05,
      "loss": 0.0181,
      "step": 6251
    },
    {
      "epoch": 1.5248780487804878,
      "grad_norm": 0.14335747063159943,
      "learning_rate": 2.435190679770237e-05,
      "loss": 0.019,
      "step": 6252
    },
    {
      "epoch": 1.5251219512195122,
      "grad_norm": 0.124705009162426,
      "learning_rate": 2.434552309495616e-05,
      "loss": 0.0265,
      "step": 6253
    },
    {
      "epoch": 1.5253658536585366,
      "grad_norm": 0.07214060425758362,
      "learning_rate": 2.4339139434912555e-05,
      "loss": 0.0173,
      "step": 6254
    },
    {
      "epoch": 1.525609756097561,
      "grad_norm": 0.1430753618478775,
      "learning_rate": 2.433275581798808e-05,
      "loss": 0.0214,
      "step": 6255
    },
    {
      "epoch": 1.5258536585365854,
      "grad_norm": 0.09768756479024887,
      "learning_rate": 2.4326372244599247e-05,
      "loss": 0.0315,
      "step": 6256
    },
    {
      "epoch": 1.5260975609756098,
      "grad_norm": 0.40669795870780945,
      "learning_rate": 2.4319988715162563e-05,
      "loss": 0.0179,
      "step": 6257
    },
    {
      "epoch": 1.5263414634146342,
      "grad_norm": 0.14983968436717987,
      "learning_rate": 2.4313605230094532e-05,
      "loss": 0.021,
      "step": 6258
    },
    {
      "epoch": 1.5265853658536586,
      "grad_norm": 0.16232486069202423,
      "learning_rate": 2.430722178981166e-05,
      "loss": 0.0231,
      "step": 6259
    },
    {
      "epoch": 1.526829268292683,
      "grad_norm": 0.11896298825740814,
      "learning_rate": 2.4300838394730442e-05,
      "loss": 0.0084,
      "step": 6260
    },
    {
      "epoch": 1.5270731707317073,
      "grad_norm": 0.1345331221818924,
      "learning_rate": 2.4294455045267385e-05,
      "loss": 0.0237,
      "step": 6261
    },
    {
      "epoch": 1.5273170731707317,
      "grad_norm": 0.1156415343284607,
      "learning_rate": 2.4288071741838973e-05,
      "loss": 0.0205,
      "step": 6262
    },
    {
      "epoch": 1.5275609756097561,
      "grad_norm": 0.13842880725860596,
      "learning_rate": 2.42816884848617e-05,
      "loss": 0.0154,
      "step": 6263
    },
    {
      "epoch": 1.5278048780487805,
      "grad_norm": 0.09867168217897415,
      "learning_rate": 2.4275305274752058e-05,
      "loss": 0.0212,
      "step": 6264
    },
    {
      "epoch": 1.528048780487805,
      "grad_norm": 0.100039042532444,
      "learning_rate": 2.4268922111926525e-05,
      "loss": 0.0183,
      "step": 6265
    },
    {
      "epoch": 1.5282926829268293,
      "grad_norm": 0.12658102810382843,
      "learning_rate": 2.426253899680158e-05,
      "loss": 0.0239,
      "step": 6266
    },
    {
      "epoch": 1.5285365853658537,
      "grad_norm": 0.1086924597620964,
      "learning_rate": 2.4256155929793725e-05,
      "loss": 0.0254,
      "step": 6267
    },
    {
      "epoch": 1.528780487804878,
      "grad_norm": 0.22773338854312897,
      "learning_rate": 2.4249772911319407e-05,
      "loss": 0.0345,
      "step": 6268
    },
    {
      "epoch": 1.5290243902439025,
      "grad_norm": 0.12048143148422241,
      "learning_rate": 2.424338994179511e-05,
      "loss": 0.0225,
      "step": 6269
    },
    {
      "epoch": 1.5292682926829269,
      "grad_norm": 0.10077868402004242,
      "learning_rate": 2.4237007021637305e-05,
      "loss": 0.0211,
      "step": 6270
    },
    {
      "epoch": 1.5295121951219512,
      "grad_norm": 0.2893564701080322,
      "learning_rate": 2.4230624151262453e-05,
      "loss": 0.0337,
      "step": 6271
    },
    {
      "epoch": 1.5297560975609756,
      "grad_norm": 0.23706991970539093,
      "learning_rate": 2.422424133108702e-05,
      "loss": 0.0115,
      "step": 6272
    },
    {
      "epoch": 1.53,
      "grad_norm": 0.10487426817417145,
      "learning_rate": 2.4217858561527464e-05,
      "loss": 0.024,
      "step": 6273
    },
    {
      "epoch": 1.5302439024390244,
      "grad_norm": 0.12333662807941437,
      "learning_rate": 2.421147584300025e-05,
      "loss": 0.0138,
      "step": 6274
    },
    {
      "epoch": 1.5304878048780488,
      "grad_norm": 0.06550345569849014,
      "learning_rate": 2.420509317592182e-05,
      "loss": 0.0144,
      "step": 6275
    },
    {
      "epoch": 1.5307317073170732,
      "grad_norm": 0.11087353527545929,
      "learning_rate": 2.419871056070862e-05,
      "loss": 0.0214,
      "step": 6276
    },
    {
      "epoch": 1.5309756097560976,
      "grad_norm": 0.16254304349422455,
      "learning_rate": 2.4192327997777113e-05,
      "loss": 0.0368,
      "step": 6277
    },
    {
      "epoch": 1.531219512195122,
      "grad_norm": 0.05404094234108925,
      "learning_rate": 2.4185945487543726e-05,
      "loss": 0.0118,
      "step": 6278
    },
    {
      "epoch": 1.5314634146341464,
      "grad_norm": 0.10419591516256332,
      "learning_rate": 2.4179563030424905e-05,
      "loss": 0.0208,
      "step": 6279
    },
    {
      "epoch": 1.5317073170731708,
      "grad_norm": 0.08408436179161072,
      "learning_rate": 2.417318062683708e-05,
      "loss": 0.018,
      "step": 6280
    },
    {
      "epoch": 1.5319512195121952,
      "grad_norm": 0.21125976741313934,
      "learning_rate": 2.4166798277196697e-05,
      "loss": 0.0167,
      "step": 6281
    },
    {
      "epoch": 1.5321951219512195,
      "grad_norm": 0.14721998572349548,
      "learning_rate": 2.416041598192017e-05,
      "loss": 0.0199,
      "step": 6282
    },
    {
      "epoch": 1.532439024390244,
      "grad_norm": 0.0737762376666069,
      "learning_rate": 2.415403374142394e-05,
      "loss": 0.0121,
      "step": 6283
    },
    {
      "epoch": 1.5326829268292683,
      "grad_norm": 0.11339245736598969,
      "learning_rate": 2.414765155612441e-05,
      "loss": 0.0189,
      "step": 6284
    },
    {
      "epoch": 1.5329268292682927,
      "grad_norm": 0.06625586003065109,
      "learning_rate": 2.4141269426438003e-05,
      "loss": 0.0141,
      "step": 6285
    },
    {
      "epoch": 1.533170731707317,
      "grad_norm": 0.13118408620357513,
      "learning_rate": 2.4134887352781138e-05,
      "loss": 0.0401,
      "step": 6286
    },
    {
      "epoch": 1.5334146341463415,
      "grad_norm": 0.1391453593969345,
      "learning_rate": 2.412850533557023e-05,
      "loss": 0.0191,
      "step": 6287
    },
    {
      "epoch": 1.5336585365853659,
      "grad_norm": 0.07819130271673203,
      "learning_rate": 2.4122123375221677e-05,
      "loss": 0.0112,
      "step": 6288
    },
    {
      "epoch": 1.5339024390243903,
      "grad_norm": 0.2178427129983902,
      "learning_rate": 2.4115741472151888e-05,
      "loss": 0.0351,
      "step": 6289
    },
    {
      "epoch": 1.5341463414634147,
      "grad_norm": 0.1402733027935028,
      "learning_rate": 2.4109359626777263e-05,
      "loss": 0.0197,
      "step": 6290
    },
    {
      "epoch": 1.534390243902439,
      "grad_norm": 0.05906654894351959,
      "learning_rate": 2.410297783951419e-05,
      "loss": 0.0108,
      "step": 6291
    },
    {
      "epoch": 1.5346341463414634,
      "grad_norm": 0.12973237037658691,
      "learning_rate": 2.4096596110779073e-05,
      "loss": 0.0314,
      "step": 6292
    },
    {
      "epoch": 1.5348780487804878,
      "grad_norm": 0.08778269588947296,
      "learning_rate": 2.409021444098829e-05,
      "loss": 0.0229,
      "step": 6293
    },
    {
      "epoch": 1.5351219512195122,
      "grad_norm": 0.05610373988747597,
      "learning_rate": 2.4083832830558233e-05,
      "loss": 0.0147,
      "step": 6294
    },
    {
      "epoch": 1.5353658536585366,
      "grad_norm": 0.12461404502391815,
      "learning_rate": 2.4077451279905277e-05,
      "loss": 0.0197,
      "step": 6295
    },
    {
      "epoch": 1.535609756097561,
      "grad_norm": 0.19628240168094635,
      "learning_rate": 2.40710697894458e-05,
      "loss": 0.016,
      "step": 6296
    },
    {
      "epoch": 1.5358536585365854,
      "grad_norm": 0.09012532979249954,
      "learning_rate": 2.4064688359596173e-05,
      "loss": 0.0086,
      "step": 6297
    },
    {
      "epoch": 1.5360975609756098,
      "grad_norm": 0.062285520136356354,
      "learning_rate": 2.4058306990772778e-05,
      "loss": 0.0109,
      "step": 6298
    },
    {
      "epoch": 1.5363414634146342,
      "grad_norm": 0.12290716171264648,
      "learning_rate": 2.405192568339196e-05,
      "loss": 0.0221,
      "step": 6299
    },
    {
      "epoch": 1.5365853658536586,
      "grad_norm": 0.2054951936006546,
      "learning_rate": 2.40455444378701e-05,
      "loss": 0.0192,
      "step": 6300
    },
    {
      "epoch": 1.536829268292683,
      "grad_norm": 0.08039981871843338,
      "learning_rate": 2.4039163254623532e-05,
      "loss": 0.0098,
      "step": 6301
    },
    {
      "epoch": 1.5370731707317074,
      "grad_norm": 0.14757274091243744,
      "learning_rate": 2.403278213406862e-05,
      "loss": 0.0181,
      "step": 6302
    },
    {
      "epoch": 1.5373170731707317,
      "grad_norm": 0.13769002258777618,
      "learning_rate": 2.402640107662172e-05,
      "loss": 0.0172,
      "step": 6303
    },
    {
      "epoch": 1.5375609756097561,
      "grad_norm": 0.12889860570430756,
      "learning_rate": 2.4020020082699162e-05,
      "loss": 0.0238,
      "step": 6304
    },
    {
      "epoch": 1.5378048780487805,
      "grad_norm": 0.11657936871051788,
      "learning_rate": 2.4013639152717296e-05,
      "loss": 0.0236,
      "step": 6305
    },
    {
      "epoch": 1.538048780487805,
      "grad_norm": 0.1518828570842743,
      "learning_rate": 2.4007258287092454e-05,
      "loss": 0.017,
      "step": 6306
    },
    {
      "epoch": 1.5382926829268293,
      "grad_norm": 0.07396256923675537,
      "learning_rate": 2.4000877486240977e-05,
      "loss": 0.0158,
      "step": 6307
    },
    {
      "epoch": 1.5385365853658537,
      "grad_norm": 0.09635597467422485,
      "learning_rate": 2.399449675057918e-05,
      "loss": 0.0182,
      "step": 6308
    },
    {
      "epoch": 1.538780487804878,
      "grad_norm": 0.10115950554609299,
      "learning_rate": 2.3988116080523386e-05,
      "loss": 0.023,
      "step": 6309
    },
    {
      "epoch": 1.5390243902439025,
      "grad_norm": 0.07639197260141373,
      "learning_rate": 2.3981735476489928e-05,
      "loss": 0.0215,
      "step": 6310
    },
    {
      "epoch": 1.5392682926829269,
      "grad_norm": 0.16063697636127472,
      "learning_rate": 2.3975354938895115e-05,
      "loss": 0.0236,
      "step": 6311
    },
    {
      "epoch": 1.5395121951219513,
      "grad_norm": 0.09670878201723099,
      "learning_rate": 2.3968974468155248e-05,
      "loss": 0.0101,
      "step": 6312
    },
    {
      "epoch": 1.5397560975609756,
      "grad_norm": 0.10595420002937317,
      "learning_rate": 2.3962594064686643e-05,
      "loss": 0.0199,
      "step": 6313
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.14797089993953705,
      "learning_rate": 2.39562137289056e-05,
      "loss": 0.0185,
      "step": 6314
    },
    {
      "epoch": 1.5402439024390244,
      "grad_norm": 0.12636931240558624,
      "learning_rate": 2.3949833461228423e-05,
      "loss": 0.0129,
      "step": 6315
    },
    {
      "epoch": 1.5404878048780488,
      "grad_norm": 0.1551915407180786,
      "learning_rate": 2.394345326207139e-05,
      "loss": 0.0213,
      "step": 6316
    },
    {
      "epoch": 1.5407317073170732,
      "grad_norm": 0.14285874366760254,
      "learning_rate": 2.3937073131850803e-05,
      "loss": 0.0177,
      "step": 6317
    },
    {
      "epoch": 1.5409756097560976,
      "grad_norm": 0.05725200101733208,
      "learning_rate": 2.3930693070982935e-05,
      "loss": 0.0184,
      "step": 6318
    },
    {
      "epoch": 1.541219512195122,
      "grad_norm": 0.09843603521585464,
      "learning_rate": 2.392431307988407e-05,
      "loss": 0.0139,
      "step": 6319
    },
    {
      "epoch": 1.5414634146341464,
      "grad_norm": 0.08353850245475769,
      "learning_rate": 2.391793315897049e-05,
      "loss": 0.01,
      "step": 6320
    },
    {
      "epoch": 1.5417073170731708,
      "grad_norm": 0.20985402166843414,
      "learning_rate": 2.391155330865845e-05,
      "loss": 0.0278,
      "step": 6321
    },
    {
      "epoch": 1.5419512195121952,
      "grad_norm": 0.09953075647354126,
      "learning_rate": 2.3905173529364227e-05,
      "loss": 0.0154,
      "step": 6322
    },
    {
      "epoch": 1.5421951219512195,
      "grad_norm": 0.07427503913640976,
      "learning_rate": 2.3898793821504082e-05,
      "loss": 0.012,
      "step": 6323
    },
    {
      "epoch": 1.542439024390244,
      "grad_norm": 0.1434379369020462,
      "learning_rate": 2.3892414185494273e-05,
      "loss": 0.0233,
      "step": 6324
    },
    {
      "epoch": 1.5426829268292683,
      "grad_norm": 0.16547881066799164,
      "learning_rate": 2.3886034621751038e-05,
      "loss": 0.0345,
      "step": 6325
    },
    {
      "epoch": 1.5429268292682927,
      "grad_norm": 0.11401966959238052,
      "learning_rate": 2.387965513069064e-05,
      "loss": 0.0224,
      "step": 6326
    },
    {
      "epoch": 1.5431707317073171,
      "grad_norm": 0.19167865812778473,
      "learning_rate": 2.387327571272931e-05,
      "loss": 0.0334,
      "step": 6327
    },
    {
      "epoch": 1.5434146341463415,
      "grad_norm": 0.21149933338165283,
      "learning_rate": 2.3866896368283297e-05,
      "loss": 0.0187,
      "step": 6328
    },
    {
      "epoch": 1.543658536585366,
      "grad_norm": 0.08174345642328262,
      "learning_rate": 2.3860517097768825e-05,
      "loss": 0.0132,
      "step": 6329
    },
    {
      "epoch": 1.5439024390243903,
      "grad_norm": 0.2899557948112488,
      "learning_rate": 2.3854137901602124e-05,
      "loss": 0.027,
      "step": 6330
    },
    {
      "epoch": 1.5441463414634147,
      "grad_norm": 0.12742872536182404,
      "learning_rate": 2.3847758780199412e-05,
      "loss": 0.0127,
      "step": 6331
    },
    {
      "epoch": 1.544390243902439,
      "grad_norm": 0.15092945098876953,
      "learning_rate": 2.384137973397692e-05,
      "loss": 0.029,
      "step": 6332
    },
    {
      "epoch": 1.5446341463414635,
      "grad_norm": 0.16257549822330475,
      "learning_rate": 2.383500076335086e-05,
      "loss": 0.0219,
      "step": 6333
    },
    {
      "epoch": 1.5448780487804878,
      "grad_norm": 0.09920191019773483,
      "learning_rate": 2.3828621868737423e-05,
      "loss": 0.0166,
      "step": 6334
    },
    {
      "epoch": 1.5451219512195122,
      "grad_norm": 0.1572604775428772,
      "learning_rate": 2.382224305055282e-05,
      "loss": 0.0195,
      "step": 6335
    },
    {
      "epoch": 1.5453658536585366,
      "grad_norm": 0.06283045560121536,
      "learning_rate": 2.3815864309213255e-05,
      "loss": 0.0104,
      "step": 6336
    },
    {
      "epoch": 1.545609756097561,
      "grad_norm": 0.14711487293243408,
      "learning_rate": 2.3809485645134928e-05,
      "loss": 0.0308,
      "step": 6337
    },
    {
      "epoch": 1.5458536585365854,
      "grad_norm": 0.2956613302230835,
      "learning_rate": 2.3803107058734008e-05,
      "loss": 0.0259,
      "step": 6338
    },
    {
      "epoch": 1.5460975609756098,
      "grad_norm": 0.11353069543838501,
      "learning_rate": 2.379672855042669e-05,
      "loss": 0.0179,
      "step": 6339
    },
    {
      "epoch": 1.5463414634146342,
      "grad_norm": 0.12996645271778107,
      "learning_rate": 2.379035012062915e-05,
      "loss": 0.0141,
      "step": 6340
    },
    {
      "epoch": 1.5465853658536586,
      "grad_norm": 0.07958287745714188,
      "learning_rate": 2.3783971769757564e-05,
      "loss": 0.0102,
      "step": 6341
    },
    {
      "epoch": 1.546829268292683,
      "grad_norm": 0.12793953716754913,
      "learning_rate": 2.3777593498228095e-05,
      "loss": 0.0189,
      "step": 6342
    },
    {
      "epoch": 1.5470731707317074,
      "grad_norm": 0.11642773449420929,
      "learning_rate": 2.3771215306456908e-05,
      "loss": 0.02,
      "step": 6343
    },
    {
      "epoch": 1.5473170731707317,
      "grad_norm": 0.1624215841293335,
      "learning_rate": 2.376483719486016e-05,
      "loss": 0.023,
      "step": 6344
    },
    {
      "epoch": 1.5475609756097561,
      "grad_norm": 0.12149971723556519,
      "learning_rate": 2.3758459163854015e-05,
      "loss": 0.0123,
      "step": 6345
    },
    {
      "epoch": 1.5478048780487805,
      "grad_norm": 0.10014866292476654,
      "learning_rate": 2.3752081213854596e-05,
      "loss": 0.0144,
      "step": 6346
    },
    {
      "epoch": 1.548048780487805,
      "grad_norm": 0.18156303465366364,
      "learning_rate": 2.374570334527806e-05,
      "loss": 0.0179,
      "step": 6347
    },
    {
      "epoch": 1.5482926829268293,
      "grad_norm": 0.17994295060634613,
      "learning_rate": 2.373932555854054e-05,
      "loss": 0.016,
      "step": 6348
    },
    {
      "epoch": 1.5485365853658537,
      "grad_norm": 0.11385461688041687,
      "learning_rate": 2.373294785405818e-05,
      "loss": 0.0191,
      "step": 6349
    },
    {
      "epoch": 1.548780487804878,
      "grad_norm": 0.12936222553253174,
      "learning_rate": 2.3726570232247085e-05,
      "loss": 0.0234,
      "step": 6350
    },
    {
      "epoch": 1.5490243902439025,
      "grad_norm": 0.08662735670804977,
      "learning_rate": 2.3720192693523382e-05,
      "loss": 0.0209,
      "step": 6351
    },
    {
      "epoch": 1.5492682926829269,
      "grad_norm": 0.08642496168613434,
      "learning_rate": 2.371381523830319e-05,
      "loss": 0.018,
      "step": 6352
    },
    {
      "epoch": 1.5495121951219513,
      "grad_norm": 0.1364983171224594,
      "learning_rate": 2.3707437867002614e-05,
      "loss": 0.0264,
      "step": 6353
    },
    {
      "epoch": 1.5497560975609757,
      "grad_norm": 0.17595531046390533,
      "learning_rate": 2.3701060580037764e-05,
      "loss": 0.0199,
      "step": 6354
    },
    {
      "epoch": 1.55,
      "grad_norm": 0.10036908090114594,
      "learning_rate": 2.369468337782473e-05,
      "loss": 0.015,
      "step": 6355
    },
    {
      "epoch": 1.5502439024390244,
      "grad_norm": 0.08544532209634781,
      "learning_rate": 2.368830626077961e-05,
      "loss": 0.0089,
      "step": 6356
    },
    {
      "epoch": 1.5504878048780488,
      "grad_norm": 0.16323770582675934,
      "learning_rate": 2.368192922931849e-05,
      "loss": 0.0332,
      "step": 6357
    },
    {
      "epoch": 1.5507317073170732,
      "grad_norm": 0.07115403562784195,
      "learning_rate": 2.367555228385746e-05,
      "loss": 0.0234,
      "step": 6358
    },
    {
      "epoch": 1.5509756097560976,
      "grad_norm": 0.23633283376693726,
      "learning_rate": 2.3669175424812585e-05,
      "loss": 0.022,
      "step": 6359
    },
    {
      "epoch": 1.551219512195122,
      "grad_norm": 0.06468843668699265,
      "learning_rate": 2.3662798652599937e-05,
      "loss": 0.0128,
      "step": 6360
    },
    {
      "epoch": 1.5514634146341464,
      "grad_norm": 0.07486201822757721,
      "learning_rate": 2.3656421967635582e-05,
      "loss": 0.0231,
      "step": 6361
    },
    {
      "epoch": 1.5517073170731708,
      "grad_norm": 0.20732805132865906,
      "learning_rate": 2.365004537033559e-05,
      "loss": 0.0401,
      "step": 6362
    },
    {
      "epoch": 1.5519512195121952,
      "grad_norm": 0.2081502079963684,
      "learning_rate": 2.3643668861115996e-05,
      "loss": 0.0167,
      "step": 6363
    },
    {
      "epoch": 1.5521951219512196,
      "grad_norm": 0.06340596079826355,
      "learning_rate": 2.363729244039286e-05,
      "loss": 0.0128,
      "step": 6364
    },
    {
      "epoch": 1.552439024390244,
      "grad_norm": 0.14526624977588654,
      "learning_rate": 2.3630916108582222e-05,
      "loss": 0.0127,
      "step": 6365
    },
    {
      "epoch": 1.5526829268292683,
      "grad_norm": 0.13200879096984863,
      "learning_rate": 2.362453986610012e-05,
      "loss": 0.027,
      "step": 6366
    },
    {
      "epoch": 1.5529268292682927,
      "grad_norm": 0.10456695407629013,
      "learning_rate": 2.361816371336258e-05,
      "loss": 0.0158,
      "step": 6367
    },
    {
      "epoch": 1.5531707317073171,
      "grad_norm": 0.20489786565303802,
      "learning_rate": 2.3611787650785623e-05,
      "loss": 0.0315,
      "step": 6368
    },
    {
      "epoch": 1.5534146341463415,
      "grad_norm": 0.08685984462499619,
      "learning_rate": 2.3605411678785268e-05,
      "loss": 0.0159,
      "step": 6369
    },
    {
      "epoch": 1.553658536585366,
      "grad_norm": 0.09320056438446045,
      "learning_rate": 2.3599035797777535e-05,
      "loss": 0.0149,
      "step": 6370
    },
    {
      "epoch": 1.5539024390243903,
      "grad_norm": 0.10202553123235703,
      "learning_rate": 2.3592660008178434e-05,
      "loss": 0.0168,
      "step": 6371
    },
    {
      "epoch": 1.5541463414634147,
      "grad_norm": 0.12931041419506073,
      "learning_rate": 2.3586284310403952e-05,
      "loss": 0.0211,
      "step": 6372
    },
    {
      "epoch": 1.554390243902439,
      "grad_norm": 0.13395416736602783,
      "learning_rate": 2.357990870487009e-05,
      "loss": 0.008,
      "step": 6373
    },
    {
      "epoch": 1.5546341463414635,
      "grad_norm": 0.07334352284669876,
      "learning_rate": 2.3573533191992842e-05,
      "loss": 0.0195,
      "step": 6374
    },
    {
      "epoch": 1.5548780487804879,
      "grad_norm": 0.14602968096733093,
      "learning_rate": 2.3567157772188187e-05,
      "loss": 0.0252,
      "step": 6375
    },
    {
      "epoch": 1.5551219512195122,
      "grad_norm": 0.09593649208545685,
      "learning_rate": 2.35607824458721e-05,
      "loss": 0.021,
      "step": 6376
    },
    {
      "epoch": 1.5553658536585366,
      "grad_norm": 0.15830808877944946,
      "learning_rate": 2.3554407213460548e-05,
      "loss": 0.019,
      "step": 6377
    },
    {
      "epoch": 1.555609756097561,
      "grad_norm": 0.24128514528274536,
      "learning_rate": 2.354803207536951e-05,
      "loss": 0.0218,
      "step": 6378
    },
    {
      "epoch": 1.5558536585365854,
      "grad_norm": 0.06644456088542938,
      "learning_rate": 2.3541657032014922e-05,
      "loss": 0.0081,
      "step": 6379
    },
    {
      "epoch": 1.5560975609756098,
      "grad_norm": 0.07982959598302841,
      "learning_rate": 2.3535282083812754e-05,
      "loss": 0.0143,
      "step": 6380
    },
    {
      "epoch": 1.5563414634146342,
      "grad_norm": 0.2827763557434082,
      "learning_rate": 2.3528907231178944e-05,
      "loss": 0.0204,
      "step": 6381
    },
    {
      "epoch": 1.5565853658536586,
      "grad_norm": 0.14448867738246918,
      "learning_rate": 2.3522532474529444e-05,
      "loss": 0.0282,
      "step": 6382
    },
    {
      "epoch": 1.556829268292683,
      "grad_norm": 0.17212235927581787,
      "learning_rate": 2.3516157814280168e-05,
      "loss": 0.016,
      "step": 6383
    },
    {
      "epoch": 1.5570731707317074,
      "grad_norm": 0.10306083410978317,
      "learning_rate": 2.3509783250847056e-05,
      "loss": 0.0174,
      "step": 6384
    },
    {
      "epoch": 1.5573170731707318,
      "grad_norm": 0.11536981910467148,
      "learning_rate": 2.350340878464602e-05,
      "loss": 0.0233,
      "step": 6385
    },
    {
      "epoch": 1.5575609756097561,
      "grad_norm": 0.18324998021125793,
      "learning_rate": 2.349703441609298e-05,
      "loss": 0.0201,
      "step": 6386
    },
    {
      "epoch": 1.5578048780487805,
      "grad_norm": 0.11745493859052658,
      "learning_rate": 2.3490660145603845e-05,
      "loss": 0.0255,
      "step": 6387
    },
    {
      "epoch": 1.558048780487805,
      "grad_norm": 0.09555881470441818,
      "learning_rate": 2.3484285973594517e-05,
      "loss": 0.0212,
      "step": 6388
    },
    {
      "epoch": 1.5582926829268293,
      "grad_norm": 0.24437236785888672,
      "learning_rate": 2.3477911900480883e-05,
      "loss": 0.0343,
      "step": 6389
    },
    {
      "epoch": 1.5585365853658537,
      "grad_norm": 0.09362263977527618,
      "learning_rate": 2.347153792667884e-05,
      "loss": 0.023,
      "step": 6390
    },
    {
      "epoch": 1.558780487804878,
      "grad_norm": 0.12971991300582886,
      "learning_rate": 2.3465164052604268e-05,
      "loss": 0.0228,
      "step": 6391
    },
    {
      "epoch": 1.5590243902439025,
      "grad_norm": 0.0605536513030529,
      "learning_rate": 2.345879027867305e-05,
      "loss": 0.0125,
      "step": 6392
    },
    {
      "epoch": 1.5592682926829269,
      "grad_norm": 0.15699726343154907,
      "learning_rate": 2.3452416605301043e-05,
      "loss": 0.0199,
      "step": 6393
    },
    {
      "epoch": 1.5595121951219513,
      "grad_norm": 0.19079424440860748,
      "learning_rate": 2.344604303290411e-05,
      "loss": 0.0325,
      "step": 6394
    },
    {
      "epoch": 1.5597560975609757,
      "grad_norm": 0.19497402012348175,
      "learning_rate": 2.3439669561898124e-05,
      "loss": 0.0228,
      "step": 6395
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.09044940024614334,
      "learning_rate": 2.3433296192698912e-05,
      "loss": 0.0153,
      "step": 6396
    },
    {
      "epoch": 1.5602439024390244,
      "grad_norm": 0.10846824198961258,
      "learning_rate": 2.3426922925722326e-05,
      "loss": 0.0143,
      "step": 6397
    },
    {
      "epoch": 1.5604878048780488,
      "grad_norm": 0.19362689554691315,
      "learning_rate": 2.342054976138421e-05,
      "loss": 0.0168,
      "step": 6398
    },
    {
      "epoch": 1.5607317073170732,
      "grad_norm": 0.21865177154541016,
      "learning_rate": 2.3414176700100393e-05,
      "loss": 0.0342,
      "step": 6399
    },
    {
      "epoch": 1.5609756097560976,
      "grad_norm": 0.1162237748503685,
      "learning_rate": 2.3407803742286683e-05,
      "loss": 0.0196,
      "step": 6400
    },
    {
      "epoch": 1.561219512195122,
      "grad_norm": 0.0663745328783989,
      "learning_rate": 2.3401430888358907e-05,
      "loss": 0.0158,
      "step": 6401
    },
    {
      "epoch": 1.5614634146341464,
      "grad_norm": 0.08489543944597244,
      "learning_rate": 2.339505813873287e-05,
      "loss": 0.0171,
      "step": 6402
    },
    {
      "epoch": 1.5617073170731708,
      "grad_norm": 0.08691786974668503,
      "learning_rate": 2.3388685493824377e-05,
      "loss": 0.0255,
      "step": 6403
    },
    {
      "epoch": 1.5619512195121952,
      "grad_norm": 0.09533993899822235,
      "learning_rate": 2.338231295404922e-05,
      "loss": 0.017,
      "step": 6404
    },
    {
      "epoch": 1.5621951219512196,
      "grad_norm": 0.15962804853916168,
      "learning_rate": 2.3375940519823195e-05,
      "loss": 0.0329,
      "step": 6405
    },
    {
      "epoch": 1.562439024390244,
      "grad_norm": 0.08063182234764099,
      "learning_rate": 2.3369568191562074e-05,
      "loss": 0.0131,
      "step": 6406
    },
    {
      "epoch": 1.5626829268292681,
      "grad_norm": 0.19874727725982666,
      "learning_rate": 2.336319596968164e-05,
      "loss": 0.0129,
      "step": 6407
    },
    {
      "epoch": 1.5629268292682927,
      "grad_norm": 0.12419422715902328,
      "learning_rate": 2.3356823854597662e-05,
      "loss": 0.0209,
      "step": 6408
    },
    {
      "epoch": 1.563170731707317,
      "grad_norm": 0.0801347941160202,
      "learning_rate": 2.335045184672589e-05,
      "loss": 0.0081,
      "step": 6409
    },
    {
      "epoch": 1.5634146341463415,
      "grad_norm": 0.11604909598827362,
      "learning_rate": 2.3344079946482088e-05,
      "loss": 0.018,
      "step": 6410
    },
    {
      "epoch": 1.5636585365853657,
      "grad_norm": 0.1326606720685959,
      "learning_rate": 2.3337708154282e-05,
      "loss": 0.0217,
      "step": 6411
    },
    {
      "epoch": 1.5639024390243903,
      "grad_norm": 0.0544661283493042,
      "learning_rate": 2.333133647054137e-05,
      "loss": 0.0087,
      "step": 6412
    },
    {
      "epoch": 1.5641463414634145,
      "grad_norm": 0.16672199964523315,
      "learning_rate": 2.3324964895675924e-05,
      "loss": 0.0183,
      "step": 6413
    },
    {
      "epoch": 1.564390243902439,
      "grad_norm": 0.12314774841070175,
      "learning_rate": 2.331859343010139e-05,
      "loss": 0.0178,
      "step": 6414
    },
    {
      "epoch": 1.5646341463414632,
      "grad_norm": 0.13255935907363892,
      "learning_rate": 2.3312222074233497e-05,
      "loss": 0.0096,
      "step": 6415
    },
    {
      "epoch": 1.5648780487804879,
      "grad_norm": 0.12227068096399307,
      "learning_rate": 2.3305850828487937e-05,
      "loss": 0.0164,
      "step": 6416
    },
    {
      "epoch": 1.565121951219512,
      "grad_norm": 0.06966502964496613,
      "learning_rate": 2.3299479693280423e-05,
      "loss": 0.0112,
      "step": 6417
    },
    {
      "epoch": 1.5653658536585366,
      "grad_norm": 0.22679027915000916,
      "learning_rate": 2.329310866902666e-05,
      "loss": 0.037,
      "step": 6418
    },
    {
      "epoch": 1.5656097560975608,
      "grad_norm": 0.13055305182933807,
      "learning_rate": 2.3286737756142327e-05,
      "loss": 0.0097,
      "step": 6419
    },
    {
      "epoch": 1.5658536585365854,
      "grad_norm": 0.09345788508653641,
      "learning_rate": 2.3280366955043108e-05,
      "loss": 0.0217,
      "step": 6420
    },
    {
      "epoch": 1.5660975609756096,
      "grad_norm": 0.16859027743339539,
      "learning_rate": 2.3273996266144688e-05,
      "loss": 0.0179,
      "step": 6421
    },
    {
      "epoch": 1.5663414634146342,
      "grad_norm": 0.11200126260519028,
      "learning_rate": 2.326762568986272e-05,
      "loss": 0.0145,
      "step": 6422
    },
    {
      "epoch": 1.5665853658536584,
      "grad_norm": 0.18823912739753723,
      "learning_rate": 2.3261255226612878e-05,
      "loss": 0.0131,
      "step": 6423
    },
    {
      "epoch": 1.566829268292683,
      "grad_norm": 0.1575833112001419,
      "learning_rate": 2.3254884876810803e-05,
      "loss": 0.0259,
      "step": 6424
    },
    {
      "epoch": 1.5670731707317072,
      "grad_norm": 0.1621248573064804,
      "learning_rate": 2.3248514640872156e-05,
      "loss": 0.0301,
      "step": 6425
    },
    {
      "epoch": 1.5673170731707318,
      "grad_norm": 0.11190182715654373,
      "learning_rate": 2.324214451921256e-05,
      "loss": 0.0216,
      "step": 6426
    },
    {
      "epoch": 1.567560975609756,
      "grad_norm": 0.10492052137851715,
      "learning_rate": 2.3235774512247655e-05,
      "loss": 0.0189,
      "step": 6427
    },
    {
      "epoch": 1.5678048780487805,
      "grad_norm": 0.13132435083389282,
      "learning_rate": 2.3229404620393062e-05,
      "loss": 0.0363,
      "step": 6428
    },
    {
      "epoch": 1.5680487804878047,
      "grad_norm": 0.13197670876979828,
      "learning_rate": 2.32230348440644e-05,
      "loss": 0.0304,
      "step": 6429
    },
    {
      "epoch": 1.5682926829268293,
      "grad_norm": 0.08617432415485382,
      "learning_rate": 2.3216665183677274e-05,
      "loss": 0.0249,
      "step": 6430
    },
    {
      "epoch": 1.5685365853658535,
      "grad_norm": 0.10704970359802246,
      "learning_rate": 2.321029563964728e-05,
      "loss": 0.0256,
      "step": 6431
    },
    {
      "epoch": 1.568780487804878,
      "grad_norm": 0.07704494893550873,
      "learning_rate": 2.3203926212390033e-05,
      "loss": 0.0123,
      "step": 6432
    },
    {
      "epoch": 1.5690243902439023,
      "grad_norm": 0.08047648519277573,
      "learning_rate": 2.3197556902321084e-05,
      "loss": 0.0143,
      "step": 6433
    },
    {
      "epoch": 1.569268292682927,
      "grad_norm": 0.27525070309638977,
      "learning_rate": 2.319118770985604e-05,
      "loss": 0.0167,
      "step": 6434
    },
    {
      "epoch": 1.569512195121951,
      "grad_norm": 0.12472217530012131,
      "learning_rate": 2.3184818635410454e-05,
      "loss": 0.0202,
      "step": 6435
    },
    {
      "epoch": 1.5697560975609757,
      "grad_norm": 0.12241790443658829,
      "learning_rate": 2.3178449679399898e-05,
      "loss": 0.0171,
      "step": 6436
    },
    {
      "epoch": 1.5699999999999998,
      "grad_norm": 0.12379495054483414,
      "learning_rate": 2.317208084223992e-05,
      "loss": 0.0199,
      "step": 6437
    },
    {
      "epoch": 1.5702439024390245,
      "grad_norm": 0.24861742556095123,
      "learning_rate": 2.3165712124346077e-05,
      "loss": 0.0368,
      "step": 6438
    },
    {
      "epoch": 1.5704878048780486,
      "grad_norm": 0.13029319047927856,
      "learning_rate": 2.31593435261339e-05,
      "loss": 0.0183,
      "step": 6439
    },
    {
      "epoch": 1.5707317073170732,
      "grad_norm": 0.09373673796653748,
      "learning_rate": 2.315297504801892e-05,
      "loss": 0.0133,
      "step": 6440
    },
    {
      "epoch": 1.5709756097560974,
      "grad_norm": 0.1280965805053711,
      "learning_rate": 2.3146606690416666e-05,
      "loss": 0.026,
      "step": 6441
    },
    {
      "epoch": 1.571219512195122,
      "grad_norm": 0.11498569697141647,
      "learning_rate": 2.3140238453742658e-05,
      "loss": 0.0116,
      "step": 6442
    },
    {
      "epoch": 1.5714634146341462,
      "grad_norm": 0.1474163830280304,
      "learning_rate": 2.313387033841239e-05,
      "loss": 0.019,
      "step": 6443
    },
    {
      "epoch": 1.5717073170731708,
      "grad_norm": 0.11296913027763367,
      "learning_rate": 2.3127502344841374e-05,
      "loss": 0.0306,
      "step": 6444
    },
    {
      "epoch": 1.571951219512195,
      "grad_norm": 0.15215423703193665,
      "learning_rate": 2.3121134473445093e-05,
      "loss": 0.0193,
      "step": 6445
    },
    {
      "epoch": 1.5721951219512196,
      "grad_norm": 0.08820193260908127,
      "learning_rate": 2.3114766724639042e-05,
      "loss": 0.0164,
      "step": 6446
    },
    {
      "epoch": 1.5724390243902437,
      "grad_norm": 0.19089001417160034,
      "learning_rate": 2.3108399098838683e-05,
      "loss": 0.0124,
      "step": 6447
    },
    {
      "epoch": 1.5726829268292684,
      "grad_norm": 0.09980756789445877,
      "learning_rate": 2.310203159645951e-05,
      "loss": 0.0262,
      "step": 6448
    },
    {
      "epoch": 1.5729268292682925,
      "grad_norm": 0.07784666866064072,
      "learning_rate": 2.3095664217916947e-05,
      "loss": 0.0164,
      "step": 6449
    },
    {
      "epoch": 1.5731707317073171,
      "grad_norm": 0.09369322657585144,
      "learning_rate": 2.308929696362647e-05,
      "loss": 0.0144,
      "step": 6450
    },
    {
      "epoch": 1.5734146341463413,
      "grad_norm": 0.10485455393791199,
      "learning_rate": 2.308292983400352e-05,
      "loss": 0.023,
      "step": 6451
    },
    {
      "epoch": 1.573658536585366,
      "grad_norm": 0.2041923701763153,
      "learning_rate": 2.307656282946352e-05,
      "loss": 0.0133,
      "step": 6452
    },
    {
      "epoch": 1.57390243902439,
      "grad_norm": 0.06889898329973221,
      "learning_rate": 2.3070195950421912e-05,
      "loss": 0.0084,
      "step": 6453
    },
    {
      "epoch": 1.5741463414634147,
      "grad_norm": 0.15865802764892578,
      "learning_rate": 2.306382919729411e-05,
      "loss": 0.0357,
      "step": 6454
    },
    {
      "epoch": 1.5743902439024389,
      "grad_norm": 0.22876067459583282,
      "learning_rate": 2.305746257049553e-05,
      "loss": 0.0299,
      "step": 6455
    },
    {
      "epoch": 1.5746341463414635,
      "grad_norm": 0.18431226909160614,
      "learning_rate": 2.3051096070441567e-05,
      "loss": 0.0193,
      "step": 6456
    },
    {
      "epoch": 1.5748780487804876,
      "grad_norm": 0.16779829561710358,
      "learning_rate": 2.3044729697547616e-05,
      "loss": 0.025,
      "step": 6457
    },
    {
      "epoch": 1.5751219512195123,
      "grad_norm": 0.2504270076751709,
      "learning_rate": 2.303836345222907e-05,
      "loss": 0.0281,
      "step": 6458
    },
    {
      "epoch": 1.5753658536585364,
      "grad_norm": 0.10619299113750458,
      "learning_rate": 2.3031997334901307e-05,
      "loss": 0.0188,
      "step": 6459
    },
    {
      "epoch": 1.575609756097561,
      "grad_norm": 0.11388114839792252,
      "learning_rate": 2.302563134597969e-05,
      "loss": 0.0128,
      "step": 6460
    },
    {
      "epoch": 1.5758536585365852,
      "grad_norm": 0.1292850822210312,
      "learning_rate": 2.3019265485879582e-05,
      "loss": 0.026,
      "step": 6461
    },
    {
      "epoch": 1.5760975609756098,
      "grad_norm": 0.25038817524909973,
      "learning_rate": 2.3012899755016338e-05,
      "loss": 0.0285,
      "step": 6462
    },
    {
      "epoch": 1.576341463414634,
      "grad_norm": 0.08846566081047058,
      "learning_rate": 2.3006534153805307e-05,
      "loss": 0.0195,
      "step": 6463
    },
    {
      "epoch": 1.5765853658536586,
      "grad_norm": 0.12865188717842102,
      "learning_rate": 2.3000168682661812e-05,
      "loss": 0.011,
      "step": 6464
    },
    {
      "epoch": 1.5768292682926828,
      "grad_norm": 0.06534819304943085,
      "learning_rate": 2.2993803342001202e-05,
      "loss": 0.0115,
      "step": 6465
    },
    {
      "epoch": 1.5770731707317074,
      "grad_norm": 0.09208358079195023,
      "learning_rate": 2.298743813223877e-05,
      "loss": 0.014,
      "step": 6466
    },
    {
      "epoch": 1.5773170731707316,
      "grad_norm": 0.08379607647657394,
      "learning_rate": 2.298107305378984e-05,
      "loss": 0.022,
      "step": 6467
    },
    {
      "epoch": 1.5775609756097562,
      "grad_norm": 0.13954144716262817,
      "learning_rate": 2.297470810706972e-05,
      "loss": 0.0137,
      "step": 6468
    },
    {
      "epoch": 1.5778048780487803,
      "grad_norm": 0.13195456564426422,
      "learning_rate": 2.296834329249369e-05,
      "loss": 0.0212,
      "step": 6469
    },
    {
      "epoch": 1.578048780487805,
      "grad_norm": 0.10835660248994827,
      "learning_rate": 2.2961978610477036e-05,
      "loss": 0.0226,
      "step": 6470
    },
    {
      "epoch": 1.5782926829268291,
      "grad_norm": 0.122840516269207,
      "learning_rate": 2.295561406143504e-05,
      "loss": 0.0173,
      "step": 6471
    },
    {
      "epoch": 1.5785365853658537,
      "grad_norm": 0.1434735655784607,
      "learning_rate": 2.294924964578298e-05,
      "loss": 0.0222,
      "step": 6472
    },
    {
      "epoch": 1.578780487804878,
      "grad_norm": 0.13383565843105316,
      "learning_rate": 2.294288536393609e-05,
      "loss": 0.0261,
      "step": 6473
    },
    {
      "epoch": 1.5790243902439025,
      "grad_norm": 0.10886415094137192,
      "learning_rate": 2.2936521216309633e-05,
      "loss": 0.0313,
      "step": 6474
    },
    {
      "epoch": 1.5792682926829267,
      "grad_norm": 0.21024852991104126,
      "learning_rate": 2.2930157203318853e-05,
      "loss": 0.0278,
      "step": 6475
    },
    {
      "epoch": 1.5795121951219513,
      "grad_norm": 0.05839641019701958,
      "learning_rate": 2.2923793325378985e-05,
      "loss": 0.0243,
      "step": 6476
    },
    {
      "epoch": 1.5797560975609755,
      "grad_norm": 0.20599278807640076,
      "learning_rate": 2.291742958290524e-05,
      "loss": 0.0176,
      "step": 6477
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.1028103306889534,
      "learning_rate": 2.2911065976312837e-05,
      "loss": 0.018,
      "step": 6478
    },
    {
      "epoch": 1.5802439024390242,
      "grad_norm": 0.10819561034440994,
      "learning_rate": 2.2904702506016988e-05,
      "loss": 0.0178,
      "step": 6479
    },
    {
      "epoch": 1.5804878048780489,
      "grad_norm": 0.08002676069736481,
      "learning_rate": 2.2898339172432886e-05,
      "loss": 0.0119,
      "step": 6480
    },
    {
      "epoch": 1.580731707317073,
      "grad_norm": 0.13538673520088196,
      "learning_rate": 2.289197597597573e-05,
      "loss": 0.0104,
      "step": 6481
    },
    {
      "epoch": 1.5809756097560976,
      "grad_norm": 0.26991957426071167,
      "learning_rate": 2.2885612917060674e-05,
      "loss": 0.0172,
      "step": 6482
    },
    {
      "epoch": 1.5812195121951218,
      "grad_norm": 0.14490725100040436,
      "learning_rate": 2.2879249996102906e-05,
      "loss": 0.0118,
      "step": 6483
    },
    {
      "epoch": 1.5814634146341464,
      "grad_norm": 0.09057147055864334,
      "learning_rate": 2.2872887213517582e-05,
      "loss": 0.0225,
      "step": 6484
    },
    {
      "epoch": 1.5817073170731706,
      "grad_norm": 0.04635104909539223,
      "learning_rate": 2.286652456971986e-05,
      "loss": 0.0113,
      "step": 6485
    },
    {
      "epoch": 1.5819512195121952,
      "grad_norm": 0.0672077164053917,
      "learning_rate": 2.2860162065124875e-05,
      "loss": 0.0138,
      "step": 6486
    },
    {
      "epoch": 1.5821951219512194,
      "grad_norm": 0.04149667173624039,
      "learning_rate": 2.2853799700147763e-05,
      "loss": 0.0108,
      "step": 6487
    },
    {
      "epoch": 1.582439024390244,
      "grad_norm": 0.15887399017810822,
      "learning_rate": 2.284743747520365e-05,
      "loss": 0.0317,
      "step": 6488
    },
    {
      "epoch": 1.5826829268292681,
      "grad_norm": 0.12188292294740677,
      "learning_rate": 2.2841075390707658e-05,
      "loss": 0.0154,
      "step": 6489
    },
    {
      "epoch": 1.5829268292682928,
      "grad_norm": 0.08493339270353317,
      "learning_rate": 2.2834713447074885e-05,
      "loss": 0.0121,
      "step": 6490
    },
    {
      "epoch": 1.583170731707317,
      "grad_norm": 0.10220513492822647,
      "learning_rate": 2.2828351644720428e-05,
      "loss": 0.0185,
      "step": 6491
    },
    {
      "epoch": 1.5834146341463415,
      "grad_norm": 0.04325983673334122,
      "learning_rate": 2.2821989984059377e-05,
      "loss": 0.0067,
      "step": 6492
    },
    {
      "epoch": 1.5836585365853657,
      "grad_norm": 0.11335323005914688,
      "learning_rate": 2.281562846550682e-05,
      "loss": 0.0248,
      "step": 6493
    },
    {
      "epoch": 1.5839024390243903,
      "grad_norm": 0.1396511346101761,
      "learning_rate": 2.280926708947781e-05,
      "loss": 0.0165,
      "step": 6494
    },
    {
      "epoch": 1.5841463414634145,
      "grad_norm": 0.13480032980442047,
      "learning_rate": 2.2802905856387422e-05,
      "loss": 0.0177,
      "step": 6495
    },
    {
      "epoch": 1.584390243902439,
      "grad_norm": 0.16689763963222504,
      "learning_rate": 2.27965447666507e-05,
      "loss": 0.0236,
      "step": 6496
    },
    {
      "epoch": 1.5846341463414633,
      "grad_norm": 0.10964097082614899,
      "learning_rate": 2.279018382068269e-05,
      "loss": 0.0199,
      "step": 6497
    },
    {
      "epoch": 1.5848780487804879,
      "grad_norm": 0.1496485024690628,
      "learning_rate": 2.278382301889842e-05,
      "loss": 0.0114,
      "step": 6498
    },
    {
      "epoch": 1.585121951219512,
      "grad_norm": 0.08970318734645844,
      "learning_rate": 2.277746236171291e-05,
      "loss": 0.0125,
      "step": 6499
    },
    {
      "epoch": 1.5853658536585367,
      "grad_norm": 0.05284741148352623,
      "learning_rate": 2.2771101849541173e-05,
      "loss": 0.0148,
      "step": 6500
    },
    {
      "epoch": 1.5856097560975608,
      "grad_norm": 0.15454785525798798,
      "learning_rate": 2.276474148279822e-05,
      "loss": 0.0098,
      "step": 6501
    },
    {
      "epoch": 1.5858536585365854,
      "grad_norm": 0.06947854906320572,
      "learning_rate": 2.2758381261899047e-05,
      "loss": 0.009,
      "step": 6502
    },
    {
      "epoch": 1.5860975609756096,
      "grad_norm": 0.06545699387788773,
      "learning_rate": 2.275202118725863e-05,
      "loss": 0.0095,
      "step": 6503
    },
    {
      "epoch": 1.5863414634146342,
      "grad_norm": 0.17042939364910126,
      "learning_rate": 2.274566125929195e-05,
      "loss": 0.0186,
      "step": 6504
    },
    {
      "epoch": 1.5865853658536584,
      "grad_norm": 0.09032254666090012,
      "learning_rate": 2.273930147841397e-05,
      "loss": 0.0236,
      "step": 6505
    },
    {
      "epoch": 1.586829268292683,
      "grad_norm": 0.12225788086652756,
      "learning_rate": 2.273294184503965e-05,
      "loss": 0.0212,
      "step": 6506
    },
    {
      "epoch": 1.5870731707317072,
      "grad_norm": 0.0842440277338028,
      "learning_rate": 2.2726582359583935e-05,
      "loss": 0.0146,
      "step": 6507
    },
    {
      "epoch": 1.5873170731707318,
      "grad_norm": 0.33298856019973755,
      "learning_rate": 2.2720223022461758e-05,
      "loss": 0.0218,
      "step": 6508
    },
    {
      "epoch": 1.587560975609756,
      "grad_norm": 0.11669360846281052,
      "learning_rate": 2.2713863834088053e-05,
      "loss": 0.0127,
      "step": 6509
    },
    {
      "epoch": 1.5878048780487806,
      "grad_norm": 0.04619940370321274,
      "learning_rate": 2.270750479487773e-05,
      "loss": 0.0084,
      "step": 6510
    },
    {
      "epoch": 1.5880487804878047,
      "grad_norm": 0.07364006340503693,
      "learning_rate": 2.2701145905245705e-05,
      "loss": 0.0222,
      "step": 6511
    },
    {
      "epoch": 1.5882926829268293,
      "grad_norm": 0.3070366382598877,
      "learning_rate": 2.269478716560687e-05,
      "loss": 0.0197,
      "step": 6512
    },
    {
      "epoch": 1.5885365853658535,
      "grad_norm": 0.157899409532547,
      "learning_rate": 2.268842857637612e-05,
      "loss": 0.0116,
      "step": 6513
    },
    {
      "epoch": 1.5887804878048781,
      "grad_norm": 0.1540059596300125,
      "learning_rate": 2.268207013796833e-05,
      "loss": 0.012,
      "step": 6514
    },
    {
      "epoch": 1.5890243902439023,
      "grad_norm": 0.14818793535232544,
      "learning_rate": 2.267571185079837e-05,
      "loss": 0.0205,
      "step": 6515
    },
    {
      "epoch": 1.589268292682927,
      "grad_norm": 0.20504626631736755,
      "learning_rate": 2.2669353715281087e-05,
      "loss": 0.0282,
      "step": 6516
    },
    {
      "epoch": 1.589512195121951,
      "grad_norm": 0.05846172198653221,
      "learning_rate": 2.266299573183134e-05,
      "loss": 0.0095,
      "step": 6517
    },
    {
      "epoch": 1.5897560975609757,
      "grad_norm": 0.1693531572818756,
      "learning_rate": 2.265663790086397e-05,
      "loss": 0.025,
      "step": 6518
    },
    {
      "epoch": 1.5899999999999999,
      "grad_norm": 0.1602349728345871,
      "learning_rate": 2.2650280222793803e-05,
      "loss": 0.0153,
      "step": 6519
    },
    {
      "epoch": 1.5902439024390245,
      "grad_norm": 0.14408455789089203,
      "learning_rate": 2.264392269803566e-05,
      "loss": 0.0286,
      "step": 6520
    },
    {
      "epoch": 1.5904878048780486,
      "grad_norm": 0.08605773001909256,
      "learning_rate": 2.2637565327004343e-05,
      "loss": 0.0099,
      "step": 6521
    },
    {
      "epoch": 1.5907317073170733,
      "grad_norm": 0.0838267132639885,
      "learning_rate": 2.263120811011466e-05,
      "loss": 0.018,
      "step": 6522
    },
    {
      "epoch": 1.5909756097560974,
      "grad_norm": 0.15466342866420746,
      "learning_rate": 2.2624851047781396e-05,
      "loss": 0.0063,
      "step": 6523
    },
    {
      "epoch": 1.591219512195122,
      "grad_norm": 0.1039498969912529,
      "learning_rate": 2.261849414041933e-05,
      "loss": 0.0163,
      "step": 6524
    },
    {
      "epoch": 1.5914634146341462,
      "grad_norm": 0.09283324331045151,
      "learning_rate": 2.261213738844323e-05,
      "loss": 0.0228,
      "step": 6525
    },
    {
      "epoch": 1.5917073170731708,
      "grad_norm": 0.06996576488018036,
      "learning_rate": 2.2605780792267857e-05,
      "loss": 0.0116,
      "step": 6526
    },
    {
      "epoch": 1.591951219512195,
      "grad_norm": 0.21820203959941864,
      "learning_rate": 2.2599424352307957e-05,
      "loss": 0.0264,
      "step": 6527
    },
    {
      "epoch": 1.5921951219512196,
      "grad_norm": 0.09415880590677261,
      "learning_rate": 2.259306806897827e-05,
      "loss": 0.008,
      "step": 6528
    },
    {
      "epoch": 1.5924390243902438,
      "grad_norm": 0.12290177494287491,
      "learning_rate": 2.258671194269352e-05,
      "loss": 0.0293,
      "step": 6529
    },
    {
      "epoch": 1.5926829268292684,
      "grad_norm": 0.13310469686985016,
      "learning_rate": 2.2580355973868435e-05,
      "loss": 0.0217,
      "step": 6530
    },
    {
      "epoch": 1.5929268292682925,
      "grad_norm": 0.07473614066839218,
      "learning_rate": 2.2574000162917715e-05,
      "loss": 0.0188,
      "step": 6531
    },
    {
      "epoch": 1.5931707317073172,
      "grad_norm": 0.3111083209514618,
      "learning_rate": 2.2567644510256057e-05,
      "loss": 0.0193,
      "step": 6532
    },
    {
      "epoch": 1.5934146341463413,
      "grad_norm": 0.07555314898490906,
      "learning_rate": 2.2561289016298144e-05,
      "loss": 0.0125,
      "step": 6533
    },
    {
      "epoch": 1.593658536585366,
      "grad_norm": 0.09984970092773438,
      "learning_rate": 2.2554933681458653e-05,
      "loss": 0.0174,
      "step": 6534
    },
    {
      "epoch": 1.59390243902439,
      "grad_norm": 0.20391513407230377,
      "learning_rate": 2.254857850615226e-05,
      "loss": 0.0205,
      "step": 6535
    },
    {
      "epoch": 1.5941463414634147,
      "grad_norm": 0.2559818923473358,
      "learning_rate": 2.254222349079362e-05,
      "loss": 0.0254,
      "step": 6536
    },
    {
      "epoch": 1.5943902439024389,
      "grad_norm": 0.2710261940956116,
      "learning_rate": 2.2535868635797365e-05,
      "loss": 0.0277,
      "step": 6537
    },
    {
      "epoch": 1.5946341463414635,
      "grad_norm": 0.07785169035196304,
      "learning_rate": 2.252951394157814e-05,
      "loss": 0.0068,
      "step": 6538
    },
    {
      "epoch": 1.5948780487804877,
      "grad_norm": 0.062156375497579575,
      "learning_rate": 2.252315940855057e-05,
      "loss": 0.0087,
      "step": 6539
    },
    {
      "epoch": 1.5951219512195123,
      "grad_norm": 0.09543394297361374,
      "learning_rate": 2.2516805037129264e-05,
      "loss": 0.011,
      "step": 6540
    },
    {
      "epoch": 1.5953658536585364,
      "grad_norm": 0.08961416780948639,
      "learning_rate": 2.2510450827728823e-05,
      "loss": 0.0233,
      "step": 6541
    },
    {
      "epoch": 1.595609756097561,
      "grad_norm": 0.10952826589345932,
      "learning_rate": 2.250409678076385e-05,
      "loss": 0.0206,
      "step": 6542
    },
    {
      "epoch": 1.5958536585365852,
      "grad_norm": 0.14446677267551422,
      "learning_rate": 2.2497742896648923e-05,
      "loss": 0.0281,
      "step": 6543
    },
    {
      "epoch": 1.5960975609756098,
      "grad_norm": 0.19833965599536896,
      "learning_rate": 2.2491389175798608e-05,
      "loss": 0.0226,
      "step": 6544
    },
    {
      "epoch": 1.596341463414634,
      "grad_norm": 0.03994473069906235,
      "learning_rate": 2.248503561862747e-05,
      "loss": 0.006,
      "step": 6545
    },
    {
      "epoch": 1.5965853658536586,
      "grad_norm": 0.12448325008153915,
      "learning_rate": 2.2478682225550057e-05,
      "loss": 0.0396,
      "step": 6546
    },
    {
      "epoch": 1.5968292682926828,
      "grad_norm": 0.1359041929244995,
      "learning_rate": 2.2472328996980925e-05,
      "loss": 0.0169,
      "step": 6547
    },
    {
      "epoch": 1.5970731707317074,
      "grad_norm": 0.12051419168710709,
      "learning_rate": 2.2465975933334572e-05,
      "loss": 0.0175,
      "step": 6548
    },
    {
      "epoch": 1.5973170731707316,
      "grad_norm": 0.158962219953537,
      "learning_rate": 2.245962303502554e-05,
      "loss": 0.0133,
      "step": 6549
    },
    {
      "epoch": 1.5975609756097562,
      "grad_norm": 0.13407671451568604,
      "learning_rate": 2.2453270302468325e-05,
      "loss": 0.0229,
      "step": 6550
    },
    {
      "epoch": 1.5978048780487804,
      "grad_norm": 0.12300156056880951,
      "learning_rate": 2.244691773607742e-05,
      "loss": 0.0156,
      "step": 6551
    },
    {
      "epoch": 1.598048780487805,
      "grad_norm": 0.2887261211872101,
      "learning_rate": 2.244056533626733e-05,
      "loss": 0.048,
      "step": 6552
    },
    {
      "epoch": 1.5982926829268291,
      "grad_norm": 0.13754558563232422,
      "learning_rate": 2.2434213103452506e-05,
      "loss": 0.034,
      "step": 6553
    },
    {
      "epoch": 1.5985365853658537,
      "grad_norm": 0.1874380260705948,
      "learning_rate": 2.2427861038047425e-05,
      "loss": 0.0137,
      "step": 6554
    },
    {
      "epoch": 1.598780487804878,
      "grad_norm": 0.20960304141044617,
      "learning_rate": 2.2421509140466537e-05,
      "loss": 0.029,
      "step": 6555
    },
    {
      "epoch": 1.5990243902439025,
      "grad_norm": 0.10838260501623154,
      "learning_rate": 2.2415157411124288e-05,
      "loss": 0.0164,
      "step": 6556
    },
    {
      "epoch": 1.5992682926829267,
      "grad_norm": 0.07581549882888794,
      "learning_rate": 2.24088058504351e-05,
      "loss": 0.0122,
      "step": 6557
    },
    {
      "epoch": 1.5995121951219513,
      "grad_norm": 0.16233327984809875,
      "learning_rate": 2.24024544588134e-05,
      "loss": 0.0198,
      "step": 6558
    },
    {
      "epoch": 1.5997560975609755,
      "grad_norm": 0.10428269952535629,
      "learning_rate": 2.239610323667359e-05,
      "loss": 0.017,
      "step": 6559
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.13738225400447845,
      "learning_rate": 2.2389752184430083e-05,
      "loss": 0.0238,
      "step": 6560
    },
    {
      "epoch": 1.6002439024390243,
      "grad_norm": 0.057120226323604584,
      "learning_rate": 2.2383401302497247e-05,
      "loss": 0.0136,
      "step": 6561
    },
    {
      "epoch": 1.6004878048780489,
      "grad_norm": 0.2239457666873932,
      "learning_rate": 2.2377050591289468e-05,
      "loss": 0.0204,
      "step": 6562
    },
    {
      "epoch": 1.600731707317073,
      "grad_norm": 0.10284099727869034,
      "learning_rate": 2.2370700051221105e-05,
      "loss": 0.0198,
      "step": 6563
    },
    {
      "epoch": 1.6009756097560977,
      "grad_norm": 0.12118732929229736,
      "learning_rate": 2.236434968270653e-05,
      "loss": 0.0211,
      "step": 6564
    },
    {
      "epoch": 1.6012195121951218,
      "grad_norm": 0.1949571818113327,
      "learning_rate": 2.2357999486160058e-05,
      "loss": 0.0264,
      "step": 6565
    },
    {
      "epoch": 1.6014634146341464,
      "grad_norm": 0.05105261504650116,
      "learning_rate": 2.2351649461996034e-05,
      "loss": 0.0081,
      "step": 6566
    },
    {
      "epoch": 1.6017073170731706,
      "grad_norm": 0.09185012429952621,
      "learning_rate": 2.2345299610628767e-05,
      "loss": 0.0219,
      "step": 6567
    },
    {
      "epoch": 1.6019512195121952,
      "grad_norm": 0.20469990372657776,
      "learning_rate": 2.233894993247258e-05,
      "loss": 0.0253,
      "step": 6568
    },
    {
      "epoch": 1.6021951219512194,
      "grad_norm": 0.10666611790657043,
      "learning_rate": 2.2332600427941767e-05,
      "loss": 0.0194,
      "step": 6569
    },
    {
      "epoch": 1.602439024390244,
      "grad_norm": 0.2942439317703247,
      "learning_rate": 2.2326251097450605e-05,
      "loss": 0.0259,
      "step": 6570
    },
    {
      "epoch": 1.6026829268292682,
      "grad_norm": 0.20837512612342834,
      "learning_rate": 2.2319901941413372e-05,
      "loss": 0.0241,
      "step": 6571
    },
    {
      "epoch": 1.6029268292682928,
      "grad_norm": 0.10686186701059341,
      "learning_rate": 2.2313552960244333e-05,
      "loss": 0.0249,
      "step": 6572
    },
    {
      "epoch": 1.603170731707317,
      "grad_norm": 0.05711113661527634,
      "learning_rate": 2.2307204154357748e-05,
      "loss": 0.0073,
      "step": 6573
    },
    {
      "epoch": 1.6034146341463416,
      "grad_norm": 0.20777633786201477,
      "learning_rate": 2.230085552416784e-05,
      "loss": 0.016,
      "step": 6574
    },
    {
      "epoch": 1.6036585365853657,
      "grad_norm": 0.22730481624603271,
      "learning_rate": 2.2294507070088846e-05,
      "loss": 0.0266,
      "step": 6575
    },
    {
      "epoch": 1.6039024390243903,
      "grad_norm": 0.14033903181552887,
      "learning_rate": 2.2288158792534986e-05,
      "loss": 0.0172,
      "step": 6576
    },
    {
      "epoch": 1.6041463414634145,
      "grad_norm": 0.18727080523967743,
      "learning_rate": 2.2281810691920467e-05,
      "loss": 0.0368,
      "step": 6577
    },
    {
      "epoch": 1.6043902439024391,
      "grad_norm": 0.17041006684303284,
      "learning_rate": 2.2275462768659472e-05,
      "loss": 0.0193,
      "step": 6578
    },
    {
      "epoch": 1.6046341463414633,
      "grad_norm": 0.24806740880012512,
      "learning_rate": 2.2269115023166193e-05,
      "loss": 0.0133,
      "step": 6579
    },
    {
      "epoch": 1.604878048780488,
      "grad_norm": 0.1774587631225586,
      "learning_rate": 2.226276745585481e-05,
      "loss": 0.0173,
      "step": 6580
    },
    {
      "epoch": 1.605121951219512,
      "grad_norm": 0.1052352637052536,
      "learning_rate": 2.2256420067139457e-05,
      "loss": 0.0199,
      "step": 6581
    },
    {
      "epoch": 1.6053658536585367,
      "grad_norm": 0.19962921738624573,
      "learning_rate": 2.2250072857434304e-05,
      "loss": 0.0137,
      "step": 6582
    },
    {
      "epoch": 1.6056097560975608,
      "grad_norm": 0.2306717187166214,
      "learning_rate": 2.224372582715347e-05,
      "loss": 0.0204,
      "step": 6583
    },
    {
      "epoch": 1.6058536585365855,
      "grad_norm": 0.08703300356864929,
      "learning_rate": 2.2237378976711094e-05,
      "loss": 0.0256,
      "step": 6584
    },
    {
      "epoch": 1.6060975609756096,
      "grad_norm": 0.07679970562458038,
      "learning_rate": 2.2231032306521276e-05,
      "loss": 0.0191,
      "step": 6585
    },
    {
      "epoch": 1.6063414634146342,
      "grad_norm": 0.12599030137062073,
      "learning_rate": 2.2224685816998135e-05,
      "loss": 0.0198,
      "step": 6586
    },
    {
      "epoch": 1.6065853658536584,
      "grad_norm": 0.15226902067661285,
      "learning_rate": 2.221833950855574e-05,
      "loss": 0.0353,
      "step": 6587
    },
    {
      "epoch": 1.606829268292683,
      "grad_norm": 0.138032004237175,
      "learning_rate": 2.2211993381608174e-05,
      "loss": 0.0189,
      "step": 6588
    },
    {
      "epoch": 1.6070731707317072,
      "grad_norm": 0.10277127474546432,
      "learning_rate": 2.220564743656951e-05,
      "loss": 0.0182,
      "step": 6589
    },
    {
      "epoch": 1.6073170731707318,
      "grad_norm": 0.07676451653242111,
      "learning_rate": 2.2199301673853797e-05,
      "loss": 0.0125,
      "step": 6590
    },
    {
      "epoch": 1.607560975609756,
      "grad_norm": 0.1344301998615265,
      "learning_rate": 2.2192956093875075e-05,
      "loss": 0.0187,
      "step": 6591
    },
    {
      "epoch": 1.6078048780487806,
      "grad_norm": 0.12569661438465118,
      "learning_rate": 2.2186610697047372e-05,
      "loss": 0.0291,
      "step": 6592
    },
    {
      "epoch": 1.6080487804878048,
      "grad_norm": 0.11164029687643051,
      "learning_rate": 2.218026548378471e-05,
      "loss": 0.0156,
      "step": 6593
    },
    {
      "epoch": 1.6082926829268294,
      "grad_norm": 0.08421950042247772,
      "learning_rate": 2.2173920454501097e-05,
      "loss": 0.0148,
      "step": 6594
    },
    {
      "epoch": 1.6085365853658535,
      "grad_norm": 0.1312893033027649,
      "learning_rate": 2.2167575609610516e-05,
      "loss": 0.0248,
      "step": 6595
    },
    {
      "epoch": 1.6087804878048781,
      "grad_norm": 0.09237994253635406,
      "learning_rate": 2.216123094952696e-05,
      "loss": 0.0167,
      "step": 6596
    },
    {
      "epoch": 1.6090243902439023,
      "grad_norm": 0.12413804978132248,
      "learning_rate": 2.21548864746644e-05,
      "loss": 0.0223,
      "step": 6597
    },
    {
      "epoch": 1.609268292682927,
      "grad_norm": 0.19984430074691772,
      "learning_rate": 2.214854218543678e-05,
      "loss": 0.0181,
      "step": 6598
    },
    {
      "epoch": 1.609512195121951,
      "grad_norm": 0.256981760263443,
      "learning_rate": 2.214219808225806e-05,
      "loss": 0.026,
      "step": 6599
    },
    {
      "epoch": 1.6097560975609757,
      "grad_norm": 0.18164700269699097,
      "learning_rate": 2.2135854165542158e-05,
      "loss": 0.037,
      "step": 6600
    },
    {
      "epoch": 1.6099999999999999,
      "grad_norm": 0.1047496497631073,
      "learning_rate": 2.2129510435703004e-05,
      "loss": 0.0318,
      "step": 6601
    },
    {
      "epoch": 1.6102439024390245,
      "grad_norm": 0.10086235404014587,
      "learning_rate": 2.212316689315451e-05,
      "loss": 0.0182,
      "step": 6602
    },
    {
      "epoch": 1.6104878048780487,
      "grad_norm": 0.07384372502565384,
      "learning_rate": 2.211682353831057e-05,
      "loss": 0.0204,
      "step": 6603
    },
    {
      "epoch": 1.6107317073170733,
      "grad_norm": 0.07902061939239502,
      "learning_rate": 2.211048037158507e-05,
      "loss": 0.0124,
      "step": 6604
    },
    {
      "epoch": 1.6109756097560974,
      "grad_norm": 0.18476134538650513,
      "learning_rate": 2.210413739339187e-05,
      "loss": 0.0242,
      "step": 6605
    },
    {
      "epoch": 1.611219512195122,
      "grad_norm": 0.13463391363620758,
      "learning_rate": 2.2097794604144848e-05,
      "loss": 0.0262,
      "step": 6606
    },
    {
      "epoch": 1.6114634146341462,
      "grad_norm": 0.07960732281208038,
      "learning_rate": 2.2091452004257847e-05,
      "loss": 0.016,
      "step": 6607
    },
    {
      "epoch": 1.6117073170731708,
      "grad_norm": 0.09430572390556335,
      "learning_rate": 2.2085109594144692e-05,
      "loss": 0.0123,
      "step": 6608
    },
    {
      "epoch": 1.611951219512195,
      "grad_norm": 0.11257869750261307,
      "learning_rate": 2.2078767374219213e-05,
      "loss": 0.0178,
      "step": 6609
    },
    {
      "epoch": 1.6121951219512196,
      "grad_norm": 0.13857011497020721,
      "learning_rate": 2.2072425344895222e-05,
      "loss": 0.027,
      "step": 6610
    },
    {
      "epoch": 1.6124390243902438,
      "grad_norm": 0.06842593848705292,
      "learning_rate": 2.2066083506586515e-05,
      "loss": 0.0153,
      "step": 6611
    },
    {
      "epoch": 1.6126829268292684,
      "grad_norm": 0.07395248860120773,
      "learning_rate": 2.2059741859706878e-05,
      "loss": 0.0094,
      "step": 6612
    },
    {
      "epoch": 1.6129268292682926,
      "grad_norm": 0.1853041797876358,
      "learning_rate": 2.2053400404670094e-05,
      "loss": 0.0172,
      "step": 6613
    },
    {
      "epoch": 1.6131707317073172,
      "grad_norm": 0.17471127212047577,
      "learning_rate": 2.20470591418899e-05,
      "loss": 0.0319,
      "step": 6614
    },
    {
      "epoch": 1.6134146341463413,
      "grad_norm": 0.1662537008523941,
      "learning_rate": 2.204071807178006e-05,
      "loss": 0.0339,
      "step": 6615
    },
    {
      "epoch": 1.613658536585366,
      "grad_norm": 0.1195266842842102,
      "learning_rate": 2.203437719475431e-05,
      "loss": 0.0175,
      "step": 6616
    },
    {
      "epoch": 1.6139024390243901,
      "grad_norm": 0.22162264585494995,
      "learning_rate": 2.202803651122636e-05,
      "loss": 0.0401,
      "step": 6617
    },
    {
      "epoch": 1.6141463414634147,
      "grad_norm": 0.08063416928052902,
      "learning_rate": 2.2021696021609937e-05,
      "loss": 0.0175,
      "step": 6618
    },
    {
      "epoch": 1.614390243902439,
      "grad_norm": 0.08344313502311707,
      "learning_rate": 2.2015355726318727e-05,
      "loss": 0.0137,
      "step": 6619
    },
    {
      "epoch": 1.6146341463414635,
      "grad_norm": 0.17911379039287567,
      "learning_rate": 2.2009015625766423e-05,
      "loss": 0.0302,
      "step": 6620
    },
    {
      "epoch": 1.6148780487804877,
      "grad_norm": 0.12343110889196396,
      "learning_rate": 2.2002675720366692e-05,
      "loss": 0.0241,
      "step": 6621
    },
    {
      "epoch": 1.6151219512195123,
      "grad_norm": 0.12279544770717621,
      "learning_rate": 2.199633601053319e-05,
      "loss": 0.0362,
      "step": 6622
    },
    {
      "epoch": 1.6153658536585365,
      "grad_norm": 0.14111213386058807,
      "learning_rate": 2.198999649667957e-05,
      "loss": 0.014,
      "step": 6623
    },
    {
      "epoch": 1.615609756097561,
      "grad_norm": 0.2231704294681549,
      "learning_rate": 2.1983657179219473e-05,
      "loss": 0.014,
      "step": 6624
    },
    {
      "epoch": 1.6158536585365852,
      "grad_norm": 0.11469615250825882,
      "learning_rate": 2.19773180585665e-05,
      "loss": 0.0169,
      "step": 6625
    },
    {
      "epoch": 1.6160975609756099,
      "grad_norm": 0.07174479216337204,
      "learning_rate": 2.1970979135134274e-05,
      "loss": 0.017,
      "step": 6626
    },
    {
      "epoch": 1.616341463414634,
      "grad_norm": 0.20156259834766388,
      "learning_rate": 2.1964640409336392e-05,
      "loss": 0.032,
      "step": 6627
    },
    {
      "epoch": 1.6165853658536586,
      "grad_norm": 0.12316072732210159,
      "learning_rate": 2.1958301881586425e-05,
      "loss": 0.0206,
      "step": 6628
    },
    {
      "epoch": 1.6168292682926828,
      "grad_norm": 0.11319324374198914,
      "learning_rate": 2.1951963552297948e-05,
      "loss": 0.0122,
      "step": 6629
    },
    {
      "epoch": 1.6170731707317074,
      "grad_norm": 0.21811379492282867,
      "learning_rate": 2.194562542188453e-05,
      "loss": 0.018,
      "step": 6630
    },
    {
      "epoch": 1.6173170731707316,
      "grad_norm": 0.1306772381067276,
      "learning_rate": 2.193928749075969e-05,
      "loss": 0.0125,
      "step": 6631
    },
    {
      "epoch": 1.6175609756097562,
      "grad_norm": 0.07062377780675888,
      "learning_rate": 2.1932949759336975e-05,
      "loss": 0.0146,
      "step": 6632
    },
    {
      "epoch": 1.6178048780487804,
      "grad_norm": 0.11716216057538986,
      "learning_rate": 2.1926612228029903e-05,
      "loss": 0.0265,
      "step": 6633
    },
    {
      "epoch": 1.618048780487805,
      "grad_norm": 0.11883144080638885,
      "learning_rate": 2.192027489725197e-05,
      "loss": 0.024,
      "step": 6634
    },
    {
      "epoch": 1.6182926829268292,
      "grad_norm": 0.14548029005527496,
      "learning_rate": 2.191393776741667e-05,
      "loss": 0.0228,
      "step": 6635
    },
    {
      "epoch": 1.6185365853658538,
      "grad_norm": 0.06856419146060944,
      "learning_rate": 2.1907600838937488e-05,
      "loss": 0.0128,
      "step": 6636
    },
    {
      "epoch": 1.618780487804878,
      "grad_norm": 0.15070679783821106,
      "learning_rate": 2.1901264112227886e-05,
      "loss": 0.015,
      "step": 6637
    },
    {
      "epoch": 1.6190243902439025,
      "grad_norm": 0.12000202387571335,
      "learning_rate": 2.1894927587701308e-05,
      "loss": 0.0196,
      "step": 6638
    },
    {
      "epoch": 1.6192682926829267,
      "grad_norm": 0.09120957553386688,
      "learning_rate": 2.1888591265771198e-05,
      "loss": 0.0148,
      "step": 6639
    },
    {
      "epoch": 1.6195121951219513,
      "grad_norm": 0.2248140573501587,
      "learning_rate": 2.1882255146850996e-05,
      "loss": 0.0124,
      "step": 6640
    },
    {
      "epoch": 1.6197560975609755,
      "grad_norm": 0.08420311659574509,
      "learning_rate": 2.187591923135409e-05,
      "loss": 0.0245,
      "step": 6641
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.12572886049747467,
      "learning_rate": 2.186958351969389e-05,
      "loss": 0.0155,
      "step": 6642
    },
    {
      "epoch": 1.6202439024390243,
      "grad_norm": 0.08464476466178894,
      "learning_rate": 2.1863248012283783e-05,
      "loss": 0.0182,
      "step": 6643
    },
    {
      "epoch": 1.6204878048780489,
      "grad_norm": 0.0750313401222229,
      "learning_rate": 2.185691270953715e-05,
      "loss": 0.0183,
      "step": 6644
    },
    {
      "epoch": 1.620731707317073,
      "grad_norm": 0.1915496438741684,
      "learning_rate": 2.185057761186733e-05,
      "loss": 0.016,
      "step": 6645
    },
    {
      "epoch": 1.6209756097560977,
      "grad_norm": 0.11798103898763657,
      "learning_rate": 2.184424271968769e-05,
      "loss": 0.0281,
      "step": 6646
    },
    {
      "epoch": 1.6212195121951218,
      "grad_norm": 0.08106910437345505,
      "learning_rate": 2.1837908033411548e-05,
      "loss": 0.0145,
      "step": 6647
    },
    {
      "epoch": 1.6214634146341464,
      "grad_norm": 0.16743220388889313,
      "learning_rate": 2.183157355345222e-05,
      "loss": 0.0219,
      "step": 6648
    },
    {
      "epoch": 1.6217073170731706,
      "grad_norm": 0.10281839966773987,
      "learning_rate": 2.182523928022302e-05,
      "loss": 0.0238,
      "step": 6649
    },
    {
      "epoch": 1.6219512195121952,
      "grad_norm": 0.1087036058306694,
      "learning_rate": 2.1818905214137243e-05,
      "loss": 0.0182,
      "step": 6650
    },
    {
      "epoch": 1.6221951219512194,
      "grad_norm": 0.10056795179843903,
      "learning_rate": 2.1812571355608158e-05,
      "loss": 0.0147,
      "step": 6651
    },
    {
      "epoch": 1.622439024390244,
      "grad_norm": 0.07881820201873779,
      "learning_rate": 2.180623770504904e-05,
      "loss": 0.0223,
      "step": 6652
    },
    {
      "epoch": 1.6226829268292682,
      "grad_norm": 0.1940384954214096,
      "learning_rate": 2.1799904262873126e-05,
      "loss": 0.0316,
      "step": 6653
    },
    {
      "epoch": 1.6229268292682928,
      "grad_norm": 0.15573270618915558,
      "learning_rate": 2.1793571029493672e-05,
      "loss": 0.0233,
      "step": 6654
    },
    {
      "epoch": 1.623170731707317,
      "grad_norm": 0.13494975864887238,
      "learning_rate": 2.1787238005323884e-05,
      "loss": 0.0242,
      "step": 6655
    },
    {
      "epoch": 1.6234146341463416,
      "grad_norm": 0.21370437741279602,
      "learning_rate": 2.1780905190776984e-05,
      "loss": 0.0201,
      "step": 6656
    },
    {
      "epoch": 1.6236585365853657,
      "grad_norm": 0.17569443583488464,
      "learning_rate": 2.177457258626617e-05,
      "loss": 0.0233,
      "step": 6657
    },
    {
      "epoch": 1.6239024390243904,
      "grad_norm": 0.114945188164711,
      "learning_rate": 2.1768240192204616e-05,
      "loss": 0.0122,
      "step": 6658
    },
    {
      "epoch": 1.6241463414634145,
      "grad_norm": 0.11896836757659912,
      "learning_rate": 2.1761908009005493e-05,
      "loss": 0.0172,
      "step": 6659
    },
    {
      "epoch": 1.6243902439024391,
      "grad_norm": 0.14883571863174438,
      "learning_rate": 2.1755576037081964e-05,
      "loss": 0.0268,
      "step": 6660
    },
    {
      "epoch": 1.6246341463414633,
      "grad_norm": 0.08110641688108444,
      "learning_rate": 2.1749244276847168e-05,
      "loss": 0.0096,
      "step": 6661
    },
    {
      "epoch": 1.624878048780488,
      "grad_norm": 0.09568372368812561,
      "learning_rate": 2.174291272871423e-05,
      "loss": 0.0151,
      "step": 6662
    },
    {
      "epoch": 1.625121951219512,
      "grad_norm": 0.099710613489151,
      "learning_rate": 2.1736581393096274e-05,
      "loss": 0.0135,
      "step": 6663
    },
    {
      "epoch": 1.6253658536585367,
      "grad_norm": 0.28235504031181335,
      "learning_rate": 2.1730250270406382e-05,
      "loss": 0.0337,
      "step": 6664
    },
    {
      "epoch": 1.6256097560975609,
      "grad_norm": 0.2061174511909485,
      "learning_rate": 2.172391936105765e-05,
      "loss": 0.0205,
      "step": 6665
    },
    {
      "epoch": 1.6258536585365855,
      "grad_norm": 0.12138453871011734,
      "learning_rate": 2.1717588665463152e-05,
      "loss": 0.0185,
      "step": 6666
    },
    {
      "epoch": 1.6260975609756096,
      "grad_norm": 0.14818038046360016,
      "learning_rate": 2.1711258184035945e-05,
      "loss": 0.0126,
      "step": 6667
    },
    {
      "epoch": 1.6263414634146343,
      "grad_norm": 0.1712682694196701,
      "learning_rate": 2.1704927917189075e-05,
      "loss": 0.0136,
      "step": 6668
    },
    {
      "epoch": 1.6265853658536584,
      "grad_norm": 0.10746272653341293,
      "learning_rate": 2.169859786533557e-05,
      "loss": 0.0143,
      "step": 6669
    },
    {
      "epoch": 1.626829268292683,
      "grad_norm": 0.05794486030936241,
      "learning_rate": 2.169226802888845e-05,
      "loss": 0.0117,
      "step": 6670
    },
    {
      "epoch": 1.6270731707317072,
      "grad_norm": 0.13762719929218292,
      "learning_rate": 2.1685938408260708e-05,
      "loss": 0.0089,
      "step": 6671
    },
    {
      "epoch": 1.6273170731707318,
      "grad_norm": 0.13341079652309418,
      "learning_rate": 2.1679609003865346e-05,
      "loss": 0.0112,
      "step": 6672
    },
    {
      "epoch": 1.627560975609756,
      "grad_norm": 0.1256362348794937,
      "learning_rate": 2.1673279816115327e-05,
      "loss": 0.0187,
      "step": 6673
    },
    {
      "epoch": 1.6278048780487806,
      "grad_norm": 0.15140646696090698,
      "learning_rate": 2.1666950845423623e-05,
      "loss": 0.0194,
      "step": 6674
    },
    {
      "epoch": 1.6280487804878048,
      "grad_norm": 0.07905444502830505,
      "learning_rate": 2.1660622092203166e-05,
      "loss": 0.026,
      "step": 6675
    },
    {
      "epoch": 1.6282926829268294,
      "grad_norm": 0.08367837220430374,
      "learning_rate": 2.1654293556866898e-05,
      "loss": 0.017,
      "step": 6676
    },
    {
      "epoch": 1.6285365853658536,
      "grad_norm": 0.13065789639949799,
      "learning_rate": 2.164796523982773e-05,
      "loss": 0.0261,
      "step": 6677
    },
    {
      "epoch": 1.6287804878048782,
      "grad_norm": 0.07798055559396744,
      "learning_rate": 2.1641637141498576e-05,
      "loss": 0.0171,
      "step": 6678
    },
    {
      "epoch": 1.6290243902439023,
      "grad_norm": 0.14025190472602844,
      "learning_rate": 2.1635309262292318e-05,
      "loss": 0.0281,
      "step": 6679
    },
    {
      "epoch": 1.629268292682927,
      "grad_norm": 0.06832503527402878,
      "learning_rate": 2.1628981602621828e-05,
      "loss": 0.0088,
      "step": 6680
    },
    {
      "epoch": 1.6295121951219511,
      "grad_norm": 0.12469557672739029,
      "learning_rate": 2.1622654162899967e-05,
      "loss": 0.0205,
      "step": 6681
    },
    {
      "epoch": 1.6297560975609757,
      "grad_norm": 0.09122014045715332,
      "learning_rate": 2.1616326943539578e-05,
      "loss": 0.0141,
      "step": 6682
    },
    {
      "epoch": 1.63,
      "grad_norm": 0.11650625616312027,
      "learning_rate": 2.160999994495351e-05,
      "loss": 0.0205,
      "step": 6683
    },
    {
      "epoch": 1.6302439024390245,
      "grad_norm": 0.10223458707332611,
      "learning_rate": 2.1603673167554553e-05,
      "loss": 0.0146,
      "step": 6684
    },
    {
      "epoch": 1.6304878048780487,
      "grad_norm": 0.097984179854393,
      "learning_rate": 2.1597346611755533e-05,
      "loss": 0.0149,
      "step": 6685
    },
    {
      "epoch": 1.6307317073170733,
      "grad_norm": 0.1697331964969635,
      "learning_rate": 2.1591020277969222e-05,
      "loss": 0.0364,
      "step": 6686
    },
    {
      "epoch": 1.6309756097560975,
      "grad_norm": 0.2371736317873001,
      "learning_rate": 2.1584694166608413e-05,
      "loss": 0.0235,
      "step": 6687
    },
    {
      "epoch": 1.631219512195122,
      "grad_norm": 0.20979371666908264,
      "learning_rate": 2.157836827808585e-05,
      "loss": 0.0283,
      "step": 6688
    },
    {
      "epoch": 1.6314634146341462,
      "grad_norm": 0.05211780592799187,
      "learning_rate": 2.1572042612814278e-05,
      "loss": 0.0068,
      "step": 6689
    },
    {
      "epoch": 1.6317073170731708,
      "grad_norm": 0.16910451650619507,
      "learning_rate": 2.1565717171206435e-05,
      "loss": 0.0182,
      "step": 6690
    },
    {
      "epoch": 1.631951219512195,
      "grad_norm": 0.14995715022087097,
      "learning_rate": 2.1559391953675033e-05,
      "loss": 0.026,
      "step": 6691
    },
    {
      "epoch": 1.6321951219512196,
      "grad_norm": 0.10738671571016312,
      "learning_rate": 2.1553066960632777e-05,
      "loss": 0.0151,
      "step": 6692
    },
    {
      "epoch": 1.6324390243902438,
      "grad_norm": 0.09681601077318192,
      "learning_rate": 2.1546742192492343e-05,
      "loss": 0.0117,
      "step": 6693
    },
    {
      "epoch": 1.6326829268292684,
      "grad_norm": 0.167344868183136,
      "learning_rate": 2.154041764966641e-05,
      "loss": 0.0309,
      "step": 6694
    },
    {
      "epoch": 1.6329268292682926,
      "grad_norm": 0.11372076719999313,
      "learning_rate": 2.153409333256764e-05,
      "loss": 0.019,
      "step": 6695
    },
    {
      "epoch": 1.6331707317073172,
      "grad_norm": 0.09684687852859497,
      "learning_rate": 2.1527769241608677e-05,
      "loss": 0.0198,
      "step": 6696
    },
    {
      "epoch": 1.6334146341463414,
      "grad_norm": 0.1187964528799057,
      "learning_rate": 2.1521445377202132e-05,
      "loss": 0.0134,
      "step": 6697
    },
    {
      "epoch": 1.633658536585366,
      "grad_norm": 0.1081899106502533,
      "learning_rate": 2.1515121739760623e-05,
      "loss": 0.0186,
      "step": 6698
    },
    {
      "epoch": 1.6339024390243901,
      "grad_norm": 0.19711421430110931,
      "learning_rate": 2.1508798329696757e-05,
      "loss": 0.0227,
      "step": 6699
    },
    {
      "epoch": 1.6341463414634148,
      "grad_norm": 0.10626088082790375,
      "learning_rate": 2.1502475147423114e-05,
      "loss": 0.0222,
      "step": 6700
    },
    {
      "epoch": 1.634390243902439,
      "grad_norm": 0.10483533143997192,
      "learning_rate": 2.149615219335226e-05,
      "loss": 0.0171,
      "step": 6701
    },
    {
      "epoch": 1.6346341463414635,
      "grad_norm": 0.24149565398693085,
      "learning_rate": 2.1489829467896743e-05,
      "loss": 0.0333,
      "step": 6702
    },
    {
      "epoch": 1.6348780487804877,
      "grad_norm": 0.1200508177280426,
      "learning_rate": 2.1483506971469112e-05,
      "loss": 0.0165,
      "step": 6703
    },
    {
      "epoch": 1.6351219512195123,
      "grad_norm": 0.0806519165635109,
      "learning_rate": 2.147718470448189e-05,
      "loss": 0.0167,
      "step": 6704
    },
    {
      "epoch": 1.6353658536585365,
      "grad_norm": 0.10092281550168991,
      "learning_rate": 2.1470862667347573e-05,
      "loss": 0.0078,
      "step": 6705
    },
    {
      "epoch": 1.635609756097561,
      "grad_norm": 0.1205490231513977,
      "learning_rate": 2.1464540860478667e-05,
      "loss": 0.0188,
      "step": 6706
    },
    {
      "epoch": 1.6358536585365853,
      "grad_norm": 0.14789192378520966,
      "learning_rate": 2.1458219284287648e-05,
      "loss": 0.0177,
      "step": 6707
    },
    {
      "epoch": 1.6360975609756099,
      "grad_norm": 0.16498495638370514,
      "learning_rate": 2.1451897939186983e-05,
      "loss": 0.0276,
      "step": 6708
    },
    {
      "epoch": 1.636341463414634,
      "grad_norm": 0.09198261052370071,
      "learning_rate": 2.144557682558911e-05,
      "loss": 0.0208,
      "step": 6709
    },
    {
      "epoch": 1.6365853658536587,
      "grad_norm": 0.06896461546421051,
      "learning_rate": 2.1439255943906466e-05,
      "loss": 0.0073,
      "step": 6710
    },
    {
      "epoch": 1.6368292682926828,
      "grad_norm": 0.173133984208107,
      "learning_rate": 2.143293529455147e-05,
      "loss": 0.0262,
      "step": 6711
    },
    {
      "epoch": 1.6370731707317074,
      "grad_norm": 0.09776586294174194,
      "learning_rate": 2.1426614877936542e-05,
      "loss": 0.0252,
      "step": 6712
    },
    {
      "epoch": 1.6373170731707316,
      "grad_norm": 0.07093065977096558,
      "learning_rate": 2.1420294694474046e-05,
      "loss": 0.0084,
      "step": 6713
    },
    {
      "epoch": 1.6375609756097562,
      "grad_norm": 0.1591702550649643,
      "learning_rate": 2.1413974744576358e-05,
      "loss": 0.0299,
      "step": 6714
    },
    {
      "epoch": 1.6378048780487804,
      "grad_norm": 0.06820882111787796,
      "learning_rate": 2.1407655028655838e-05,
      "loss": 0.0085,
      "step": 6715
    },
    {
      "epoch": 1.638048780487805,
      "grad_norm": 0.2101459503173828,
      "learning_rate": 2.1401335547124832e-05,
      "loss": 0.0213,
      "step": 6716
    },
    {
      "epoch": 1.6382926829268292,
      "grad_norm": 0.2012602537870407,
      "learning_rate": 2.139501630039567e-05,
      "loss": 0.0234,
      "step": 6717
    },
    {
      "epoch": 1.6385365853658538,
      "grad_norm": 0.13706520199775696,
      "learning_rate": 2.1388697288880654e-05,
      "loss": 0.0141,
      "step": 6718
    },
    {
      "epoch": 1.638780487804878,
      "grad_norm": 0.09493392705917358,
      "learning_rate": 2.138237851299208e-05,
      "loss": 0.0254,
      "step": 6719
    },
    {
      "epoch": 1.6390243902439026,
      "grad_norm": 0.2174706906080246,
      "learning_rate": 2.1376059973142236e-05,
      "loss": 0.0178,
      "step": 6720
    },
    {
      "epoch": 1.6392682926829267,
      "grad_norm": 0.10230620950460434,
      "learning_rate": 2.1369741669743393e-05,
      "loss": 0.0107,
      "step": 6721
    },
    {
      "epoch": 1.6395121951219513,
      "grad_norm": 0.10148237645626068,
      "learning_rate": 2.1363423603207787e-05,
      "loss": 0.0166,
      "step": 6722
    },
    {
      "epoch": 1.6397560975609755,
      "grad_norm": 0.15957768261432648,
      "learning_rate": 2.1357105773947654e-05,
      "loss": 0.019,
      "step": 6723
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.18759773671627045,
      "learning_rate": 2.1350788182375222e-05,
      "loss": 0.0244,
      "step": 6724
    },
    {
      "epoch": 1.6402439024390243,
      "grad_norm": 0.13963674008846283,
      "learning_rate": 2.1344470828902695e-05,
      "loss": 0.0196,
      "step": 6725
    },
    {
      "epoch": 1.640487804878049,
      "grad_norm": 0.21938996016979218,
      "learning_rate": 2.1338153713942248e-05,
      "loss": 0.0203,
      "step": 6726
    },
    {
      "epoch": 1.640731707317073,
      "grad_norm": 0.16703982651233673,
      "learning_rate": 2.1331836837906062e-05,
      "loss": 0.0137,
      "step": 6727
    },
    {
      "epoch": 1.6409756097560977,
      "grad_norm": 0.14627261459827423,
      "learning_rate": 2.1325520201206292e-05,
      "loss": 0.0301,
      "step": 6728
    },
    {
      "epoch": 1.6412195121951219,
      "grad_norm": 0.10409048199653625,
      "learning_rate": 2.1319203804255092e-05,
      "loss": 0.0156,
      "step": 6729
    },
    {
      "epoch": 1.6414634146341465,
      "grad_norm": 0.22870568931102753,
      "learning_rate": 2.131288764746457e-05,
      "loss": 0.0213,
      "step": 6730
    },
    {
      "epoch": 1.6417073170731706,
      "grad_norm": 0.13949227333068848,
      "learning_rate": 2.1306571731246834e-05,
      "loss": 0.0241,
      "step": 6731
    },
    {
      "epoch": 1.6419512195121952,
      "grad_norm": 0.15488363802433014,
      "learning_rate": 2.130025605601399e-05,
      "loss": 0.0142,
      "step": 6732
    },
    {
      "epoch": 1.6421951219512194,
      "grad_norm": 0.10470922291278839,
      "learning_rate": 2.129394062217811e-05,
      "loss": 0.0253,
      "step": 6733
    },
    {
      "epoch": 1.642439024390244,
      "grad_norm": 0.09891786426305771,
      "learning_rate": 2.1287625430151267e-05,
      "loss": 0.0158,
      "step": 6734
    },
    {
      "epoch": 1.6426829268292682,
      "grad_norm": 0.16629308462142944,
      "learning_rate": 2.1281310480345493e-05,
      "loss": 0.0238,
      "step": 6735
    },
    {
      "epoch": 1.6429268292682928,
      "grad_norm": 0.11345867067575455,
      "learning_rate": 2.1274995773172825e-05,
      "loss": 0.0113,
      "step": 6736
    },
    {
      "epoch": 1.643170731707317,
      "grad_norm": 0.09059171378612518,
      "learning_rate": 2.126868130904528e-05,
      "loss": 0.0128,
      "step": 6737
    },
    {
      "epoch": 1.6434146341463416,
      "grad_norm": 0.14170974493026733,
      "learning_rate": 2.1262367088374862e-05,
      "loss": 0.0254,
      "step": 6738
    },
    {
      "epoch": 1.6436585365853658,
      "grad_norm": 0.27191466093063354,
      "learning_rate": 2.1256053111573544e-05,
      "loss": 0.0198,
      "step": 6739
    },
    {
      "epoch": 1.6439024390243904,
      "grad_norm": 0.12328340858221054,
      "learning_rate": 2.1249739379053303e-05,
      "loss": 0.0253,
      "step": 6740
    },
    {
      "epoch": 1.6441463414634145,
      "grad_norm": 0.2626935541629791,
      "learning_rate": 2.124342589122608e-05,
      "loss": 0.0264,
      "step": 6741
    },
    {
      "epoch": 1.6443902439024392,
      "grad_norm": 0.3220939338207245,
      "learning_rate": 2.123711264850383e-05,
      "loss": 0.0218,
      "step": 6742
    },
    {
      "epoch": 1.6446341463414633,
      "grad_norm": 0.0608569011092186,
      "learning_rate": 2.123079965129845e-05,
      "loss": 0.0143,
      "step": 6743
    },
    {
      "epoch": 1.644878048780488,
      "grad_norm": 0.07269459217786789,
      "learning_rate": 2.1224486900021856e-05,
      "loss": 0.0172,
      "step": 6744
    },
    {
      "epoch": 1.645121951219512,
      "grad_norm": 0.10188425332307816,
      "learning_rate": 2.1218174395085947e-05,
      "loss": 0.0167,
      "step": 6745
    },
    {
      "epoch": 1.6453658536585367,
      "grad_norm": 0.2978832423686981,
      "learning_rate": 2.1211862136902566e-05,
      "loss": 0.0267,
      "step": 6746
    },
    {
      "epoch": 1.6456097560975609,
      "grad_norm": 0.2668145000934601,
      "learning_rate": 2.1205550125883593e-05,
      "loss": 0.0316,
      "step": 6747
    },
    {
      "epoch": 1.6458536585365855,
      "grad_norm": 0.07028722763061523,
      "learning_rate": 2.119923836244085e-05,
      "loss": 0.0113,
      "step": 6748
    },
    {
      "epoch": 1.6460975609756097,
      "grad_norm": 0.2095419317483902,
      "learning_rate": 2.1192926846986174e-05,
      "loss": 0.0182,
      "step": 6749
    },
    {
      "epoch": 1.6463414634146343,
      "grad_norm": 0.21509884297847748,
      "learning_rate": 2.118661557993136e-05,
      "loss": 0.0254,
      "step": 6750
    },
    {
      "epoch": 1.6465853658536584,
      "grad_norm": 0.09093190729618073,
      "learning_rate": 2.1180304561688214e-05,
      "loss": 0.0135,
      "step": 6751
    },
    {
      "epoch": 1.646829268292683,
      "grad_norm": 0.24212762713432312,
      "learning_rate": 2.1173993792668494e-05,
      "loss": 0.0242,
      "step": 6752
    },
    {
      "epoch": 1.6470731707317072,
      "grad_norm": 0.21461907029151917,
      "learning_rate": 2.1167683273283973e-05,
      "loss": 0.0232,
      "step": 6753
    },
    {
      "epoch": 1.6473170731707318,
      "grad_norm": 0.1361011564731598,
      "learning_rate": 2.116137300394638e-05,
      "loss": 0.0322,
      "step": 6754
    },
    {
      "epoch": 1.647560975609756,
      "grad_norm": 0.12444889545440674,
      "learning_rate": 2.1155062985067455e-05,
      "loss": 0.0255,
      "step": 6755
    },
    {
      "epoch": 1.6478048780487806,
      "grad_norm": 0.06778627634048462,
      "learning_rate": 2.1148753217058896e-05,
      "loss": 0.0212,
      "step": 6756
    },
    {
      "epoch": 1.6480487804878048,
      "grad_norm": 0.32508304715156555,
      "learning_rate": 2.1142443700332398e-05,
      "loss": 0.0194,
      "step": 6757
    },
    {
      "epoch": 1.6482926829268294,
      "grad_norm": 0.08129293471574783,
      "learning_rate": 2.1136134435299646e-05,
      "loss": 0.0099,
      "step": 6758
    },
    {
      "epoch": 1.6485365853658536,
      "grad_norm": 0.2475673109292984,
      "learning_rate": 2.1129825422372285e-05,
      "loss": 0.0334,
      "step": 6759
    },
    {
      "epoch": 1.6487804878048782,
      "grad_norm": 0.09856503456830978,
      "learning_rate": 2.112351666196197e-05,
      "loss": 0.0186,
      "step": 6760
    },
    {
      "epoch": 1.6490243902439023,
      "grad_norm": 0.31554505228996277,
      "learning_rate": 2.1117208154480328e-05,
      "loss": 0.0297,
      "step": 6761
    },
    {
      "epoch": 1.649268292682927,
      "grad_norm": 0.2358003556728363,
      "learning_rate": 2.1110899900338976e-05,
      "loss": 0.0224,
      "step": 6762
    },
    {
      "epoch": 1.6495121951219511,
      "grad_norm": 0.12705983221530914,
      "learning_rate": 2.1104591899949487e-05,
      "loss": 0.0212,
      "step": 6763
    },
    {
      "epoch": 1.6497560975609757,
      "grad_norm": 0.1100926473736763,
      "learning_rate": 2.109828415372346e-05,
      "loss": 0.0251,
      "step": 6764
    },
    {
      "epoch": 1.65,
      "grad_norm": 0.12385501712560654,
      "learning_rate": 2.109197666207244e-05,
      "loss": 0.0102,
      "step": 6765
    },
    {
      "epoch": 1.6502439024390245,
      "grad_norm": 0.21091938018798828,
      "learning_rate": 2.1085669425407982e-05,
      "loss": 0.0223,
      "step": 6766
    },
    {
      "epoch": 1.6504878048780487,
      "grad_norm": 0.22762301564216614,
      "learning_rate": 2.107936244414161e-05,
      "loss": 0.02,
      "step": 6767
    },
    {
      "epoch": 1.6507317073170733,
      "grad_norm": 0.20471176505088806,
      "learning_rate": 2.1073055718684843e-05,
      "loss": 0.0161,
      "step": 6768
    },
    {
      "epoch": 1.6509756097560975,
      "grad_norm": 0.15742148458957672,
      "learning_rate": 2.1066749249449157e-05,
      "loss": 0.0176,
      "step": 6769
    },
    {
      "epoch": 1.651219512195122,
      "grad_norm": 0.14508631825447083,
      "learning_rate": 2.106044303684605e-05,
      "loss": 0.0162,
      "step": 6770
    },
    {
      "epoch": 1.6514634146341463,
      "grad_norm": 0.09255479276180267,
      "learning_rate": 2.1054137081286974e-05,
      "loss": 0.0184,
      "step": 6771
    },
    {
      "epoch": 1.6517073170731709,
      "grad_norm": 0.11681744456291199,
      "learning_rate": 2.1047831383183374e-05,
      "loss": 0.0103,
      "step": 6772
    },
    {
      "epoch": 1.651951219512195,
      "grad_norm": 0.14888940751552582,
      "learning_rate": 2.1041525942946672e-05,
      "loss": 0.0233,
      "step": 6773
    },
    {
      "epoch": 1.6521951219512196,
      "grad_norm": 0.09964505583047867,
      "learning_rate": 2.1035220760988287e-05,
      "loss": 0.0267,
      "step": 6774
    },
    {
      "epoch": 1.6524390243902438,
      "grad_norm": 0.14365455508232117,
      "learning_rate": 2.1028915837719616e-05,
      "loss": 0.0067,
      "step": 6775
    },
    {
      "epoch": 1.6526829268292684,
      "grad_norm": 0.16340272128582,
      "learning_rate": 2.1022611173552024e-05,
      "loss": 0.0166,
      "step": 6776
    },
    {
      "epoch": 1.6529268292682926,
      "grad_norm": 0.20089653134346008,
      "learning_rate": 2.1016306768896876e-05,
      "loss": 0.0281,
      "step": 6777
    },
    {
      "epoch": 1.6531707317073172,
      "grad_norm": 0.15989969670772552,
      "learning_rate": 2.1010002624165527e-05,
      "loss": 0.0243,
      "step": 6778
    },
    {
      "epoch": 1.6534146341463414,
      "grad_norm": 0.19944548606872559,
      "learning_rate": 2.100369873976928e-05,
      "loss": 0.0141,
      "step": 6779
    },
    {
      "epoch": 1.653658536585366,
      "grad_norm": 0.14349088072776794,
      "learning_rate": 2.0997395116119462e-05,
      "loss": 0.0179,
      "step": 6780
    },
    {
      "epoch": 1.6539024390243902,
      "grad_norm": 0.09698621183633804,
      "learning_rate": 2.099109175362736e-05,
      "loss": 0.0111,
      "step": 6781
    },
    {
      "epoch": 1.6541463414634148,
      "grad_norm": 0.1165546253323555,
      "learning_rate": 2.0984788652704243e-05,
      "loss": 0.0171,
      "step": 6782
    },
    {
      "epoch": 1.654390243902439,
      "grad_norm": 0.21887150406837463,
      "learning_rate": 2.0978485813761376e-05,
      "loss": 0.0186,
      "step": 6783
    },
    {
      "epoch": 1.6546341463414636,
      "grad_norm": 0.16934041678905487,
      "learning_rate": 2.097218323721e-05,
      "loss": 0.0187,
      "step": 6784
    },
    {
      "epoch": 1.6548780487804877,
      "grad_norm": 0.12375520914793015,
      "learning_rate": 2.096588092346134e-05,
      "loss": 0.0169,
      "step": 6785
    },
    {
      "epoch": 1.6551219512195123,
      "grad_norm": 0.2082337737083435,
      "learning_rate": 2.0959578872926597e-05,
      "loss": 0.0211,
      "step": 6786
    },
    {
      "epoch": 1.6553658536585365,
      "grad_norm": 0.15741018950939178,
      "learning_rate": 2.0953277086016963e-05,
      "loss": 0.0234,
      "step": 6787
    },
    {
      "epoch": 1.6556097560975611,
      "grad_norm": 0.09283621609210968,
      "learning_rate": 2.0946975563143617e-05,
      "loss": 0.0192,
      "step": 6788
    },
    {
      "epoch": 1.6558536585365853,
      "grad_norm": 0.12256689369678497,
      "learning_rate": 2.09406743047177e-05,
      "loss": 0.0117,
      "step": 6789
    },
    {
      "epoch": 1.65609756097561,
      "grad_norm": 0.08062120527029037,
      "learning_rate": 2.093437331115036e-05,
      "loss": 0.0272,
      "step": 6790
    },
    {
      "epoch": 1.656341463414634,
      "grad_norm": 0.0698704868555069,
      "learning_rate": 2.0928072582852716e-05,
      "loss": 0.0155,
      "step": 6791
    },
    {
      "epoch": 1.6565853658536587,
      "grad_norm": 0.19516514241695404,
      "learning_rate": 2.0921772120235873e-05,
      "loss": 0.0124,
      "step": 6792
    },
    {
      "epoch": 1.6568292682926828,
      "grad_norm": 0.09698793292045593,
      "learning_rate": 2.091547192371091e-05,
      "loss": 0.0172,
      "step": 6793
    },
    {
      "epoch": 1.6570731707317075,
      "grad_norm": 0.18704639375209808,
      "learning_rate": 2.0909171993688903e-05,
      "loss": 0.0238,
      "step": 6794
    },
    {
      "epoch": 1.6573170731707316,
      "grad_norm": 0.13308241963386536,
      "learning_rate": 2.090287233058091e-05,
      "loss": 0.014,
      "step": 6795
    },
    {
      "epoch": 1.6575609756097562,
      "grad_norm": 0.2596413195133209,
      "learning_rate": 2.0896572934797943e-05,
      "loss": 0.016,
      "step": 6796
    },
    {
      "epoch": 1.6578048780487804,
      "grad_norm": 0.12201647460460663,
      "learning_rate": 2.0890273806751033e-05,
      "loss": 0.0135,
      "step": 6797
    },
    {
      "epoch": 1.658048780487805,
      "grad_norm": 0.30936360359191895,
      "learning_rate": 2.0883974946851176e-05,
      "loss": 0.0271,
      "step": 6798
    },
    {
      "epoch": 1.6582926829268292,
      "grad_norm": 0.11601469665765762,
      "learning_rate": 2.087767635550935e-05,
      "loss": 0.0146,
      "step": 6799
    },
    {
      "epoch": 1.6585365853658538,
      "grad_norm": 0.2169123888015747,
      "learning_rate": 2.0871378033136523e-05,
      "loss": 0.0239,
      "step": 6800
    },
    {
      "epoch": 1.658780487804878,
      "grad_norm": 0.08229474723339081,
      "learning_rate": 2.0865079980143647e-05,
      "loss": 0.0173,
      "step": 6801
    },
    {
      "epoch": 1.6590243902439026,
      "grad_norm": 0.312630832195282,
      "learning_rate": 2.0858782196941637e-05,
      "loss": 0.0457,
      "step": 6802
    },
    {
      "epoch": 1.6592682926829267,
      "grad_norm": 0.11434248089790344,
      "learning_rate": 2.0852484683941412e-05,
      "loss": 0.01,
      "step": 6803
    },
    {
      "epoch": 1.6595121951219514,
      "grad_norm": 0.05425767973065376,
      "learning_rate": 2.0846187441553866e-05,
      "loss": 0.0127,
      "step": 6804
    },
    {
      "epoch": 1.6597560975609755,
      "grad_norm": 0.18172340095043182,
      "learning_rate": 2.0839890470189876e-05,
      "loss": 0.0185,
      "step": 6805
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 0.07679333537817001,
      "learning_rate": 2.0833593770260294e-05,
      "loss": 0.022,
      "step": 6806
    },
    {
      "epoch": 1.6602439024390243,
      "grad_norm": 0.17885343730449677,
      "learning_rate": 2.0827297342175962e-05,
      "loss": 0.0165,
      "step": 6807
    },
    {
      "epoch": 1.660487804878049,
      "grad_norm": 0.1146654561161995,
      "learning_rate": 2.0821001186347705e-05,
      "loss": 0.0218,
      "step": 6808
    },
    {
      "epoch": 1.660731707317073,
      "grad_norm": 0.09500154852867126,
      "learning_rate": 2.0814705303186332e-05,
      "loss": 0.0187,
      "step": 6809
    },
    {
      "epoch": 1.6609756097560977,
      "grad_norm": 0.1637580692768097,
      "learning_rate": 2.0808409693102622e-05,
      "loss": 0.0275,
      "step": 6810
    },
    {
      "epoch": 1.6612195121951219,
      "grad_norm": 0.08160008490085602,
      "learning_rate": 2.080211435650736e-05,
      "loss": 0.0115,
      "step": 6811
    },
    {
      "epoch": 1.6614634146341465,
      "grad_norm": 0.11111493408679962,
      "learning_rate": 2.079581929381127e-05,
      "loss": 0.0087,
      "step": 6812
    },
    {
      "epoch": 1.6617073170731707,
      "grad_norm": 0.1025356575846672,
      "learning_rate": 2.0789524505425106e-05,
      "loss": 0.0233,
      "step": 6813
    },
    {
      "epoch": 1.6619512195121953,
      "grad_norm": 0.10707800835371017,
      "learning_rate": 2.0783229991759582e-05,
      "loss": 0.0171,
      "step": 6814
    },
    {
      "epoch": 1.6621951219512194,
      "grad_norm": 0.08119194954633713,
      "learning_rate": 2.0776935753225387e-05,
      "loss": 0.0162,
      "step": 6815
    },
    {
      "epoch": 1.662439024390244,
      "grad_norm": 0.3029094636440277,
      "learning_rate": 2.0770641790233204e-05,
      "loss": 0.0302,
      "step": 6816
    },
    {
      "epoch": 1.6626829268292682,
      "grad_norm": 0.18641383945941925,
      "learning_rate": 2.0764348103193697e-05,
      "loss": 0.0263,
      "step": 6817
    },
    {
      "epoch": 1.6629268292682928,
      "grad_norm": 0.22516781091690063,
      "learning_rate": 2.075805469251752e-05,
      "loss": 0.0256,
      "step": 6818
    },
    {
      "epoch": 1.663170731707317,
      "grad_norm": 0.08529213815927505,
      "learning_rate": 2.0751761558615278e-05,
      "loss": 0.0257,
      "step": 6819
    },
    {
      "epoch": 1.6634146341463416,
      "grad_norm": 0.11548125743865967,
      "learning_rate": 2.074546870189759e-05,
      "loss": 0.0233,
      "step": 6820
    },
    {
      "epoch": 1.6636585365853658,
      "grad_norm": 0.11720031499862671,
      "learning_rate": 2.0739176122775044e-05,
      "loss": 0.0191,
      "step": 6821
    },
    {
      "epoch": 1.6639024390243904,
      "grad_norm": 0.11951764672994614,
      "learning_rate": 2.073288382165822e-05,
      "loss": 0.0143,
      "step": 6822
    },
    {
      "epoch": 1.6641463414634146,
      "grad_norm": 0.10895285755395889,
      "learning_rate": 2.072659179895766e-05,
      "loss": 0.0156,
      "step": 6823
    },
    {
      "epoch": 1.6643902439024392,
      "grad_norm": 0.17452387511730194,
      "learning_rate": 2.07203000550839e-05,
      "loss": 0.0281,
      "step": 6824
    },
    {
      "epoch": 1.6646341463414633,
      "grad_norm": 0.15032586455345154,
      "learning_rate": 2.071400859044746e-05,
      "loss": 0.0217,
      "step": 6825
    },
    {
      "epoch": 1.664878048780488,
      "grad_norm": 0.21886666119098663,
      "learning_rate": 2.0707717405458847e-05,
      "loss": 0.0231,
      "step": 6826
    },
    {
      "epoch": 1.6651219512195121,
      "grad_norm": 0.373725026845932,
      "learning_rate": 2.0701426500528522e-05,
      "loss": 0.0404,
      "step": 6827
    },
    {
      "epoch": 1.6653658536585367,
      "grad_norm": 0.05219616740942001,
      "learning_rate": 2.0695135876066974e-05,
      "loss": 0.0131,
      "step": 6828
    },
    {
      "epoch": 1.665609756097561,
      "grad_norm": 0.21851275861263275,
      "learning_rate": 2.068884553248462e-05,
      "loss": 0.0218,
      "step": 6829
    },
    {
      "epoch": 1.6658536585365855,
      "grad_norm": 0.2513200342655182,
      "learning_rate": 2.0682555470191897e-05,
      "loss": 0.029,
      "step": 6830
    },
    {
      "epoch": 1.6660975609756097,
      "grad_norm": 0.22931325435638428,
      "learning_rate": 2.067626568959922e-05,
      "loss": 0.0201,
      "step": 6831
    },
    {
      "epoch": 1.6663414634146343,
      "grad_norm": 0.09649654477834702,
      "learning_rate": 2.0669976191116963e-05,
      "loss": 0.0115,
      "step": 6832
    },
    {
      "epoch": 1.6665853658536585,
      "grad_norm": 0.10509903728961945,
      "learning_rate": 2.0663686975155505e-05,
      "loss": 0.0188,
      "step": 6833
    },
    {
      "epoch": 1.666829268292683,
      "grad_norm": 0.17204779386520386,
      "learning_rate": 2.0657398042125196e-05,
      "loss": 0.0324,
      "step": 6834
    },
    {
      "epoch": 1.6670731707317072,
      "grad_norm": 0.10209063440561295,
      "learning_rate": 2.0651109392436376e-05,
      "loss": 0.0254,
      "step": 6835
    },
    {
      "epoch": 1.6673170731707319,
      "grad_norm": 0.09140753000974655,
      "learning_rate": 2.064482102649935e-05,
      "loss": 0.0256,
      "step": 6836
    },
    {
      "epoch": 1.667560975609756,
      "grad_norm": 0.1378859281539917,
      "learning_rate": 2.063853294472442e-05,
      "loss": 0.0211,
      "step": 6837
    },
    {
      "epoch": 1.6678048780487806,
      "grad_norm": 0.08894850313663483,
      "learning_rate": 2.0632245147521865e-05,
      "loss": 0.0162,
      "step": 6838
    },
    {
      "epoch": 1.6680487804878048,
      "grad_norm": 0.3170410096645355,
      "learning_rate": 2.0625957635301945e-05,
      "loss": 0.0172,
      "step": 6839
    },
    {
      "epoch": 1.6682926829268294,
      "grad_norm": 0.1709493100643158,
      "learning_rate": 2.0619670408474896e-05,
      "loss": 0.0224,
      "step": 6840
    },
    {
      "epoch": 1.6685365853658536,
      "grad_norm": 0.13358788192272186,
      "learning_rate": 2.0613383467450946e-05,
      "loss": 0.0137,
      "step": 6841
    },
    {
      "epoch": 1.6687804878048782,
      "grad_norm": 0.08434554189443588,
      "learning_rate": 2.0607096812640293e-05,
      "loss": 0.0142,
      "step": 6842
    },
    {
      "epoch": 1.6690243902439024,
      "grad_norm": 0.12628106772899628,
      "learning_rate": 2.0600810444453127e-05,
      "loss": 0.0178,
      "step": 6843
    },
    {
      "epoch": 1.669268292682927,
      "grad_norm": 0.12384394556283951,
      "learning_rate": 2.059452436329962e-05,
      "loss": 0.0171,
      "step": 6844
    },
    {
      "epoch": 1.6695121951219511,
      "grad_norm": 0.07528673112392426,
      "learning_rate": 2.0588238569589905e-05,
      "loss": 0.0254,
      "step": 6845
    },
    {
      "epoch": 1.6697560975609758,
      "grad_norm": 0.0838393047451973,
      "learning_rate": 2.0581953063734115e-05,
      "loss": 0.0103,
      "step": 6846
    },
    {
      "epoch": 1.67,
      "grad_norm": 0.10383336991071701,
      "learning_rate": 2.0575667846142363e-05,
      "loss": 0.0164,
      "step": 6847
    },
    {
      "epoch": 1.6702439024390245,
      "grad_norm": 0.19353216886520386,
      "learning_rate": 2.0569382917224745e-05,
      "loss": 0.0219,
      "step": 6848
    },
    {
      "epoch": 1.6704878048780487,
      "grad_norm": 0.19252891838550568,
      "learning_rate": 2.056309827739132e-05,
      "loss": 0.0241,
      "step": 6849
    },
    {
      "epoch": 1.6707317073170733,
      "grad_norm": 0.11953774839639664,
      "learning_rate": 2.0556813927052155e-05,
      "loss": 0.0223,
      "step": 6850
    },
    {
      "epoch": 1.6709756097560975,
      "grad_norm": 0.08051926642656326,
      "learning_rate": 2.0550529866617272e-05,
      "loss": 0.0095,
      "step": 6851
    },
    {
      "epoch": 1.671219512195122,
      "grad_norm": 0.13817407190799713,
      "learning_rate": 2.054424609649671e-05,
      "loss": 0.0158,
      "step": 6852
    },
    {
      "epoch": 1.6714634146341463,
      "grad_norm": 0.0653483122587204,
      "learning_rate": 2.0537962617100438e-05,
      "loss": 0.0112,
      "step": 6853
    },
    {
      "epoch": 1.6717073170731709,
      "grad_norm": 0.13264240324497223,
      "learning_rate": 2.0531679428838447e-05,
      "loss": 0.0096,
      "step": 6854
    },
    {
      "epoch": 1.671951219512195,
      "grad_norm": 0.1335975080728531,
      "learning_rate": 2.052539653212069e-05,
      "loss": 0.0242,
      "step": 6855
    },
    {
      "epoch": 1.6721951219512197,
      "grad_norm": 0.30276772379875183,
      "learning_rate": 2.051911392735712e-05,
      "loss": 0.018,
      "step": 6856
    },
    {
      "epoch": 1.6724390243902438,
      "grad_norm": 0.2920651137828827,
      "learning_rate": 2.0512831614957645e-05,
      "loss": 0.0351,
      "step": 6857
    },
    {
      "epoch": 1.6726829268292684,
      "grad_norm": 0.163991317152977,
      "learning_rate": 2.050654959533217e-05,
      "loss": 0.0121,
      "step": 6858
    },
    {
      "epoch": 1.6729268292682926,
      "grad_norm": 0.12096626311540604,
      "learning_rate": 2.0500267868890575e-05,
      "loss": 0.013,
      "step": 6859
    },
    {
      "epoch": 1.6731707317073172,
      "grad_norm": 0.23116745054721832,
      "learning_rate": 2.0493986436042738e-05,
      "loss": 0.0166,
      "step": 6860
    },
    {
      "epoch": 1.6734146341463414,
      "grad_norm": 0.08978589624166489,
      "learning_rate": 2.048770529719849e-05,
      "loss": 0.0183,
      "step": 6861
    },
    {
      "epoch": 1.673658536585366,
      "grad_norm": 0.08484019339084625,
      "learning_rate": 2.0481424452767654e-05,
      "loss": 0.0115,
      "step": 6862
    },
    {
      "epoch": 1.6739024390243902,
      "grad_norm": 0.37186306715011597,
      "learning_rate": 2.0475143903160035e-05,
      "loss": 0.0179,
      "step": 6863
    },
    {
      "epoch": 1.6741463414634148,
      "grad_norm": 0.11203189194202423,
      "learning_rate": 2.0468863648785427e-05,
      "loss": 0.0249,
      "step": 6864
    },
    {
      "epoch": 1.674390243902439,
      "grad_norm": 0.09675799310207367,
      "learning_rate": 2.0462583690053598e-05,
      "loss": 0.017,
      "step": 6865
    },
    {
      "epoch": 1.6746341463414636,
      "grad_norm": 0.12856701016426086,
      "learning_rate": 2.045630402737429e-05,
      "loss": 0.0171,
      "step": 6866
    },
    {
      "epoch": 1.6748780487804877,
      "grad_norm": 0.11218339204788208,
      "learning_rate": 2.0450024661157233e-05,
      "loss": 0.0063,
      "step": 6867
    },
    {
      "epoch": 1.6751219512195124,
      "grad_norm": 0.10224076360464096,
      "learning_rate": 2.0443745591812137e-05,
      "loss": 0.0128,
      "step": 6868
    },
    {
      "epoch": 1.6753658536585365,
      "grad_norm": 0.35257503390312195,
      "learning_rate": 2.04374668197487e-05,
      "loss": 0.0478,
      "step": 6869
    },
    {
      "epoch": 1.6756097560975611,
      "grad_norm": 0.11864370852708817,
      "learning_rate": 2.0431188345376582e-05,
      "loss": 0.0143,
      "step": 6870
    },
    {
      "epoch": 1.6758536585365853,
      "grad_norm": 0.13069085776805878,
      "learning_rate": 2.0424910169105434e-05,
      "loss": 0.0209,
      "step": 6871
    },
    {
      "epoch": 1.67609756097561,
      "grad_norm": 0.13918143510818481,
      "learning_rate": 2.0418632291344896e-05,
      "loss": 0.0218,
      "step": 6872
    },
    {
      "epoch": 1.676341463414634,
      "grad_norm": 0.10586746037006378,
      "learning_rate": 2.0412354712504578e-05,
      "loss": 0.014,
      "step": 6873
    },
    {
      "epoch": 1.6765853658536587,
      "grad_norm": 0.15429994463920593,
      "learning_rate": 2.0406077432994066e-05,
      "loss": 0.0135,
      "step": 6874
    },
    {
      "epoch": 1.6768292682926829,
      "grad_norm": 0.09000758826732635,
      "learning_rate": 2.039980045322294e-05,
      "loss": 0.022,
      "step": 6875
    },
    {
      "epoch": 1.6770731707317075,
      "grad_norm": 0.13485312461853027,
      "learning_rate": 2.0393523773600757e-05,
      "loss": 0.0154,
      "step": 6876
    },
    {
      "epoch": 1.6773170731707316,
      "grad_norm": 0.17456132173538208,
      "learning_rate": 2.0387247394537046e-05,
      "loss": 0.0266,
      "step": 6877
    },
    {
      "epoch": 1.6775609756097563,
      "grad_norm": 0.09844690561294556,
      "learning_rate": 2.038097131644132e-05,
      "loss": 0.01,
      "step": 6878
    },
    {
      "epoch": 1.6778048780487804,
      "grad_norm": 0.14421744644641876,
      "learning_rate": 2.037469553972307e-05,
      "loss": 0.0236,
      "step": 6879
    },
    {
      "epoch": 1.678048780487805,
      "grad_norm": 0.11612200736999512,
      "learning_rate": 2.0368420064791777e-05,
      "loss": 0.0137,
      "step": 6880
    },
    {
      "epoch": 1.6782926829268292,
      "grad_norm": 0.22199319303035736,
      "learning_rate": 2.03621448920569e-05,
      "loss": 0.0209,
      "step": 6881
    },
    {
      "epoch": 1.6785365853658538,
      "grad_norm": 0.17557205259799957,
      "learning_rate": 2.035587002192787e-05,
      "loss": 0.0134,
      "step": 6882
    },
    {
      "epoch": 1.678780487804878,
      "grad_norm": 0.06689164787530899,
      "learning_rate": 2.0349595454814103e-05,
      "loss": 0.0128,
      "step": 6883
    },
    {
      "epoch": 1.6790243902439026,
      "grad_norm": 0.1681406944990158,
      "learning_rate": 2.034332119112499e-05,
      "loss": 0.015,
      "step": 6884
    },
    {
      "epoch": 1.6792682926829268,
      "grad_norm": 0.13956990838050842,
      "learning_rate": 2.033704723126992e-05,
      "loss": 0.0218,
      "step": 6885
    },
    {
      "epoch": 1.6795121951219514,
      "grad_norm": 0.08211492002010345,
      "learning_rate": 2.033077357565825e-05,
      "loss": 0.0236,
      "step": 6886
    },
    {
      "epoch": 1.6797560975609755,
      "grad_norm": 0.20645307004451752,
      "learning_rate": 2.03245002246993e-05,
      "loss": 0.029,
      "step": 6887
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.16901209950447083,
      "learning_rate": 2.0318227178802403e-05,
      "loss": 0.0268,
      "step": 6888
    },
    {
      "epoch": 1.6802439024390243,
      "grad_norm": 0.11103029549121857,
      "learning_rate": 2.0311954438376852e-05,
      "loss": 0.0159,
      "step": 6889
    },
    {
      "epoch": 1.680487804878049,
      "grad_norm": 0.1248159408569336,
      "learning_rate": 2.0305682003831915e-05,
      "loss": 0.0198,
      "step": 6890
    },
    {
      "epoch": 1.680731707317073,
      "grad_norm": 0.10502463579177856,
      "learning_rate": 2.029940987557686e-05,
      "loss": 0.0164,
      "step": 6891
    },
    {
      "epoch": 1.6809756097560977,
      "grad_norm": 0.09904863685369492,
      "learning_rate": 2.029313805402092e-05,
      "loss": 0.0155,
      "step": 6892
    },
    {
      "epoch": 1.681219512195122,
      "grad_norm": 0.17787380516529083,
      "learning_rate": 2.0286866539573317e-05,
      "loss": 0.0243,
      "step": 6893
    },
    {
      "epoch": 1.6814634146341465,
      "grad_norm": 0.07995554059743881,
      "learning_rate": 2.0280595332643245e-05,
      "loss": 0.0307,
      "step": 6894
    },
    {
      "epoch": 1.6817073170731707,
      "grad_norm": 0.07995827496051788,
      "learning_rate": 2.027432443363988e-05,
      "loss": 0.0062,
      "step": 6895
    },
    {
      "epoch": 1.6819512195121953,
      "grad_norm": 0.046719856560230255,
      "learning_rate": 2.0268053842972374e-05,
      "loss": 0.0086,
      "step": 6896
    },
    {
      "epoch": 1.6821951219512195,
      "grad_norm": 0.15578749775886536,
      "learning_rate": 2.0261783561049867e-05,
      "loss": 0.0235,
      "step": 6897
    },
    {
      "epoch": 1.682439024390244,
      "grad_norm": 0.1582854986190796,
      "learning_rate": 2.025551358828148e-05,
      "loss": 0.0136,
      "step": 6898
    },
    {
      "epoch": 1.6826829268292682,
      "grad_norm": 0.19258679449558258,
      "learning_rate": 2.024924392507631e-05,
      "loss": 0.0178,
      "step": 6899
    },
    {
      "epoch": 1.6829268292682928,
      "grad_norm": 0.10471425205469131,
      "learning_rate": 2.024297457184343e-05,
      "loss": 0.0216,
      "step": 6900
    },
    {
      "epoch": 1.683170731707317,
      "grad_norm": 0.04982857033610344,
      "learning_rate": 2.0236705528991892e-05,
      "loss": 0.0103,
      "step": 6901
    },
    {
      "epoch": 1.6834146341463416,
      "grad_norm": 0.12350182235240936,
      "learning_rate": 2.0230436796930742e-05,
      "loss": 0.0277,
      "step": 6902
    },
    {
      "epoch": 1.6836585365853658,
      "grad_norm": 0.19828151166439056,
      "learning_rate": 2.0224168376068986e-05,
      "loss": 0.0168,
      "step": 6903
    },
    {
      "epoch": 1.6839024390243904,
      "grad_norm": 0.06570941209793091,
      "learning_rate": 2.0217900266815624e-05,
      "loss": 0.0137,
      "step": 6904
    },
    {
      "epoch": 1.6841463414634146,
      "grad_norm": 0.08413808792829514,
      "learning_rate": 2.021163246957963e-05,
      "loss": 0.0119,
      "step": 6905
    },
    {
      "epoch": 1.6843902439024392,
      "grad_norm": 0.09983031451702118,
      "learning_rate": 2.020536498476996e-05,
      "loss": 0.0211,
      "step": 6906
    },
    {
      "epoch": 1.6846341463414634,
      "grad_norm": 0.0777423232793808,
      "learning_rate": 2.0199097812795546e-05,
      "loss": 0.0139,
      "step": 6907
    },
    {
      "epoch": 1.684878048780488,
      "grad_norm": 0.06257323175668716,
      "learning_rate": 2.01928309540653e-05,
      "loss": 0.0147,
      "step": 6908
    },
    {
      "epoch": 1.6851219512195121,
      "grad_norm": 0.1297847032546997,
      "learning_rate": 2.018656440898812e-05,
      "loss": 0.0242,
      "step": 6909
    },
    {
      "epoch": 1.6853658536585368,
      "grad_norm": 0.23573057353496552,
      "learning_rate": 2.018029817797289e-05,
      "loss": 0.0325,
      "step": 6910
    },
    {
      "epoch": 1.685609756097561,
      "grad_norm": 0.14130811393260956,
      "learning_rate": 2.0174032261428436e-05,
      "loss": 0.0213,
      "step": 6911
    },
    {
      "epoch": 1.6858536585365855,
      "grad_norm": 0.1171950101852417,
      "learning_rate": 2.0167766659763606e-05,
      "loss": 0.0213,
      "step": 6912
    },
    {
      "epoch": 1.6860975609756097,
      "grad_norm": 0.10324814170598984,
      "learning_rate": 2.0161501373387207e-05,
      "loss": 0.0167,
      "step": 6913
    },
    {
      "epoch": 1.6863414634146343,
      "grad_norm": 0.19946794211864471,
      "learning_rate": 2.015523640270803e-05,
      "loss": 0.0221,
      "step": 6914
    },
    {
      "epoch": 1.6865853658536585,
      "grad_norm": 0.08976856619119644,
      "learning_rate": 2.0148971748134844e-05,
      "loss": 0.0191,
      "step": 6915
    },
    {
      "epoch": 1.686829268292683,
      "grad_norm": 0.31741055846214294,
      "learning_rate": 2.014270741007641e-05,
      "loss": 0.0251,
      "step": 6916
    },
    {
      "epoch": 1.6870731707317073,
      "grad_norm": 0.11568129062652588,
      "learning_rate": 2.0136443388941443e-05,
      "loss": 0.0166,
      "step": 6917
    },
    {
      "epoch": 1.6873170731707319,
      "grad_norm": 0.14547143876552582,
      "learning_rate": 2.0130179685138654e-05,
      "loss": 0.019,
      "step": 6918
    },
    {
      "epoch": 1.687560975609756,
      "grad_norm": 0.15868200361728668,
      "learning_rate": 2.0123916299076738e-05,
      "loss": 0.017,
      "step": 6919
    },
    {
      "epoch": 1.6878048780487804,
      "grad_norm": 0.18193647265434265,
      "learning_rate": 2.011765323116435e-05,
      "loss": 0.0113,
      "step": 6920
    },
    {
      "epoch": 1.6880487804878048,
      "grad_norm": 0.11548196524381638,
      "learning_rate": 2.0111390481810143e-05,
      "loss": 0.0264,
      "step": 6921
    },
    {
      "epoch": 1.6882926829268292,
      "grad_norm": 0.13508063554763794,
      "learning_rate": 2.0105128051422745e-05,
      "loss": 0.0274,
      "step": 6922
    },
    {
      "epoch": 1.6885365853658536,
      "grad_norm": 0.09801782667636871,
      "learning_rate": 2.009886594041076e-05,
      "loss": 0.01,
      "step": 6923
    },
    {
      "epoch": 1.688780487804878,
      "grad_norm": 0.09230571985244751,
      "learning_rate": 2.0092604149182762e-05,
      "loss": 0.0144,
      "step": 6924
    },
    {
      "epoch": 1.6890243902439024,
      "grad_norm": 0.2745218575000763,
      "learning_rate": 2.008634267814732e-05,
      "loss": 0.0145,
      "step": 6925
    },
    {
      "epoch": 1.6892682926829268,
      "grad_norm": 0.16958314180374146,
      "learning_rate": 2.008008152771298e-05,
      "loss": 0.0163,
      "step": 6926
    },
    {
      "epoch": 1.6895121951219512,
      "grad_norm": 0.06501583009958267,
      "learning_rate": 2.0073820698288265e-05,
      "loss": 0.0124,
      "step": 6927
    },
    {
      "epoch": 1.6897560975609756,
      "grad_norm": 0.19938457012176514,
      "learning_rate": 2.006756019028166e-05,
      "loss": 0.029,
      "step": 6928
    },
    {
      "epoch": 1.69,
      "grad_norm": 0.30722367763519287,
      "learning_rate": 2.006130000410166e-05,
      "loss": 0.0262,
      "step": 6929
    },
    {
      "epoch": 1.6902439024390243,
      "grad_norm": 0.13401836156845093,
      "learning_rate": 2.005504014015671e-05,
      "loss": 0.0095,
      "step": 6930
    },
    {
      "epoch": 1.6904878048780487,
      "grad_norm": 0.12373414635658264,
      "learning_rate": 2.004878059885525e-05,
      "loss": 0.0157,
      "step": 6931
    },
    {
      "epoch": 1.6907317073170731,
      "grad_norm": 0.14957134425640106,
      "learning_rate": 2.0042521380605707e-05,
      "loss": 0.0221,
      "step": 6932
    },
    {
      "epoch": 1.6909756097560975,
      "grad_norm": 0.12315084785223007,
      "learning_rate": 2.003626248581646e-05,
      "loss": 0.031,
      "step": 6933
    },
    {
      "epoch": 1.691219512195122,
      "grad_norm": 0.1491011530160904,
      "learning_rate": 2.0030003914895897e-05,
      "loss": 0.021,
      "step": 6934
    },
    {
      "epoch": 1.6914634146341463,
      "grad_norm": 0.060368217527866364,
      "learning_rate": 2.0023745668252355e-05,
      "loss": 0.011,
      "step": 6935
    },
    {
      "epoch": 1.6917073170731707,
      "grad_norm": 0.1771264225244522,
      "learning_rate": 2.0017487746294184e-05,
      "loss": 0.0262,
      "step": 6936
    },
    {
      "epoch": 1.691951219512195,
      "grad_norm": 0.2395658791065216,
      "learning_rate": 2.0011230149429678e-05,
      "loss": 0.0326,
      "step": 6937
    },
    {
      "epoch": 1.6921951219512195,
      "grad_norm": 0.15752464532852173,
      "learning_rate": 2.0004972878067136e-05,
      "loss": 0.0145,
      "step": 6938
    },
    {
      "epoch": 1.6924390243902439,
      "grad_norm": 0.12557555735111237,
      "learning_rate": 1.9998715932614818e-05,
      "loss": 0.018,
      "step": 6939
    },
    {
      "epoch": 1.6926829268292682,
      "grad_norm": 0.09208472073078156,
      "learning_rate": 1.999245931348098e-05,
      "loss": 0.0178,
      "step": 6940
    },
    {
      "epoch": 1.6929268292682926,
      "grad_norm": 0.2953568994998932,
      "learning_rate": 1.998620302107384e-05,
      "loss": 0.0236,
      "step": 6941
    },
    {
      "epoch": 1.693170731707317,
      "grad_norm": 0.1281118243932724,
      "learning_rate": 1.9979947055801602e-05,
      "loss": 0.0101,
      "step": 6942
    },
    {
      "epoch": 1.6934146341463414,
      "grad_norm": 0.16891878843307495,
      "learning_rate": 1.9973691418072464e-05,
      "loss": 0.0148,
      "step": 6943
    },
    {
      "epoch": 1.6936585365853658,
      "grad_norm": 0.14038990437984467,
      "learning_rate": 1.996743610829456e-05,
      "loss": 0.0161,
      "step": 6944
    },
    {
      "epoch": 1.6939024390243902,
      "grad_norm": 0.09771297872066498,
      "learning_rate": 1.9961181126876055e-05,
      "loss": 0.0317,
      "step": 6945
    },
    {
      "epoch": 1.6941463414634146,
      "grad_norm": 0.21947379410266876,
      "learning_rate": 1.995492647422505e-05,
      "loss": 0.0345,
      "step": 6946
    },
    {
      "epoch": 1.694390243902439,
      "grad_norm": 0.10089663416147232,
      "learning_rate": 1.9948672150749643e-05,
      "loss": 0.0068,
      "step": 6947
    },
    {
      "epoch": 1.6946341463414634,
      "grad_norm": 0.19056569039821625,
      "learning_rate": 1.9942418156857915e-05,
      "loss": 0.0219,
      "step": 6948
    },
    {
      "epoch": 1.6948780487804878,
      "grad_norm": 0.12197564542293549,
      "learning_rate": 1.9936164492957928e-05,
      "loss": 0.0169,
      "step": 6949
    },
    {
      "epoch": 1.6951219512195121,
      "grad_norm": 0.12252405285835266,
      "learning_rate": 1.9929911159457697e-05,
      "loss": 0.0335,
      "step": 6950
    },
    {
      "epoch": 1.6953658536585365,
      "grad_norm": 0.2664389908313751,
      "learning_rate": 1.9923658156765245e-05,
      "loss": 0.0183,
      "step": 6951
    },
    {
      "epoch": 1.695609756097561,
      "grad_norm": 0.10748541355133057,
      "learning_rate": 1.9917405485288557e-05,
      "loss": 0.0226,
      "step": 6952
    },
    {
      "epoch": 1.6958536585365853,
      "grad_norm": 0.12239140272140503,
      "learning_rate": 1.9911153145435606e-05,
      "loss": 0.0138,
      "step": 6953
    },
    {
      "epoch": 1.6960975609756097,
      "grad_norm": 0.22570398449897766,
      "learning_rate": 1.990490113761433e-05,
      "loss": 0.0273,
      "step": 6954
    },
    {
      "epoch": 1.696341463414634,
      "grad_norm": 0.1874019205570221,
      "learning_rate": 1.9898649462232656e-05,
      "loss": 0.0246,
      "step": 6955
    },
    {
      "epoch": 1.6965853658536585,
      "grad_norm": 0.14629577100276947,
      "learning_rate": 1.989239811969849e-05,
      "loss": 0.0226,
      "step": 6956
    },
    {
      "epoch": 1.6968292682926829,
      "grad_norm": 0.1432499885559082,
      "learning_rate": 1.9886147110419714e-05,
      "loss": 0.03,
      "step": 6957
    },
    {
      "epoch": 1.6970731707317073,
      "grad_norm": 0.12545102834701538,
      "learning_rate": 1.987989643480418e-05,
      "loss": 0.0156,
      "step": 6958
    },
    {
      "epoch": 1.6973170731707317,
      "grad_norm": 0.18085043132305145,
      "learning_rate": 1.9873646093259728e-05,
      "loss": 0.0174,
      "step": 6959
    },
    {
      "epoch": 1.697560975609756,
      "grad_norm": 0.10355813056230545,
      "learning_rate": 1.9867396086194186e-05,
      "loss": 0.0178,
      "step": 6960
    },
    {
      "epoch": 1.6978048780487804,
      "grad_norm": 0.10787852853536606,
      "learning_rate": 1.9861146414015327e-05,
      "loss": 0.0097,
      "step": 6961
    },
    {
      "epoch": 1.6980487804878048,
      "grad_norm": 0.38239577412605286,
      "learning_rate": 1.985489707713094e-05,
      "loss": 0.0243,
      "step": 6962
    },
    {
      "epoch": 1.6982926829268292,
      "grad_norm": 0.1974622756242752,
      "learning_rate": 1.984864807594876e-05,
      "loss": 0.014,
      "step": 6963
    },
    {
      "epoch": 1.6985365853658536,
      "grad_norm": 0.15638110041618347,
      "learning_rate": 1.9842399410876525e-05,
      "loss": 0.0252,
      "step": 6964
    },
    {
      "epoch": 1.698780487804878,
      "grad_norm": 0.128541499376297,
      "learning_rate": 1.9836151082321937e-05,
      "loss": 0.0239,
      "step": 6965
    },
    {
      "epoch": 1.6990243902439024,
      "grad_norm": 0.12570102512836456,
      "learning_rate": 1.982990309069269e-05,
      "loss": 0.0331,
      "step": 6966
    },
    {
      "epoch": 1.6992682926829268,
      "grad_norm": 0.13879939913749695,
      "learning_rate": 1.9823655436396434e-05,
      "loss": 0.0244,
      "step": 6967
    },
    {
      "epoch": 1.6995121951219512,
      "grad_norm": 0.1849382221698761,
      "learning_rate": 1.9817408119840813e-05,
      "loss": 0.0346,
      "step": 6968
    },
    {
      "epoch": 1.6997560975609756,
      "grad_norm": 0.10112941265106201,
      "learning_rate": 1.981116114143345e-05,
      "loss": 0.0205,
      "step": 6969
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.10343100875616074,
      "learning_rate": 1.9804914501581943e-05,
      "loss": 0.0164,
      "step": 6970
    },
    {
      "epoch": 1.7002439024390243,
      "grad_norm": 0.18953916430473328,
      "learning_rate": 1.9798668200693855e-05,
      "loss": 0.0303,
      "step": 6971
    },
    {
      "epoch": 1.7004878048780487,
      "grad_norm": 0.162181094288826,
      "learning_rate": 1.9792422239176738e-05,
      "loss": 0.0152,
      "step": 6972
    },
    {
      "epoch": 1.7007317073170731,
      "grad_norm": 0.13118721544742584,
      "learning_rate": 1.9786176617438136e-05,
      "loss": 0.0257,
      "step": 6973
    },
    {
      "epoch": 1.7009756097560975,
      "grad_norm": 0.09344034641981125,
      "learning_rate": 1.977993133588555e-05,
      "loss": 0.0259,
      "step": 6974
    },
    {
      "epoch": 1.701219512195122,
      "grad_norm": 0.15850721299648285,
      "learning_rate": 1.977368639492646e-05,
      "loss": 0.0232,
      "step": 6975
    },
    {
      "epoch": 1.7014634146341463,
      "grad_norm": 0.15972304344177246,
      "learning_rate": 1.9767441794968345e-05,
      "loss": 0.0161,
      "step": 6976
    },
    {
      "epoch": 1.7017073170731707,
      "grad_norm": 0.11160274595022202,
      "learning_rate": 1.9761197536418625e-05,
      "loss": 0.0187,
      "step": 6977
    },
    {
      "epoch": 1.701951219512195,
      "grad_norm": 0.1431739181280136,
      "learning_rate": 1.9754953619684725e-05,
      "loss": 0.0237,
      "step": 6978
    },
    {
      "epoch": 1.7021951219512195,
      "grad_norm": 0.09147361665964127,
      "learning_rate": 1.9748710045174053e-05,
      "loss": 0.0265,
      "step": 6979
    },
    {
      "epoch": 1.7024390243902439,
      "grad_norm": 0.15977251529693604,
      "learning_rate": 1.9742466813293965e-05,
      "loss": 0.0228,
      "step": 6980
    },
    {
      "epoch": 1.7026829268292683,
      "grad_norm": 0.07590578496456146,
      "learning_rate": 1.9736223924451825e-05,
      "loss": 0.0176,
      "step": 6981
    },
    {
      "epoch": 1.7029268292682926,
      "grad_norm": 0.19247183203697205,
      "learning_rate": 1.972998137905496e-05,
      "loss": 0.0212,
      "step": 6982
    },
    {
      "epoch": 1.703170731707317,
      "grad_norm": 0.1323166787624359,
      "learning_rate": 1.972373917751068e-05,
      "loss": 0.0239,
      "step": 6983
    },
    {
      "epoch": 1.7034146341463414,
      "grad_norm": 0.12795871496200562,
      "learning_rate": 1.9717497320226268e-05,
      "loss": 0.0292,
      "step": 6984
    },
    {
      "epoch": 1.7036585365853658,
      "grad_norm": 0.21623937785625458,
      "learning_rate": 1.9711255807608977e-05,
      "loss": 0.0224,
      "step": 6985
    },
    {
      "epoch": 1.7039024390243902,
      "grad_norm": 0.1292436420917511,
      "learning_rate": 1.9705014640066058e-05,
      "loss": 0.0189,
      "step": 6986
    },
    {
      "epoch": 1.7041463414634146,
      "grad_norm": 0.19760476052761078,
      "learning_rate": 1.969877381800473e-05,
      "loss": 0.0314,
      "step": 6987
    },
    {
      "epoch": 1.704390243902439,
      "grad_norm": 0.13868090510368347,
      "learning_rate": 1.9692533341832176e-05,
      "loss": 0.0364,
      "step": 6988
    },
    {
      "epoch": 1.7046341463414634,
      "grad_norm": 0.13364572823047638,
      "learning_rate": 1.968629321195557e-05,
      "loss": 0.0234,
      "step": 6989
    },
    {
      "epoch": 1.7048780487804878,
      "grad_norm": 0.18304912745952606,
      "learning_rate": 1.968005342878207e-05,
      "loss": 0.0284,
      "step": 6990
    },
    {
      "epoch": 1.7051219512195122,
      "grad_norm": 0.13960020244121552,
      "learning_rate": 1.96738139927188e-05,
      "loss": 0.03,
      "step": 6991
    },
    {
      "epoch": 1.7053658536585365,
      "grad_norm": 0.1330072581768036,
      "learning_rate": 1.9667574904172858e-05,
      "loss": 0.0169,
      "step": 6992
    },
    {
      "epoch": 1.705609756097561,
      "grad_norm": 0.2883757948875427,
      "learning_rate": 1.966133616355134e-05,
      "loss": 0.021,
      "step": 6993
    },
    {
      "epoch": 1.7058536585365853,
      "grad_norm": 0.19475992023944855,
      "learning_rate": 1.9655097771261282e-05,
      "loss": 0.0208,
      "step": 6994
    },
    {
      "epoch": 1.7060975609756097,
      "grad_norm": 0.053542863577604294,
      "learning_rate": 1.9648859727709734e-05,
      "loss": 0.0106,
      "step": 6995
    },
    {
      "epoch": 1.706341463414634,
      "grad_norm": 0.08248420059680939,
      "learning_rate": 1.9642622033303713e-05,
      "loss": 0.0187,
      "step": 6996
    },
    {
      "epoch": 1.7065853658536585,
      "grad_norm": 0.10235189646482468,
      "learning_rate": 1.96363846884502e-05,
      "loss": 0.0167,
      "step": 6997
    },
    {
      "epoch": 1.7068292682926829,
      "grad_norm": 0.06620629131793976,
      "learning_rate": 1.963014769355616e-05,
      "loss": 0.0126,
      "step": 6998
    },
    {
      "epoch": 1.7070731707317073,
      "grad_norm": 0.11179495602846146,
      "learning_rate": 1.962391104902855e-05,
      "loss": 0.0149,
      "step": 6999
    },
    {
      "epoch": 1.7073170731707317,
      "grad_norm": 0.11498963087797165,
      "learning_rate": 1.961767475527429e-05,
      "loss": 0.0105,
      "step": 7000
    },
    {
      "epoch": 1.707560975609756,
      "grad_norm": 0.3349592387676239,
      "learning_rate": 1.961143881270027e-05,
      "loss": 0.018,
      "step": 7001
    },
    {
      "epoch": 1.7078048780487805,
      "grad_norm": 0.1318347305059433,
      "learning_rate": 1.9605203221713372e-05,
      "loss": 0.0284,
      "step": 7002
    },
    {
      "epoch": 1.7080487804878048,
      "grad_norm": 0.10101602971553802,
      "learning_rate": 1.959896798272045e-05,
      "loss": 0.0135,
      "step": 7003
    },
    {
      "epoch": 1.7082926829268292,
      "grad_norm": 0.09844134747982025,
      "learning_rate": 1.9592733096128336e-05,
      "loss": 0.0156,
      "step": 7004
    },
    {
      "epoch": 1.7085365853658536,
      "grad_norm": 0.14335498213768005,
      "learning_rate": 1.958649856234383e-05,
      "loss": 0.0178,
      "step": 7005
    },
    {
      "epoch": 1.708780487804878,
      "grad_norm": 0.09043950587511063,
      "learning_rate": 1.958026438177372e-05,
      "loss": 0.0121,
      "step": 7006
    },
    {
      "epoch": 1.7090243902439024,
      "grad_norm": 0.18831168115139008,
      "learning_rate": 1.957403055482477e-05,
      "loss": 0.0202,
      "step": 7007
    },
    {
      "epoch": 1.7092682926829268,
      "grad_norm": 0.1339479684829712,
      "learning_rate": 1.9567797081903716e-05,
      "loss": 0.0241,
      "step": 7008
    },
    {
      "epoch": 1.7095121951219512,
      "grad_norm": 0.19723385572433472,
      "learning_rate": 1.9561563963417278e-05,
      "loss": 0.0394,
      "step": 7009
    },
    {
      "epoch": 1.7097560975609756,
      "grad_norm": 0.13506168127059937,
      "learning_rate": 1.9555331199772138e-05,
      "loss": 0.0389,
      "step": 7010
    },
    {
      "epoch": 1.71,
      "grad_norm": 0.11871456354856491,
      "learning_rate": 1.9549098791374965e-05,
      "loss": 0.0215,
      "step": 7011
    },
    {
      "epoch": 1.7102439024390244,
      "grad_norm": 0.1013561487197876,
      "learning_rate": 1.954286673863241e-05,
      "loss": 0.0156,
      "step": 7012
    },
    {
      "epoch": 1.7104878048780487,
      "grad_norm": 0.09068997949361801,
      "learning_rate": 1.95366350419511e-05,
      "loss": 0.021,
      "step": 7013
    },
    {
      "epoch": 1.7107317073170731,
      "grad_norm": 0.10772193968296051,
      "learning_rate": 1.9530403701737625e-05,
      "loss": 0.0211,
      "step": 7014
    },
    {
      "epoch": 1.7109756097560975,
      "grad_norm": 0.1607835590839386,
      "learning_rate": 1.9524172718398565e-05,
      "loss": 0.0194,
      "step": 7015
    },
    {
      "epoch": 1.711219512195122,
      "grad_norm": 0.11943427473306656,
      "learning_rate": 1.951794209234047e-05,
      "loss": 0.0241,
      "step": 7016
    },
    {
      "epoch": 1.7114634146341463,
      "grad_norm": 0.10698289424180984,
      "learning_rate": 1.9511711823969875e-05,
      "loss": 0.0337,
      "step": 7017
    },
    {
      "epoch": 1.7117073170731707,
      "grad_norm": 0.1633470356464386,
      "learning_rate": 1.950548191369328e-05,
      "loss": 0.0184,
      "step": 7018
    },
    {
      "epoch": 1.711951219512195,
      "grad_norm": 0.15118086338043213,
      "learning_rate": 1.9499252361917168e-05,
      "loss": 0.0238,
      "step": 7019
    },
    {
      "epoch": 1.7121951219512195,
      "grad_norm": 0.1199861466884613,
      "learning_rate": 1.9493023169048004e-05,
      "loss": 0.0286,
      "step": 7020
    },
    {
      "epoch": 1.7124390243902439,
      "grad_norm": 0.09051667898893356,
      "learning_rate": 1.9486794335492218e-05,
      "loss": 0.0154,
      "step": 7021
    },
    {
      "epoch": 1.7126829268292683,
      "grad_norm": 0.24999137222766876,
      "learning_rate": 1.9480565861656218e-05,
      "loss": 0.0253,
      "step": 7022
    },
    {
      "epoch": 1.7129268292682926,
      "grad_norm": 0.142018660902977,
      "learning_rate": 1.9474337747946405e-05,
      "loss": 0.0237,
      "step": 7023
    },
    {
      "epoch": 1.713170731707317,
      "grad_norm": 0.12471751868724823,
      "learning_rate": 1.9468109994769143e-05,
      "loss": 0.0112,
      "step": 7024
    },
    {
      "epoch": 1.7134146341463414,
      "grad_norm": 0.07650507241487503,
      "learning_rate": 1.9461882602530763e-05,
      "loss": 0.0123,
      "step": 7025
    },
    {
      "epoch": 1.7136585365853658,
      "grad_norm": 0.13230988383293152,
      "learning_rate": 1.94556555716376e-05,
      "loss": 0.024,
      "step": 7026
    },
    {
      "epoch": 1.7139024390243902,
      "grad_norm": 0.10565539449453354,
      "learning_rate": 1.9449428902495926e-05,
      "loss": 0.0134,
      "step": 7027
    },
    {
      "epoch": 1.7141463414634146,
      "grad_norm": 0.12428409606218338,
      "learning_rate": 1.9443202595512022e-05,
      "loss": 0.0177,
      "step": 7028
    },
    {
      "epoch": 1.714390243902439,
      "grad_norm": 0.11924461275339127,
      "learning_rate": 1.9436976651092144e-05,
      "loss": 0.0214,
      "step": 7029
    },
    {
      "epoch": 1.7146341463414634,
      "grad_norm": 0.08407869935035706,
      "learning_rate": 1.943075106964251e-05,
      "loss": 0.0152,
      "step": 7030
    },
    {
      "epoch": 1.7148780487804878,
      "grad_norm": 0.14884281158447266,
      "learning_rate": 1.9424525851569315e-05,
      "loss": 0.0157,
      "step": 7031
    },
    {
      "epoch": 1.7151219512195122,
      "grad_norm": 0.037212006747722626,
      "learning_rate": 1.9418300997278743e-05,
      "loss": 0.0074,
      "step": 7032
    },
    {
      "epoch": 1.7153658536585366,
      "grad_norm": 0.2127280831336975,
      "learning_rate": 1.9412076507176946e-05,
      "loss": 0.0184,
      "step": 7033
    },
    {
      "epoch": 1.715609756097561,
      "grad_norm": 0.10657911747694016,
      "learning_rate": 1.9405852381670046e-05,
      "loss": 0.0292,
      "step": 7034
    },
    {
      "epoch": 1.7158536585365853,
      "grad_norm": 0.14420083165168762,
      "learning_rate": 1.939962862116415e-05,
      "loss": 0.0236,
      "step": 7035
    },
    {
      "epoch": 1.7160975609756097,
      "grad_norm": 0.093436099588871,
      "learning_rate": 1.939340522606535e-05,
      "loss": 0.021,
      "step": 7036
    },
    {
      "epoch": 1.7163414634146341,
      "grad_norm": 0.16561773419380188,
      "learning_rate": 1.9387182196779693e-05,
      "loss": 0.0217,
      "step": 7037
    },
    {
      "epoch": 1.7165853658536585,
      "grad_norm": 0.05618220940232277,
      "learning_rate": 1.9380959533713215e-05,
      "loss": 0.0095,
      "step": 7038
    },
    {
      "epoch": 1.716829268292683,
      "grad_norm": 0.06675945967435837,
      "learning_rate": 1.9374737237271923e-05,
      "loss": 0.0173,
      "step": 7039
    },
    {
      "epoch": 1.7170731707317073,
      "grad_norm": 0.24829590320587158,
      "learning_rate": 1.9368515307861806e-05,
      "loss": 0.0306,
      "step": 7040
    },
    {
      "epoch": 1.7173170731707317,
      "grad_norm": 0.10868538171052933,
      "learning_rate": 1.9362293745888828e-05,
      "loss": 0.0251,
      "step": 7041
    },
    {
      "epoch": 1.717560975609756,
      "grad_norm": 0.14901508390903473,
      "learning_rate": 1.935607255175893e-05,
      "loss": 0.0232,
      "step": 7042
    },
    {
      "epoch": 1.7178048780487805,
      "grad_norm": 0.15394756197929382,
      "learning_rate": 1.934985172587802e-05,
      "loss": 0.0214,
      "step": 7043
    },
    {
      "epoch": 1.7180487804878048,
      "grad_norm": 0.16721759736537933,
      "learning_rate": 1.934363126865198e-05,
      "loss": 0.0134,
      "step": 7044
    },
    {
      "epoch": 1.7182926829268292,
      "grad_norm": 0.12777332961559296,
      "learning_rate": 1.9337411180486684e-05,
      "loss": 0.015,
      "step": 7045
    },
    {
      "epoch": 1.7185365853658536,
      "grad_norm": 0.1507657915353775,
      "learning_rate": 1.9331191461787974e-05,
      "loss": 0.0182,
      "step": 7046
    },
    {
      "epoch": 1.718780487804878,
      "grad_norm": 0.16534008085727692,
      "learning_rate": 1.932497211296167e-05,
      "loss": 0.0234,
      "step": 7047
    },
    {
      "epoch": 1.7190243902439024,
      "grad_norm": 0.11125683784484863,
      "learning_rate": 1.931875313441356e-05,
      "loss": 0.0214,
      "step": 7048
    },
    {
      "epoch": 1.7192682926829268,
      "grad_norm": 0.137700617313385,
      "learning_rate": 1.931253452654942e-05,
      "loss": 0.021,
      "step": 7049
    },
    {
      "epoch": 1.7195121951219512,
      "grad_norm": 0.13641732931137085,
      "learning_rate": 1.9306316289774988e-05,
      "loss": 0.021,
      "step": 7050
    },
    {
      "epoch": 1.7197560975609756,
      "grad_norm": 0.12689802050590515,
      "learning_rate": 1.930009842449599e-05,
      "loss": 0.0264,
      "step": 7051
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.12851598858833313,
      "learning_rate": 1.929388093111812e-05,
      "loss": 0.0153,
      "step": 7052
    },
    {
      "epoch": 1.7202439024390244,
      "grad_norm": 0.10966452211141586,
      "learning_rate": 1.9287663810047047e-05,
      "loss": 0.0254,
      "step": 7053
    },
    {
      "epoch": 1.7204878048780488,
      "grad_norm": 0.10704545676708221,
      "learning_rate": 1.928144706168843e-05,
      "loss": 0.0266,
      "step": 7054
    },
    {
      "epoch": 1.7207317073170731,
      "grad_norm": 0.14233706891536713,
      "learning_rate": 1.927523068644788e-05,
      "loss": 0.0247,
      "step": 7055
    },
    {
      "epoch": 1.7209756097560975,
      "grad_norm": 0.1199982538819313,
      "learning_rate": 1.9269014684731006e-05,
      "loss": 0.0108,
      "step": 7056
    },
    {
      "epoch": 1.721219512195122,
      "grad_norm": 0.09784766286611557,
      "learning_rate": 1.9262799056943377e-05,
      "loss": 0.0139,
      "step": 7057
    },
    {
      "epoch": 1.7214634146341463,
      "grad_norm": 0.08688991516828537,
      "learning_rate": 1.9256583803490553e-05,
      "loss": 0.0075,
      "step": 7058
    },
    {
      "epoch": 1.7217073170731707,
      "grad_norm": 0.11917977035045624,
      "learning_rate": 1.9250368924778053e-05,
      "loss": 0.0231,
      "step": 7059
    },
    {
      "epoch": 1.721951219512195,
      "grad_norm": 0.14155329763889313,
      "learning_rate": 1.924415442121138e-05,
      "loss": 0.0209,
      "step": 7060
    },
    {
      "epoch": 1.7221951219512195,
      "grad_norm": 0.13167299330234528,
      "learning_rate": 1.9237940293196006e-05,
      "loss": 0.0191,
      "step": 7061
    },
    {
      "epoch": 1.7224390243902439,
      "grad_norm": 0.14938917756080627,
      "learning_rate": 1.9231726541137385e-05,
      "loss": 0.041,
      "step": 7062
    },
    {
      "epoch": 1.7226829268292683,
      "grad_norm": 0.08869317173957825,
      "learning_rate": 1.9225513165440957e-05,
      "loss": 0.0127,
      "step": 7063
    },
    {
      "epoch": 1.7229268292682927,
      "grad_norm": 0.09962359815835953,
      "learning_rate": 1.921930016651211e-05,
      "loss": 0.0215,
      "step": 7064
    },
    {
      "epoch": 1.723170731707317,
      "grad_norm": 0.253823846578598,
      "learning_rate": 1.921308754475623e-05,
      "loss": 0.0254,
      "step": 7065
    },
    {
      "epoch": 1.7234146341463414,
      "grad_norm": 0.11026421189308167,
      "learning_rate": 1.9206875300578676e-05,
      "loss": 0.0247,
      "step": 7066
    },
    {
      "epoch": 1.7236585365853658,
      "grad_norm": 0.12771257758140564,
      "learning_rate": 1.9200663434384777e-05,
      "loss": 0.026,
      "step": 7067
    },
    {
      "epoch": 1.7239024390243902,
      "grad_norm": 0.1238134577870369,
      "learning_rate": 1.9194451946579834e-05,
      "loss": 0.0277,
      "step": 7068
    },
    {
      "epoch": 1.7241463414634146,
      "grad_norm": 0.07774243503808975,
      "learning_rate": 1.9188240837569126e-05,
      "loss": 0.0173,
      "step": 7069
    },
    {
      "epoch": 1.724390243902439,
      "grad_norm": 0.16945746541023254,
      "learning_rate": 1.9182030107757908e-05,
      "loss": 0.03,
      "step": 7070
    },
    {
      "epoch": 1.7246341463414634,
      "grad_norm": 0.15644972026348114,
      "learning_rate": 1.9175819757551423e-05,
      "loss": 0.0371,
      "step": 7071
    },
    {
      "epoch": 1.7248780487804878,
      "grad_norm": 0.11458208411931992,
      "learning_rate": 1.9169609787354866e-05,
      "loss": 0.0125,
      "step": 7072
    },
    {
      "epoch": 1.7251219512195122,
      "grad_norm": 0.15039494633674622,
      "learning_rate": 1.9163400197573423e-05,
      "loss": 0.0129,
      "step": 7073
    },
    {
      "epoch": 1.7253658536585366,
      "grad_norm": 0.1233067736029625,
      "learning_rate": 1.9157190988612247e-05,
      "loss": 0.0216,
      "step": 7074
    },
    {
      "epoch": 1.725609756097561,
      "grad_norm": 0.08119823783636093,
      "learning_rate": 1.9150982160876484e-05,
      "loss": 0.0183,
      "step": 7075
    },
    {
      "epoch": 1.7258536585365853,
      "grad_norm": 0.06947124749422073,
      "learning_rate": 1.9144773714771222e-05,
      "loss": 0.0084,
      "step": 7076
    },
    {
      "epoch": 1.7260975609756097,
      "grad_norm": 0.1069609746336937,
      "learning_rate": 1.913856565070155e-05,
      "loss": 0.0163,
      "step": 7077
    },
    {
      "epoch": 1.7263414634146341,
      "grad_norm": 0.14633861184120178,
      "learning_rate": 1.913235796907252e-05,
      "loss": 0.0234,
      "step": 7078
    },
    {
      "epoch": 1.7265853658536585,
      "grad_norm": 0.152476504445076,
      "learning_rate": 1.9126150670289172e-05,
      "loss": 0.0225,
      "step": 7079
    },
    {
      "epoch": 1.726829268292683,
      "grad_norm": 0.14770886301994324,
      "learning_rate": 1.9119943754756518e-05,
      "loss": 0.0135,
      "step": 7080
    },
    {
      "epoch": 1.7270731707317073,
      "grad_norm": 0.10284020751714706,
      "learning_rate": 1.9113737222879526e-05,
      "loss": 0.0223,
      "step": 7081
    },
    {
      "epoch": 1.7273170731707317,
      "grad_norm": 0.08319828659296036,
      "learning_rate": 1.9107531075063163e-05,
      "loss": 0.0145,
      "step": 7082
    },
    {
      "epoch": 1.727560975609756,
      "grad_norm": 0.11402386426925659,
      "learning_rate": 1.9101325311712358e-05,
      "loss": 0.0074,
      "step": 7083
    },
    {
      "epoch": 1.7278048780487805,
      "grad_norm": 0.22133494913578033,
      "learning_rate": 1.9095119933232022e-05,
      "loss": 0.0196,
      "step": 7084
    },
    {
      "epoch": 1.7280487804878049,
      "grad_norm": 0.08326509594917297,
      "learning_rate": 1.9088914940027032e-05,
      "loss": 0.0341,
      "step": 7085
    },
    {
      "epoch": 1.7282926829268292,
      "grad_norm": 0.2267410010099411,
      "learning_rate": 1.9082710332502245e-05,
      "loss": 0.0138,
      "step": 7086
    },
    {
      "epoch": 1.7285365853658536,
      "grad_norm": 0.132320374250412,
      "learning_rate": 1.907650611106249e-05,
      "loss": 0.0083,
      "step": 7087
    },
    {
      "epoch": 1.728780487804878,
      "grad_norm": 0.21904857456684113,
      "learning_rate": 1.907030227611259e-05,
      "loss": 0.0184,
      "step": 7088
    },
    {
      "epoch": 1.7290243902439024,
      "grad_norm": 0.1290457397699356,
      "learning_rate": 1.9064098828057306e-05,
      "loss": 0.0079,
      "step": 7089
    },
    {
      "epoch": 1.7292682926829268,
      "grad_norm": 0.0775296688079834,
      "learning_rate": 1.90578957673014e-05,
      "loss": 0.0112,
      "step": 7090
    },
    {
      "epoch": 1.7295121951219512,
      "grad_norm": 0.10735198110342026,
      "learning_rate": 1.9051693094249607e-05,
      "loss": 0.02,
      "step": 7091
    },
    {
      "epoch": 1.7297560975609756,
      "grad_norm": 0.07401032745838165,
      "learning_rate": 1.904549080930664e-05,
      "loss": 0.0166,
      "step": 7092
    },
    {
      "epoch": 1.73,
      "grad_norm": 0.13451442122459412,
      "learning_rate": 1.903928891287716e-05,
      "loss": 0.0129,
      "step": 7093
    },
    {
      "epoch": 1.7302439024390244,
      "grad_norm": 0.15218114852905273,
      "learning_rate": 1.9033087405365835e-05,
      "loss": 0.0205,
      "step": 7094
    },
    {
      "epoch": 1.7304878048780488,
      "grad_norm": 0.11221764236688614,
      "learning_rate": 1.9026886287177282e-05,
      "loss": 0.0191,
      "step": 7095
    },
    {
      "epoch": 1.7307317073170732,
      "grad_norm": 0.05720992386341095,
      "learning_rate": 1.9020685558716117e-05,
      "loss": 0.007,
      "step": 7096
    },
    {
      "epoch": 1.7309756097560975,
      "grad_norm": 0.08134730160236359,
      "learning_rate": 1.901448522038692e-05,
      "loss": 0.0119,
      "step": 7097
    },
    {
      "epoch": 1.731219512195122,
      "grad_norm": 0.14781388640403748,
      "learning_rate": 1.900828527259423e-05,
      "loss": 0.0173,
      "step": 7098
    },
    {
      "epoch": 1.7314634146341463,
      "grad_norm": 0.08029764145612717,
      "learning_rate": 1.9002085715742588e-05,
      "loss": 0.0182,
      "step": 7099
    },
    {
      "epoch": 1.7317073170731707,
      "grad_norm": 0.1794692724943161,
      "learning_rate": 1.8995886550236493e-05,
      "loss": 0.023,
      "step": 7100
    },
    {
      "epoch": 1.731951219512195,
      "grad_norm": 0.10423334687948227,
      "learning_rate": 1.898968777648042e-05,
      "loss": 0.0134,
      "step": 7101
    },
    {
      "epoch": 1.7321951219512195,
      "grad_norm": 0.16372109949588776,
      "learning_rate": 1.8983489394878817e-05,
      "loss": 0.0179,
      "step": 7102
    },
    {
      "epoch": 1.7324390243902439,
      "grad_norm": 0.1355905681848526,
      "learning_rate": 1.8977291405836115e-05,
      "loss": 0.0217,
      "step": 7103
    },
    {
      "epoch": 1.7326829268292683,
      "grad_norm": 0.12377317994832993,
      "learning_rate": 1.8971093809756707e-05,
      "loss": 0.0218,
      "step": 7104
    },
    {
      "epoch": 1.7329268292682927,
      "grad_norm": 0.15703491866588593,
      "learning_rate": 1.896489660704498e-05,
      "loss": 0.0244,
      "step": 7105
    },
    {
      "epoch": 1.733170731707317,
      "grad_norm": 0.1102873682975769,
      "learning_rate": 1.895869979810527e-05,
      "loss": 0.022,
      "step": 7106
    },
    {
      "epoch": 1.7334146341463414,
      "grad_norm": 0.14667199552059174,
      "learning_rate": 1.89525033833419e-05,
      "loss": 0.0246,
      "step": 7107
    },
    {
      "epoch": 1.7336585365853658,
      "grad_norm": 0.1442093402147293,
      "learning_rate": 1.8946307363159177e-05,
      "loss": 0.024,
      "step": 7108
    },
    {
      "epoch": 1.7339024390243902,
      "grad_norm": 0.09476162493228912,
      "learning_rate": 1.894011173796137e-05,
      "loss": 0.0157,
      "step": 7109
    },
    {
      "epoch": 1.7341463414634146,
      "grad_norm": 0.11591134965419769,
      "learning_rate": 1.8933916508152723e-05,
      "loss": 0.013,
      "step": 7110
    },
    {
      "epoch": 1.734390243902439,
      "grad_norm": 0.1182047426700592,
      "learning_rate": 1.8927721674137445e-05,
      "loss": 0.0278,
      "step": 7111
    },
    {
      "epoch": 1.7346341463414634,
      "grad_norm": 0.12143267691135406,
      "learning_rate": 1.8921527236319735e-05,
      "loss": 0.0216,
      "step": 7112
    },
    {
      "epoch": 1.7348780487804878,
      "grad_norm": 0.12113425135612488,
      "learning_rate": 1.8915333195103772e-05,
      "loss": 0.0178,
      "step": 7113
    },
    {
      "epoch": 1.7351219512195122,
      "grad_norm": 0.12805722653865814,
      "learning_rate": 1.8909139550893693e-05,
      "loss": 0.0223,
      "step": 7114
    },
    {
      "epoch": 1.7353658536585366,
      "grad_norm": 0.1302306354045868,
      "learning_rate": 1.8902946304093604e-05,
      "loss": 0.0091,
      "step": 7115
    },
    {
      "epoch": 1.735609756097561,
      "grad_norm": 0.1047305092215538,
      "learning_rate": 1.8896753455107607e-05,
      "loss": 0.0095,
      "step": 7116
    },
    {
      "epoch": 1.7358536585365854,
      "grad_norm": 0.08829545974731445,
      "learning_rate": 1.889056100433976e-05,
      "loss": 0.0206,
      "step": 7117
    },
    {
      "epoch": 1.7360975609756097,
      "grad_norm": 0.2169303596019745,
      "learning_rate": 1.8884368952194118e-05,
      "loss": 0.0219,
      "step": 7118
    },
    {
      "epoch": 1.7363414634146341,
      "grad_norm": 0.12886463105678558,
      "learning_rate": 1.8878177299074668e-05,
      "loss": 0.0226,
      "step": 7119
    },
    {
      "epoch": 1.7365853658536585,
      "grad_norm": 0.1509641706943512,
      "learning_rate": 1.887198604538541e-05,
      "loss": 0.0222,
      "step": 7120
    },
    {
      "epoch": 1.736829268292683,
      "grad_norm": 0.11731714755296707,
      "learning_rate": 1.8865795191530305e-05,
      "loss": 0.0224,
      "step": 7121
    },
    {
      "epoch": 1.7370731707317073,
      "grad_norm": 0.0912993997335434,
      "learning_rate": 1.885960473791329e-05,
      "loss": 0.0111,
      "step": 7122
    },
    {
      "epoch": 1.7373170731707317,
      "grad_norm": 0.11931503564119339,
      "learning_rate": 1.8853414684938258e-05,
      "loss": 0.016,
      "step": 7123
    },
    {
      "epoch": 1.737560975609756,
      "grad_norm": 0.1227351650595665,
      "learning_rate": 1.8847225033009104e-05,
      "loss": 0.0181,
      "step": 7124
    },
    {
      "epoch": 1.7378048780487805,
      "grad_norm": 0.14969059824943542,
      "learning_rate": 1.8841035782529695e-05,
      "loss": 0.0184,
      "step": 7125
    },
    {
      "epoch": 1.7380487804878049,
      "grad_norm": 0.13221660256385803,
      "learning_rate": 1.8834846933903834e-05,
      "loss": 0.017,
      "step": 7126
    },
    {
      "epoch": 1.7382926829268293,
      "grad_norm": 0.3158467710018158,
      "learning_rate": 1.8828658487535342e-05,
      "loss": 0.026,
      "step": 7127
    },
    {
      "epoch": 1.7385365853658536,
      "grad_norm": 0.1479274034500122,
      "learning_rate": 1.8822470443827984e-05,
      "loss": 0.0163,
      "step": 7128
    },
    {
      "epoch": 1.738780487804878,
      "grad_norm": 0.15603341162204742,
      "learning_rate": 1.8816282803185523e-05,
      "loss": 0.0258,
      "step": 7129
    },
    {
      "epoch": 1.7390243902439024,
      "grad_norm": 0.22785554826259613,
      "learning_rate": 1.8810095566011675e-05,
      "loss": 0.0216,
      "step": 7130
    },
    {
      "epoch": 1.7392682926829268,
      "grad_norm": 0.21732117235660553,
      "learning_rate": 1.880390873271015e-05,
      "loss": 0.022,
      "step": 7131
    },
    {
      "epoch": 1.7395121951219512,
      "grad_norm": 0.06829048693180084,
      "learning_rate": 1.8797722303684606e-05,
      "loss": 0.0059,
      "step": 7132
    },
    {
      "epoch": 1.7397560975609756,
      "grad_norm": 0.08453648537397385,
      "learning_rate": 1.8791536279338696e-05,
      "loss": 0.0088,
      "step": 7133
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.11958207190036774,
      "learning_rate": 1.8785350660076033e-05,
      "loss": 0.0193,
      "step": 7134
    },
    {
      "epoch": 1.7402439024390244,
      "grad_norm": 0.18417809903621674,
      "learning_rate": 1.8779165446300226e-05,
      "loss": 0.0235,
      "step": 7135
    },
    {
      "epoch": 1.7404878048780488,
      "grad_norm": 0.05480913817882538,
      "learning_rate": 1.877298063841482e-05,
      "loss": 0.0152,
      "step": 7136
    },
    {
      "epoch": 1.7407317073170732,
      "grad_norm": 0.09626313298940659,
      "learning_rate": 1.876679623682337e-05,
      "loss": 0.0212,
      "step": 7137
    },
    {
      "epoch": 1.7409756097560976,
      "grad_norm": 0.17808425426483154,
      "learning_rate": 1.8760612241929385e-05,
      "loss": 0.0254,
      "step": 7138
    },
    {
      "epoch": 1.741219512195122,
      "grad_norm": 0.35340631008148193,
      "learning_rate": 1.8754428654136346e-05,
      "loss": 0.0209,
      "step": 7139
    },
    {
      "epoch": 1.7414634146341463,
      "grad_norm": 0.15835259854793549,
      "learning_rate": 1.8748245473847718e-05,
      "loss": 0.0264,
      "step": 7140
    },
    {
      "epoch": 1.7417073170731707,
      "grad_norm": 0.10540545731782913,
      "learning_rate": 1.8742062701466934e-05,
      "loss": 0.0147,
      "step": 7141
    },
    {
      "epoch": 1.7419512195121951,
      "grad_norm": 0.12754687666893005,
      "learning_rate": 1.873588033739741e-05,
      "loss": 0.0262,
      "step": 7142
    },
    {
      "epoch": 1.7421951219512195,
      "grad_norm": 0.1007089838385582,
      "learning_rate": 1.872969838204251e-05,
      "loss": 0.0147,
      "step": 7143
    },
    {
      "epoch": 1.742439024390244,
      "grad_norm": 0.15846899151802063,
      "learning_rate": 1.87235168358056e-05,
      "loss": 0.0168,
      "step": 7144
    },
    {
      "epoch": 1.7426829268292683,
      "grad_norm": 0.1466098427772522,
      "learning_rate": 1.8717335699089997e-05,
      "loss": 0.044,
      "step": 7145
    },
    {
      "epoch": 1.7429268292682927,
      "grad_norm": 0.09007689356803894,
      "learning_rate": 1.8711154972299005e-05,
      "loss": 0.0153,
      "step": 7146
    },
    {
      "epoch": 1.743170731707317,
      "grad_norm": 0.13565438985824585,
      "learning_rate": 1.8704974655835903e-05,
      "loss": 0.0233,
      "step": 7147
    },
    {
      "epoch": 1.7434146341463415,
      "grad_norm": 0.16618509590625763,
      "learning_rate": 1.8698794750103936e-05,
      "loss": 0.0204,
      "step": 7148
    },
    {
      "epoch": 1.7436585365853658,
      "grad_norm": 0.2082400619983673,
      "learning_rate": 1.869261525550632e-05,
      "loss": 0.0161,
      "step": 7149
    },
    {
      "epoch": 1.7439024390243902,
      "grad_norm": 0.1285521239042282,
      "learning_rate": 1.868643617244624e-05,
      "loss": 0.0085,
      "step": 7150
    },
    {
      "epoch": 1.7441463414634146,
      "grad_norm": 0.16548410058021545,
      "learning_rate": 1.8680257501326886e-05,
      "loss": 0.0227,
      "step": 7151
    },
    {
      "epoch": 1.744390243902439,
      "grad_norm": 0.08386199176311493,
      "learning_rate": 1.8674079242551378e-05,
      "loss": 0.0087,
      "step": 7152
    },
    {
      "epoch": 1.7446341463414634,
      "grad_norm": 0.10121048241853714,
      "learning_rate": 1.866790139652283e-05,
      "loss": 0.0145,
      "step": 7153
    },
    {
      "epoch": 1.7448780487804878,
      "grad_norm": 0.10638957470655441,
      "learning_rate": 1.8661723963644333e-05,
      "loss": 0.0137,
      "step": 7154
    },
    {
      "epoch": 1.7451219512195122,
      "grad_norm": 0.10634678602218628,
      "learning_rate": 1.865554694431895e-05,
      "loss": 0.0168,
      "step": 7155
    },
    {
      "epoch": 1.7453658536585366,
      "grad_norm": 0.13471488654613495,
      "learning_rate": 1.8649370338949703e-05,
      "loss": 0.0091,
      "step": 7156
    },
    {
      "epoch": 1.745609756097561,
      "grad_norm": 0.22079019248485565,
      "learning_rate": 1.8643194147939598e-05,
      "loss": 0.0225,
      "step": 7157
    },
    {
      "epoch": 1.7458536585365854,
      "grad_norm": 0.12930695712566376,
      "learning_rate": 1.8637018371691627e-05,
      "loss": 0.0211,
      "step": 7158
    },
    {
      "epoch": 1.7460975609756098,
      "grad_norm": 0.15554961562156677,
      "learning_rate": 1.8630843010608717e-05,
      "loss": 0.0228,
      "step": 7159
    },
    {
      "epoch": 1.7463414634146341,
      "grad_norm": 0.17618252336978912,
      "learning_rate": 1.8624668065093805e-05,
      "loss": 0.0231,
      "step": 7160
    },
    {
      "epoch": 1.7465853658536585,
      "grad_norm": 0.18782322108745575,
      "learning_rate": 1.8618493535549787e-05,
      "loss": 0.0244,
      "step": 7161
    },
    {
      "epoch": 1.746829268292683,
      "grad_norm": 0.20033341646194458,
      "learning_rate": 1.8612319422379525e-05,
      "loss": 0.0128,
      "step": 7162
    },
    {
      "epoch": 1.7470731707317073,
      "grad_norm": 0.15423819422721863,
      "learning_rate": 1.8606145725985865e-05,
      "loss": 0.0207,
      "step": 7163
    },
    {
      "epoch": 1.7473170731707317,
      "grad_norm": 0.21333728730678558,
      "learning_rate": 1.8599972446771622e-05,
      "loss": 0.012,
      "step": 7164
    },
    {
      "epoch": 1.747560975609756,
      "grad_norm": 0.11913555860519409,
      "learning_rate": 1.8593799585139595e-05,
      "loss": 0.0224,
      "step": 7165
    },
    {
      "epoch": 1.7478048780487805,
      "grad_norm": 0.1102868840098381,
      "learning_rate": 1.8587627141492526e-05,
      "loss": 0.0179,
      "step": 7166
    },
    {
      "epoch": 1.7480487804878049,
      "grad_norm": 0.1187824085354805,
      "learning_rate": 1.8581455116233156e-05,
      "loss": 0.0243,
      "step": 7167
    },
    {
      "epoch": 1.7482926829268293,
      "grad_norm": 0.09539428353309631,
      "learning_rate": 1.8575283509764196e-05,
      "loss": 0.0091,
      "step": 7168
    },
    {
      "epoch": 1.7485365853658537,
      "grad_norm": 0.08113904297351837,
      "learning_rate": 1.8569112322488314e-05,
      "loss": 0.0214,
      "step": 7169
    },
    {
      "epoch": 1.748780487804878,
      "grad_norm": 0.08322317153215408,
      "learning_rate": 1.8562941554808166e-05,
      "loss": 0.0058,
      "step": 7170
    },
    {
      "epoch": 1.7490243902439024,
      "grad_norm": 0.18694749474525452,
      "learning_rate": 1.855677120712637e-05,
      "loss": 0.02,
      "step": 7171
    },
    {
      "epoch": 1.7492682926829268,
      "grad_norm": 0.15035006403923035,
      "learning_rate": 1.855060127984554e-05,
      "loss": 0.0302,
      "step": 7172
    },
    {
      "epoch": 1.7495121951219512,
      "grad_norm": 0.07038722187280655,
      "learning_rate": 1.854443177336823e-05,
      "loss": 0.0189,
      "step": 7173
    },
    {
      "epoch": 1.7497560975609756,
      "grad_norm": 0.069266676902771,
      "learning_rate": 1.853826268809698e-05,
      "loss": 0.0072,
      "step": 7174
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.16338008642196655,
      "learning_rate": 1.853209402443432e-05,
      "loss": 0.0259,
      "step": 7175
    },
    {
      "epoch": 1.7502439024390244,
      "grad_norm": 0.18441739678382874,
      "learning_rate": 1.8525925782782713e-05,
      "loss": 0.0264,
      "step": 7176
    },
    {
      "epoch": 1.7504878048780488,
      "grad_norm": 0.09397097676992416,
      "learning_rate": 1.851975796354463e-05,
      "loss": 0.0209,
      "step": 7177
    },
    {
      "epoch": 1.7507317073170732,
      "grad_norm": 0.0726422667503357,
      "learning_rate": 1.851359056712251e-05,
      "loss": 0.0111,
      "step": 7178
    },
    {
      "epoch": 1.7509756097560976,
      "grad_norm": 0.11451105773448944,
      "learning_rate": 1.850742359391874e-05,
      "loss": 0.0263,
      "step": 7179
    },
    {
      "epoch": 1.751219512195122,
      "grad_norm": 0.23412024974822998,
      "learning_rate": 1.8501257044335708e-05,
      "loss": 0.0216,
      "step": 7180
    },
    {
      "epoch": 1.7514634146341463,
      "grad_norm": 0.06730109453201294,
      "learning_rate": 1.8495090918775758e-05,
      "loss": 0.0094,
      "step": 7181
    },
    {
      "epoch": 1.7517073170731707,
      "grad_norm": 0.06644231081008911,
      "learning_rate": 1.8488925217641213e-05,
      "loss": 0.0191,
      "step": 7182
    },
    {
      "epoch": 1.7519512195121951,
      "grad_norm": 0.1203298345208168,
      "learning_rate": 1.8482759941334365e-05,
      "loss": 0.0153,
      "step": 7183
    },
    {
      "epoch": 1.7521951219512195,
      "grad_norm": 0.17969204485416412,
      "learning_rate": 1.8476595090257478e-05,
      "loss": 0.0254,
      "step": 7184
    },
    {
      "epoch": 1.752439024390244,
      "grad_norm": 0.16369186341762543,
      "learning_rate": 1.8470430664812796e-05,
      "loss": 0.017,
      "step": 7185
    },
    {
      "epoch": 1.7526829268292683,
      "grad_norm": 0.27169153094291687,
      "learning_rate": 1.846426666540252e-05,
      "loss": 0.0239,
      "step": 7186
    },
    {
      "epoch": 1.7529268292682927,
      "grad_norm": 0.12783360481262207,
      "learning_rate": 1.8458103092428835e-05,
      "loss": 0.0236,
      "step": 7187
    },
    {
      "epoch": 1.753170731707317,
      "grad_norm": 0.29873520135879517,
      "learning_rate": 1.84519399462939e-05,
      "loss": 0.0298,
      "step": 7188
    },
    {
      "epoch": 1.7534146341463415,
      "grad_norm": 0.09949497878551483,
      "learning_rate": 1.844577722739984e-05,
      "loss": 0.02,
      "step": 7189
    },
    {
      "epoch": 1.7536585365853659,
      "grad_norm": 0.12188572436571121,
      "learning_rate": 1.8439614936148745e-05,
      "loss": 0.0126,
      "step": 7190
    },
    {
      "epoch": 1.7539024390243902,
      "grad_norm": 0.11012465506792068,
      "learning_rate": 1.8433453072942707e-05,
      "loss": 0.0215,
      "step": 7191
    },
    {
      "epoch": 1.7541463414634146,
      "grad_norm": 0.1458406299352646,
      "learning_rate": 1.842729163818374e-05,
      "loss": 0.0337,
      "step": 7192
    },
    {
      "epoch": 1.754390243902439,
      "grad_norm": 0.16941961646080017,
      "learning_rate": 1.8421130632273877e-05,
      "loss": 0.0211,
      "step": 7193
    },
    {
      "epoch": 1.7546341463414634,
      "grad_norm": 0.1640060991048813,
      "learning_rate": 1.84149700556151e-05,
      "loss": 0.0156,
      "step": 7194
    },
    {
      "epoch": 1.7548780487804878,
      "grad_norm": 0.1553047001361847,
      "learning_rate": 1.8408809908609366e-05,
      "loss": 0.0121,
      "step": 7195
    },
    {
      "epoch": 1.7551219512195122,
      "grad_norm": 0.12183915823698044,
      "learning_rate": 1.8402650191658605e-05,
      "loss": 0.0197,
      "step": 7196
    },
    {
      "epoch": 1.7553658536585366,
      "grad_norm": 0.10840476304292679,
      "learning_rate": 1.8396490905164725e-05,
      "loss": 0.0155,
      "step": 7197
    },
    {
      "epoch": 1.755609756097561,
      "grad_norm": 0.4235883355140686,
      "learning_rate": 1.8390332049529604e-05,
      "loss": 0.0289,
      "step": 7198
    },
    {
      "epoch": 1.7558536585365854,
      "grad_norm": 0.1552635133266449,
      "learning_rate": 1.8384173625155078e-05,
      "loss": 0.0207,
      "step": 7199
    },
    {
      "epoch": 1.7560975609756098,
      "grad_norm": 0.13392487168312073,
      "learning_rate": 1.8378015632442968e-05,
      "loss": 0.0244,
      "step": 7200
    },
    {
      "epoch": 1.7563414634146342,
      "grad_norm": 0.11887330561876297,
      "learning_rate": 1.8371858071795066e-05,
      "loss": 0.0099,
      "step": 7201
    },
    {
      "epoch": 1.7565853658536585,
      "grad_norm": 0.12977153062820435,
      "learning_rate": 1.836570094361314e-05,
      "loss": 0.0338,
      "step": 7202
    },
    {
      "epoch": 1.756829268292683,
      "grad_norm": 0.12825067341327667,
      "learning_rate": 1.8359544248298913e-05,
      "loss": 0.0076,
      "step": 7203
    },
    {
      "epoch": 1.7570731707317073,
      "grad_norm": 0.17286416888237,
      "learning_rate": 1.8353387986254095e-05,
      "loss": 0.0233,
      "step": 7204
    },
    {
      "epoch": 1.7573170731707317,
      "grad_norm": 0.15318413078784943,
      "learning_rate": 1.8347232157880362e-05,
      "loss": 0.0196,
      "step": 7205
    },
    {
      "epoch": 1.757560975609756,
      "grad_norm": 0.08513625711202621,
      "learning_rate": 1.8341076763579372e-05,
      "loss": 0.0196,
      "step": 7206
    },
    {
      "epoch": 1.7578048780487805,
      "grad_norm": 0.10297968238592148,
      "learning_rate": 1.8334921803752732e-05,
      "loss": 0.0074,
      "step": 7207
    },
    {
      "epoch": 1.7580487804878049,
      "grad_norm": 0.16108573973178864,
      "learning_rate": 1.8328767278802053e-05,
      "loss": 0.0174,
      "step": 7208
    },
    {
      "epoch": 1.7582926829268293,
      "grad_norm": 0.16035804152488708,
      "learning_rate": 1.8322613189128873e-05,
      "loss": 0.0264,
      "step": 7209
    },
    {
      "epoch": 1.7585365853658537,
      "grad_norm": 0.13903726637363434,
      "learning_rate": 1.8316459535134746e-05,
      "loss": 0.0196,
      "step": 7210
    },
    {
      "epoch": 1.758780487804878,
      "grad_norm": 0.11252710968255997,
      "learning_rate": 1.8310306317221177e-05,
      "loss": 0.0162,
      "step": 7211
    },
    {
      "epoch": 1.7590243902439024,
      "grad_norm": 0.15054923295974731,
      "learning_rate": 1.8304153535789632e-05,
      "loss": 0.0117,
      "step": 7212
    },
    {
      "epoch": 1.7592682926829268,
      "grad_norm": 0.06550399959087372,
      "learning_rate": 1.8298001191241577e-05,
      "loss": 0.0152,
      "step": 7213
    },
    {
      "epoch": 1.7595121951219512,
      "grad_norm": 0.14190249145030975,
      "learning_rate": 1.8291849283978425e-05,
      "loss": 0.0254,
      "step": 7214
    },
    {
      "epoch": 1.7597560975609756,
      "grad_norm": 0.10872288048267365,
      "learning_rate": 1.8285697814401574e-05,
      "loss": 0.0172,
      "step": 7215
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.25019681453704834,
      "learning_rate": 1.8279546782912383e-05,
      "loss": 0.0239,
      "step": 7216
    },
    {
      "epoch": 1.7602439024390244,
      "grad_norm": 0.1613592505455017,
      "learning_rate": 1.8273396189912193e-05,
      "loss": 0.0215,
      "step": 7217
    },
    {
      "epoch": 1.7604878048780488,
      "grad_norm": 0.10143842548131943,
      "learning_rate": 1.8267246035802307e-05,
      "loss": 0.0205,
      "step": 7218
    },
    {
      "epoch": 1.7607317073170732,
      "grad_norm": 0.2824580669403076,
      "learning_rate": 1.8261096320984012e-05,
      "loss": 0.0291,
      "step": 7219
    },
    {
      "epoch": 1.7609756097560976,
      "grad_norm": 0.17944927513599396,
      "learning_rate": 1.825494704585855e-05,
      "loss": 0.013,
      "step": 7220
    },
    {
      "epoch": 1.761219512195122,
      "grad_norm": 0.12218891829252243,
      "learning_rate": 1.824879821082714e-05,
      "loss": 0.0105,
      "step": 7221
    },
    {
      "epoch": 1.7614634146341464,
      "grad_norm": 0.11684595793485641,
      "learning_rate": 1.824264981629098e-05,
      "loss": 0.02,
      "step": 7222
    },
    {
      "epoch": 1.7617073170731707,
      "grad_norm": 0.12626297771930695,
      "learning_rate": 1.8236501862651242e-05,
      "loss": 0.0122,
      "step": 7223
    },
    {
      "epoch": 1.7619512195121951,
      "grad_norm": 0.21718426048755646,
      "learning_rate": 1.8230354350309058e-05,
      "loss": 0.0112,
      "step": 7224
    },
    {
      "epoch": 1.7621951219512195,
      "grad_norm": 0.1692737638950348,
      "learning_rate": 1.8224207279665512e-05,
      "loss": 0.0309,
      "step": 7225
    },
    {
      "epoch": 1.762439024390244,
      "grad_norm": 0.09797339141368866,
      "learning_rate": 1.8218060651121706e-05,
      "loss": 0.013,
      "step": 7226
    },
    {
      "epoch": 1.7626829268292683,
      "grad_norm": 0.14384402334690094,
      "learning_rate": 1.8211914465078675e-05,
      "loss": 0.0228,
      "step": 7227
    },
    {
      "epoch": 1.7629268292682927,
      "grad_norm": 0.20219795405864716,
      "learning_rate": 1.8205768721937453e-05,
      "loss": 0.0162,
      "step": 7228
    },
    {
      "epoch": 1.763170731707317,
      "grad_norm": 0.15523676574230194,
      "learning_rate": 1.8199623422099015e-05,
      "loss": 0.015,
      "step": 7229
    },
    {
      "epoch": 1.7634146341463415,
      "grad_norm": 0.14321482181549072,
      "learning_rate": 1.8193478565964333e-05,
      "loss": 0.0121,
      "step": 7230
    },
    {
      "epoch": 1.7636585365853659,
      "grad_norm": 0.1477016806602478,
      "learning_rate": 1.8187334153934338e-05,
      "loss": 0.0164,
      "step": 7231
    },
    {
      "epoch": 1.7639024390243903,
      "grad_norm": 0.07435151934623718,
      "learning_rate": 1.818119018640994e-05,
      "loss": 0.0159,
      "step": 7232
    },
    {
      "epoch": 1.7641463414634146,
      "grad_norm": 0.23486419022083282,
      "learning_rate": 1.8175046663792e-05,
      "loss": 0.0167,
      "step": 7233
    },
    {
      "epoch": 1.764390243902439,
      "grad_norm": 0.21009932458400726,
      "learning_rate": 1.8168903586481374e-05,
      "loss": 0.0247,
      "step": 7234
    },
    {
      "epoch": 1.7646341463414634,
      "grad_norm": 0.2136082500219345,
      "learning_rate": 1.816276095487888e-05,
      "loss": 0.0216,
      "step": 7235
    },
    {
      "epoch": 1.7648780487804878,
      "grad_norm": 0.09450244903564453,
      "learning_rate": 1.81566187693853e-05,
      "loss": 0.0338,
      "step": 7236
    },
    {
      "epoch": 1.7651219512195122,
      "grad_norm": 0.08754700422286987,
      "learning_rate": 1.8150477030401397e-05,
      "loss": 0.0259,
      "step": 7237
    },
    {
      "epoch": 1.7653658536585366,
      "grad_norm": 0.07433989644050598,
      "learning_rate": 1.8144335738327894e-05,
      "loss": 0.014,
      "step": 7238
    },
    {
      "epoch": 1.765609756097561,
      "grad_norm": 0.07478536665439606,
      "learning_rate": 1.81381948935655e-05,
      "loss": 0.0157,
      "step": 7239
    },
    {
      "epoch": 1.7658536585365854,
      "grad_norm": 0.18280313909053802,
      "learning_rate": 1.8132054496514887e-05,
      "loss": 0.0148,
      "step": 7240
    },
    {
      "epoch": 1.7660975609756098,
      "grad_norm": 0.11736205965280533,
      "learning_rate": 1.8125914547576704e-05,
      "loss": 0.0165,
      "step": 7241
    },
    {
      "epoch": 1.7663414634146342,
      "grad_norm": 0.2258506417274475,
      "learning_rate": 1.811977504715154e-05,
      "loss": 0.0261,
      "step": 7242
    },
    {
      "epoch": 1.7665853658536586,
      "grad_norm": 0.0553518682718277,
      "learning_rate": 1.8113635995639987e-05,
      "loss": 0.0059,
      "step": 7243
    },
    {
      "epoch": 1.766829268292683,
      "grad_norm": 0.09778805077075958,
      "learning_rate": 1.8107497393442607e-05,
      "loss": 0.0175,
      "step": 7244
    },
    {
      "epoch": 1.7670731707317073,
      "grad_norm": 0.13659562170505524,
      "learning_rate": 1.8101359240959932e-05,
      "loss": 0.0258,
      "step": 7245
    },
    {
      "epoch": 1.7673170731707317,
      "grad_norm": 0.07673391699790955,
      "learning_rate": 1.809522153859244e-05,
      "loss": 0.019,
      "step": 7246
    },
    {
      "epoch": 1.7675609756097561,
      "grad_norm": 0.06758233159780502,
      "learning_rate": 1.8089084286740597e-05,
      "loss": 0.0184,
      "step": 7247
    },
    {
      "epoch": 1.7678048780487805,
      "grad_norm": 0.10510925948619843,
      "learning_rate": 1.808294748580486e-05,
      "loss": 0.0124,
      "step": 7248
    },
    {
      "epoch": 1.768048780487805,
      "grad_norm": 0.17092841863632202,
      "learning_rate": 1.807681113618562e-05,
      "loss": 0.0304,
      "step": 7249
    },
    {
      "epoch": 1.7682926829268293,
      "grad_norm": 0.2334153801202774,
      "learning_rate": 1.807067523828326e-05,
      "loss": 0.0265,
      "step": 7250
    },
    {
      "epoch": 1.7685365853658537,
      "grad_norm": 0.16367191076278687,
      "learning_rate": 1.8064539792498126e-05,
      "loss": 0.0234,
      "step": 7251
    },
    {
      "epoch": 1.768780487804878,
      "grad_norm": 0.09110154211521149,
      "learning_rate": 1.8058404799230542e-05,
      "loss": 0.0185,
      "step": 7252
    },
    {
      "epoch": 1.7690243902439025,
      "grad_norm": 0.14499986171722412,
      "learning_rate": 1.8052270258880795e-05,
      "loss": 0.0098,
      "step": 7253
    },
    {
      "epoch": 1.7692682926829268,
      "grad_norm": 0.10639262199401855,
      "learning_rate": 1.8046136171849143e-05,
      "loss": 0.0265,
      "step": 7254
    },
    {
      "epoch": 1.7695121951219512,
      "grad_norm": 0.17699818313121796,
      "learning_rate": 1.8040002538535815e-05,
      "loss": 0.0315,
      "step": 7255
    },
    {
      "epoch": 1.7697560975609756,
      "grad_norm": 0.07380124181509018,
      "learning_rate": 1.803386935934102e-05,
      "loss": 0.0142,
      "step": 7256
    },
    {
      "epoch": 1.77,
      "grad_norm": 0.16178499162197113,
      "learning_rate": 1.8027736634664927e-05,
      "loss": 0.0234,
      "step": 7257
    },
    {
      "epoch": 1.7702439024390244,
      "grad_norm": 0.22478429973125458,
      "learning_rate": 1.8021604364907673e-05,
      "loss": 0.022,
      "step": 7258
    },
    {
      "epoch": 1.7704878048780488,
      "grad_norm": 0.20493155717849731,
      "learning_rate": 1.8015472550469366e-05,
      "loss": 0.0227,
      "step": 7259
    },
    {
      "epoch": 1.7707317073170732,
      "grad_norm": 0.14187555015087128,
      "learning_rate": 1.8009341191750095e-05,
      "loss": 0.0204,
      "step": 7260
    },
    {
      "epoch": 1.7709756097560976,
      "grad_norm": 0.1688457429409027,
      "learning_rate": 1.8003210289149907e-05,
      "loss": 0.013,
      "step": 7261
    },
    {
      "epoch": 1.771219512195122,
      "grad_norm": 0.19506672024726868,
      "learning_rate": 1.7997079843068837e-05,
      "loss": 0.0375,
      "step": 7262
    },
    {
      "epoch": 1.7714634146341464,
      "grad_norm": 0.18157905340194702,
      "learning_rate": 1.7990949853906862e-05,
      "loss": 0.0337,
      "step": 7263
    },
    {
      "epoch": 1.7717073170731708,
      "grad_norm": 0.1548849195241928,
      "learning_rate": 1.7984820322063955e-05,
      "loss": 0.022,
      "step": 7264
    },
    {
      "epoch": 1.7719512195121951,
      "grad_norm": 0.1261863261461258,
      "learning_rate": 1.7978691247940045e-05,
      "loss": 0.0211,
      "step": 7265
    },
    {
      "epoch": 1.7721951219512195,
      "grad_norm": 0.13734368979930878,
      "learning_rate": 1.797256263193504e-05,
      "loss": 0.0123,
      "step": 7266
    },
    {
      "epoch": 1.772439024390244,
      "grad_norm": 0.13653136789798737,
      "learning_rate": 1.7966434474448806e-05,
      "loss": 0.0166,
      "step": 7267
    },
    {
      "epoch": 1.7726829268292683,
      "grad_norm": 0.08394061774015427,
      "learning_rate": 1.796030677588119e-05,
      "loss": 0.0127,
      "step": 7268
    },
    {
      "epoch": 1.7729268292682927,
      "grad_norm": 0.20174477994441986,
      "learning_rate": 1.7954179536632016e-05,
      "loss": 0.0185,
      "step": 7269
    },
    {
      "epoch": 1.773170731707317,
      "grad_norm": 0.29308053851127625,
      "learning_rate": 1.7948052757101047e-05,
      "loss": 0.0159,
      "step": 7270
    },
    {
      "epoch": 1.7734146341463415,
      "grad_norm": 0.1716732382774353,
      "learning_rate": 1.794192643768805e-05,
      "loss": 0.0339,
      "step": 7271
    },
    {
      "epoch": 1.7736585365853659,
      "grad_norm": 0.06585892289876938,
      "learning_rate": 1.793580057879275e-05,
      "loss": 0.0132,
      "step": 7272
    },
    {
      "epoch": 1.7739024390243903,
      "grad_norm": 0.17893289029598236,
      "learning_rate": 1.7929675180814836e-05,
      "loss": 0.0447,
      "step": 7273
    },
    {
      "epoch": 1.7741463414634147,
      "grad_norm": 0.15795449912548065,
      "learning_rate": 1.7923550244153976e-05,
      "loss": 0.0258,
      "step": 7274
    },
    {
      "epoch": 1.774390243902439,
      "grad_norm": 0.09078869968652725,
      "learning_rate": 1.7917425769209793e-05,
      "loss": 0.0188,
      "step": 7275
    },
    {
      "epoch": 1.7746341463414634,
      "grad_norm": 0.1525227427482605,
      "learning_rate": 1.7911301756381897e-05,
      "loss": 0.0161,
      "step": 7276
    },
    {
      "epoch": 1.7748780487804878,
      "grad_norm": 0.05864417925477028,
      "learning_rate": 1.790517820606986e-05,
      "loss": 0.0096,
      "step": 7277
    },
    {
      "epoch": 1.7751219512195122,
      "grad_norm": 0.08192608505487442,
      "learning_rate": 1.789905511867323e-05,
      "loss": 0.0106,
      "step": 7278
    },
    {
      "epoch": 1.7753658536585366,
      "grad_norm": 0.44420576095581055,
      "learning_rate": 1.7892932494591513e-05,
      "loss": 0.0179,
      "step": 7279
    },
    {
      "epoch": 1.775609756097561,
      "grad_norm": 0.10002540051937103,
      "learning_rate": 1.7886810334224192e-05,
      "loss": 0.0254,
      "step": 7280
    },
    {
      "epoch": 1.7758536585365854,
      "grad_norm": 0.15287289023399353,
      "learning_rate": 1.7880688637970722e-05,
      "loss": 0.0409,
      "step": 7281
    },
    {
      "epoch": 1.7760975609756098,
      "grad_norm": 0.12834148108959198,
      "learning_rate": 1.7874567406230526e-05,
      "loss": 0.0114,
      "step": 7282
    },
    {
      "epoch": 1.7763414634146342,
      "grad_norm": 0.1589745283126831,
      "learning_rate": 1.7868446639402992e-05,
      "loss": 0.0116,
      "step": 7283
    },
    {
      "epoch": 1.7765853658536586,
      "grad_norm": 0.11897169053554535,
      "learning_rate": 1.786232633788748e-05,
      "loss": 0.0125,
      "step": 7284
    },
    {
      "epoch": 1.776829268292683,
      "grad_norm": 0.11274375021457672,
      "learning_rate": 1.7856206502083326e-05,
      "loss": 0.0119,
      "step": 7285
    },
    {
      "epoch": 1.7770731707317073,
      "grad_norm": 0.09750615060329437,
      "learning_rate": 1.785008713238984e-05,
      "loss": 0.0117,
      "step": 7286
    },
    {
      "epoch": 1.7773170731707317,
      "grad_norm": 0.17754502594470978,
      "learning_rate": 1.7843968229206265e-05,
      "loss": 0.0147,
      "step": 7287
    },
    {
      "epoch": 1.7775609756097561,
      "grad_norm": 0.13688936829566956,
      "learning_rate": 1.783784979293186e-05,
      "loss": 0.0222,
      "step": 7288
    },
    {
      "epoch": 1.7778048780487805,
      "grad_norm": 0.10880070179700851,
      "learning_rate": 1.783173182396583e-05,
      "loss": 0.0237,
      "step": 7289
    },
    {
      "epoch": 1.778048780487805,
      "grad_norm": 0.12677253782749176,
      "learning_rate": 1.782561432270737e-05,
      "loss": 0.0117,
      "step": 7290
    },
    {
      "epoch": 1.7782926829268293,
      "grad_norm": 0.12506254017353058,
      "learning_rate": 1.7819497289555594e-05,
      "loss": 0.0218,
      "step": 7291
    },
    {
      "epoch": 1.7785365853658537,
      "grad_norm": 0.22231116890907288,
      "learning_rate": 1.7813380724909644e-05,
      "loss": 0.0306,
      "step": 7292
    },
    {
      "epoch": 1.778780487804878,
      "grad_norm": 0.06779325753450394,
      "learning_rate": 1.7807264629168602e-05,
      "loss": 0.0136,
      "step": 7293
    },
    {
      "epoch": 1.7790243902439025,
      "grad_norm": 0.17512840032577515,
      "learning_rate": 1.780114900273152e-05,
      "loss": 0.0245,
      "step": 7294
    },
    {
      "epoch": 1.7792682926829269,
      "grad_norm": 0.16727189719676971,
      "learning_rate": 1.779503384599743e-05,
      "loss": 0.0166,
      "step": 7295
    },
    {
      "epoch": 1.7795121951219512,
      "grad_norm": 0.16641758382320404,
      "learning_rate": 1.778891915936533e-05,
      "loss": 0.0378,
      "step": 7296
    },
    {
      "epoch": 1.7797560975609756,
      "grad_norm": 0.1746494174003601,
      "learning_rate": 1.778280494323417e-05,
      "loss": 0.031,
      "step": 7297
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.10340903699398041,
      "learning_rate": 1.7776691198002897e-05,
      "loss": 0.0193,
      "step": 7298
    },
    {
      "epoch": 1.7802439024390244,
      "grad_norm": 0.1592475026845932,
      "learning_rate": 1.777057792407042e-05,
      "loss": 0.0169,
      "step": 7299
    },
    {
      "epoch": 1.7804878048780488,
      "grad_norm": 0.1667807698249817,
      "learning_rate": 1.7764465121835593e-05,
      "loss": 0.0153,
      "step": 7300
    },
    {
      "epoch": 1.7807317073170732,
      "grad_norm": 0.0922040194272995,
      "learning_rate": 1.7758352791697265e-05,
      "loss": 0.0117,
      "step": 7301
    },
    {
      "epoch": 1.7809756097560976,
      "grad_norm": 0.08664457499980927,
      "learning_rate": 1.7752240934054248e-05,
      "loss": 0.0216,
      "step": 7302
    },
    {
      "epoch": 1.781219512195122,
      "grad_norm": 0.32207438349723816,
      "learning_rate": 1.774612954930533e-05,
      "loss": 0.0268,
      "step": 7303
    },
    {
      "epoch": 1.7814634146341464,
      "grad_norm": 0.17773005366325378,
      "learning_rate": 1.774001863784925e-05,
      "loss": 0.0153,
      "step": 7304
    },
    {
      "epoch": 1.7817073170731708,
      "grad_norm": 0.15332211554050446,
      "learning_rate": 1.7733908200084725e-05,
      "loss": 0.0336,
      "step": 7305
    },
    {
      "epoch": 1.7819512195121952,
      "grad_norm": 0.06576564908027649,
      "learning_rate": 1.7727798236410447e-05,
      "loss": 0.0093,
      "step": 7306
    },
    {
      "epoch": 1.7821951219512195,
      "grad_norm": 0.12019266933202744,
      "learning_rate": 1.7721688747225084e-05,
      "loss": 0.0182,
      "step": 7307
    },
    {
      "epoch": 1.782439024390244,
      "grad_norm": 0.23656511306762695,
      "learning_rate": 1.7715579732927234e-05,
      "loss": 0.0146,
      "step": 7308
    },
    {
      "epoch": 1.7826829268292683,
      "grad_norm": 0.09297338873147964,
      "learning_rate": 1.7709471193915516e-05,
      "loss": 0.0229,
      "step": 7309
    },
    {
      "epoch": 1.7829268292682927,
      "grad_norm": 0.18968670070171356,
      "learning_rate": 1.770336313058848e-05,
      "loss": 0.0159,
      "step": 7310
    },
    {
      "epoch": 1.783170731707317,
      "grad_norm": 0.07354214042425156,
      "learning_rate": 1.769725554334466e-05,
      "loss": 0.0243,
      "step": 7311
    },
    {
      "epoch": 1.7834146341463415,
      "grad_norm": 0.19762256741523743,
      "learning_rate": 1.7691148432582567e-05,
      "loss": 0.023,
      "step": 7312
    },
    {
      "epoch": 1.7836585365853659,
      "grad_norm": 0.15300728380680084,
      "learning_rate": 1.7685041798700657e-05,
      "loss": 0.0242,
      "step": 7313
    },
    {
      "epoch": 1.7839024390243903,
      "grad_norm": 0.09568840265274048,
      "learning_rate": 1.7678935642097378e-05,
      "loss": 0.0141,
      "step": 7314
    },
    {
      "epoch": 1.7841463414634147,
      "grad_norm": 0.12118292599916458,
      "learning_rate": 1.7672829963171133e-05,
      "loss": 0.013,
      "step": 7315
    },
    {
      "epoch": 1.784390243902439,
      "grad_norm": 0.08107317984104156,
      "learning_rate": 1.7666724762320315e-05,
      "loss": 0.0101,
      "step": 7316
    },
    {
      "epoch": 1.7846341463414634,
      "grad_norm": 0.13944463431835175,
      "learning_rate": 1.7660620039943244e-05,
      "loss": 0.023,
      "step": 7317
    },
    {
      "epoch": 1.7848780487804878,
      "grad_norm": 0.0866844654083252,
      "learning_rate": 1.765451579643825e-05,
      "loss": 0.0172,
      "step": 7318
    },
    {
      "epoch": 1.7851219512195122,
      "grad_norm": 0.12451760470867157,
      "learning_rate": 1.7648412032203615e-05,
      "loss": 0.015,
      "step": 7319
    },
    {
      "epoch": 1.7853658536585366,
      "grad_norm": 0.14557018876075745,
      "learning_rate": 1.7642308747637592e-05,
      "loss": 0.0204,
      "step": 7320
    },
    {
      "epoch": 1.785609756097561,
      "grad_norm": 0.13018395006656647,
      "learning_rate": 1.76362059431384e-05,
      "loss": 0.0095,
      "step": 7321
    },
    {
      "epoch": 1.7858536585365854,
      "grad_norm": 0.22560884058475494,
      "learning_rate": 1.7630103619104222e-05,
      "loss": 0.0253,
      "step": 7322
    },
    {
      "epoch": 1.7860975609756098,
      "grad_norm": 0.17375032603740692,
      "learning_rate": 1.762400177593323e-05,
      "loss": 0.0287,
      "step": 7323
    },
    {
      "epoch": 1.7863414634146342,
      "grad_norm": 0.13354842364788055,
      "learning_rate": 1.7617900414023534e-05,
      "loss": 0.0274,
      "step": 7324
    },
    {
      "epoch": 1.7865853658536586,
      "grad_norm": 0.21051499247550964,
      "learning_rate": 1.7611799533773245e-05,
      "loss": 0.0255,
      "step": 7325
    },
    {
      "epoch": 1.786829268292683,
      "grad_norm": 0.2336919605731964,
      "learning_rate": 1.760569913558041e-05,
      "loss": 0.0262,
      "step": 7326
    },
    {
      "epoch": 1.7870731707317074,
      "grad_norm": 0.1343264877796173,
      "learning_rate": 1.7599599219843075e-05,
      "loss": 0.0271,
      "step": 7327
    },
    {
      "epoch": 1.7873170731707317,
      "grad_norm": 0.15815822780132294,
      "learning_rate": 1.7593499786959234e-05,
      "loss": 0.0306,
      "step": 7328
    },
    {
      "epoch": 1.7875609756097561,
      "grad_norm": 0.11291009187698364,
      "learning_rate": 1.758740083732686e-05,
      "loss": 0.0222,
      "step": 7329
    },
    {
      "epoch": 1.7878048780487805,
      "grad_norm": 0.1360287219285965,
      "learning_rate": 1.758130237134389e-05,
      "loss": 0.0291,
      "step": 7330
    },
    {
      "epoch": 1.788048780487805,
      "grad_norm": 0.08765065670013428,
      "learning_rate": 1.7575204389408224e-05,
      "loss": 0.0227,
      "step": 7331
    },
    {
      "epoch": 1.7882926829268293,
      "grad_norm": 0.08846117556095123,
      "learning_rate": 1.7569106891917746e-05,
      "loss": 0.0117,
      "step": 7332
    },
    {
      "epoch": 1.7885365853658537,
      "grad_norm": 0.09102331846952438,
      "learning_rate": 1.75630098792703e-05,
      "loss": 0.0207,
      "step": 7333
    },
    {
      "epoch": 1.788780487804878,
      "grad_norm": 0.15057061612606049,
      "learning_rate": 1.7556913351863687e-05,
      "loss": 0.0169,
      "step": 7334
    },
    {
      "epoch": 1.7890243902439025,
      "grad_norm": 0.18690121173858643,
      "learning_rate": 1.7550817310095687e-05,
      "loss": 0.0271,
      "step": 7335
    },
    {
      "epoch": 1.7892682926829269,
      "grad_norm": 0.09704414010047913,
      "learning_rate": 1.754472175436406e-05,
      "loss": 0.018,
      "step": 7336
    },
    {
      "epoch": 1.7895121951219513,
      "grad_norm": 0.09291773289442062,
      "learning_rate": 1.753862668506652e-05,
      "loss": 0.0136,
      "step": 7337
    },
    {
      "epoch": 1.7897560975609756,
      "grad_norm": 0.1530061960220337,
      "learning_rate": 1.7532532102600738e-05,
      "loss": 0.0192,
      "step": 7338
    },
    {
      "epoch": 1.79,
      "grad_norm": 0.17158712446689606,
      "learning_rate": 1.7526438007364378e-05,
      "loss": 0.0266,
      "step": 7339
    },
    {
      "epoch": 1.7902439024390244,
      "grad_norm": 0.060875292867422104,
      "learning_rate": 1.7520344399755074e-05,
      "loss": 0.0089,
      "step": 7340
    },
    {
      "epoch": 1.7904878048780488,
      "grad_norm": 0.08712205290794373,
      "learning_rate": 1.7514251280170386e-05,
      "loss": 0.0215,
      "step": 7341
    },
    {
      "epoch": 1.7907317073170732,
      "grad_norm": 0.10682419687509537,
      "learning_rate": 1.750815864900789e-05,
      "loss": 0.0277,
      "step": 7342
    },
    {
      "epoch": 1.7909756097560976,
      "grad_norm": 0.14052364230155945,
      "learning_rate": 1.7502066506665104e-05,
      "loss": 0.0124,
      "step": 7343
    },
    {
      "epoch": 1.791219512195122,
      "grad_norm": 0.11576680094003677,
      "learning_rate": 1.7495974853539528e-05,
      "loss": 0.0252,
      "step": 7344
    },
    {
      "epoch": 1.7914634146341464,
      "grad_norm": 0.08962918072938919,
      "learning_rate": 1.7489883690028618e-05,
      "loss": 0.0102,
      "step": 7345
    },
    {
      "epoch": 1.7917073170731708,
      "grad_norm": 0.14424854516983032,
      "learning_rate": 1.748379301652981e-05,
      "loss": 0.0268,
      "step": 7346
    },
    {
      "epoch": 1.7919512195121952,
      "grad_norm": 0.0892491564154625,
      "learning_rate": 1.7477702833440497e-05,
      "loss": 0.0216,
      "step": 7347
    },
    {
      "epoch": 1.7921951219512195,
      "grad_norm": 0.2597837746143341,
      "learning_rate": 1.7471613141158043e-05,
      "loss": 0.026,
      "step": 7348
    },
    {
      "epoch": 1.792439024390244,
      "grad_norm": 0.11519887298345566,
      "learning_rate": 1.7465523940079788e-05,
      "loss": 0.0218,
      "step": 7349
    },
    {
      "epoch": 1.7926829268292683,
      "grad_norm": 0.2083563208580017,
      "learning_rate": 1.7459435230603033e-05,
      "loss": 0.0196,
      "step": 7350
    },
    {
      "epoch": 1.7929268292682927,
      "grad_norm": 0.08479151129722595,
      "learning_rate": 1.7453347013125036e-05,
      "loss": 0.0202,
      "step": 7351
    },
    {
      "epoch": 1.7931707317073171,
      "grad_norm": 0.10064254701137543,
      "learning_rate": 1.7447259288043053e-05,
      "loss": 0.0172,
      "step": 7352
    },
    {
      "epoch": 1.7934146341463415,
      "grad_norm": 0.13968585431575775,
      "learning_rate": 1.7441172055754275e-05,
      "loss": 0.0402,
      "step": 7353
    },
    {
      "epoch": 1.793658536585366,
      "grad_norm": 0.10305843502283096,
      "learning_rate": 1.7435085316655885e-05,
      "loss": 0.0149,
      "step": 7354
    },
    {
      "epoch": 1.7939024390243903,
      "grad_norm": 0.11461985856294632,
      "learning_rate": 1.742899907114502e-05,
      "loss": 0.0214,
      "step": 7355
    },
    {
      "epoch": 1.7941463414634147,
      "grad_norm": 0.1125817745923996,
      "learning_rate": 1.7422913319618793e-05,
      "loss": 0.0204,
      "step": 7356
    },
    {
      "epoch": 1.794390243902439,
      "grad_norm": 0.09364952892065048,
      "learning_rate": 1.741682806247427e-05,
      "loss": 0.0137,
      "step": 7357
    },
    {
      "epoch": 1.7946341463414635,
      "grad_norm": 0.17934681475162506,
      "learning_rate": 1.7410743300108496e-05,
      "loss": 0.0163,
      "step": 7358
    },
    {
      "epoch": 1.7948780487804878,
      "grad_norm": 0.1050121933221817,
      "learning_rate": 1.7404659032918503e-05,
      "loss": 0.0183,
      "step": 7359
    },
    {
      "epoch": 1.7951219512195122,
      "grad_norm": 0.13962145149707794,
      "learning_rate": 1.7398575261301243e-05,
      "loss": 0.0265,
      "step": 7360
    },
    {
      "epoch": 1.7953658536585366,
      "grad_norm": 0.10031608492136002,
      "learning_rate": 1.739249198565368e-05,
      "loss": 0.0197,
      "step": 7361
    },
    {
      "epoch": 1.795609756097561,
      "grad_norm": 0.1932249367237091,
      "learning_rate": 1.7386409206372728e-05,
      "loss": 0.0185,
      "step": 7362
    },
    {
      "epoch": 1.7958536585365854,
      "grad_norm": 0.1078159362077713,
      "learning_rate": 1.738032692385527e-05,
      "loss": 0.0206,
      "step": 7363
    },
    {
      "epoch": 1.7960975609756098,
      "grad_norm": 0.14158274233341217,
      "learning_rate": 1.737424513849815e-05,
      "loss": 0.0215,
      "step": 7364
    },
    {
      "epoch": 1.7963414634146342,
      "grad_norm": 0.12966252863407135,
      "learning_rate": 1.7368163850698187e-05,
      "loss": 0.031,
      "step": 7365
    },
    {
      "epoch": 1.7965853658536586,
      "grad_norm": 0.08848622441291809,
      "learning_rate": 1.7362083060852173e-05,
      "loss": 0.0083,
      "step": 7366
    },
    {
      "epoch": 1.796829268292683,
      "grad_norm": 0.2503780722618103,
      "learning_rate": 1.7356002769356865e-05,
      "loss": 0.0366,
      "step": 7367
    },
    {
      "epoch": 1.7970731707317074,
      "grad_norm": 0.12017913907766342,
      "learning_rate": 1.734992297660897e-05,
      "loss": 0.0115,
      "step": 7368
    },
    {
      "epoch": 1.7973170731707317,
      "grad_norm": 0.16738015413284302,
      "learning_rate": 1.734384368300518e-05,
      "loss": 0.019,
      "step": 7369
    },
    {
      "epoch": 1.7975609756097561,
      "grad_norm": 0.09974786639213562,
      "learning_rate": 1.733776488894215e-05,
      "loss": 0.0274,
      "step": 7370
    },
    {
      "epoch": 1.7978048780487805,
      "grad_norm": 0.08242250233888626,
      "learning_rate": 1.733168659481651e-05,
      "loss": 0.0194,
      "step": 7371
    },
    {
      "epoch": 1.798048780487805,
      "grad_norm": 0.18749211728572845,
      "learning_rate": 1.732560880102484e-05,
      "loss": 0.0234,
      "step": 7372
    },
    {
      "epoch": 1.7982926829268293,
      "grad_norm": 0.13278402388095856,
      "learning_rate": 1.7319531507963714e-05,
      "loss": 0.017,
      "step": 7373
    },
    {
      "epoch": 1.7985365853658537,
      "grad_norm": 0.09446610510349274,
      "learning_rate": 1.7313454716029633e-05,
      "loss": 0.0096,
      "step": 7374
    },
    {
      "epoch": 1.798780487804878,
      "grad_norm": 0.12923859059810638,
      "learning_rate": 1.7307378425619104e-05,
      "loss": 0.011,
      "step": 7375
    },
    {
      "epoch": 1.7990243902439025,
      "grad_norm": 0.9147636294364929,
      "learning_rate": 1.7301302637128586e-05,
      "loss": 0.0361,
      "step": 7376
    },
    {
      "epoch": 1.7992682926829269,
      "grad_norm": 0.1710508167743683,
      "learning_rate": 1.7295227350954496e-05,
      "loss": 0.0183,
      "step": 7377
    },
    {
      "epoch": 1.7995121951219513,
      "grad_norm": 0.0650525689125061,
      "learning_rate": 1.728915256749324e-05,
      "loss": 0.0163,
      "step": 7378
    },
    {
      "epoch": 1.7997560975609757,
      "grad_norm": 0.10242591798305511,
      "learning_rate": 1.7283078287141168e-05,
      "loss": 0.0232,
      "step": 7379
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.15619952976703644,
      "learning_rate": 1.727700451029462e-05,
      "loss": 0.021,
      "step": 7380
    },
    {
      "epoch": 1.8002439024390244,
      "grad_norm": 0.13498377799987793,
      "learning_rate": 1.7270931237349886e-05,
      "loss": 0.0228,
      "step": 7381
    },
    {
      "epoch": 1.8004878048780488,
      "grad_norm": 0.11075358092784882,
      "learning_rate": 1.726485846870322e-05,
      "loss": 0.0148,
      "step": 7382
    },
    {
      "epoch": 1.8007317073170732,
      "grad_norm": 0.09903710335493088,
      "learning_rate": 1.7258786204750865e-05,
      "loss": 0.0199,
      "step": 7383
    },
    {
      "epoch": 1.8009756097560976,
      "grad_norm": 0.10260450094938278,
      "learning_rate": 1.725271444588902e-05,
      "loss": 0.0252,
      "step": 7384
    },
    {
      "epoch": 1.801219512195122,
      "grad_norm": 0.1350809633731842,
      "learning_rate": 1.7246643192513827e-05,
      "loss": 0.0119,
      "step": 7385
    },
    {
      "epoch": 1.8014634146341464,
      "grad_norm": 0.12106557935476303,
      "learning_rate": 1.7240572445021436e-05,
      "loss": 0.0199,
      "step": 7386
    },
    {
      "epoch": 1.8017073170731708,
      "grad_norm": 0.12468688935041428,
      "learning_rate": 1.723450220380794e-05,
      "loss": 0.0162,
      "step": 7387
    },
    {
      "epoch": 1.8019512195121952,
      "grad_norm": 0.21550117433071136,
      "learning_rate": 1.7228432469269402e-05,
      "loss": 0.0199,
      "step": 7388
    },
    {
      "epoch": 1.8021951219512196,
      "grad_norm": 0.11778100579977036,
      "learning_rate": 1.722236324180186e-05,
      "loss": 0.0168,
      "step": 7389
    },
    {
      "epoch": 1.802439024390244,
      "grad_norm": 0.09350532293319702,
      "learning_rate": 1.72162945218013e-05,
      "loss": 0.01,
      "step": 7390
    },
    {
      "epoch": 1.8026829268292683,
      "grad_norm": 0.07896099984645844,
      "learning_rate": 1.7210226309663697e-05,
      "loss": 0.0149,
      "step": 7391
    },
    {
      "epoch": 1.8029268292682927,
      "grad_norm": 0.09697649627923965,
      "learning_rate": 1.7204158605784976e-05,
      "loss": 0.015,
      "step": 7392
    },
    {
      "epoch": 1.8031707317073171,
      "grad_norm": 0.09109098464250565,
      "learning_rate": 1.7198091410561046e-05,
      "loss": 0.0131,
      "step": 7393
    },
    {
      "epoch": 1.8034146341463415,
      "grad_norm": 0.20065803825855255,
      "learning_rate": 1.7192024724387763e-05,
      "loss": 0.0222,
      "step": 7394
    },
    {
      "epoch": 1.803658536585366,
      "grad_norm": 0.24048127233982086,
      "learning_rate": 1.7185958547660963e-05,
      "loss": 0.0265,
      "step": 7395
    },
    {
      "epoch": 1.8039024390243903,
      "grad_norm": 0.1023685410618782,
      "learning_rate": 1.717989288077645e-05,
      "loss": 0.0098,
      "step": 7396
    },
    {
      "epoch": 1.8041463414634147,
      "grad_norm": 0.17875231802463531,
      "learning_rate": 1.717382772412999e-05,
      "loss": 0.0164,
      "step": 7397
    },
    {
      "epoch": 1.804390243902439,
      "grad_norm": 0.11234917491674423,
      "learning_rate": 1.716776307811731e-05,
      "loss": 0.0219,
      "step": 7398
    },
    {
      "epoch": 1.8046341463414635,
      "grad_norm": 0.07685382664203644,
      "learning_rate": 1.7161698943134107e-05,
      "loss": 0.0094,
      "step": 7399
    },
    {
      "epoch": 1.8048780487804879,
      "grad_norm": 0.13659030199050903,
      "learning_rate": 1.7155635319576062e-05,
      "loss": 0.0142,
      "step": 7400
    },
    {
      "epoch": 1.8051219512195122,
      "grad_norm": 0.15118783712387085,
      "learning_rate": 1.7149572207838794e-05,
      "loss": 0.0172,
      "step": 7401
    },
    {
      "epoch": 1.8053658536585366,
      "grad_norm": 0.1422421634197235,
      "learning_rate": 1.7143509608317905e-05,
      "loss": 0.0153,
      "step": 7402
    },
    {
      "epoch": 1.805609756097561,
      "grad_norm": 0.14380818605422974,
      "learning_rate": 1.713744752140896e-05,
      "loss": 0.0209,
      "step": 7403
    },
    {
      "epoch": 1.8058536585365854,
      "grad_norm": 0.1158767119050026,
      "learning_rate": 1.7131385947507507e-05,
      "loss": 0.0282,
      "step": 7404
    },
    {
      "epoch": 1.8060975609756098,
      "grad_norm": 0.09992091357707977,
      "learning_rate": 1.712532488700902e-05,
      "loss": 0.0115,
      "step": 7405
    },
    {
      "epoch": 1.8063414634146342,
      "grad_norm": 0.08176606148481369,
      "learning_rate": 1.7119264340308994e-05,
      "loss": 0.0159,
      "step": 7406
    },
    {
      "epoch": 1.8065853658536586,
      "grad_norm": 0.12021416425704956,
      "learning_rate": 1.7113204307802833e-05,
      "loss": 0.0149,
      "step": 7407
    },
    {
      "epoch": 1.806829268292683,
      "grad_norm": 0.13196313381195068,
      "learning_rate": 1.7107144789885943e-05,
      "loss": 0.0169,
      "step": 7408
    },
    {
      "epoch": 1.8070731707317074,
      "grad_norm": 0.24925605952739716,
      "learning_rate": 1.7101085786953697e-05,
      "loss": 0.0349,
      "step": 7409
    },
    {
      "epoch": 1.8073170731707318,
      "grad_norm": 0.13929666578769684,
      "learning_rate": 1.709502729940143e-05,
      "loss": 0.0241,
      "step": 7410
    },
    {
      "epoch": 1.8075609756097561,
      "grad_norm": 0.19565826654434204,
      "learning_rate": 1.708896932762442e-05,
      "loss": 0.0227,
      "step": 7411
    },
    {
      "epoch": 1.8078048780487805,
      "grad_norm": 0.0932338535785675,
      "learning_rate": 1.7082911872017948e-05,
      "loss": 0.0223,
      "step": 7412
    },
    {
      "epoch": 1.808048780487805,
      "grad_norm": 0.16042190790176392,
      "learning_rate": 1.7076854932977243e-05,
      "loss": 0.0367,
      "step": 7413
    },
    {
      "epoch": 1.8082926829268293,
      "grad_norm": 0.13545216619968414,
      "learning_rate": 1.7070798510897494e-05,
      "loss": 0.0225,
      "step": 7414
    },
    {
      "epoch": 1.8085365853658537,
      "grad_norm": 0.12800800800323486,
      "learning_rate": 1.706474260617387e-05,
      "loss": 0.0153,
      "step": 7415
    },
    {
      "epoch": 1.808780487804878,
      "grad_norm": 0.09136470407247543,
      "learning_rate": 1.7058687219201496e-05,
      "loss": 0.0262,
      "step": 7416
    },
    {
      "epoch": 1.8090243902439025,
      "grad_norm": 0.1471845507621765,
      "learning_rate": 1.705263235037548e-05,
      "loss": 0.0177,
      "step": 7417
    },
    {
      "epoch": 1.8092682926829269,
      "grad_norm": 0.25576648116111755,
      "learning_rate": 1.7046578000090867e-05,
      "loss": 0.0216,
      "step": 7418
    },
    {
      "epoch": 1.8095121951219513,
      "grad_norm": 0.12810304760932922,
      "learning_rate": 1.7040524168742688e-05,
      "loss": 0.0163,
      "step": 7419
    },
    {
      "epoch": 1.8097560975609757,
      "grad_norm": 0.2209550142288208,
      "learning_rate": 1.7034470856725944e-05,
      "loss": 0.0162,
      "step": 7420
    },
    {
      "epoch": 1.81,
      "grad_norm": 0.06867094337940216,
      "learning_rate": 1.7028418064435596e-05,
      "loss": 0.0158,
      "step": 7421
    },
    {
      "epoch": 1.8102439024390244,
      "grad_norm": 0.3618789315223694,
      "learning_rate": 1.7022365792266564e-05,
      "loss": 0.0148,
      "step": 7422
    },
    {
      "epoch": 1.8104878048780488,
      "grad_norm": 0.15794850885868073,
      "learning_rate": 1.7016314040613746e-05,
      "loss": 0.0277,
      "step": 7423
    },
    {
      "epoch": 1.8107317073170732,
      "grad_norm": 0.18019962310791016,
      "learning_rate": 1.701026280987199e-05,
      "loss": 0.0158,
      "step": 7424
    },
    {
      "epoch": 1.8109756097560976,
      "grad_norm": 0.14889457821846008,
      "learning_rate": 1.7004212100436127e-05,
      "loss": 0.0198,
      "step": 7425
    },
    {
      "epoch": 1.811219512195122,
      "grad_norm": 0.10692200809717178,
      "learning_rate": 1.6998161912700948e-05,
      "loss": 0.0163,
      "step": 7426
    },
    {
      "epoch": 1.8114634146341464,
      "grad_norm": 0.22949844598770142,
      "learning_rate": 1.699211224706121e-05,
      "loss": 0.0095,
      "step": 7427
    },
    {
      "epoch": 1.8117073170731708,
      "grad_norm": 0.15080344676971436,
      "learning_rate": 1.6986063103911632e-05,
      "loss": 0.0192,
      "step": 7428
    },
    {
      "epoch": 1.8119512195121952,
      "grad_norm": 0.19403952360153198,
      "learning_rate": 1.69800144836469e-05,
      "loss": 0.0149,
      "step": 7429
    },
    {
      "epoch": 1.8121951219512196,
      "grad_norm": 0.09740349650382996,
      "learning_rate": 1.6973966386661678e-05,
      "loss": 0.0159,
      "step": 7430
    },
    {
      "epoch": 1.812439024390244,
      "grad_norm": 0.08144393563270569,
      "learning_rate": 1.6967918813350574e-05,
      "loss": 0.0139,
      "step": 7431
    },
    {
      "epoch": 1.8126829268292681,
      "grad_norm": 0.12803389132022858,
      "learning_rate": 1.696187176410818e-05,
      "loss": 0.0241,
      "step": 7432
    },
    {
      "epoch": 1.8129268292682927,
      "grad_norm": 0.14770035445690155,
      "learning_rate": 1.6955825239329044e-05,
      "loss": 0.0288,
      "step": 7433
    },
    {
      "epoch": 1.813170731707317,
      "grad_norm": 0.13502584397792816,
      "learning_rate": 1.6949779239407694e-05,
      "loss": 0.0155,
      "step": 7434
    },
    {
      "epoch": 1.8134146341463415,
      "grad_norm": 0.1170789822936058,
      "learning_rate": 1.6943733764738596e-05,
      "loss": 0.013,
      "step": 7435
    },
    {
      "epoch": 1.8136585365853657,
      "grad_norm": 0.05586686357855797,
      "learning_rate": 1.6937688815716207e-05,
      "loss": 0.0082,
      "step": 7436
    },
    {
      "epoch": 1.8139024390243903,
      "grad_norm": 0.07323527336120605,
      "learning_rate": 1.6931644392734947e-05,
      "loss": 0.0117,
      "step": 7437
    },
    {
      "epoch": 1.8141463414634145,
      "grad_norm": 0.13196827471256256,
      "learning_rate": 1.6925600496189193e-05,
      "loss": 0.0256,
      "step": 7438
    },
    {
      "epoch": 1.814390243902439,
      "grad_norm": 0.0885755643248558,
      "learning_rate": 1.6919557126473283e-05,
      "loss": 0.0103,
      "step": 7439
    },
    {
      "epoch": 1.8146341463414632,
      "grad_norm": 0.1260240226984024,
      "learning_rate": 1.691351428398154e-05,
      "loss": 0.0166,
      "step": 7440
    },
    {
      "epoch": 1.8148780487804879,
      "grad_norm": 0.10109172016382217,
      "learning_rate": 1.690747196910823e-05,
      "loss": 0.0172,
      "step": 7441
    },
    {
      "epoch": 1.815121951219512,
      "grad_norm": 0.13773763179779053,
      "learning_rate": 1.6901430182247595e-05,
      "loss": 0.0234,
      "step": 7442
    },
    {
      "epoch": 1.8153658536585366,
      "grad_norm": 0.29074031114578247,
      "learning_rate": 1.689538892379386e-05,
      "loss": 0.0315,
      "step": 7443
    },
    {
      "epoch": 1.8156097560975608,
      "grad_norm": 0.14886151254177094,
      "learning_rate": 1.688934819414118e-05,
      "loss": 0.0208,
      "step": 7444
    },
    {
      "epoch": 1.8158536585365854,
      "grad_norm": 0.1176827996969223,
      "learning_rate": 1.68833079936837e-05,
      "loss": 0.0166,
      "step": 7445
    },
    {
      "epoch": 1.8160975609756096,
      "grad_norm": 0.14147253334522247,
      "learning_rate": 1.6877268322815526e-05,
      "loss": 0.0296,
      "step": 7446
    },
    {
      "epoch": 1.8163414634146342,
      "grad_norm": 0.2002013623714447,
      "learning_rate": 1.6871229181930735e-05,
      "loss": 0.0206,
      "step": 7447
    },
    {
      "epoch": 1.8165853658536584,
      "grad_norm": 0.12105658650398254,
      "learning_rate": 1.686519057142335e-05,
      "loss": 0.0143,
      "step": 7448
    },
    {
      "epoch": 1.816829268292683,
      "grad_norm": 0.14697600901126862,
      "learning_rate": 1.6859152491687374e-05,
      "loss": 0.0252,
      "step": 7449
    },
    {
      "epoch": 1.8170731707317072,
      "grad_norm": 0.10858121514320374,
      "learning_rate": 1.6853114943116778e-05,
      "loss": 0.0122,
      "step": 7450
    },
    {
      "epoch": 1.8173170731707318,
      "grad_norm": 0.1403081715106964,
      "learning_rate": 1.68470779261055e-05,
      "loss": 0.0198,
      "step": 7451
    },
    {
      "epoch": 1.817560975609756,
      "grad_norm": 0.06916269659996033,
      "learning_rate": 1.684104144104742e-05,
      "loss": 0.0173,
      "step": 7452
    },
    {
      "epoch": 1.8178048780487805,
      "grad_norm": 0.06807736307382584,
      "learning_rate": 1.683500548833641e-05,
      "loss": 0.0201,
      "step": 7453
    },
    {
      "epoch": 1.8180487804878047,
      "grad_norm": 0.23472310602664948,
      "learning_rate": 1.6828970068366295e-05,
      "loss": 0.0278,
      "step": 7454
    },
    {
      "epoch": 1.8182926829268293,
      "grad_norm": 0.15015658736228943,
      "learning_rate": 1.682293518153088e-05,
      "loss": 0.0298,
      "step": 7455
    },
    {
      "epoch": 1.8185365853658535,
      "grad_norm": 0.24811683595180511,
      "learning_rate": 1.6816900828223908e-05,
      "loss": 0.0189,
      "step": 7456
    },
    {
      "epoch": 1.818780487804878,
      "grad_norm": 0.13690075278282166,
      "learning_rate": 1.6810867008839106e-05,
      "loss": 0.0219,
      "step": 7457
    },
    {
      "epoch": 1.8190243902439023,
      "grad_norm": 0.08445920050144196,
      "learning_rate": 1.6804833723770154e-05,
      "loss": 0.0194,
      "step": 7458
    },
    {
      "epoch": 1.819268292682927,
      "grad_norm": 0.08910585939884186,
      "learning_rate": 1.6798800973410716e-05,
      "loss": 0.0136,
      "step": 7459
    },
    {
      "epoch": 1.819512195121951,
      "grad_norm": 0.1637047380208969,
      "learning_rate": 1.6792768758154416e-05,
      "loss": 0.0129,
      "step": 7460
    },
    {
      "epoch": 1.8197560975609757,
      "grad_norm": 0.18848979473114014,
      "learning_rate": 1.678673707839482e-05,
      "loss": 0.0358,
      "step": 7461
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 0.12154293060302734,
      "learning_rate": 1.6780705934525488e-05,
      "loss": 0.0123,
      "step": 7462
    },
    {
      "epoch": 1.8202439024390245,
      "grad_norm": 0.20234301686286926,
      "learning_rate": 1.6774675326939937e-05,
      "loss": 0.0239,
      "step": 7463
    },
    {
      "epoch": 1.8204878048780486,
      "grad_norm": 0.09669335931539536,
      "learning_rate": 1.676864525603164e-05,
      "loss": 0.0194,
      "step": 7464
    },
    {
      "epoch": 1.8207317073170732,
      "grad_norm": 0.18360993266105652,
      "learning_rate": 1.676261572219404e-05,
      "loss": 0.0215,
      "step": 7465
    },
    {
      "epoch": 1.8209756097560974,
      "grad_norm": 0.16609874367713928,
      "learning_rate": 1.6756586725820543e-05,
      "loss": 0.0105,
      "step": 7466
    },
    {
      "epoch": 1.821219512195122,
      "grad_norm": 0.09139598906040192,
      "learning_rate": 1.675055826730453e-05,
      "loss": 0.0186,
      "step": 7467
    },
    {
      "epoch": 1.8214634146341462,
      "grad_norm": 0.24615810811519623,
      "learning_rate": 1.674453034703934e-05,
      "loss": 0.0197,
      "step": 7468
    },
    {
      "epoch": 1.8217073170731708,
      "grad_norm": 0.08477959781885147,
      "learning_rate": 1.6738502965418268e-05,
      "loss": 0.0145,
      "step": 7469
    },
    {
      "epoch": 1.821951219512195,
      "grad_norm": 0.1304086297750473,
      "learning_rate": 1.673247612283459e-05,
      "loss": 0.0188,
      "step": 7470
    },
    {
      "epoch": 1.8221951219512196,
      "grad_norm": 0.10293605178594589,
      "learning_rate": 1.672644981968153e-05,
      "loss": 0.0165,
      "step": 7471
    },
    {
      "epoch": 1.8224390243902437,
      "grad_norm": 0.16333243250846863,
      "learning_rate": 1.67204240563523e-05,
      "loss": 0.0126,
      "step": 7472
    },
    {
      "epoch": 1.8226829268292684,
      "grad_norm": 0.15813182294368744,
      "learning_rate": 1.6714398833240058e-05,
      "loss": 0.0224,
      "step": 7473
    },
    {
      "epoch": 1.8229268292682925,
      "grad_norm": 0.14341233670711517,
      "learning_rate": 1.6708374150737915e-05,
      "loss": 0.0173,
      "step": 7474
    },
    {
      "epoch": 1.8231707317073171,
      "grad_norm": 0.12139911949634552,
      "learning_rate": 1.6702350009238976e-05,
      "loss": 0.013,
      "step": 7475
    },
    {
      "epoch": 1.8234146341463413,
      "grad_norm": 0.06370458006858826,
      "learning_rate": 1.66963264091363e-05,
      "loss": 0.0075,
      "step": 7476
    },
    {
      "epoch": 1.823658536585366,
      "grad_norm": 0.11368570476770401,
      "learning_rate": 1.669030335082291e-05,
      "loss": 0.0121,
      "step": 7477
    },
    {
      "epoch": 1.82390243902439,
      "grad_norm": 0.14090393483638763,
      "learning_rate": 1.6684280834691783e-05,
      "loss": 0.0199,
      "step": 7478
    },
    {
      "epoch": 1.8241463414634147,
      "grad_norm": 0.10285305976867676,
      "learning_rate": 1.6678258861135877e-05,
      "loss": 0.0238,
      "step": 7479
    },
    {
      "epoch": 1.8243902439024389,
      "grad_norm": 0.14093416929244995,
      "learning_rate": 1.6672237430548098e-05,
      "loss": 0.0196,
      "step": 7480
    },
    {
      "epoch": 1.8246341463414635,
      "grad_norm": 0.16504761576652527,
      "learning_rate": 1.6666216543321346e-05,
      "loss": 0.0188,
      "step": 7481
    },
    {
      "epoch": 1.8248780487804876,
      "grad_norm": 0.17734161019325256,
      "learning_rate": 1.6660196199848445e-05,
      "loss": 0.014,
      "step": 7482
    },
    {
      "epoch": 1.8251219512195123,
      "grad_norm": 0.13826197385787964,
      "learning_rate": 1.6654176400522207e-05,
      "loss": 0.0151,
      "step": 7483
    },
    {
      "epoch": 1.8253658536585364,
      "grad_norm": 0.12794490158557892,
      "learning_rate": 1.6648157145735417e-05,
      "loss": 0.0105,
      "step": 7484
    },
    {
      "epoch": 1.825609756097561,
      "grad_norm": 0.06894524395465851,
      "learning_rate": 1.6642138435880805e-05,
      "loss": 0.0117,
      "step": 7485
    },
    {
      "epoch": 1.8258536585365852,
      "grad_norm": 0.08935815095901489,
      "learning_rate": 1.663612027135107e-05,
      "loss": 0.0149,
      "step": 7486
    },
    {
      "epoch": 1.8260975609756098,
      "grad_norm": 0.1268208771944046,
      "learning_rate": 1.6630102652538886e-05,
      "loss": 0.014,
      "step": 7487
    },
    {
      "epoch": 1.826341463414634,
      "grad_norm": 0.2290053367614746,
      "learning_rate": 1.662408557983689e-05,
      "loss": 0.0255,
      "step": 7488
    },
    {
      "epoch": 1.8265853658536586,
      "grad_norm": 0.1352570801973343,
      "learning_rate": 1.6618069053637658e-05,
      "loss": 0.0239,
      "step": 7489
    },
    {
      "epoch": 1.8268292682926828,
      "grad_norm": 0.05130050331354141,
      "learning_rate": 1.6612053074333763e-05,
      "loss": 0.008,
      "step": 7490
    },
    {
      "epoch": 1.8270731707317074,
      "grad_norm": 0.07791335135698318,
      "learning_rate": 1.6606037642317726e-05,
      "loss": 0.0146,
      "step": 7491
    },
    {
      "epoch": 1.8273170731707316,
      "grad_norm": 0.11165919154882431,
      "learning_rate": 1.6600022757982036e-05,
      "loss": 0.0125,
      "step": 7492
    },
    {
      "epoch": 1.8275609756097562,
      "grad_norm": 0.046747270971536636,
      "learning_rate": 1.6594008421719143e-05,
      "loss": 0.008,
      "step": 7493
    },
    {
      "epoch": 1.8278048780487803,
      "grad_norm": 0.15802855789661407,
      "learning_rate": 1.658799463392148e-05,
      "loss": 0.0254,
      "step": 7494
    },
    {
      "epoch": 1.828048780487805,
      "grad_norm": 0.08221515268087387,
      "learning_rate": 1.65819813949814e-05,
      "loss": 0.0118,
      "step": 7495
    },
    {
      "epoch": 1.8282926829268291,
      "grad_norm": 0.1471886932849884,
      "learning_rate": 1.657596870529127e-05,
      "loss": 0.0239,
      "step": 7496
    },
    {
      "epoch": 1.8285365853658537,
      "grad_norm": 0.13090062141418457,
      "learning_rate": 1.6569956565243394e-05,
      "loss": 0.0079,
      "step": 7497
    },
    {
      "epoch": 1.828780487804878,
      "grad_norm": 0.09258272498846054,
      "learning_rate": 1.656394497523005e-05,
      "loss": 0.0135,
      "step": 7498
    },
    {
      "epoch": 1.8290243902439025,
      "grad_norm": 0.13726429641246796,
      "learning_rate": 1.6557933935643462e-05,
      "loss": 0.0169,
      "step": 7499
    },
    {
      "epoch": 1.8292682926829267,
      "grad_norm": 0.25588300824165344,
      "learning_rate": 1.6551923446875846e-05,
      "loss": 0.029,
      "step": 7500
    },
    {
      "epoch": 1.8295121951219513,
      "grad_norm": 0.1189199909567833,
      "learning_rate": 1.6545913509319362e-05,
      "loss": 0.0227,
      "step": 7501
    },
    {
      "epoch": 1.8297560975609755,
      "grad_norm": 0.13990181684494019,
      "learning_rate": 1.6539904123366142e-05,
      "loss": 0.0086,
      "step": 7502
    },
    {
      "epoch": 1.83,
      "grad_norm": 0.12852969765663147,
      "learning_rate": 1.653389528940828e-05,
      "loss": 0.0348,
      "step": 7503
    },
    {
      "epoch": 1.8302439024390242,
      "grad_norm": 0.15931014716625214,
      "learning_rate": 1.652788700783783e-05,
      "loss": 0.0191,
      "step": 7504
    },
    {
      "epoch": 1.8304878048780489,
      "grad_norm": 0.08206013590097427,
      "learning_rate": 1.6521879279046827e-05,
      "loss": 0.0141,
      "step": 7505
    },
    {
      "epoch": 1.830731707317073,
      "grad_norm": 0.11189880222082138,
      "learning_rate": 1.6515872103427238e-05,
      "loss": 0.0204,
      "step": 7506
    },
    {
      "epoch": 1.8309756097560976,
      "grad_norm": 0.11092716455459595,
      "learning_rate": 1.6509865481371026e-05,
      "loss": 0.0236,
      "step": 7507
    },
    {
      "epoch": 1.8312195121951218,
      "grad_norm": 0.12915369868278503,
      "learning_rate": 1.6503859413270096e-05,
      "loss": 0.0301,
      "step": 7508
    },
    {
      "epoch": 1.8314634146341464,
      "grad_norm": 0.11356376856565475,
      "learning_rate": 1.649785389951633e-05,
      "loss": 0.0174,
      "step": 7509
    },
    {
      "epoch": 1.8317073170731706,
      "grad_norm": 0.09707794338464737,
      "learning_rate": 1.649184894050157e-05,
      "loss": 0.0198,
      "step": 7510
    },
    {
      "epoch": 1.8319512195121952,
      "grad_norm": 0.07043495029211044,
      "learning_rate": 1.6485844536617626e-05,
      "loss": 0.0173,
      "step": 7511
    },
    {
      "epoch": 1.8321951219512194,
      "grad_norm": 0.1978764533996582,
      "learning_rate": 1.6479840688256252e-05,
      "loss": 0.0209,
      "step": 7512
    },
    {
      "epoch": 1.832439024390244,
      "grad_norm": 0.19119952619075775,
      "learning_rate": 1.64738373958092e-05,
      "loss": 0.024,
      "step": 7513
    },
    {
      "epoch": 1.8326829268292681,
      "grad_norm": 0.05866732820868492,
      "learning_rate": 1.6467834659668146e-05,
      "loss": 0.0181,
      "step": 7514
    },
    {
      "epoch": 1.8329268292682928,
      "grad_norm": 0.12636698782444,
      "learning_rate": 1.6461832480224773e-05,
      "loss": 0.0226,
      "step": 7515
    },
    {
      "epoch": 1.833170731707317,
      "grad_norm": 0.08633330464363098,
      "learning_rate": 1.6455830857870684e-05,
      "loss": 0.0113,
      "step": 7516
    },
    {
      "epoch": 1.8334146341463415,
      "grad_norm": 0.2259209156036377,
      "learning_rate": 1.644982979299748e-05,
      "loss": 0.0164,
      "step": 7517
    },
    {
      "epoch": 1.8336585365853657,
      "grad_norm": 0.11284089833498001,
      "learning_rate": 1.644382928599671e-05,
      "loss": 0.0257,
      "step": 7518
    },
    {
      "epoch": 1.8339024390243903,
      "grad_norm": 0.15153267979621887,
      "learning_rate": 1.643782933725988e-05,
      "loss": 0.0233,
      "step": 7519
    },
    {
      "epoch": 1.8341463414634145,
      "grad_norm": 0.13710136711597443,
      "learning_rate": 1.6431829947178476e-05,
      "loss": 0.0183,
      "step": 7520
    },
    {
      "epoch": 1.834390243902439,
      "grad_norm": 0.40180379152297974,
      "learning_rate": 1.6425831116143947e-05,
      "loss": 0.0396,
      "step": 7521
    },
    {
      "epoch": 1.8346341463414633,
      "grad_norm": 0.14975574612617493,
      "learning_rate": 1.641983284454768e-05,
      "loss": 0.0125,
      "step": 7522
    },
    {
      "epoch": 1.8348780487804879,
      "grad_norm": 0.19383400678634644,
      "learning_rate": 1.6413835132781052e-05,
      "loss": 0.0236,
      "step": 7523
    },
    {
      "epoch": 1.835121951219512,
      "grad_norm": 0.15400145947933197,
      "learning_rate": 1.6407837981235408e-05,
      "loss": 0.0182,
      "step": 7524
    },
    {
      "epoch": 1.8353658536585367,
      "grad_norm": 0.14662621915340424,
      "learning_rate": 1.640184139030202e-05,
      "loss": 0.0148,
      "step": 7525
    },
    {
      "epoch": 1.8356097560975608,
      "grad_norm": 0.12257782369852066,
      "learning_rate": 1.6395845360372163e-05,
      "loss": 0.0191,
      "step": 7526
    },
    {
      "epoch": 1.8358536585365854,
      "grad_norm": 0.09507337212562561,
      "learning_rate": 1.6389849891837057e-05,
      "loss": 0.0185,
      "step": 7527
    },
    {
      "epoch": 1.8360975609756096,
      "grad_norm": 0.10245784372091293,
      "learning_rate": 1.6383854985087895e-05,
      "loss": 0.0087,
      "step": 7528
    },
    {
      "epoch": 1.8363414634146342,
      "grad_norm": 0.17582079768180847,
      "learning_rate": 1.6377860640515807e-05,
      "loss": 0.0269,
      "step": 7529
    },
    {
      "epoch": 1.8365853658536584,
      "grad_norm": 0.09538818895816803,
      "learning_rate": 1.637186685851192e-05,
      "loss": 0.0101,
      "step": 7530
    },
    {
      "epoch": 1.836829268292683,
      "grad_norm": 0.20918558537960052,
      "learning_rate": 1.6365873639467315e-05,
      "loss": 0.0261,
      "step": 7531
    },
    {
      "epoch": 1.8370731707317072,
      "grad_norm": 0.1421177238225937,
      "learning_rate": 1.6359880983773014e-05,
      "loss": 0.0155,
      "step": 7532
    },
    {
      "epoch": 1.8373170731707318,
      "grad_norm": 0.13857972621917725,
      "learning_rate": 1.635388889182003e-05,
      "loss": 0.0117,
      "step": 7533
    },
    {
      "epoch": 1.837560975609756,
      "grad_norm": 0.11787223815917969,
      "learning_rate": 1.6347897363999333e-05,
      "loss": 0.0319,
      "step": 7534
    },
    {
      "epoch": 1.8378048780487806,
      "grad_norm": 0.12930583953857422,
      "learning_rate": 1.6341906400701844e-05,
      "loss": 0.0097,
      "step": 7535
    },
    {
      "epoch": 1.8380487804878047,
      "grad_norm": 0.07416258752346039,
      "learning_rate": 1.6335916002318456e-05,
      "loss": 0.013,
      "step": 7536
    },
    {
      "epoch": 1.8382926829268293,
      "grad_norm": 0.18097525835037231,
      "learning_rate": 1.6329926169240024e-05,
      "loss": 0.0153,
      "step": 7537
    },
    {
      "epoch": 1.8385365853658535,
      "grad_norm": 0.13774198293685913,
      "learning_rate": 1.6323936901857378e-05,
      "loss": 0.014,
      "step": 7538
    },
    {
      "epoch": 1.8387804878048781,
      "grad_norm": 0.0711868554353714,
      "learning_rate": 1.631794820056128e-05,
      "loss": 0.0132,
      "step": 7539
    },
    {
      "epoch": 1.8390243902439023,
      "grad_norm": 0.12618018686771393,
      "learning_rate": 1.6311960065742486e-05,
      "loss": 0.0351,
      "step": 7540
    },
    {
      "epoch": 1.839268292682927,
      "grad_norm": 0.22866322100162506,
      "learning_rate": 1.63059724977917e-05,
      "loss": 0.028,
      "step": 7541
    },
    {
      "epoch": 1.839512195121951,
      "grad_norm": 0.12312314659357071,
      "learning_rate": 1.6299985497099597e-05,
      "loss": 0.0114,
      "step": 7542
    },
    {
      "epoch": 1.8397560975609757,
      "grad_norm": 0.10725562274456024,
      "learning_rate": 1.62939990640568e-05,
      "loss": 0.0131,
      "step": 7543
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.12210729718208313,
      "learning_rate": 1.628801319905392e-05,
      "loss": 0.019,
      "step": 7544
    },
    {
      "epoch": 1.8402439024390245,
      "grad_norm": 0.1851433366537094,
      "learning_rate": 1.6282027902481504e-05,
      "loss": 0.0195,
      "step": 7545
    },
    {
      "epoch": 1.8404878048780486,
      "grad_norm": 0.0789211094379425,
      "learning_rate": 1.6276043174730075e-05,
      "loss": 0.0138,
      "step": 7546
    },
    {
      "epoch": 1.8407317073170733,
      "grad_norm": 0.10226504504680634,
      "learning_rate": 1.627005901619013e-05,
      "loss": 0.0158,
      "step": 7547
    },
    {
      "epoch": 1.8409756097560974,
      "grad_norm": 0.10551490634679794,
      "learning_rate": 1.6264075427252106e-05,
      "loss": 0.0168,
      "step": 7548
    },
    {
      "epoch": 1.841219512195122,
      "grad_norm": 0.09436175972223282,
      "learning_rate": 1.625809240830642e-05,
      "loss": 0.0088,
      "step": 7549
    },
    {
      "epoch": 1.8414634146341462,
      "grad_norm": 0.09425298869609833,
      "learning_rate": 1.6252109959743438e-05,
      "loss": 0.0148,
      "step": 7550
    },
    {
      "epoch": 1.8417073170731708,
      "grad_norm": 0.44673147797584534,
      "learning_rate": 1.62461280819535e-05,
      "loss": 0.0242,
      "step": 7551
    },
    {
      "epoch": 1.841951219512195,
      "grad_norm": 0.08458652347326279,
      "learning_rate": 1.6240146775326915e-05,
      "loss": 0.0147,
      "step": 7552
    },
    {
      "epoch": 1.8421951219512196,
      "grad_norm": 0.12906880676746368,
      "learning_rate": 1.6234166040253928e-05,
      "loss": 0.0161,
      "step": 7553
    },
    {
      "epoch": 1.8424390243902438,
      "grad_norm": 0.06271813064813614,
      "learning_rate": 1.6228185877124786e-05,
      "loss": 0.0162,
      "step": 7554
    },
    {
      "epoch": 1.8426829268292684,
      "grad_norm": 0.09349748492240906,
      "learning_rate": 1.622220628632965e-05,
      "loss": 0.03,
      "step": 7555
    },
    {
      "epoch": 1.8429268292682925,
      "grad_norm": 0.07999443262815475,
      "learning_rate": 1.621622726825868e-05,
      "loss": 0.0092,
      "step": 7556
    },
    {
      "epoch": 1.8431707317073172,
      "grad_norm": 0.16877295076847076,
      "learning_rate": 1.6210248823302e-05,
      "loss": 0.0202,
      "step": 7557
    },
    {
      "epoch": 1.8434146341463413,
      "grad_norm": 0.19252343475818634,
      "learning_rate": 1.6204270951849674e-05,
      "loss": 0.0232,
      "step": 7558
    },
    {
      "epoch": 1.843658536585366,
      "grad_norm": 0.21004419028759003,
      "learning_rate": 1.6198293654291736e-05,
      "loss": 0.0084,
      "step": 7559
    },
    {
      "epoch": 1.84390243902439,
      "grad_norm": 0.11638574302196503,
      "learning_rate": 1.6192316931018196e-05,
      "loss": 0.0248,
      "step": 7560
    },
    {
      "epoch": 1.8441463414634147,
      "grad_norm": 0.1397673487663269,
      "learning_rate": 1.6186340782419018e-05,
      "loss": 0.0089,
      "step": 7561
    },
    {
      "epoch": 1.8443902439024389,
      "grad_norm": 0.16982319951057434,
      "learning_rate": 1.6180365208884118e-05,
      "loss": 0.0217,
      "step": 7562
    },
    {
      "epoch": 1.8446341463414635,
      "grad_norm": 0.1187959536910057,
      "learning_rate": 1.6174390210803385e-05,
      "loss": 0.0198,
      "step": 7563
    },
    {
      "epoch": 1.8448780487804877,
      "grad_norm": 0.1398555040359497,
      "learning_rate": 1.6168415788566678e-05,
      "loss": 0.0134,
      "step": 7564
    },
    {
      "epoch": 1.8451219512195123,
      "grad_norm": 0.12463443726301193,
      "learning_rate": 1.6162441942563805e-05,
      "loss": 0.0149,
      "step": 7565
    },
    {
      "epoch": 1.8453658536585364,
      "grad_norm": 0.12862852215766907,
      "learning_rate": 1.6156468673184537e-05,
      "loss": 0.0239,
      "step": 7566
    },
    {
      "epoch": 1.845609756097561,
      "grad_norm": 0.15996992588043213,
      "learning_rate": 1.615049598081862e-05,
      "loss": 0.0184,
      "step": 7567
    },
    {
      "epoch": 1.8458536585365852,
      "grad_norm": 0.11446800082921982,
      "learning_rate": 1.6144523865855745e-05,
      "loss": 0.0065,
      "step": 7568
    },
    {
      "epoch": 1.8460975609756098,
      "grad_norm": 0.17403191328048706,
      "learning_rate": 1.6138552328685586e-05,
      "loss": 0.0337,
      "step": 7569
    },
    {
      "epoch": 1.846341463414634,
      "grad_norm": 0.14888395369052887,
      "learning_rate": 1.6132581369697752e-05,
      "loss": 0.0202,
      "step": 7570
    },
    {
      "epoch": 1.8465853658536586,
      "grad_norm": 0.22368672490119934,
      "learning_rate": 1.6126610989281848e-05,
      "loss": 0.0163,
      "step": 7571
    },
    {
      "epoch": 1.8468292682926828,
      "grad_norm": 0.11920717358589172,
      "learning_rate": 1.6120641187827406e-05,
      "loss": 0.0283,
      "step": 7572
    },
    {
      "epoch": 1.8470731707317074,
      "grad_norm": 0.05423992499709129,
      "learning_rate": 1.6114671965723945e-05,
      "loss": 0.006,
      "step": 7573
    },
    {
      "epoch": 1.8473170731707316,
      "grad_norm": 0.07537388801574707,
      "learning_rate": 1.610870332336094e-05,
      "loss": 0.0091,
      "step": 7574
    },
    {
      "epoch": 1.8475609756097562,
      "grad_norm": 0.17961592972278595,
      "learning_rate": 1.6102735261127816e-05,
      "loss": 0.0182,
      "step": 7575
    },
    {
      "epoch": 1.8478048780487804,
      "grad_norm": 0.06794730573892593,
      "learning_rate": 1.6096767779413986e-05,
      "loss": 0.0121,
      "step": 7576
    },
    {
      "epoch": 1.848048780487805,
      "grad_norm": 0.1831604689359665,
      "learning_rate": 1.6090800878608796e-05,
      "loss": 0.0176,
      "step": 7577
    },
    {
      "epoch": 1.8482926829268291,
      "grad_norm": 0.1433103233575821,
      "learning_rate": 1.6084834559101585e-05,
      "loss": 0.033,
      "step": 7578
    },
    {
      "epoch": 1.8485365853658537,
      "grad_norm": 0.16923166811466217,
      "learning_rate": 1.607886882128162e-05,
      "loss": 0.0216,
      "step": 7579
    },
    {
      "epoch": 1.848780487804878,
      "grad_norm": 0.17925232648849487,
      "learning_rate": 1.6072903665538156e-05,
      "loss": 0.0256,
      "step": 7580
    },
    {
      "epoch": 1.8490243902439025,
      "grad_norm": 0.1924538016319275,
      "learning_rate": 1.6066939092260398e-05,
      "loss": 0.0235,
      "step": 7581
    },
    {
      "epoch": 1.8492682926829267,
      "grad_norm": 0.12453033030033112,
      "learning_rate": 1.6060975101837524e-05,
      "loss": 0.0165,
      "step": 7582
    },
    {
      "epoch": 1.8495121951219513,
      "grad_norm": 0.0949954092502594,
      "learning_rate": 1.605501169465865e-05,
      "loss": 0.0132,
      "step": 7583
    },
    {
      "epoch": 1.8497560975609755,
      "grad_norm": 0.18017029762268066,
      "learning_rate": 1.604904887111288e-05,
      "loss": 0.0264,
      "step": 7584
    },
    {
      "epoch": 1.85,
      "grad_norm": 0.19677066802978516,
      "learning_rate": 1.6043086631589278e-05,
      "loss": 0.0144,
      "step": 7585
    },
    {
      "epoch": 1.8502439024390243,
      "grad_norm": 0.09727691113948822,
      "learning_rate": 1.6037124976476853e-05,
      "loss": 0.0129,
      "step": 7586
    },
    {
      "epoch": 1.8504878048780489,
      "grad_norm": 0.1815924495458603,
      "learning_rate": 1.603116390616459e-05,
      "loss": 0.0152,
      "step": 7587
    },
    {
      "epoch": 1.850731707317073,
      "grad_norm": 0.14593428373336792,
      "learning_rate": 1.6025203421041415e-05,
      "loss": 0.0279,
      "step": 7588
    },
    {
      "epoch": 1.8509756097560977,
      "grad_norm": 0.13835978507995605,
      "learning_rate": 1.6019243521496242e-05,
      "loss": 0.0239,
      "step": 7589
    },
    {
      "epoch": 1.8512195121951218,
      "grad_norm": 0.09432875365018845,
      "learning_rate": 1.601328420791794e-05,
      "loss": 0.0291,
      "step": 7590
    },
    {
      "epoch": 1.8514634146341464,
      "grad_norm": 0.13190683722496033,
      "learning_rate": 1.6007325480695335e-05,
      "loss": 0.0228,
      "step": 7591
    },
    {
      "epoch": 1.8517073170731706,
      "grad_norm": 0.1255134642124176,
      "learning_rate": 1.600136734021721e-05,
      "loss": 0.0214,
      "step": 7592
    },
    {
      "epoch": 1.8519512195121952,
      "grad_norm": 0.08996184170246124,
      "learning_rate": 1.5995409786872318e-05,
      "loss": 0.0167,
      "step": 7593
    },
    {
      "epoch": 1.8521951219512194,
      "grad_norm": 0.22918160259723663,
      "learning_rate": 1.598945282104937e-05,
      "loss": 0.0257,
      "step": 7594
    },
    {
      "epoch": 1.852439024390244,
      "grad_norm": 0.06714186072349548,
      "learning_rate": 1.5983496443137046e-05,
      "loss": 0.0087,
      "step": 7595
    },
    {
      "epoch": 1.8526829268292682,
      "grad_norm": 0.0812053456902504,
      "learning_rate": 1.5977540653523974e-05,
      "loss": 0.0136,
      "step": 7596
    },
    {
      "epoch": 1.8529268292682928,
      "grad_norm": 0.08203903585672379,
      "learning_rate": 1.5971585452598754e-05,
      "loss": 0.0114,
      "step": 7597
    },
    {
      "epoch": 1.853170731707317,
      "grad_norm": 0.15195247530937195,
      "learning_rate": 1.5965630840749942e-05,
      "loss": 0.0298,
      "step": 7598
    },
    {
      "epoch": 1.8534146341463416,
      "grad_norm": 0.17816153168678284,
      "learning_rate": 1.595967681836607e-05,
      "loss": 0.0207,
      "step": 7599
    },
    {
      "epoch": 1.8536585365853657,
      "grad_norm": 0.26013556122779846,
      "learning_rate": 1.5953723385835602e-05,
      "loss": 0.0219,
      "step": 7600
    },
    {
      "epoch": 1.8539024390243903,
      "grad_norm": 0.078949473798275,
      "learning_rate": 1.5947770543546993e-05,
      "loss": 0.0122,
      "step": 7601
    },
    {
      "epoch": 1.8541463414634145,
      "grad_norm": 0.10904417932033539,
      "learning_rate": 1.594181829188864e-05,
      "loss": 0.0416,
      "step": 7602
    },
    {
      "epoch": 1.8543902439024391,
      "grad_norm": 0.09622463583946228,
      "learning_rate": 1.5935866631248923e-05,
      "loss": 0.0267,
      "step": 7603
    },
    {
      "epoch": 1.8546341463414633,
      "grad_norm": 0.1273903250694275,
      "learning_rate": 1.592991556201616e-05,
      "loss": 0.04,
      "step": 7604
    },
    {
      "epoch": 1.854878048780488,
      "grad_norm": 0.10928892344236374,
      "learning_rate": 1.5923965084578635e-05,
      "loss": 0.0205,
      "step": 7605
    },
    {
      "epoch": 1.855121951219512,
      "grad_norm": 0.18002387881278992,
      "learning_rate": 1.5918015199324603e-05,
      "loss": 0.0173,
      "step": 7606
    },
    {
      "epoch": 1.8553658536585367,
      "grad_norm": 0.20251788198947906,
      "learning_rate": 1.5912065906642275e-05,
      "loss": 0.0292,
      "step": 7607
    },
    {
      "epoch": 1.8556097560975608,
      "grad_norm": 0.08517350256443024,
      "learning_rate": 1.5906117206919835e-05,
      "loss": 0.0156,
      "step": 7608
    },
    {
      "epoch": 1.8558536585365855,
      "grad_norm": 0.16454194486141205,
      "learning_rate": 1.5900169100545402e-05,
      "loss": 0.016,
      "step": 7609
    },
    {
      "epoch": 1.8560975609756096,
      "grad_norm": 0.09160705655813217,
      "learning_rate": 1.5894221587907078e-05,
      "loss": 0.0142,
      "step": 7610
    },
    {
      "epoch": 1.8563414634146342,
      "grad_norm": 0.1472165882587433,
      "learning_rate": 1.5888274669392917e-05,
      "loss": 0.0181,
      "step": 7611
    },
    {
      "epoch": 1.8565853658536584,
      "grad_norm": 0.22054900228977203,
      "learning_rate": 1.5882328345390944e-05,
      "loss": 0.0239,
      "step": 7612
    },
    {
      "epoch": 1.856829268292683,
      "grad_norm": 0.14949052035808563,
      "learning_rate": 1.587638261628913e-05,
      "loss": 0.0307,
      "step": 7613
    },
    {
      "epoch": 1.8570731707317072,
      "grad_norm": 0.17056111991405487,
      "learning_rate": 1.5870437482475426e-05,
      "loss": 0.0166,
      "step": 7614
    },
    {
      "epoch": 1.8573170731707318,
      "grad_norm": 0.039729006588459015,
      "learning_rate": 1.5864492944337722e-05,
      "loss": 0.0027,
      "step": 7615
    },
    {
      "epoch": 1.857560975609756,
      "grad_norm": 0.11683277785778046,
      "learning_rate": 1.5858549002263893e-05,
      "loss": 0.0164,
      "step": 7616
    },
    {
      "epoch": 1.8578048780487806,
      "grad_norm": 0.234971821308136,
      "learning_rate": 1.5852605656641755e-05,
      "loss": 0.0231,
      "step": 7617
    },
    {
      "epoch": 1.8580487804878048,
      "grad_norm": 0.1239464282989502,
      "learning_rate": 1.5846662907859093e-05,
      "loss": 0.0204,
      "step": 7618
    },
    {
      "epoch": 1.8582926829268294,
      "grad_norm": 0.08938061445951462,
      "learning_rate": 1.584072075630366e-05,
      "loss": 0.0157,
      "step": 7619
    },
    {
      "epoch": 1.8585365853658535,
      "grad_norm": 0.21259163320064545,
      "learning_rate": 1.583477920236316e-05,
      "loss": 0.0149,
      "step": 7620
    },
    {
      "epoch": 1.8587804878048781,
      "grad_norm": 0.09580311924219131,
      "learning_rate": 1.5828838246425256e-05,
      "loss": 0.0198,
      "step": 7621
    },
    {
      "epoch": 1.8590243902439023,
      "grad_norm": 0.2332301288843155,
      "learning_rate": 1.582289788887758e-05,
      "loss": 0.0276,
      "step": 7622
    },
    {
      "epoch": 1.859268292682927,
      "grad_norm": 0.11013591289520264,
      "learning_rate": 1.581695813010772e-05,
      "loss": 0.0153,
      "step": 7623
    },
    {
      "epoch": 1.859512195121951,
      "grad_norm": 0.11969049274921417,
      "learning_rate": 1.581101897050323e-05,
      "loss": 0.0197,
      "step": 7624
    },
    {
      "epoch": 1.8597560975609757,
      "grad_norm": 0.19948796927928925,
      "learning_rate": 1.5805080410451628e-05,
      "loss": 0.0264,
      "step": 7625
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 0.09299811720848083,
      "learning_rate": 1.5799142450340375e-05,
      "loss": 0.0154,
      "step": 7626
    },
    {
      "epoch": 1.8602439024390245,
      "grad_norm": 0.11238297820091248,
      "learning_rate": 1.5793205090556913e-05,
      "loss": 0.0177,
      "step": 7627
    },
    {
      "epoch": 1.8604878048780487,
      "grad_norm": 0.10133910179138184,
      "learning_rate": 1.5787268331488634e-05,
      "loss": 0.018,
      "step": 7628
    },
    {
      "epoch": 1.8607317073170733,
      "grad_norm": 0.1294659823179245,
      "learning_rate": 1.5781332173522893e-05,
      "loss": 0.0109,
      "step": 7629
    },
    {
      "epoch": 1.8609756097560974,
      "grad_norm": 0.21976138651371002,
      "learning_rate": 1.577539661704701e-05,
      "loss": 0.0243,
      "step": 7630
    },
    {
      "epoch": 1.861219512195122,
      "grad_norm": 0.16654448211193085,
      "learning_rate": 1.5769461662448255e-05,
      "loss": 0.0209,
      "step": 7631
    },
    {
      "epoch": 1.8614634146341462,
      "grad_norm": 0.09994102269411087,
      "learning_rate": 1.576352731011387e-05,
      "loss": 0.0166,
      "step": 7632
    },
    {
      "epoch": 1.8617073170731708,
      "grad_norm": 0.10862106829881668,
      "learning_rate": 1.5757593560431053e-05,
      "loss": 0.0161,
      "step": 7633
    },
    {
      "epoch": 1.861951219512195,
      "grad_norm": 0.08570434153079987,
      "learning_rate": 1.5751660413786963e-05,
      "loss": 0.0129,
      "step": 7634
    },
    {
      "epoch": 1.8621951219512196,
      "grad_norm": 0.1410321295261383,
      "learning_rate": 1.5745727870568714e-05,
      "loss": 0.0151,
      "step": 7635
    },
    {
      "epoch": 1.8624390243902438,
      "grad_norm": 0.20410126447677612,
      "learning_rate": 1.5739795931163398e-05,
      "loss": 0.0221,
      "step": 7636
    },
    {
      "epoch": 1.8626829268292684,
      "grad_norm": 0.14664192497730255,
      "learning_rate": 1.573386459595805e-05,
      "loss": 0.0174,
      "step": 7637
    },
    {
      "epoch": 1.8629268292682926,
      "grad_norm": 0.2726263403892517,
      "learning_rate": 1.5727933865339673e-05,
      "loss": 0.0234,
      "step": 7638
    },
    {
      "epoch": 1.8631707317073172,
      "grad_norm": 0.12145332247018814,
      "learning_rate": 1.572200373969522e-05,
      "loss": 0.0164,
      "step": 7639
    },
    {
      "epoch": 1.8634146341463413,
      "grad_norm": 0.13042734563350677,
      "learning_rate": 1.571607421941162e-05,
      "loss": 0.0128,
      "step": 7640
    },
    {
      "epoch": 1.863658536585366,
      "grad_norm": 0.1688656210899353,
      "learning_rate": 1.5710145304875754e-05,
      "loss": 0.0239,
      "step": 7641
    },
    {
      "epoch": 1.8639024390243901,
      "grad_norm": 0.22899436950683594,
      "learning_rate": 1.5704216996474474e-05,
      "loss": 0.0198,
      "step": 7642
    },
    {
      "epoch": 1.8641463414634147,
      "grad_norm": 0.12794780731201172,
      "learning_rate": 1.5698289294594574e-05,
      "loss": 0.0228,
      "step": 7643
    },
    {
      "epoch": 1.864390243902439,
      "grad_norm": 0.16100598871707916,
      "learning_rate": 1.5692362199622825e-05,
      "loss": 0.0272,
      "step": 7644
    },
    {
      "epoch": 1.8646341463414635,
      "grad_norm": 0.16771870851516724,
      "learning_rate": 1.5686435711945946e-05,
      "loss": 0.0261,
      "step": 7645
    },
    {
      "epoch": 1.8648780487804877,
      "grad_norm": 0.13763472437858582,
      "learning_rate": 1.568050983195063e-05,
      "loss": 0.018,
      "step": 7646
    },
    {
      "epoch": 1.8651219512195123,
      "grad_norm": 0.1284446269273758,
      "learning_rate": 1.5674584560023514e-05,
      "loss": 0.0188,
      "step": 7647
    },
    {
      "epoch": 1.8653658536585365,
      "grad_norm": 0.20443099737167358,
      "learning_rate": 1.5668659896551207e-05,
      "loss": 0.0229,
      "step": 7648
    },
    {
      "epoch": 1.865609756097561,
      "grad_norm": 0.0867885872721672,
      "learning_rate": 1.5662735841920277e-05,
      "loss": 0.0112,
      "step": 7649
    },
    {
      "epoch": 1.8658536585365852,
      "grad_norm": 0.07291465252637863,
      "learning_rate": 1.565681239651725e-05,
      "loss": 0.0128,
      "step": 7650
    },
    {
      "epoch": 1.8660975609756099,
      "grad_norm": 0.12921138107776642,
      "learning_rate": 1.565088956072861e-05,
      "loss": 0.0247,
      "step": 7651
    },
    {
      "epoch": 1.866341463414634,
      "grad_norm": 0.09777019172906876,
      "learning_rate": 1.564496733494081e-05,
      "loss": 0.0119,
      "step": 7652
    },
    {
      "epoch": 1.8665853658536586,
      "grad_norm": 0.1297944188117981,
      "learning_rate": 1.563904571954026e-05,
      "loss": 0.0281,
      "step": 7653
    },
    {
      "epoch": 1.8668292682926828,
      "grad_norm": 0.16748052835464478,
      "learning_rate": 1.5633124714913304e-05,
      "loss": 0.0192,
      "step": 7654
    },
    {
      "epoch": 1.8670731707317074,
      "grad_norm": 0.1400986611843109,
      "learning_rate": 1.5627204321446296e-05,
      "loss": 0.024,
      "step": 7655
    },
    {
      "epoch": 1.8673170731707316,
      "grad_norm": 0.12465516477823257,
      "learning_rate": 1.562128453952551e-05,
      "loss": 0.0239,
      "step": 7656
    },
    {
      "epoch": 1.8675609756097562,
      "grad_norm": 0.1567000150680542,
      "learning_rate": 1.5615365369537192e-05,
      "loss": 0.0225,
      "step": 7657
    },
    {
      "epoch": 1.8678048780487804,
      "grad_norm": 0.09891847521066666,
      "learning_rate": 1.5609446811867557e-05,
      "loss": 0.0207,
      "step": 7658
    },
    {
      "epoch": 1.868048780487805,
      "grad_norm": 0.14695382118225098,
      "learning_rate": 1.5603528866902772e-05,
      "loss": 0.0151,
      "step": 7659
    },
    {
      "epoch": 1.8682926829268292,
      "grad_norm": 0.15232841670513153,
      "learning_rate": 1.5597611535028962e-05,
      "loss": 0.0113,
      "step": 7660
    },
    {
      "epoch": 1.8685365853658538,
      "grad_norm": 0.20441433787345886,
      "learning_rate": 1.5591694816632215e-05,
      "loss": 0.0171,
      "step": 7661
    },
    {
      "epoch": 1.868780487804878,
      "grad_norm": 0.1985592395067215,
      "learning_rate": 1.5585778712098585e-05,
      "loss": 0.0253,
      "step": 7662
    },
    {
      "epoch": 1.8690243902439025,
      "grad_norm": 0.22192136943340302,
      "learning_rate": 1.557986322181407e-05,
      "loss": 0.0237,
      "step": 7663
    },
    {
      "epoch": 1.8692682926829267,
      "grad_norm": 0.08972933888435364,
      "learning_rate": 1.557394834616464e-05,
      "loss": 0.0176,
      "step": 7664
    },
    {
      "epoch": 1.8695121951219513,
      "grad_norm": 0.08699538558721542,
      "learning_rate": 1.5568034085536227e-05,
      "loss": 0.0054,
      "step": 7665
    },
    {
      "epoch": 1.8697560975609755,
      "grad_norm": 0.20094344019889832,
      "learning_rate": 1.5562120440314722e-05,
      "loss": 0.0213,
      "step": 7666
    },
    {
      "epoch": 1.87,
      "grad_norm": 0.3317919671535492,
      "learning_rate": 1.5556207410885953e-05,
      "loss": 0.0226,
      "step": 7667
    },
    {
      "epoch": 1.8702439024390243,
      "grad_norm": 0.1021721288561821,
      "learning_rate": 1.5550294997635747e-05,
      "loss": 0.0128,
      "step": 7668
    },
    {
      "epoch": 1.8704878048780489,
      "grad_norm": 0.12506955862045288,
      "learning_rate": 1.5544383200949863e-05,
      "loss": 0.0219,
      "step": 7669
    },
    {
      "epoch": 1.870731707317073,
      "grad_norm": 0.17186293005943298,
      "learning_rate": 1.553847202121404e-05,
      "loss": 0.0154,
      "step": 7670
    },
    {
      "epoch": 1.8709756097560977,
      "grad_norm": 0.1269879937171936,
      "learning_rate": 1.553256145881394e-05,
      "loss": 0.0117,
      "step": 7671
    },
    {
      "epoch": 1.8712195121951218,
      "grad_norm": 0.217967227101326,
      "learning_rate": 1.552665151413523e-05,
      "loss": 0.0276,
      "step": 7672
    },
    {
      "epoch": 1.8714634146341464,
      "grad_norm": 0.09007537364959717,
      "learning_rate": 1.5520742187563503e-05,
      "loss": 0.0262,
      "step": 7673
    },
    {
      "epoch": 1.8717073170731706,
      "grad_norm": 0.10430757701396942,
      "learning_rate": 1.5514833479484324e-05,
      "loss": 0.027,
      "step": 7674
    },
    {
      "epoch": 1.8719512195121952,
      "grad_norm": 0.09781187772750854,
      "learning_rate": 1.5508925390283236e-05,
      "loss": 0.0174,
      "step": 7675
    },
    {
      "epoch": 1.8721951219512194,
      "grad_norm": 0.13938066363334656,
      "learning_rate": 1.5503017920345702e-05,
      "loss": 0.0236,
      "step": 7676
    },
    {
      "epoch": 1.872439024390244,
      "grad_norm": 0.17128883302211761,
      "learning_rate": 1.5497111070057178e-05,
      "loss": 0.0442,
      "step": 7677
    },
    {
      "epoch": 1.8726829268292682,
      "grad_norm": 0.23399250209331512,
      "learning_rate": 1.5491204839803063e-05,
      "loss": 0.0232,
      "step": 7678
    },
    {
      "epoch": 1.8729268292682928,
      "grad_norm": 0.1397080421447754,
      "learning_rate": 1.548529922996873e-05,
      "loss": 0.0186,
      "step": 7679
    },
    {
      "epoch": 1.873170731707317,
      "grad_norm": 0.0789981260895729,
      "learning_rate": 1.547939424093949e-05,
      "loss": 0.0066,
      "step": 7680
    },
    {
      "epoch": 1.8734146341463416,
      "grad_norm": 0.15635241568088531,
      "learning_rate": 1.547348987310063e-05,
      "loss": 0.0162,
      "step": 7681
    },
    {
      "epoch": 1.8736585365853657,
      "grad_norm": 0.0960034728050232,
      "learning_rate": 1.5467586126837393e-05,
      "loss": 0.0187,
      "step": 7682
    },
    {
      "epoch": 1.8739024390243904,
      "grad_norm": 0.13564026355743408,
      "learning_rate": 1.546168300253498e-05,
      "loss": 0.0322,
      "step": 7683
    },
    {
      "epoch": 1.8741463414634145,
      "grad_norm": 0.14567355811595917,
      "learning_rate": 1.5455780500578553e-05,
      "loss": 0.0177,
      "step": 7684
    },
    {
      "epoch": 1.8743902439024391,
      "grad_norm": 0.07573458552360535,
      "learning_rate": 1.5449878621353225e-05,
      "loss": 0.008,
      "step": 7685
    },
    {
      "epoch": 1.8746341463414633,
      "grad_norm": 0.13935072720050812,
      "learning_rate": 1.5443977365244097e-05,
      "loss": 0.0174,
      "step": 7686
    },
    {
      "epoch": 1.874878048780488,
      "grad_norm": 0.19611109793186188,
      "learning_rate": 1.5438076732636182e-05,
      "loss": 0.0208,
      "step": 7687
    },
    {
      "epoch": 1.875121951219512,
      "grad_norm": 0.10757714509963989,
      "learning_rate": 1.543217672391448e-05,
      "loss": 0.0229,
      "step": 7688
    },
    {
      "epoch": 1.8753658536585367,
      "grad_norm": 0.09886273741722107,
      "learning_rate": 1.542627733946397e-05,
      "loss": 0.0299,
      "step": 7689
    },
    {
      "epoch": 1.8756097560975609,
      "grad_norm": 0.08399215340614319,
      "learning_rate": 1.5420378579669547e-05,
      "loss": 0.0148,
      "step": 7690
    },
    {
      "epoch": 1.8758536585365855,
      "grad_norm": 0.15495549142360687,
      "learning_rate": 1.5414480444916095e-05,
      "loss": 0.0165,
      "step": 7691
    },
    {
      "epoch": 1.8760975609756096,
      "grad_norm": 0.09169885516166687,
      "learning_rate": 1.540858293558846e-05,
      "loss": 0.0242,
      "step": 7692
    },
    {
      "epoch": 1.8763414634146343,
      "grad_norm": 0.14146749675273895,
      "learning_rate": 1.5402686052071417e-05,
      "loss": 0.0298,
      "step": 7693
    },
    {
      "epoch": 1.8765853658536584,
      "grad_norm": 0.17252857983112335,
      "learning_rate": 1.539678979474973e-05,
      "loss": 0.0153,
      "step": 7694
    },
    {
      "epoch": 1.876829268292683,
      "grad_norm": 0.08552044630050659,
      "learning_rate": 1.5390894164008112e-05,
      "loss": 0.0164,
      "step": 7695
    },
    {
      "epoch": 1.8770731707317072,
      "grad_norm": 0.1786337047815323,
      "learning_rate": 1.5384999160231237e-05,
      "loss": 0.0155,
      "step": 7696
    },
    {
      "epoch": 1.8773170731707318,
      "grad_norm": 0.3088321089744568,
      "learning_rate": 1.5379104783803727e-05,
      "loss": 0.0341,
      "step": 7697
    },
    {
      "epoch": 1.877560975609756,
      "grad_norm": 0.2851656377315521,
      "learning_rate": 1.5373211035110177e-05,
      "loss": 0.0237,
      "step": 7698
    },
    {
      "epoch": 1.8778048780487806,
      "grad_norm": 0.18225228786468506,
      "learning_rate": 1.5367317914535138e-05,
      "loss": 0.0164,
      "step": 7699
    },
    {
      "epoch": 1.8780487804878048,
      "grad_norm": 0.14572973549365997,
      "learning_rate": 1.536142542246312e-05,
      "loss": 0.0201,
      "step": 7700
    },
    {
      "epoch": 1.8782926829268294,
      "grad_norm": 0.26453185081481934,
      "learning_rate": 1.5355533559278585e-05,
      "loss": 0.0259,
      "step": 7701
    },
    {
      "epoch": 1.8785365853658536,
      "grad_norm": 0.2082863599061966,
      "learning_rate": 1.5349642325365956e-05,
      "loss": 0.0135,
      "step": 7702
    },
    {
      "epoch": 1.8787804878048782,
      "grad_norm": 0.10899484157562256,
      "learning_rate": 1.5343751721109637e-05,
      "loss": 0.0163,
      "step": 7703
    },
    {
      "epoch": 1.8790243902439023,
      "grad_norm": 0.17161786556243896,
      "learning_rate": 1.5337861746893945e-05,
      "loss": 0.0184,
      "step": 7704
    },
    {
      "epoch": 1.879268292682927,
      "grad_norm": 0.08165957778692245,
      "learning_rate": 1.53319724031032e-05,
      "loss": 0.0091,
      "step": 7705
    },
    {
      "epoch": 1.8795121951219511,
      "grad_norm": 0.1794082224369049,
      "learning_rate": 1.5326083690121658e-05,
      "loss": 0.0121,
      "step": 7706
    },
    {
      "epoch": 1.8797560975609757,
      "grad_norm": 0.1731981635093689,
      "learning_rate": 1.5320195608333536e-05,
      "loss": 0.0232,
      "step": 7707
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.08791651576757431,
      "learning_rate": 1.5314308158123018e-05,
      "loss": 0.0148,
      "step": 7708
    },
    {
      "epoch": 1.8802439024390245,
      "grad_norm": 0.11983360350131989,
      "learning_rate": 1.5308421339874252e-05,
      "loss": 0.0174,
      "step": 7709
    },
    {
      "epoch": 1.8804878048780487,
      "grad_norm": 0.2428634613752365,
      "learning_rate": 1.5302535153971315e-05,
      "loss": 0.0207,
      "step": 7710
    },
    {
      "epoch": 1.8807317073170733,
      "grad_norm": 0.1418541669845581,
      "learning_rate": 1.5296649600798273e-05,
      "loss": 0.0145,
      "step": 7711
    },
    {
      "epoch": 1.8809756097560975,
      "grad_norm": 0.09162887185811996,
      "learning_rate": 1.529076468073914e-05,
      "loss": 0.0105,
      "step": 7712
    },
    {
      "epoch": 1.881219512195122,
      "grad_norm": 0.1296485811471939,
      "learning_rate": 1.528488039417789e-05,
      "loss": 0.0258,
      "step": 7713
    },
    {
      "epoch": 1.8814634146341462,
      "grad_norm": 0.08651719987392426,
      "learning_rate": 1.527899674149846e-05,
      "loss": 0.0074,
      "step": 7714
    },
    {
      "epoch": 1.8817073170731708,
      "grad_norm": 0.08064393699169159,
      "learning_rate": 1.5273113723084722e-05,
      "loss": 0.018,
      "step": 7715
    },
    {
      "epoch": 1.881951219512195,
      "grad_norm": 0.19635072350502014,
      "learning_rate": 1.526723133932054e-05,
      "loss": 0.0139,
      "step": 7716
    },
    {
      "epoch": 1.8821951219512196,
      "grad_norm": 0.1824309527873993,
      "learning_rate": 1.5261349590589724e-05,
      "loss": 0.0191,
      "step": 7717
    },
    {
      "epoch": 1.8824390243902438,
      "grad_norm": 0.26812148094177246,
      "learning_rate": 1.5255468477276031e-05,
      "loss": 0.0253,
      "step": 7718
    },
    {
      "epoch": 1.8826829268292684,
      "grad_norm": 0.06187153235077858,
      "learning_rate": 1.5249587999763198e-05,
      "loss": 0.0151,
      "step": 7719
    },
    {
      "epoch": 1.8829268292682926,
      "grad_norm": 0.08645489066839218,
      "learning_rate": 1.5243708158434888e-05,
      "loss": 0.0087,
      "step": 7720
    },
    {
      "epoch": 1.8831707317073172,
      "grad_norm": 0.10596916824579239,
      "learning_rate": 1.5237828953674754e-05,
      "loss": 0.024,
      "step": 7721
    },
    {
      "epoch": 1.8834146341463414,
      "grad_norm": 0.10162275284528732,
      "learning_rate": 1.5231950385866404e-05,
      "loss": 0.0167,
      "step": 7722
    },
    {
      "epoch": 1.883658536585366,
      "grad_norm": 0.2716365158557892,
      "learning_rate": 1.5226072455393381e-05,
      "loss": 0.0177,
      "step": 7723
    },
    {
      "epoch": 1.8839024390243901,
      "grad_norm": 0.10468900203704834,
      "learning_rate": 1.5220195162639208e-05,
      "loss": 0.0128,
      "step": 7724
    },
    {
      "epoch": 1.8841463414634148,
      "grad_norm": 0.12870748341083527,
      "learning_rate": 1.5214318507987363e-05,
      "loss": 0.0167,
      "step": 7725
    },
    {
      "epoch": 1.884390243902439,
      "grad_norm": 0.22884249687194824,
      "learning_rate": 1.5208442491821285e-05,
      "loss": 0.0151,
      "step": 7726
    },
    {
      "epoch": 1.8846341463414635,
      "grad_norm": 0.2570129632949829,
      "learning_rate": 1.5202567114524352e-05,
      "loss": 0.024,
      "step": 7727
    },
    {
      "epoch": 1.8848780487804877,
      "grad_norm": 0.086156465113163,
      "learning_rate": 1.5196692376479924e-05,
      "loss": 0.012,
      "step": 7728
    },
    {
      "epoch": 1.8851219512195123,
      "grad_norm": 0.09859669208526611,
      "learning_rate": 1.5190818278071306e-05,
      "loss": 0.0222,
      "step": 7729
    },
    {
      "epoch": 1.8853658536585365,
      "grad_norm": 0.1310737133026123,
      "learning_rate": 1.5184944819681774e-05,
      "loss": 0.0204,
      "step": 7730
    },
    {
      "epoch": 1.885609756097561,
      "grad_norm": 0.13197201490402222,
      "learning_rate": 1.5179072001694539e-05,
      "loss": 0.0194,
      "step": 7731
    },
    {
      "epoch": 1.8858536585365853,
      "grad_norm": 0.15852287411689758,
      "learning_rate": 1.517319982449279e-05,
      "loss": 0.0224,
      "step": 7732
    },
    {
      "epoch": 1.8860975609756099,
      "grad_norm": 0.1955212950706482,
      "learning_rate": 1.516732828845967e-05,
      "loss": 0.0254,
      "step": 7733
    },
    {
      "epoch": 1.886341463414634,
      "grad_norm": 0.10334653407335281,
      "learning_rate": 1.5161457393978284e-05,
      "loss": 0.0149,
      "step": 7734
    },
    {
      "epoch": 1.8865853658536587,
      "grad_norm": 0.15767480432987213,
      "learning_rate": 1.5155587141431682e-05,
      "loss": 0.012,
      "step": 7735
    },
    {
      "epoch": 1.8868292682926828,
      "grad_norm": 0.25818705558776855,
      "learning_rate": 1.5149717531202889e-05,
      "loss": 0.0142,
      "step": 7736
    },
    {
      "epoch": 1.8870731707317074,
      "grad_norm": 0.09272520244121552,
      "learning_rate": 1.5143848563674865e-05,
      "loss": 0.0108,
      "step": 7737
    },
    {
      "epoch": 1.8873170731707316,
      "grad_norm": 0.13047416508197784,
      "learning_rate": 1.5137980239230548e-05,
      "loss": 0.0164,
      "step": 7738
    },
    {
      "epoch": 1.8875609756097562,
      "grad_norm": 0.2313338965177536,
      "learning_rate": 1.5132112558252837e-05,
      "loss": 0.0254,
      "step": 7739
    },
    {
      "epoch": 1.8878048780487804,
      "grad_norm": 0.1810239851474762,
      "learning_rate": 1.5126245521124565e-05,
      "loss": 0.0288,
      "step": 7740
    },
    {
      "epoch": 1.888048780487805,
      "grad_norm": 0.2141072005033493,
      "learning_rate": 1.5120379128228546e-05,
      "loss": 0.0232,
      "step": 7741
    },
    {
      "epoch": 1.8882926829268292,
      "grad_norm": 0.1282481700181961,
      "learning_rate": 1.5114513379947548e-05,
      "loss": 0.0253,
      "step": 7742
    },
    {
      "epoch": 1.8885365853658538,
      "grad_norm": 0.10543882846832275,
      "learning_rate": 1.5108648276664289e-05,
      "loss": 0.0197,
      "step": 7743
    },
    {
      "epoch": 1.888780487804878,
      "grad_norm": 0.21881063282489777,
      "learning_rate": 1.5102783818761451e-05,
      "loss": 0.032,
      "step": 7744
    },
    {
      "epoch": 1.8890243902439026,
      "grad_norm": 0.1535903960466385,
      "learning_rate": 1.5096920006621667e-05,
      "loss": 0.0278,
      "step": 7745
    },
    {
      "epoch": 1.8892682926829267,
      "grad_norm": 0.09723231941461563,
      "learning_rate": 1.5091056840627538e-05,
      "loss": 0.0099,
      "step": 7746
    },
    {
      "epoch": 1.8895121951219513,
      "grad_norm": 0.2582624554634094,
      "learning_rate": 1.508519432116162e-05,
      "loss": 0.0145,
      "step": 7747
    },
    {
      "epoch": 1.8897560975609755,
      "grad_norm": 0.22885888814926147,
      "learning_rate": 1.5079332448606415e-05,
      "loss": 0.0167,
      "step": 7748
    },
    {
      "epoch": 1.8900000000000001,
      "grad_norm": 0.13510827720165253,
      "learning_rate": 1.5073471223344399e-05,
      "loss": 0.0155,
      "step": 7749
    },
    {
      "epoch": 1.8902439024390243,
      "grad_norm": 0.12554973363876343,
      "learning_rate": 1.5067610645758002e-05,
      "loss": 0.0194,
      "step": 7750
    },
    {
      "epoch": 1.890487804878049,
      "grad_norm": 0.08452695608139038,
      "learning_rate": 1.5061750716229597e-05,
      "loss": 0.0141,
      "step": 7751
    },
    {
      "epoch": 1.890731707317073,
      "grad_norm": 0.10648408532142639,
      "learning_rate": 1.5055891435141545e-05,
      "loss": 0.0131,
      "step": 7752
    },
    {
      "epoch": 1.8909756097560977,
      "grad_norm": 0.18606914579868317,
      "learning_rate": 1.505003280287613e-05,
      "loss": 0.018,
      "step": 7753
    },
    {
      "epoch": 1.8912195121951219,
      "grad_norm": 0.10326358675956726,
      "learning_rate": 1.504417481981561e-05,
      "loss": 0.0153,
      "step": 7754
    },
    {
      "epoch": 1.8914634146341465,
      "grad_norm": 0.24080955982208252,
      "learning_rate": 1.5038317486342204e-05,
      "loss": 0.0292,
      "step": 7755
    },
    {
      "epoch": 1.8917073170731706,
      "grad_norm": 0.0916227251291275,
      "learning_rate": 1.5032460802838095e-05,
      "loss": 0.0187,
      "step": 7756
    },
    {
      "epoch": 1.8919512195121952,
      "grad_norm": 0.16995678842067719,
      "learning_rate": 1.5026604769685399e-05,
      "loss": 0.017,
      "step": 7757
    },
    {
      "epoch": 1.8921951219512194,
      "grad_norm": 0.1479305922985077,
      "learning_rate": 1.5020749387266209e-05,
      "loss": 0.0221,
      "step": 7758
    },
    {
      "epoch": 1.892439024390244,
      "grad_norm": 0.12897908687591553,
      "learning_rate": 1.5014894655962575e-05,
      "loss": 0.0286,
      "step": 7759
    },
    {
      "epoch": 1.8926829268292682,
      "grad_norm": 0.08572901040315628,
      "learning_rate": 1.5009040576156497e-05,
      "loss": 0.0197,
      "step": 7760
    },
    {
      "epoch": 1.8929268292682928,
      "grad_norm": 0.20703142881393433,
      "learning_rate": 1.5003187148229936e-05,
      "loss": 0.0324,
      "step": 7761
    },
    {
      "epoch": 1.893170731707317,
      "grad_norm": 0.06370311975479126,
      "learning_rate": 1.4997334372564808e-05,
      "loss": 0.0102,
      "step": 7762
    },
    {
      "epoch": 1.8934146341463416,
      "grad_norm": 0.12512612342834473,
      "learning_rate": 1.4991482249542994e-05,
      "loss": 0.0229,
      "step": 7763
    },
    {
      "epoch": 1.8936585365853658,
      "grad_norm": 0.20037515461444855,
      "learning_rate": 1.4985630779546325e-05,
      "loss": 0.0248,
      "step": 7764
    },
    {
      "epoch": 1.8939024390243904,
      "grad_norm": 0.16724835336208344,
      "learning_rate": 1.4979779962956588e-05,
      "loss": 0.0205,
      "step": 7765
    },
    {
      "epoch": 1.8941463414634145,
      "grad_norm": 0.16815820336341858,
      "learning_rate": 1.4973929800155533e-05,
      "loss": 0.0174,
      "step": 7766
    },
    {
      "epoch": 1.8943902439024392,
      "grad_norm": 0.19024622440338135,
      "learning_rate": 1.4968080291524871e-05,
      "loss": 0.0322,
      "step": 7767
    },
    {
      "epoch": 1.8946341463414633,
      "grad_norm": 0.17909406125545502,
      "learning_rate": 1.4962231437446256e-05,
      "loss": 0.0226,
      "step": 7768
    },
    {
      "epoch": 1.894878048780488,
      "grad_norm": 0.11539437621831894,
      "learning_rate": 1.4956383238301318e-05,
      "loss": 0.0254,
      "step": 7769
    },
    {
      "epoch": 1.895121951219512,
      "grad_norm": 0.27428969740867615,
      "learning_rate": 1.4950535694471622e-05,
      "loss": 0.0299,
      "step": 7770
    },
    {
      "epoch": 1.8953658536585367,
      "grad_norm": 0.23181068897247314,
      "learning_rate": 1.4944688806338703e-05,
      "loss": 0.0329,
      "step": 7771
    },
    {
      "epoch": 1.8956097560975609,
      "grad_norm": 0.1114240363240242,
      "learning_rate": 1.4938842574284057e-05,
      "loss": 0.0264,
      "step": 7772
    },
    {
      "epoch": 1.8958536585365855,
      "grad_norm": 0.11361809819936752,
      "learning_rate": 1.4932996998689141e-05,
      "loss": 0.0198,
      "step": 7773
    },
    {
      "epoch": 1.8960975609756097,
      "grad_norm": 0.15924488008022308,
      "learning_rate": 1.4927152079935347e-05,
      "loss": 0.0101,
      "step": 7774
    },
    {
      "epoch": 1.8963414634146343,
      "grad_norm": 0.16440142691135406,
      "learning_rate": 1.4921307818404043e-05,
      "loss": 0.0101,
      "step": 7775
    },
    {
      "epoch": 1.8965853658536584,
      "grad_norm": 0.1135040819644928,
      "learning_rate": 1.4915464214476548e-05,
      "loss": 0.0102,
      "step": 7776
    },
    {
      "epoch": 1.896829268292683,
      "grad_norm": 0.06413403153419495,
      "learning_rate": 1.4909621268534152e-05,
      "loss": 0.0147,
      "step": 7777
    },
    {
      "epoch": 1.8970731707317072,
      "grad_norm": 0.07525838166475296,
      "learning_rate": 1.4903778980958066e-05,
      "loss": 0.0129,
      "step": 7778
    },
    {
      "epoch": 1.8973170731707318,
      "grad_norm": 0.3600413203239441,
      "learning_rate": 1.4897937352129499e-05,
      "loss": 0.0451,
      "step": 7779
    },
    {
      "epoch": 1.897560975609756,
      "grad_norm": 0.12338519096374512,
      "learning_rate": 1.4892096382429599e-05,
      "loss": 0.0202,
      "step": 7780
    },
    {
      "epoch": 1.8978048780487806,
      "grad_norm": 0.17703872919082642,
      "learning_rate": 1.4886256072239462e-05,
      "loss": 0.0156,
      "step": 7781
    },
    {
      "epoch": 1.8980487804878048,
      "grad_norm": 0.11927293241024017,
      "learning_rate": 1.4880416421940155e-05,
      "loss": 0.0188,
      "step": 7782
    },
    {
      "epoch": 1.8982926829268294,
      "grad_norm": 0.12622471153736115,
      "learning_rate": 1.4874577431912695e-05,
      "loss": 0.0214,
      "step": 7783
    },
    {
      "epoch": 1.8985365853658536,
      "grad_norm": 0.12278860062360764,
      "learning_rate": 1.4868739102538069e-05,
      "loss": 0.0229,
      "step": 7784
    },
    {
      "epoch": 1.8987804878048782,
      "grad_norm": 0.09519131481647491,
      "learning_rate": 1.4862901434197202e-05,
      "loss": 0.0112,
      "step": 7785
    },
    {
      "epoch": 1.8990243902439023,
      "grad_norm": 0.06815088540315628,
      "learning_rate": 1.4857064427270978e-05,
      "loss": 0.0118,
      "step": 7786
    },
    {
      "epoch": 1.899268292682927,
      "grad_norm": 0.24586769938468933,
      "learning_rate": 1.485122808214025e-05,
      "loss": 0.0463,
      "step": 7787
    },
    {
      "epoch": 1.8995121951219511,
      "grad_norm": 0.16674649715423584,
      "learning_rate": 1.484539239918582e-05,
      "loss": 0.0306,
      "step": 7788
    },
    {
      "epoch": 1.8997560975609757,
      "grad_norm": 0.16637474298477173,
      "learning_rate": 1.4839557378788448e-05,
      "loss": 0.0281,
      "step": 7789
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.12662635743618011,
      "learning_rate": 1.483372302132886e-05,
      "loss": 0.0331,
      "step": 7790
    },
    {
      "epoch": 1.9002439024390245,
      "grad_norm": 0.1420418620109558,
      "learning_rate": 1.4827889327187718e-05,
      "loss": 0.0123,
      "step": 7791
    },
    {
      "epoch": 1.9004878048780487,
      "grad_norm": 0.1937485635280609,
      "learning_rate": 1.4822056296745656e-05,
      "loss": 0.0276,
      "step": 7792
    },
    {
      "epoch": 1.9007317073170733,
      "grad_norm": 0.058837663382291794,
      "learning_rate": 1.4816223930383271e-05,
      "loss": 0.01,
      "step": 7793
    },
    {
      "epoch": 1.9009756097560975,
      "grad_norm": 0.07669579237699509,
      "learning_rate": 1.4810392228481093e-05,
      "loss": 0.0185,
      "step": 7794
    },
    {
      "epoch": 1.901219512195122,
      "grad_norm": 0.08128625154495239,
      "learning_rate": 1.4804561191419628e-05,
      "loss": 0.0121,
      "step": 7795
    },
    {
      "epoch": 1.9014634146341463,
      "grad_norm": 0.1021445095539093,
      "learning_rate": 1.4798730819579332e-05,
      "loss": 0.0193,
      "step": 7796
    },
    {
      "epoch": 1.9017073170731709,
      "grad_norm": 0.13876736164093018,
      "learning_rate": 1.479290111334063e-05,
      "loss": 0.0241,
      "step": 7797
    },
    {
      "epoch": 1.901951219512195,
      "grad_norm": 0.12449467182159424,
      "learning_rate": 1.4787072073083874e-05,
      "loss": 0.0107,
      "step": 7798
    },
    {
      "epoch": 1.9021951219512196,
      "grad_norm": 0.24701137840747833,
      "learning_rate": 1.4781243699189404e-05,
      "loss": 0.0254,
      "step": 7799
    },
    {
      "epoch": 1.9024390243902438,
      "grad_norm": 0.13595719635486603,
      "learning_rate": 1.47754159920375e-05,
      "loss": 0.023,
      "step": 7800
    },
    {
      "epoch": 1.9026829268292684,
      "grad_norm": 0.1598983108997345,
      "learning_rate": 1.4769588952008404e-05,
      "loss": 0.0168,
      "step": 7801
    },
    {
      "epoch": 1.9029268292682926,
      "grad_norm": 0.1761922389268875,
      "learning_rate": 1.4763762579482312e-05,
      "loss": 0.026,
      "step": 7802
    },
    {
      "epoch": 1.9031707317073172,
      "grad_norm": 0.09917707741260529,
      "learning_rate": 1.4757936874839375e-05,
      "loss": 0.0228,
      "step": 7803
    },
    {
      "epoch": 1.9034146341463414,
      "grad_norm": 0.09948187321424484,
      "learning_rate": 1.4752111838459697e-05,
      "loss": 0.0117,
      "step": 7804
    },
    {
      "epoch": 1.903658536585366,
      "grad_norm": 0.08241346478462219,
      "learning_rate": 1.4746287470723355e-05,
      "loss": 0.0158,
      "step": 7805
    },
    {
      "epoch": 1.9039024390243902,
      "grad_norm": 0.13393835723400116,
      "learning_rate": 1.4740463772010366e-05,
      "loss": 0.0125,
      "step": 7806
    },
    {
      "epoch": 1.9041463414634148,
      "grad_norm": 0.14386555552482605,
      "learning_rate": 1.4734640742700706e-05,
      "loss": 0.0294,
      "step": 7807
    },
    {
      "epoch": 1.904390243902439,
      "grad_norm": 0.28777578473091125,
      "learning_rate": 1.4728818383174312e-05,
      "loss": 0.0287,
      "step": 7808
    },
    {
      "epoch": 1.9046341463414636,
      "grad_norm": 0.1040174588561058,
      "learning_rate": 1.4722996693811076e-05,
      "loss": 0.0183,
      "step": 7809
    },
    {
      "epoch": 1.9048780487804877,
      "grad_norm": 0.1408773809671402,
      "learning_rate": 1.4717175674990846e-05,
      "loss": 0.0186,
      "step": 7810
    },
    {
      "epoch": 1.9051219512195123,
      "grad_norm": 0.2510327994823456,
      "learning_rate": 1.4711355327093424e-05,
      "loss": 0.01,
      "step": 7811
    },
    {
      "epoch": 1.9053658536585365,
      "grad_norm": 0.09391503781080246,
      "learning_rate": 1.4705535650498565e-05,
      "loss": 0.0152,
      "step": 7812
    },
    {
      "epoch": 1.9056097560975611,
      "grad_norm": 0.14110349118709564,
      "learning_rate": 1.4699716645585993e-05,
      "loss": 0.0277,
      "step": 7813
    },
    {
      "epoch": 1.9058536585365853,
      "grad_norm": 0.15383487939834595,
      "learning_rate": 1.4693898312735385e-05,
      "loss": 0.0156,
      "step": 7814
    },
    {
      "epoch": 1.90609756097561,
      "grad_norm": 0.20528282225131989,
      "learning_rate": 1.4688080652326356e-05,
      "loss": 0.0172,
      "step": 7815
    },
    {
      "epoch": 1.906341463414634,
      "grad_norm": 0.09596534073352814,
      "learning_rate": 1.4682263664738494e-05,
      "loss": 0.0155,
      "step": 7816
    },
    {
      "epoch": 1.9065853658536587,
      "grad_norm": 0.32203608751296997,
      "learning_rate": 1.4676447350351347e-05,
      "loss": 0.0352,
      "step": 7817
    },
    {
      "epoch": 1.9068292682926828,
      "grad_norm": 0.09834697842597961,
      "learning_rate": 1.4670631709544414e-05,
      "loss": 0.025,
      "step": 7818
    },
    {
      "epoch": 1.9070731707317075,
      "grad_norm": 0.12907420098781586,
      "learning_rate": 1.4664816742697129e-05,
      "loss": 0.0194,
      "step": 7819
    },
    {
      "epoch": 1.9073170731707316,
      "grad_norm": 0.1380680650472641,
      "learning_rate": 1.465900245018892e-05,
      "loss": 0.0247,
      "step": 7820
    },
    {
      "epoch": 1.9075609756097562,
      "grad_norm": 0.07459209114313126,
      "learning_rate": 1.465318883239914e-05,
      "loss": 0.0094,
      "step": 7821
    },
    {
      "epoch": 1.9078048780487804,
      "grad_norm": 0.15441164374351501,
      "learning_rate": 1.464737588970711e-05,
      "loss": 0.0276,
      "step": 7822
    },
    {
      "epoch": 1.908048780487805,
      "grad_norm": 0.11485657840967178,
      "learning_rate": 1.4641563622492122e-05,
      "loss": 0.0173,
      "step": 7823
    },
    {
      "epoch": 1.9082926829268292,
      "grad_norm": 0.09132023900747299,
      "learning_rate": 1.4635752031133387e-05,
      "loss": 0.0189,
      "step": 7824
    },
    {
      "epoch": 1.9085365853658538,
      "grad_norm": 0.12935885787010193,
      "learning_rate": 1.4629941116010104e-05,
      "loss": 0.0176,
      "step": 7825
    },
    {
      "epoch": 1.908780487804878,
      "grad_norm": 0.3714942932128906,
      "learning_rate": 1.4624130877501422e-05,
      "loss": 0.0328,
      "step": 7826
    },
    {
      "epoch": 1.9090243902439026,
      "grad_norm": 0.07938776910305023,
      "learning_rate": 1.4618321315986438e-05,
      "loss": 0.0104,
      "step": 7827
    },
    {
      "epoch": 1.9092682926829267,
      "grad_norm": 0.1119636744260788,
      "learning_rate": 1.4612512431844202e-05,
      "loss": 0.0236,
      "step": 7828
    },
    {
      "epoch": 1.9095121951219514,
      "grad_norm": 0.147664412856102,
      "learning_rate": 1.4606704225453732e-05,
      "loss": 0.0139,
      "step": 7829
    },
    {
      "epoch": 1.9097560975609755,
      "grad_norm": 0.13452604413032532,
      "learning_rate": 1.4600896697193995e-05,
      "loss": 0.0291,
      "step": 7830
    },
    {
      "epoch": 1.9100000000000001,
      "grad_norm": 0.1729833036661148,
      "learning_rate": 1.4595089847443911e-05,
      "loss": 0.0236,
      "step": 7831
    },
    {
      "epoch": 1.9102439024390243,
      "grad_norm": 0.08583223074674606,
      "learning_rate": 1.4589283676582369e-05,
      "loss": 0.0142,
      "step": 7832
    },
    {
      "epoch": 1.910487804878049,
      "grad_norm": 0.10958400368690491,
      "learning_rate": 1.4583478184988198e-05,
      "loss": 0.0149,
      "step": 7833
    },
    {
      "epoch": 1.910731707317073,
      "grad_norm": 0.11610575020313263,
      "learning_rate": 1.4577673373040173e-05,
      "loss": 0.0105,
      "step": 7834
    },
    {
      "epoch": 1.9109756097560977,
      "grad_norm": 0.10793296992778778,
      "learning_rate": 1.4571869241117078e-05,
      "loss": 0.0249,
      "step": 7835
    },
    {
      "epoch": 1.9112195121951219,
      "grad_norm": 0.12660355865955353,
      "learning_rate": 1.4566065789597571e-05,
      "loss": 0.0095,
      "step": 7836
    },
    {
      "epoch": 1.9114634146341465,
      "grad_norm": 0.10545254498720169,
      "learning_rate": 1.456026301886034e-05,
      "loss": 0.0293,
      "step": 7837
    },
    {
      "epoch": 1.9117073170731707,
      "grad_norm": 0.16140349209308624,
      "learning_rate": 1.4554460929283987e-05,
      "loss": 0.0161,
      "step": 7838
    },
    {
      "epoch": 1.9119512195121953,
      "grad_norm": 0.10551083832979202,
      "learning_rate": 1.454865952124707e-05,
      "loss": 0.0152,
      "step": 7839
    },
    {
      "epoch": 1.9121951219512194,
      "grad_norm": 0.16015008091926575,
      "learning_rate": 1.454285879512814e-05,
      "loss": 0.0203,
      "step": 7840
    },
    {
      "epoch": 1.912439024390244,
      "grad_norm": 0.09634798765182495,
      "learning_rate": 1.453705875130565e-05,
      "loss": 0.0153,
      "step": 7841
    },
    {
      "epoch": 1.9126829268292682,
      "grad_norm": 0.133613720536232,
      "learning_rate": 1.4531259390158053e-05,
      "loss": 0.0155,
      "step": 7842
    },
    {
      "epoch": 1.9129268292682928,
      "grad_norm": 0.10721319168806076,
      "learning_rate": 1.4525460712063731e-05,
      "loss": 0.0164,
      "step": 7843
    },
    {
      "epoch": 1.913170731707317,
      "grad_norm": 0.29779064655303955,
      "learning_rate": 1.4519662717401028e-05,
      "loss": 0.0332,
      "step": 7844
    },
    {
      "epoch": 1.9134146341463416,
      "grad_norm": 0.2699929177761078,
      "learning_rate": 1.4513865406548255e-05,
      "loss": 0.0214,
      "step": 7845
    },
    {
      "epoch": 1.9136585365853658,
      "grad_norm": 0.13933050632476807,
      "learning_rate": 1.4508068779883665e-05,
      "loss": 0.0139,
      "step": 7846
    },
    {
      "epoch": 1.9139024390243904,
      "grad_norm": 0.13362403213977814,
      "learning_rate": 1.4502272837785458e-05,
      "loss": 0.0206,
      "step": 7847
    },
    {
      "epoch": 1.9141463414634146,
      "grad_norm": 0.1259174644947052,
      "learning_rate": 1.449647758063182e-05,
      "loss": 0.0152,
      "step": 7848
    },
    {
      "epoch": 1.9143902439024392,
      "grad_norm": 0.13691268861293793,
      "learning_rate": 1.4490683008800859e-05,
      "loss": 0.0142,
      "step": 7849
    },
    {
      "epoch": 1.9146341463414633,
      "grad_norm": 0.1130133792757988,
      "learning_rate": 1.4484889122670665e-05,
      "loss": 0.0219,
      "step": 7850
    },
    {
      "epoch": 1.914878048780488,
      "grad_norm": 0.0594097264111042,
      "learning_rate": 1.4479095922619279e-05,
      "loss": 0.0085,
      "step": 7851
    },
    {
      "epoch": 1.9151219512195121,
      "grad_norm": 0.15992499887943268,
      "learning_rate": 1.4473303409024657e-05,
      "loss": 0.0194,
      "step": 7852
    },
    {
      "epoch": 1.9153658536585367,
      "grad_norm": 0.11225571483373642,
      "learning_rate": 1.446751158226477e-05,
      "loss": 0.0269,
      "step": 7853
    },
    {
      "epoch": 1.915609756097561,
      "grad_norm": 0.12120702117681503,
      "learning_rate": 1.4461720442717503e-05,
      "loss": 0.0289,
      "step": 7854
    },
    {
      "epoch": 1.9158536585365855,
      "grad_norm": 0.061132341623306274,
      "learning_rate": 1.4455929990760723e-05,
      "loss": 0.0119,
      "step": 7855
    },
    {
      "epoch": 1.9160975609756097,
      "grad_norm": 0.13323041796684265,
      "learning_rate": 1.4450140226772233e-05,
      "loss": 0.0396,
      "step": 7856
    },
    {
      "epoch": 1.9163414634146343,
      "grad_norm": 0.10500813275575638,
      "learning_rate": 1.4444351151129786e-05,
      "loss": 0.0273,
      "step": 7857
    },
    {
      "epoch": 1.9165853658536585,
      "grad_norm": 0.06008587032556534,
      "learning_rate": 1.4438562764211121e-05,
      "loss": 0.0146,
      "step": 7858
    },
    {
      "epoch": 1.916829268292683,
      "grad_norm": 0.16525688767433167,
      "learning_rate": 1.443277506639391e-05,
      "loss": 0.0314,
      "step": 7859
    },
    {
      "epoch": 1.9170731707317072,
      "grad_norm": 0.09765542298555374,
      "learning_rate": 1.4426988058055763e-05,
      "loss": 0.0148,
      "step": 7860
    },
    {
      "epoch": 1.9173170731707319,
      "grad_norm": 0.20160503685474396,
      "learning_rate": 1.4421201739574286e-05,
      "loss": 0.0167,
      "step": 7861
    },
    {
      "epoch": 1.917560975609756,
      "grad_norm": 0.09455633163452148,
      "learning_rate": 1.4415416111327007e-05,
      "loss": 0.0076,
      "step": 7862
    },
    {
      "epoch": 1.9178048780487806,
      "grad_norm": 0.06852567940950394,
      "learning_rate": 1.4409631173691429e-05,
      "loss": 0.0156,
      "step": 7863
    },
    {
      "epoch": 1.9180487804878048,
      "grad_norm": 0.2842481732368469,
      "learning_rate": 1.4403846927044997e-05,
      "loss": 0.0238,
      "step": 7864
    },
    {
      "epoch": 1.9182926829268294,
      "grad_norm": 0.06134156882762909,
      "learning_rate": 1.4398063371765108e-05,
      "loss": 0.0082,
      "step": 7865
    },
    {
      "epoch": 1.9185365853658536,
      "grad_norm": 0.11957427114248276,
      "learning_rate": 1.4392280508229133e-05,
      "loss": 0.0152,
      "step": 7866
    },
    {
      "epoch": 1.9187804878048782,
      "grad_norm": 0.19195882976055145,
      "learning_rate": 1.4386498336814386e-05,
      "loss": 0.033,
      "step": 7867
    },
    {
      "epoch": 1.9190243902439024,
      "grad_norm": 0.13040320575237274,
      "learning_rate": 1.438071685789813e-05,
      "loss": 0.0165,
      "step": 7868
    },
    {
      "epoch": 1.919268292682927,
      "grad_norm": 0.07660453766584396,
      "learning_rate": 1.4374936071857592e-05,
      "loss": 0.0056,
      "step": 7869
    },
    {
      "epoch": 1.9195121951219511,
      "grad_norm": 0.2873619496822357,
      "learning_rate": 1.4369155979069937e-05,
      "loss": 0.0125,
      "step": 7870
    },
    {
      "epoch": 1.9197560975609758,
      "grad_norm": 0.13009437918663025,
      "learning_rate": 1.4363376579912318e-05,
      "loss": 0.0313,
      "step": 7871
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.06452144682407379,
      "learning_rate": 1.4357597874761819e-05,
      "loss": 0.0104,
      "step": 7872
    },
    {
      "epoch": 1.9202439024390245,
      "grad_norm": 0.13656340539455414,
      "learning_rate": 1.4351819863995469e-05,
      "loss": 0.0241,
      "step": 7873
    },
    {
      "epoch": 1.9204878048780487,
      "grad_norm": 0.12077721953392029,
      "learning_rate": 1.4346042547990286e-05,
      "loss": 0.0137,
      "step": 7874
    },
    {
      "epoch": 1.9207317073170733,
      "grad_norm": 0.2078728824853897,
      "learning_rate": 1.43402659271232e-05,
      "loss": 0.0361,
      "step": 7875
    },
    {
      "epoch": 1.9209756097560975,
      "grad_norm": 0.07486832141876221,
      "learning_rate": 1.433449000177114e-05,
      "loss": 0.0116,
      "step": 7876
    },
    {
      "epoch": 1.921219512195122,
      "grad_norm": 0.10625021159648895,
      "learning_rate": 1.4328714772310959e-05,
      "loss": 0.0131,
      "step": 7877
    },
    {
      "epoch": 1.9214634146341463,
      "grad_norm": 0.11339661478996277,
      "learning_rate": 1.4322940239119458e-05,
      "loss": 0.0126,
      "step": 7878
    },
    {
      "epoch": 1.9217073170731709,
      "grad_norm": 0.16782192885875702,
      "learning_rate": 1.4317166402573434e-05,
      "loss": 0.0163,
      "step": 7879
    },
    {
      "epoch": 1.921951219512195,
      "grad_norm": 0.103792205452919,
      "learning_rate": 1.4311393263049596e-05,
      "loss": 0.0213,
      "step": 7880
    },
    {
      "epoch": 1.9221951219512197,
      "grad_norm": 0.1806844025850296,
      "learning_rate": 1.430562082092462e-05,
      "loss": 0.0172,
      "step": 7881
    },
    {
      "epoch": 1.9224390243902438,
      "grad_norm": 0.13402865827083588,
      "learning_rate": 1.4299849076575154e-05,
      "loss": 0.0172,
      "step": 7882
    },
    {
      "epoch": 1.9226829268292684,
      "grad_norm": 0.12890608608722687,
      "learning_rate": 1.429407803037778e-05,
      "loss": 0.019,
      "step": 7883
    },
    {
      "epoch": 1.9229268292682926,
      "grad_norm": 0.11409222334623337,
      "learning_rate": 1.4288307682709041e-05,
      "loss": 0.0241,
      "step": 7884
    },
    {
      "epoch": 1.9231707317073172,
      "grad_norm": 0.1673360913991928,
      "learning_rate": 1.4282538033945433e-05,
      "loss": 0.0387,
      "step": 7885
    },
    {
      "epoch": 1.9234146341463414,
      "grad_norm": 0.23378437757492065,
      "learning_rate": 1.4276769084463399e-05,
      "loss": 0.0227,
      "step": 7886
    },
    {
      "epoch": 1.923658536585366,
      "grad_norm": 0.1268574446439743,
      "learning_rate": 1.4271000834639367e-05,
      "loss": 0.0169,
      "step": 7887
    },
    {
      "epoch": 1.9239024390243902,
      "grad_norm": 0.20114202797412872,
      "learning_rate": 1.4265233284849678e-05,
      "loss": 0.0233,
      "step": 7888
    },
    {
      "epoch": 1.9241463414634148,
      "grad_norm": 0.21424837410449982,
      "learning_rate": 1.4259466435470658e-05,
      "loss": 0.0199,
      "step": 7889
    },
    {
      "epoch": 1.924390243902439,
      "grad_norm": 0.15415619313716888,
      "learning_rate": 1.4253700286878579e-05,
      "loss": 0.0194,
      "step": 7890
    },
    {
      "epoch": 1.9246341463414636,
      "grad_norm": 0.11245692521333694,
      "learning_rate": 1.4247934839449645e-05,
      "loss": 0.0149,
      "step": 7891
    },
    {
      "epoch": 1.9248780487804877,
      "grad_norm": 0.07572298496961594,
      "learning_rate": 1.424217009356006e-05,
      "loss": 0.015,
      "step": 7892
    },
    {
      "epoch": 1.9251219512195124,
      "grad_norm": 0.08891977369785309,
      "learning_rate": 1.4236406049585944e-05,
      "loss": 0.0198,
      "step": 7893
    },
    {
      "epoch": 1.9253658536585365,
      "grad_norm": 0.07784730195999146,
      "learning_rate": 1.4230642707903371e-05,
      "loss": 0.0128,
      "step": 7894
    },
    {
      "epoch": 1.9256097560975611,
      "grad_norm": 0.10316893458366394,
      "learning_rate": 1.4224880068888402e-05,
      "loss": 0.0173,
      "step": 7895
    },
    {
      "epoch": 1.9258536585365853,
      "grad_norm": 0.1254405379295349,
      "learning_rate": 1.4219118132917025e-05,
      "loss": 0.0193,
      "step": 7896
    },
    {
      "epoch": 1.92609756097561,
      "grad_norm": 0.16591677069664001,
      "learning_rate": 1.4213356900365177e-05,
      "loss": 0.0222,
      "step": 7897
    },
    {
      "epoch": 1.926341463414634,
      "grad_norm": 0.2466028332710266,
      "learning_rate": 1.4207596371608778e-05,
      "loss": 0.0182,
      "step": 7898
    },
    {
      "epoch": 1.9265853658536587,
      "grad_norm": 0.12036361545324326,
      "learning_rate": 1.4201836547023668e-05,
      "loss": 0.0235,
      "step": 7899
    },
    {
      "epoch": 1.9268292682926829,
      "grad_norm": 0.1712113469839096,
      "learning_rate": 1.4196077426985677e-05,
      "loss": 0.0241,
      "step": 7900
    },
    {
      "epoch": 1.9270731707317075,
      "grad_norm": 0.08503235876560211,
      "learning_rate": 1.4190319011870563e-05,
      "loss": 0.0122,
      "step": 7901
    },
    {
      "epoch": 1.9273170731707316,
      "grad_norm": 0.145196795463562,
      "learning_rate": 1.4184561302054036e-05,
      "loss": 0.0261,
      "step": 7902
    },
    {
      "epoch": 1.9275609756097563,
      "grad_norm": 0.10545085370540619,
      "learning_rate": 1.417880429791178e-05,
      "loss": 0.0182,
      "step": 7903
    },
    {
      "epoch": 1.9278048780487804,
      "grad_norm": 0.14195549488067627,
      "learning_rate": 1.4173047999819403e-05,
      "loss": 0.0194,
      "step": 7904
    },
    {
      "epoch": 1.928048780487805,
      "grad_norm": 0.2149410843849182,
      "learning_rate": 1.4167292408152511e-05,
      "loss": 0.02,
      "step": 7905
    },
    {
      "epoch": 1.9282926829268292,
      "grad_norm": 0.1371883749961853,
      "learning_rate": 1.4161537523286627e-05,
      "loss": 0.0185,
      "step": 7906
    },
    {
      "epoch": 1.9285365853658538,
      "grad_norm": 0.1046161875128746,
      "learning_rate": 1.415578334559723e-05,
      "loss": 0.0213,
      "step": 7907
    },
    {
      "epoch": 1.928780487804878,
      "grad_norm": 0.0975603237748146,
      "learning_rate": 1.415002987545978e-05,
      "loss": 0.0119,
      "step": 7908
    },
    {
      "epoch": 1.9290243902439026,
      "grad_norm": 0.13508743047714233,
      "learning_rate": 1.414427711324967e-05,
      "loss": 0.0174,
      "step": 7909
    },
    {
      "epoch": 1.9292682926829268,
      "grad_norm": 0.11905644834041595,
      "learning_rate": 1.4138525059342234e-05,
      "loss": 0.0201,
      "step": 7910
    },
    {
      "epoch": 1.9295121951219514,
      "grad_norm": 0.15577678382396698,
      "learning_rate": 1.4132773714112796e-05,
      "loss": 0.0283,
      "step": 7911
    },
    {
      "epoch": 1.9297560975609755,
      "grad_norm": 0.0818672627210617,
      "learning_rate": 1.4127023077936596e-05,
      "loss": 0.0174,
      "step": 7912
    },
    {
      "epoch": 1.9300000000000002,
      "grad_norm": 0.07617037743330002,
      "learning_rate": 1.4121273151188868e-05,
      "loss": 0.0141,
      "step": 7913
    },
    {
      "epoch": 1.9302439024390243,
      "grad_norm": 0.1747385561466217,
      "learning_rate": 1.4115523934244762e-05,
      "loss": 0.02,
      "step": 7914
    },
    {
      "epoch": 1.930487804878049,
      "grad_norm": 0.23599892854690552,
      "learning_rate": 1.4109775427479388e-05,
      "loss": 0.0211,
      "step": 7915
    },
    {
      "epoch": 1.930731707317073,
      "grad_norm": 0.08813726156949997,
      "learning_rate": 1.4104027631267838e-05,
      "loss": 0.0184,
      "step": 7916
    },
    {
      "epoch": 1.9309756097560977,
      "grad_norm": 0.14469067752361298,
      "learning_rate": 1.4098280545985133e-05,
      "loss": 0.03,
      "step": 7917
    },
    {
      "epoch": 1.931219512195122,
      "grad_norm": 0.13735325634479523,
      "learning_rate": 1.4092534172006244e-05,
      "loss": 0.0147,
      "step": 7918
    },
    {
      "epoch": 1.9314634146341465,
      "grad_norm": 0.18644662201404572,
      "learning_rate": 1.4086788509706112e-05,
      "loss": 0.0118,
      "step": 7919
    },
    {
      "epoch": 1.9317073170731707,
      "grad_norm": 0.12149842083454132,
      "learning_rate": 1.408104355945961e-05,
      "loss": 0.0269,
      "step": 7920
    },
    {
      "epoch": 1.9319512195121953,
      "grad_norm": 0.08518368005752563,
      "learning_rate": 1.4075299321641605e-05,
      "loss": 0.0141,
      "step": 7921
    },
    {
      "epoch": 1.9321951219512195,
      "grad_norm": 0.11326923221349716,
      "learning_rate": 1.406955579662686e-05,
      "loss": 0.0192,
      "step": 7922
    },
    {
      "epoch": 1.932439024390244,
      "grad_norm": 0.08083216100931168,
      "learning_rate": 1.4063812984790147e-05,
      "loss": 0.0175,
      "step": 7923
    },
    {
      "epoch": 1.9326829268292682,
      "grad_norm": 0.11177631467580795,
      "learning_rate": 1.4058070886506165e-05,
      "loss": 0.0253,
      "step": 7924
    },
    {
      "epoch": 1.9329268292682928,
      "grad_norm": 0.11383102089166641,
      "learning_rate": 1.4052329502149544e-05,
      "loss": 0.012,
      "step": 7925
    },
    {
      "epoch": 1.933170731707317,
      "grad_norm": 0.4119531512260437,
      "learning_rate": 1.404658883209492e-05,
      "loss": 0.0259,
      "step": 7926
    },
    {
      "epoch": 1.9334146341463416,
      "grad_norm": 0.10392463207244873,
      "learning_rate": 1.4040848876716845e-05,
      "loss": 0.0144,
      "step": 7927
    },
    {
      "epoch": 1.9336585365853658,
      "grad_norm": 0.20387063920497894,
      "learning_rate": 1.4035109636389821e-05,
      "loss": 0.0306,
      "step": 7928
    },
    {
      "epoch": 1.9339024390243904,
      "grad_norm": 0.08817196637392044,
      "learning_rate": 1.4029371111488337e-05,
      "loss": 0.0159,
      "step": 7929
    },
    {
      "epoch": 1.9341463414634146,
      "grad_norm": 0.07611971348524094,
      "learning_rate": 1.4023633302386802e-05,
      "loss": 0.0158,
      "step": 7930
    },
    {
      "epoch": 1.9343902439024392,
      "grad_norm": 0.1685517132282257,
      "learning_rate": 1.4017896209459585e-05,
      "loss": 0.0265,
      "step": 7931
    },
    {
      "epoch": 1.9346341463414634,
      "grad_norm": 0.15611447393894196,
      "learning_rate": 1.401215983308103e-05,
      "loss": 0.0229,
      "step": 7932
    },
    {
      "epoch": 1.934878048780488,
      "grad_norm": 0.1265852451324463,
      "learning_rate": 1.4006424173625399e-05,
      "loss": 0.0248,
      "step": 7933
    },
    {
      "epoch": 1.9351219512195121,
      "grad_norm": 0.12293513864278793,
      "learning_rate": 1.4000689231466955e-05,
      "loss": 0.0198,
      "step": 7934
    },
    {
      "epoch": 1.9353658536585368,
      "grad_norm": 0.12412384152412415,
      "learning_rate": 1.3994955006979845e-05,
      "loss": 0.0215,
      "step": 7935
    },
    {
      "epoch": 1.935609756097561,
      "grad_norm": 0.17981547117233276,
      "learning_rate": 1.3989221500538244e-05,
      "loss": 0.0243,
      "step": 7936
    },
    {
      "epoch": 1.9358536585365855,
      "grad_norm": 0.14854972064495087,
      "learning_rate": 1.3983488712516229e-05,
      "loss": 0.0118,
      "step": 7937
    },
    {
      "epoch": 1.9360975609756097,
      "grad_norm": 0.1690671741962433,
      "learning_rate": 1.3977756643287845e-05,
      "loss": 0.0193,
      "step": 7938
    },
    {
      "epoch": 1.9363414634146343,
      "grad_norm": 0.18848514556884766,
      "learning_rate": 1.3972025293227107e-05,
      "loss": 0.0158,
      "step": 7939
    },
    {
      "epoch": 1.9365853658536585,
      "grad_norm": 0.17260928452014923,
      "learning_rate": 1.3966294662707957e-05,
      "loss": 0.0141,
      "step": 7940
    },
    {
      "epoch": 1.936829268292683,
      "grad_norm": 0.13632555305957794,
      "learning_rate": 1.3960564752104294e-05,
      "loss": 0.0154,
      "step": 7941
    },
    {
      "epoch": 1.9370731707317073,
      "grad_norm": 0.1683947890996933,
      "learning_rate": 1.3954835561789997e-05,
      "loss": 0.0196,
      "step": 7942
    },
    {
      "epoch": 1.9373170731707319,
      "grad_norm": 0.19703148305416107,
      "learning_rate": 1.3949107092138864e-05,
      "loss": 0.0351,
      "step": 7943
    },
    {
      "epoch": 1.937560975609756,
      "grad_norm": 0.1122429147362709,
      "learning_rate": 1.3943379343524658e-05,
      "loss": 0.0193,
      "step": 7944
    },
    {
      "epoch": 1.9378048780487804,
      "grad_norm": 0.1296531856060028,
      "learning_rate": 1.3937652316321109e-05,
      "loss": 0.0306,
      "step": 7945
    },
    {
      "epoch": 1.9380487804878048,
      "grad_norm": 0.1621851921081543,
      "learning_rate": 1.3931926010901869e-05,
      "loss": 0.0245,
      "step": 7946
    },
    {
      "epoch": 1.9382926829268292,
      "grad_norm": 0.11670716851949692,
      "learning_rate": 1.392620042764059e-05,
      "loss": 0.0139,
      "step": 7947
    },
    {
      "epoch": 1.9385365853658536,
      "grad_norm": 0.14194925129413605,
      "learning_rate": 1.3920475566910829e-05,
      "loss": 0.0117,
      "step": 7948
    },
    {
      "epoch": 1.938780487804878,
      "grad_norm": 0.12110503017902374,
      "learning_rate": 1.3914751429086109e-05,
      "loss": 0.0171,
      "step": 7949
    },
    {
      "epoch": 1.9390243902439024,
      "grad_norm": 0.07623130083084106,
      "learning_rate": 1.3909028014539944e-05,
      "loss": 0.0143,
      "step": 7950
    },
    {
      "epoch": 1.9392682926829268,
      "grad_norm": 0.08110134303569794,
      "learning_rate": 1.3903305323645729e-05,
      "loss": 0.0175,
      "step": 7951
    },
    {
      "epoch": 1.9395121951219512,
      "grad_norm": 0.16852112114429474,
      "learning_rate": 1.3897583356776883e-05,
      "loss": 0.0149,
      "step": 7952
    },
    {
      "epoch": 1.9397560975609756,
      "grad_norm": 0.15634049475193024,
      "learning_rate": 1.3891862114306731e-05,
      "loss": 0.0134,
      "step": 7953
    },
    {
      "epoch": 1.94,
      "grad_norm": 0.07335177809000015,
      "learning_rate": 1.3886141596608566e-05,
      "loss": 0.015,
      "step": 7954
    },
    {
      "epoch": 1.9402439024390243,
      "grad_norm": 0.065250463783741,
      "learning_rate": 1.3880421804055643e-05,
      "loss": 0.0076,
      "step": 7955
    },
    {
      "epoch": 1.9404878048780487,
      "grad_norm": 0.17864398658275604,
      "learning_rate": 1.3874702737021162e-05,
      "loss": 0.0199,
      "step": 7956
    },
    {
      "epoch": 1.9407317073170731,
      "grad_norm": 0.160111665725708,
      "learning_rate": 1.3868984395878256e-05,
      "loss": 0.0313,
      "step": 7957
    },
    {
      "epoch": 1.9409756097560975,
      "grad_norm": 0.12141354382038116,
      "learning_rate": 1.3863266781000056e-05,
      "loss": 0.0114,
      "step": 7958
    },
    {
      "epoch": 1.941219512195122,
      "grad_norm": 0.11061709374189377,
      "learning_rate": 1.3857549892759592e-05,
      "loss": 0.0239,
      "step": 7959
    },
    {
      "epoch": 1.9414634146341463,
      "grad_norm": 0.09912879765033722,
      "learning_rate": 1.38518337315299e-05,
      "loss": 0.0138,
      "step": 7960
    },
    {
      "epoch": 1.9417073170731707,
      "grad_norm": 0.09225386381149292,
      "learning_rate": 1.384611829768393e-05,
      "loss": 0.016,
      "step": 7961
    },
    {
      "epoch": 1.941951219512195,
      "grad_norm": 0.09698896110057831,
      "learning_rate": 1.3840403591594584e-05,
      "loss": 0.0101,
      "step": 7962
    },
    {
      "epoch": 1.9421951219512195,
      "grad_norm": 0.10563526302576065,
      "learning_rate": 1.383468961363475e-05,
      "loss": 0.0084,
      "step": 7963
    },
    {
      "epoch": 1.9424390243902439,
      "grad_norm": 0.09766524285078049,
      "learning_rate": 1.382897636417724e-05,
      "loss": 0.0154,
      "step": 7964
    },
    {
      "epoch": 1.9426829268292682,
      "grad_norm": 0.16176524758338928,
      "learning_rate": 1.3823263843594814e-05,
      "loss": 0.0268,
      "step": 7965
    },
    {
      "epoch": 1.9429268292682926,
      "grad_norm": 0.38764074444770813,
      "learning_rate": 1.3817552052260218e-05,
      "loss": 0.0243,
      "step": 7966
    },
    {
      "epoch": 1.943170731707317,
      "grad_norm": 0.11637818813323975,
      "learning_rate": 1.3811840990546115e-05,
      "loss": 0.0142,
      "step": 7967
    },
    {
      "epoch": 1.9434146341463414,
      "grad_norm": 0.13817206025123596,
      "learning_rate": 1.3806130658825139e-05,
      "loss": 0.0223,
      "step": 7968
    },
    {
      "epoch": 1.9436585365853658,
      "grad_norm": 0.1260567307472229,
      "learning_rate": 1.3800421057469871e-05,
      "loss": 0.0198,
      "step": 7969
    },
    {
      "epoch": 1.9439024390243902,
      "grad_norm": 0.12252877652645111,
      "learning_rate": 1.3794712186852832e-05,
      "loss": 0.0236,
      "step": 7970
    },
    {
      "epoch": 1.9441463414634146,
      "grad_norm": 0.1409202516078949,
      "learning_rate": 1.3789004047346532e-05,
      "loss": 0.0123,
      "step": 7971
    },
    {
      "epoch": 1.944390243902439,
      "grad_norm": 0.08367211371660233,
      "learning_rate": 1.3783296639323387e-05,
      "loss": 0.0113,
      "step": 7972
    },
    {
      "epoch": 1.9446341463414634,
      "grad_norm": 0.1291550248861313,
      "learning_rate": 1.3777589963155808e-05,
      "loss": 0.0182,
      "step": 7973
    },
    {
      "epoch": 1.9448780487804878,
      "grad_norm": 0.11762254685163498,
      "learning_rate": 1.377188401921613e-05,
      "loss": 0.0227,
      "step": 7974
    },
    {
      "epoch": 1.9451219512195121,
      "grad_norm": 0.09721539914608002,
      "learning_rate": 1.3766178807876634e-05,
      "loss": 0.0254,
      "step": 7975
    },
    {
      "epoch": 1.9453658536585365,
      "grad_norm": 0.07364696264266968,
      "learning_rate": 1.3760474329509593e-05,
      "loss": 0.0105,
      "step": 7976
    },
    {
      "epoch": 1.945609756097561,
      "grad_norm": 0.13366852700710297,
      "learning_rate": 1.3754770584487195e-05,
      "loss": 0.0248,
      "step": 7977
    },
    {
      "epoch": 1.9458536585365853,
      "grad_norm": 0.1279081255197525,
      "learning_rate": 1.3749067573181581e-05,
      "loss": 0.0244,
      "step": 7978
    },
    {
      "epoch": 1.9460975609756097,
      "grad_norm": 0.20942309498786926,
      "learning_rate": 1.3743365295964878e-05,
      "loss": 0.0273,
      "step": 7979
    },
    {
      "epoch": 1.946341463414634,
      "grad_norm": 0.1285526156425476,
      "learning_rate": 1.3737663753209113e-05,
      "loss": 0.0164,
      "step": 7980
    },
    {
      "epoch": 1.9465853658536585,
      "grad_norm": 0.14175762236118317,
      "learning_rate": 1.3731962945286326e-05,
      "loss": 0.0177,
      "step": 7981
    },
    {
      "epoch": 1.9468292682926829,
      "grad_norm": 0.12601564824581146,
      "learning_rate": 1.372626287256846e-05,
      "loss": 0.0123,
      "step": 7982
    },
    {
      "epoch": 1.9470731707317073,
      "grad_norm": 0.06905529648065567,
      "learning_rate": 1.372056353542743e-05,
      "loss": 0.0185,
      "step": 7983
    },
    {
      "epoch": 1.9473170731707317,
      "grad_norm": 0.09263942390680313,
      "learning_rate": 1.37148649342351e-05,
      "loss": 0.0155,
      "step": 7984
    },
    {
      "epoch": 1.947560975609756,
      "grad_norm": 0.11974821239709854,
      "learning_rate": 1.3709167069363276e-05,
      "loss": 0.0267,
      "step": 7985
    },
    {
      "epoch": 1.9478048780487804,
      "grad_norm": 0.23757882416248322,
      "learning_rate": 1.3703469941183745e-05,
      "loss": 0.0235,
      "step": 7986
    },
    {
      "epoch": 1.9480487804878048,
      "grad_norm": 0.16853968799114227,
      "learning_rate": 1.3697773550068221e-05,
      "loss": 0.0139,
      "step": 7987
    },
    {
      "epoch": 1.9482926829268292,
      "grad_norm": 0.1258818507194519,
      "learning_rate": 1.369207789638836e-05,
      "loss": 0.0336,
      "step": 7988
    },
    {
      "epoch": 1.9485365853658536,
      "grad_norm": 0.12309251725673676,
      "learning_rate": 1.3686382980515811e-05,
      "loss": 0.0239,
      "step": 7989
    },
    {
      "epoch": 1.948780487804878,
      "grad_norm": 0.18617132306098938,
      "learning_rate": 1.3680688802822139e-05,
      "loss": 0.0233,
      "step": 7990
    },
    {
      "epoch": 1.9490243902439024,
      "grad_norm": 0.19867786765098572,
      "learning_rate": 1.3674995363678861e-05,
      "loss": 0.0237,
      "step": 7991
    },
    {
      "epoch": 1.9492682926829268,
      "grad_norm": 0.17542602121829987,
      "learning_rate": 1.3669302663457472e-05,
      "loss": 0.0186,
      "step": 7992
    },
    {
      "epoch": 1.9495121951219512,
      "grad_norm": 0.0994405448436737,
      "learning_rate": 1.3663610702529394e-05,
      "loss": 0.0177,
      "step": 7993
    },
    {
      "epoch": 1.9497560975609756,
      "grad_norm": 0.14064094424247742,
      "learning_rate": 1.3657919481266018e-05,
      "loss": 0.0319,
      "step": 7994
    },
    {
      "epoch": 1.95,
      "grad_norm": 0.13178963959217072,
      "learning_rate": 1.365222900003868e-05,
      "loss": 0.0151,
      "step": 7995
    },
    {
      "epoch": 1.9502439024390243,
      "grad_norm": 0.1141672283411026,
      "learning_rate": 1.3646539259218647e-05,
      "loss": 0.009,
      "step": 7996
    },
    {
      "epoch": 1.9504878048780487,
      "grad_norm": 0.13454164564609528,
      "learning_rate": 1.3640850259177183e-05,
      "loss": 0.0193,
      "step": 7997
    },
    {
      "epoch": 1.9507317073170731,
      "grad_norm": 0.12094591557979584,
      "learning_rate": 1.3635162000285463e-05,
      "loss": 0.0243,
      "step": 7998
    },
    {
      "epoch": 1.9509756097560975,
      "grad_norm": 0.06436028331518173,
      "learning_rate": 1.3629474482914623e-05,
      "loss": 0.0107,
      "step": 7999
    },
    {
      "epoch": 1.951219512195122,
      "grad_norm": 0.09942449629306793,
      "learning_rate": 1.3623787707435786e-05,
      "loss": 0.0263,
      "step": 8000
    },
    {
      "epoch": 1.9514634146341463,
      "grad_norm": 0.12638023495674133,
      "learning_rate": 1.3618101674219952e-05,
      "loss": 0.0233,
      "step": 8001
    },
    {
      "epoch": 1.9517073170731707,
      "grad_norm": 0.2110375612974167,
      "learning_rate": 1.3612416383638155e-05,
      "loss": 0.0119,
      "step": 8002
    },
    {
      "epoch": 1.951951219512195,
      "grad_norm": 0.0780370831489563,
      "learning_rate": 1.3606731836061326e-05,
      "loss": 0.0199,
      "step": 8003
    },
    {
      "epoch": 1.9521951219512195,
      "grad_norm": 0.13373051583766937,
      "learning_rate": 1.3601048031860354e-05,
      "loss": 0.0261,
      "step": 8004
    },
    {
      "epoch": 1.9524390243902439,
      "grad_norm": 0.12639161944389343,
      "learning_rate": 1.3595364971406115e-05,
      "loss": 0.0222,
      "step": 8005
    },
    {
      "epoch": 1.9526829268292683,
      "grad_norm": 0.18339671194553375,
      "learning_rate": 1.3589682655069388e-05,
      "loss": 0.021,
      "step": 8006
    },
    {
      "epoch": 1.9529268292682926,
      "grad_norm": 0.11311507970094681,
      "learning_rate": 1.3584001083220948e-05,
      "loss": 0.0123,
      "step": 8007
    },
    {
      "epoch": 1.953170731707317,
      "grad_norm": 0.1115831807255745,
      "learning_rate": 1.3578320256231488e-05,
      "loss": 0.0215,
      "step": 8008
    },
    {
      "epoch": 1.9534146341463414,
      "grad_norm": 0.11985159665346146,
      "learning_rate": 1.3572640174471657e-05,
      "loss": 0.0276,
      "step": 8009
    },
    {
      "epoch": 1.9536585365853658,
      "grad_norm": 0.14499497413635254,
      "learning_rate": 1.3566960838312082e-05,
      "loss": 0.0191,
      "step": 8010
    },
    {
      "epoch": 1.9539024390243902,
      "grad_norm": 0.1324671506881714,
      "learning_rate": 1.3561282248123314e-05,
      "loss": 0.0144,
      "step": 8011
    },
    {
      "epoch": 1.9541463414634146,
      "grad_norm": 0.08451536297798157,
      "learning_rate": 1.3555604404275851e-05,
      "loss": 0.0134,
      "step": 8012
    },
    {
      "epoch": 1.954390243902439,
      "grad_norm": 0.10193413496017456,
      "learning_rate": 1.3549927307140175e-05,
      "loss": 0.0112,
      "step": 8013
    },
    {
      "epoch": 1.9546341463414634,
      "grad_norm": 0.2666729986667633,
      "learning_rate": 1.3544250957086691e-05,
      "loss": 0.0214,
      "step": 8014
    },
    {
      "epoch": 1.9548780487804878,
      "grad_norm": 0.11511582881212234,
      "learning_rate": 1.3538575354485754e-05,
      "loss": 0.0123,
      "step": 8015
    },
    {
      "epoch": 1.9551219512195122,
      "grad_norm": 0.1256025731563568,
      "learning_rate": 1.3532900499707708e-05,
      "loss": 0.0141,
      "step": 8016
    },
    {
      "epoch": 1.9553658536585365,
      "grad_norm": 0.17564000189304352,
      "learning_rate": 1.3527226393122778e-05,
      "loss": 0.0145,
      "step": 8017
    },
    {
      "epoch": 1.955609756097561,
      "grad_norm": 0.14945188164710999,
      "learning_rate": 1.3521553035101214e-05,
      "loss": 0.016,
      "step": 8018
    },
    {
      "epoch": 1.9558536585365853,
      "grad_norm": 0.15782734751701355,
      "learning_rate": 1.351588042601317e-05,
      "loss": 0.0234,
      "step": 8019
    },
    {
      "epoch": 1.9560975609756097,
      "grad_norm": 0.09820955991744995,
      "learning_rate": 1.3510208566228777e-05,
      "loss": 0.0102,
      "step": 8020
    },
    {
      "epoch": 1.956341463414634,
      "grad_norm": 0.11304526031017303,
      "learning_rate": 1.3504537456118105e-05,
      "loss": 0.0135,
      "step": 8021
    },
    {
      "epoch": 1.9565853658536585,
      "grad_norm": 0.2552875876426697,
      "learning_rate": 1.349886709605116e-05,
      "loss": 0.0396,
      "step": 8022
    },
    {
      "epoch": 1.9568292682926829,
      "grad_norm": 0.10024367272853851,
      "learning_rate": 1.349319748639794e-05,
      "loss": 0.0111,
      "step": 8023
    },
    {
      "epoch": 1.9570731707317073,
      "grad_norm": 0.16551458835601807,
      "learning_rate": 1.348752862752836e-05,
      "loss": 0.0354,
      "step": 8024
    },
    {
      "epoch": 1.9573170731707317,
      "grad_norm": 0.11109614372253418,
      "learning_rate": 1.3481860519812284e-05,
      "loss": 0.0133,
      "step": 8025
    },
    {
      "epoch": 1.957560975609756,
      "grad_norm": 0.12261182814836502,
      "learning_rate": 1.3476193163619554e-05,
      "loss": 0.0164,
      "step": 8026
    },
    {
      "epoch": 1.9578048780487805,
      "grad_norm": 0.12784555554389954,
      "learning_rate": 1.3470526559319947e-05,
      "loss": 0.0259,
      "step": 8027
    },
    {
      "epoch": 1.9580487804878048,
      "grad_norm": 0.3089909851551056,
      "learning_rate": 1.3464860707283174e-05,
      "loss": 0.0232,
      "step": 8028
    },
    {
      "epoch": 1.9582926829268292,
      "grad_norm": 0.1664627194404602,
      "learning_rate": 1.3459195607878939e-05,
      "loss": 0.013,
      "step": 8029
    },
    {
      "epoch": 1.9585365853658536,
      "grad_norm": 0.10647406429052353,
      "learning_rate": 1.345353126147685e-05,
      "loss": 0.0091,
      "step": 8030
    },
    {
      "epoch": 1.958780487804878,
      "grad_norm": 0.09261433780193329,
      "learning_rate": 1.3447867668446511e-05,
      "loss": 0.0204,
      "step": 8031
    },
    {
      "epoch": 1.9590243902439024,
      "grad_norm": 0.0700429156422615,
      "learning_rate": 1.3442204829157439e-05,
      "loss": 0.0132,
      "step": 8032
    },
    {
      "epoch": 1.9592682926829268,
      "grad_norm": 0.15413513779640198,
      "learning_rate": 1.3436542743979125e-05,
      "loss": 0.0292,
      "step": 8033
    },
    {
      "epoch": 1.9595121951219512,
      "grad_norm": 0.06743016839027405,
      "learning_rate": 1.3430881413280994e-05,
      "loss": 0.0171,
      "step": 8034
    },
    {
      "epoch": 1.9597560975609756,
      "grad_norm": 0.06525164842605591,
      "learning_rate": 1.3425220837432423e-05,
      "loss": 0.0109,
      "step": 8035
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.1929812878370285,
      "learning_rate": 1.3419561016802773e-05,
      "loss": 0.0186,
      "step": 8036
    },
    {
      "epoch": 1.9602439024390244,
      "grad_norm": 0.11284954100847244,
      "learning_rate": 1.341390195176131e-05,
      "loss": 0.0143,
      "step": 8037
    },
    {
      "epoch": 1.9604878048780487,
      "grad_norm": 0.2045086771249771,
      "learning_rate": 1.3408243642677271e-05,
      "loss": 0.0136,
      "step": 8038
    },
    {
      "epoch": 1.9607317073170731,
      "grad_norm": 0.20899294316768646,
      "learning_rate": 1.3402586089919855e-05,
      "loss": 0.0209,
      "step": 8039
    },
    {
      "epoch": 1.9609756097560975,
      "grad_norm": 0.10212161391973495,
      "learning_rate": 1.3396929293858196e-05,
      "loss": 0.0176,
      "step": 8040
    },
    {
      "epoch": 1.961219512195122,
      "grad_norm": 0.1409292221069336,
      "learning_rate": 1.339127325486137e-05,
      "loss": 0.0248,
      "step": 8041
    },
    {
      "epoch": 1.9614634146341463,
      "grad_norm": 0.21829074621200562,
      "learning_rate": 1.3385617973298437e-05,
      "loss": 0.0326,
      "step": 8042
    },
    {
      "epoch": 1.9617073170731707,
      "grad_norm": 0.1312154233455658,
      "learning_rate": 1.3379963449538364e-05,
      "loss": 0.0225,
      "step": 8043
    },
    {
      "epoch": 1.961951219512195,
      "grad_norm": 0.09770647436380386,
      "learning_rate": 1.3374309683950114e-05,
      "loss": 0.0187,
      "step": 8044
    },
    {
      "epoch": 1.9621951219512195,
      "grad_norm": 0.0985443964600563,
      "learning_rate": 1.3368656676902569e-05,
      "loss": 0.0099,
      "step": 8045
    },
    {
      "epoch": 1.9624390243902439,
      "grad_norm": 0.18278567492961884,
      "learning_rate": 1.336300442876456e-05,
      "loss": 0.0242,
      "step": 8046
    },
    {
      "epoch": 1.9626829268292683,
      "grad_norm": 0.17726756632328033,
      "learning_rate": 1.3357352939904893e-05,
      "loss": 0.0297,
      "step": 8047
    },
    {
      "epoch": 1.9629268292682926,
      "grad_norm": 0.1348484605550766,
      "learning_rate": 1.3351702210692307e-05,
      "loss": 0.0124,
      "step": 8048
    },
    {
      "epoch": 1.963170731707317,
      "grad_norm": 0.09416735917329788,
      "learning_rate": 1.3346052241495484e-05,
      "loss": 0.0101,
      "step": 8049
    },
    {
      "epoch": 1.9634146341463414,
      "grad_norm": 0.09715328365564346,
      "learning_rate": 1.3340403032683097e-05,
      "loss": 0.0055,
      "step": 8050
    },
    {
      "epoch": 1.9636585365853658,
      "grad_norm": 0.1613832265138626,
      "learning_rate": 1.33347545846237e-05,
      "loss": 0.0136,
      "step": 8051
    },
    {
      "epoch": 1.9639024390243902,
      "grad_norm": 0.15481434762477875,
      "learning_rate": 1.3329106897685862e-05,
      "loss": 0.0163,
      "step": 8052
    },
    {
      "epoch": 1.9641463414634146,
      "grad_norm": 0.1127903014421463,
      "learning_rate": 1.332345997223806e-05,
      "loss": 0.0191,
      "step": 8053
    },
    {
      "epoch": 1.964390243902439,
      "grad_norm": 0.15679965913295746,
      "learning_rate": 1.3317813808648764e-05,
      "loss": 0.0405,
      "step": 8054
    },
    {
      "epoch": 1.9646341463414634,
      "grad_norm": 0.22098180651664734,
      "learning_rate": 1.3312168407286352e-05,
      "loss": 0.0299,
      "step": 8055
    },
    {
      "epoch": 1.9648780487804878,
      "grad_norm": 0.32716578245162964,
      "learning_rate": 1.3306523768519161e-05,
      "loss": 0.0378,
      "step": 8056
    },
    {
      "epoch": 1.9651219512195122,
      "grad_norm": 0.14315608143806458,
      "learning_rate": 1.3300879892715507e-05,
      "loss": 0.0299,
      "step": 8057
    },
    {
      "epoch": 1.9653658536585366,
      "grad_norm": 0.11264318972826004,
      "learning_rate": 1.3295236780243625e-05,
      "loss": 0.0254,
      "step": 8058
    },
    {
      "epoch": 1.965609756097561,
      "grad_norm": 0.09104640036821365,
      "learning_rate": 1.3289594431471703e-05,
      "loss": 0.02,
      "step": 8059
    },
    {
      "epoch": 1.9658536585365853,
      "grad_norm": 0.08577406406402588,
      "learning_rate": 1.3283952846767906e-05,
      "loss": 0.0157,
      "step": 8060
    },
    {
      "epoch": 1.9660975609756097,
      "grad_norm": 0.5876876711845398,
      "learning_rate": 1.3278312026500318e-05,
      "loss": 0.0273,
      "step": 8061
    },
    {
      "epoch": 1.9663414634146341,
      "grad_norm": 0.09336314350366592,
      "learning_rate": 1.3272671971036976e-05,
      "loss": 0.0205,
      "step": 8062
    },
    {
      "epoch": 1.9665853658536585,
      "grad_norm": 0.07998613268136978,
      "learning_rate": 1.3267032680745897e-05,
      "loss": 0.0101,
      "step": 8063
    },
    {
      "epoch": 1.966829268292683,
      "grad_norm": 0.09490150213241577,
      "learning_rate": 1.326139415599501e-05,
      "loss": 0.0154,
      "step": 8064
    },
    {
      "epoch": 1.9670731707317073,
      "grad_norm": 0.11902705579996109,
      "learning_rate": 1.3255756397152228e-05,
      "loss": 0.0148,
      "step": 8065
    },
    {
      "epoch": 1.9673170731707317,
      "grad_norm": 0.16805359721183777,
      "learning_rate": 1.3250119404585388e-05,
      "loss": 0.0311,
      "step": 8066
    },
    {
      "epoch": 1.967560975609756,
      "grad_norm": 0.2304549515247345,
      "learning_rate": 1.3244483178662286e-05,
      "loss": 0.0307,
      "step": 8067
    },
    {
      "epoch": 1.9678048780487805,
      "grad_norm": 0.14667348563671112,
      "learning_rate": 1.323884771975067e-05,
      "loss": 0.0161,
      "step": 8068
    },
    {
      "epoch": 1.9680487804878048,
      "grad_norm": 0.06227982044219971,
      "learning_rate": 1.3233213028218228e-05,
      "loss": 0.0156,
      "step": 8069
    },
    {
      "epoch": 1.9682926829268292,
      "grad_norm": 0.15750062465667725,
      "learning_rate": 1.3227579104432623e-05,
      "loss": 0.0289,
      "step": 8070
    },
    {
      "epoch": 1.9685365853658536,
      "grad_norm": 0.07958779484033585,
      "learning_rate": 1.3221945948761441e-05,
      "loss": 0.0094,
      "step": 8071
    },
    {
      "epoch": 1.968780487804878,
      "grad_norm": 0.09260164946317673,
      "learning_rate": 1.3216313561572222e-05,
      "loss": 0.0153,
      "step": 8072
    },
    {
      "epoch": 1.9690243902439024,
      "grad_norm": 0.1491154581308365,
      "learning_rate": 1.3210681943232478e-05,
      "loss": 0.0301,
      "step": 8073
    },
    {
      "epoch": 1.9692682926829268,
      "grad_norm": 0.2014380544424057,
      "learning_rate": 1.3205051094109649e-05,
      "loss": 0.0197,
      "step": 8074
    },
    {
      "epoch": 1.9695121951219512,
      "grad_norm": 0.178896963596344,
      "learning_rate": 1.3199421014571117e-05,
      "loss": 0.0147,
      "step": 8075
    },
    {
      "epoch": 1.9697560975609756,
      "grad_norm": 0.22560591995716095,
      "learning_rate": 1.3193791704984249e-05,
      "loss": 0.0491,
      "step": 8076
    },
    {
      "epoch": 1.97,
      "grad_norm": 0.0833282321691513,
      "learning_rate": 1.3188163165716317e-05,
      "loss": 0.0146,
      "step": 8077
    },
    {
      "epoch": 1.9702439024390244,
      "grad_norm": 0.05852700024843216,
      "learning_rate": 1.3182535397134593e-05,
      "loss": 0.0088,
      "step": 8078
    },
    {
      "epoch": 1.9704878048780488,
      "grad_norm": 0.14513128995895386,
      "learning_rate": 1.3176908399606256e-05,
      "loss": 0.0121,
      "step": 8079
    },
    {
      "epoch": 1.9707317073170731,
      "grad_norm": 0.10770308971405029,
      "learning_rate": 1.3171282173498442e-05,
      "loss": 0.0172,
      "step": 8080
    },
    {
      "epoch": 1.9709756097560975,
      "grad_norm": 0.09241873770952225,
      "learning_rate": 1.3165656719178265e-05,
      "loss": 0.0153,
      "step": 8081
    },
    {
      "epoch": 1.971219512195122,
      "grad_norm": 0.1227409690618515,
      "learning_rate": 1.316003203701276e-05,
      "loss": 0.0146,
      "step": 8082
    },
    {
      "epoch": 1.9714634146341463,
      "grad_norm": 0.12985216081142426,
      "learning_rate": 1.3154408127368916e-05,
      "loss": 0.0217,
      "step": 8083
    },
    {
      "epoch": 1.9717073170731707,
      "grad_norm": 0.2022208869457245,
      "learning_rate": 1.3148784990613683e-05,
      "loss": 0.0238,
      "step": 8084
    },
    {
      "epoch": 1.971951219512195,
      "grad_norm": 0.28992557525634766,
      "learning_rate": 1.3143162627113937e-05,
      "loss": 0.0357,
      "step": 8085
    },
    {
      "epoch": 1.9721951219512195,
      "grad_norm": 0.0691232681274414,
      "learning_rate": 1.3137541037236539e-05,
      "loss": 0.0136,
      "step": 8086
    },
    {
      "epoch": 1.9724390243902439,
      "grad_norm": 0.10923455655574799,
      "learning_rate": 1.313192022134828e-05,
      "loss": 0.0197,
      "step": 8087
    },
    {
      "epoch": 1.9726829268292683,
      "grad_norm": 0.11297167092561722,
      "learning_rate": 1.312630017981588e-05,
      "loss": 0.023,
      "step": 8088
    },
    {
      "epoch": 1.9729268292682927,
      "grad_norm": 0.10122233629226685,
      "learning_rate": 1.3120680913006056e-05,
      "loss": 0.0269,
      "step": 8089
    },
    {
      "epoch": 1.973170731707317,
      "grad_norm": 0.10005208849906921,
      "learning_rate": 1.311506242128543e-05,
      "loss": 0.0145,
      "step": 8090
    },
    {
      "epoch": 1.9734146341463414,
      "grad_norm": 0.13048386573791504,
      "learning_rate": 1.3109444705020602e-05,
      "loss": 0.026,
      "step": 8091
    },
    {
      "epoch": 1.9736585365853658,
      "grad_norm": 0.1023963913321495,
      "learning_rate": 1.3103827764578109e-05,
      "loss": 0.0105,
      "step": 8092
    },
    {
      "epoch": 1.9739024390243902,
      "grad_norm": 0.2176552712917328,
      "learning_rate": 1.3098211600324428e-05,
      "loss": 0.0186,
      "step": 8093
    },
    {
      "epoch": 1.9741463414634146,
      "grad_norm": 0.1567385047674179,
      "learning_rate": 1.3092596212626013e-05,
      "loss": 0.0166,
      "step": 8094
    },
    {
      "epoch": 1.974390243902439,
      "grad_norm": 0.09392212331295013,
      "learning_rate": 1.3086981601849244e-05,
      "loss": 0.0123,
      "step": 8095
    },
    {
      "epoch": 1.9746341463414634,
      "grad_norm": 0.11290109902620316,
      "learning_rate": 1.3081367768360447e-05,
      "loss": 0.0093,
      "step": 8096
    },
    {
      "epoch": 1.9748780487804878,
      "grad_norm": 0.12319695204496384,
      "learning_rate": 1.3075754712525923e-05,
      "loss": 0.021,
      "step": 8097
    },
    {
      "epoch": 1.9751219512195122,
      "grad_norm": 0.20971301198005676,
      "learning_rate": 1.3070142434711896e-05,
      "loss": 0.0218,
      "step": 8098
    },
    {
      "epoch": 1.9753658536585366,
      "grad_norm": 0.07925701141357422,
      "learning_rate": 1.3064530935284568e-05,
      "loss": 0.0047,
      "step": 8099
    },
    {
      "epoch": 1.975609756097561,
      "grad_norm": 0.23619160056114197,
      "learning_rate": 1.3058920214610054e-05,
      "loss": 0.0503,
      "step": 8100
    },
    {
      "epoch": 1.9758536585365853,
      "grad_norm": 0.15619045495986938,
      "learning_rate": 1.3053310273054432e-05,
      "loss": 0.0119,
      "step": 8101
    },
    {
      "epoch": 1.9760975609756097,
      "grad_norm": 0.1325758546590805,
      "learning_rate": 1.3047701110983745e-05,
      "loss": 0.0182,
      "step": 8102
    },
    {
      "epoch": 1.9763414634146341,
      "grad_norm": 0.10256871581077576,
      "learning_rate": 1.3042092728763966e-05,
      "loss": 0.0126,
      "step": 8103
    },
    {
      "epoch": 1.9765853658536585,
      "grad_norm": 0.11427649855613708,
      "learning_rate": 1.3036485126761039e-05,
      "loss": 0.0192,
      "step": 8104
    },
    {
      "epoch": 1.976829268292683,
      "grad_norm": 0.11476637423038483,
      "learning_rate": 1.3030878305340833e-05,
      "loss": 0.0221,
      "step": 8105
    },
    {
      "epoch": 1.9770731707317073,
      "grad_norm": 0.08361666649580002,
      "learning_rate": 1.302527226486917e-05,
      "loss": 0.0169,
      "step": 8106
    },
    {
      "epoch": 1.9773170731707317,
      "grad_norm": 0.185808464884758,
      "learning_rate": 1.301966700571184e-05,
      "loss": 0.0239,
      "step": 8107
    },
    {
      "epoch": 1.977560975609756,
      "grad_norm": 0.12016505748033524,
      "learning_rate": 1.3014062528234561e-05,
      "loss": 0.0173,
      "step": 8108
    },
    {
      "epoch": 1.9778048780487805,
      "grad_norm": 0.10949871689081192,
      "learning_rate": 1.3008458832803005e-05,
      "loss": 0.0168,
      "step": 8109
    },
    {
      "epoch": 1.9780487804878049,
      "grad_norm": 0.11084183305501938,
      "learning_rate": 1.3002855919782809e-05,
      "loss": 0.013,
      "step": 8110
    },
    {
      "epoch": 1.9782926829268292,
      "grad_norm": 0.1440565437078476,
      "learning_rate": 1.2997253789539524e-05,
      "loss": 0.0148,
      "step": 8111
    },
    {
      "epoch": 1.9785365853658536,
      "grad_norm": 0.13585975766181946,
      "learning_rate": 1.2991652442438695e-05,
      "loss": 0.0204,
      "step": 8112
    },
    {
      "epoch": 1.978780487804878,
      "grad_norm": 0.12288523465394974,
      "learning_rate": 1.2986051878845787e-05,
      "loss": 0.0219,
      "step": 8113
    },
    {
      "epoch": 1.9790243902439024,
      "grad_norm": 0.1304168552160263,
      "learning_rate": 1.2980452099126205e-05,
      "loss": 0.0118,
      "step": 8114
    },
    {
      "epoch": 1.9792682926829268,
      "grad_norm": 0.09726432710886002,
      "learning_rate": 1.2974853103645335e-05,
      "loss": 0.0125,
      "step": 8115
    },
    {
      "epoch": 1.9795121951219512,
      "grad_norm": 0.13218232989311218,
      "learning_rate": 1.296925489276849e-05,
      "loss": 0.0205,
      "step": 8116
    },
    {
      "epoch": 1.9797560975609756,
      "grad_norm": 0.15685103833675385,
      "learning_rate": 1.2963657466860935e-05,
      "loss": 0.032,
      "step": 8117
    },
    {
      "epoch": 1.98,
      "grad_norm": 0.10809429734945297,
      "learning_rate": 1.2958060826287882e-05,
      "loss": 0.0207,
      "step": 8118
    },
    {
      "epoch": 1.9802439024390244,
      "grad_norm": 0.19368787109851837,
      "learning_rate": 1.2952464971414486e-05,
      "loss": 0.0187,
      "step": 8119
    },
    {
      "epoch": 1.9804878048780488,
      "grad_norm": 0.13605253398418427,
      "learning_rate": 1.294686990260588e-05,
      "loss": 0.0165,
      "step": 8120
    },
    {
      "epoch": 1.9807317073170732,
      "grad_norm": 0.16027821600437164,
      "learning_rate": 1.2941275620227112e-05,
      "loss": 0.0237,
      "step": 8121
    },
    {
      "epoch": 1.9809756097560975,
      "grad_norm": 0.10906826704740524,
      "learning_rate": 1.2935682124643186e-05,
      "loss": 0.022,
      "step": 8122
    },
    {
      "epoch": 1.981219512195122,
      "grad_norm": 0.12095700949430466,
      "learning_rate": 1.293008941621908e-05,
      "loss": 0.0155,
      "step": 8123
    },
    {
      "epoch": 1.9814634146341463,
      "grad_norm": 0.09153637290000916,
      "learning_rate": 1.2924497495319682e-05,
      "loss": 0.0219,
      "step": 8124
    },
    {
      "epoch": 1.9817073170731707,
      "grad_norm": 0.16164375841617584,
      "learning_rate": 1.2918906362309863e-05,
      "loss": 0.0174,
      "step": 8125
    },
    {
      "epoch": 1.981951219512195,
      "grad_norm": 0.08633796125650406,
      "learning_rate": 1.2913316017554417e-05,
      "loss": 0.0213,
      "step": 8126
    },
    {
      "epoch": 1.9821951219512195,
      "grad_norm": 0.071688711643219,
      "learning_rate": 1.2907726461418096e-05,
      "loss": 0.0155,
      "step": 8127
    },
    {
      "epoch": 1.9824390243902439,
      "grad_norm": 0.204531729221344,
      "learning_rate": 1.2902137694265611e-05,
      "loss": 0.0244,
      "step": 8128
    },
    {
      "epoch": 1.9826829268292683,
      "grad_norm": 0.10310570895671844,
      "learning_rate": 1.2896549716461609e-05,
      "loss": 0.0215,
      "step": 8129
    },
    {
      "epoch": 1.9829268292682927,
      "grad_norm": 0.13770316541194916,
      "learning_rate": 1.2890962528370676e-05,
      "loss": 0.0173,
      "step": 8130
    },
    {
      "epoch": 1.983170731707317,
      "grad_norm": 0.25594744086265564,
      "learning_rate": 1.2885376130357376e-05,
      "loss": 0.0195,
      "step": 8131
    },
    {
      "epoch": 1.9834146341463414,
      "grad_norm": 0.15162378549575806,
      "learning_rate": 1.28797905227862e-05,
      "loss": 0.0257,
      "step": 8132
    },
    {
      "epoch": 1.9836585365853658,
      "grad_norm": 0.15210218727588654,
      "learning_rate": 1.2874205706021585e-05,
      "loss": 0.0217,
      "step": 8133
    },
    {
      "epoch": 1.9839024390243902,
      "grad_norm": 0.11554920673370361,
      "learning_rate": 1.2868621680427931e-05,
      "loss": 0.0268,
      "step": 8134
    },
    {
      "epoch": 1.9841463414634146,
      "grad_norm": 0.36032089591026306,
      "learning_rate": 1.2863038446369562e-05,
      "loss": 0.0256,
      "step": 8135
    },
    {
      "epoch": 1.984390243902439,
      "grad_norm": 0.06189921870827675,
      "learning_rate": 1.2857456004210794e-05,
      "loss": 0.0166,
      "step": 8136
    },
    {
      "epoch": 1.9846341463414634,
      "grad_norm": 0.07536941021680832,
      "learning_rate": 1.2851874354315835e-05,
      "loss": 0.0185,
      "step": 8137
    },
    {
      "epoch": 1.9848780487804878,
      "grad_norm": 0.18216468393802643,
      "learning_rate": 1.2846293497048898e-05,
      "loss": 0.0304,
      "step": 8138
    },
    {
      "epoch": 1.9851219512195122,
      "grad_norm": 0.16464903950691223,
      "learning_rate": 1.2840713432774104e-05,
      "loss": 0.019,
      "step": 8139
    },
    {
      "epoch": 1.9853658536585366,
      "grad_norm": 0.11353758722543716,
      "learning_rate": 1.2835134161855523e-05,
      "loss": 0.02,
      "step": 8140
    },
    {
      "epoch": 1.985609756097561,
      "grad_norm": 0.1403363049030304,
      "learning_rate": 1.2829555684657211e-05,
      "loss": 0.0199,
      "step": 8141
    },
    {
      "epoch": 1.9858536585365854,
      "grad_norm": 0.11454571038484573,
      "learning_rate": 1.2823978001543129e-05,
      "loss": 0.0111,
      "step": 8142
    },
    {
      "epoch": 1.9860975609756097,
      "grad_norm": 0.19302864372730255,
      "learning_rate": 1.2818401112877205e-05,
      "loss": 0.0222,
      "step": 8143
    },
    {
      "epoch": 1.9863414634146341,
      "grad_norm": 0.25765469670295715,
      "learning_rate": 1.281282501902332e-05,
      "loss": 0.0178,
      "step": 8144
    },
    {
      "epoch": 1.9865853658536585,
      "grad_norm": 0.1461222916841507,
      "learning_rate": 1.2807249720345296e-05,
      "loss": 0.0222,
      "step": 8145
    },
    {
      "epoch": 1.986829268292683,
      "grad_norm": 0.10310488939285278,
      "learning_rate": 1.2801675217206893e-05,
      "loss": 0.0172,
      "step": 8146
    },
    {
      "epoch": 1.9870731707317073,
      "grad_norm": 0.05774044990539551,
      "learning_rate": 1.279610150997185e-05,
      "loss": 0.0094,
      "step": 8147
    },
    {
      "epoch": 1.9873170731707317,
      "grad_norm": 0.08782321959733963,
      "learning_rate": 1.2790528599003815e-05,
      "loss": 0.0203,
      "step": 8148
    },
    {
      "epoch": 1.987560975609756,
      "grad_norm": 0.11436474323272705,
      "learning_rate": 1.2784956484666427e-05,
      "loss": 0.0203,
      "step": 8149
    },
    {
      "epoch": 1.9878048780487805,
      "grad_norm": 0.0682234913110733,
      "learning_rate": 1.2779385167323218e-05,
      "loss": 0.0191,
      "step": 8150
    },
    {
      "epoch": 1.9880487804878049,
      "grad_norm": 0.11299602687358856,
      "learning_rate": 1.2773814647337726e-05,
      "loss": 0.0295,
      "step": 8151
    },
    {
      "epoch": 1.9882926829268293,
      "grad_norm": 0.15621419250965118,
      "learning_rate": 1.27682449250734e-05,
      "loss": 0.0228,
      "step": 8152
    },
    {
      "epoch": 1.9885365853658536,
      "grad_norm": 0.21325230598449707,
      "learning_rate": 1.2762676000893637e-05,
      "loss": 0.0189,
      "step": 8153
    },
    {
      "epoch": 1.988780487804878,
      "grad_norm": 0.11310136318206787,
      "learning_rate": 1.2757107875161815e-05,
      "loss": 0.0274,
      "step": 8154
    },
    {
      "epoch": 1.9890243902439024,
      "grad_norm": 0.12141957879066467,
      "learning_rate": 1.2751540548241222e-05,
      "loss": 0.0082,
      "step": 8155
    },
    {
      "epoch": 1.9892682926829268,
      "grad_norm": 0.09564732015132904,
      "learning_rate": 1.2745974020495106e-05,
      "loss": 0.0207,
      "step": 8156
    },
    {
      "epoch": 1.9895121951219512,
      "grad_norm": 0.12758879363536835,
      "learning_rate": 1.2740408292286677e-05,
      "loss": 0.0236,
      "step": 8157
    },
    {
      "epoch": 1.9897560975609756,
      "grad_norm": 0.0961412861943245,
      "learning_rate": 1.273484336397908e-05,
      "loss": 0.0108,
      "step": 8158
    },
    {
      "epoch": 1.99,
      "grad_norm": 0.07535535097122192,
      "learning_rate": 1.2729279235935396e-05,
      "loss": 0.0131,
      "step": 8159
    },
    {
      "epoch": 1.9902439024390244,
      "grad_norm": 0.17096887528896332,
      "learning_rate": 1.2723715908518685e-05,
      "loss": 0.0239,
      "step": 8160
    },
    {
      "epoch": 1.9904878048780488,
      "grad_norm": 0.13758933544158936,
      "learning_rate": 1.2718153382091924e-05,
      "loss": 0.0077,
      "step": 8161
    },
    {
      "epoch": 1.9907317073170732,
      "grad_norm": 0.07291338592767715,
      "learning_rate": 1.2712591657018064e-05,
      "loss": 0.0105,
      "step": 8162
    },
    {
      "epoch": 1.9909756097560976,
      "grad_norm": 0.05933534353971481,
      "learning_rate": 1.2707030733659984e-05,
      "loss": 0.0106,
      "step": 8163
    },
    {
      "epoch": 1.991219512195122,
      "grad_norm": 0.03992294892668724,
      "learning_rate": 1.2701470612380506e-05,
      "loss": 0.0052,
      "step": 8164
    },
    {
      "epoch": 1.9914634146341463,
      "grad_norm": 0.19932439923286438,
      "learning_rate": 1.2695911293542439e-05,
      "loss": 0.0329,
      "step": 8165
    },
    {
      "epoch": 1.9917073170731707,
      "grad_norm": 0.14851751923561096,
      "learning_rate": 1.2690352777508474e-05,
      "loss": 0.0151,
      "step": 8166
    },
    {
      "epoch": 1.9919512195121951,
      "grad_norm": 0.10850825160741806,
      "learning_rate": 1.268479506464132e-05,
      "loss": 0.0068,
      "step": 8167
    },
    {
      "epoch": 1.9921951219512195,
      "grad_norm": 0.2347681075334549,
      "learning_rate": 1.2679238155303585e-05,
      "loss": 0.0157,
      "step": 8168
    },
    {
      "epoch": 1.992439024390244,
      "grad_norm": 0.11972571164369583,
      "learning_rate": 1.2673682049857833e-05,
      "loss": 0.0223,
      "step": 8169
    },
    {
      "epoch": 1.9926829268292683,
      "grad_norm": 0.18737481534481049,
      "learning_rate": 1.2668126748666604e-05,
      "loss": 0.016,
      "step": 8170
    },
    {
      "epoch": 1.9929268292682927,
      "grad_norm": 0.07219008356332779,
      "learning_rate": 1.2662572252092353e-05,
      "loss": 0.0152,
      "step": 8171
    },
    {
      "epoch": 1.993170731707317,
      "grad_norm": 0.2457728534936905,
      "learning_rate": 1.2657018560497483e-05,
      "loss": 0.0189,
      "step": 8172
    },
    {
      "epoch": 1.9934146341463415,
      "grad_norm": 0.18042133748531342,
      "learning_rate": 1.2651465674244379e-05,
      "loss": 0.0164,
      "step": 8173
    },
    {
      "epoch": 1.9936585365853658,
      "grad_norm": 0.19323627650737762,
      "learning_rate": 1.2645913593695324e-05,
      "loss": 0.0199,
      "step": 8174
    },
    {
      "epoch": 1.9939024390243902,
      "grad_norm": 0.17345720529556274,
      "learning_rate": 1.2640362319212599e-05,
      "loss": 0.0207,
      "step": 8175
    },
    {
      "epoch": 1.9941463414634146,
      "grad_norm": 0.11599072813987732,
      "learning_rate": 1.2634811851158396e-05,
      "loss": 0.0214,
      "step": 8176
    },
    {
      "epoch": 1.994390243902439,
      "grad_norm": 0.13510659337043762,
      "learning_rate": 1.2629262189894855e-05,
      "loss": 0.0137,
      "step": 8177
    },
    {
      "epoch": 1.9946341463414634,
      "grad_norm": 0.12675981223583221,
      "learning_rate": 1.2623713335784099e-05,
      "loss": 0.0177,
      "step": 8178
    },
    {
      "epoch": 1.9948780487804878,
      "grad_norm": 0.11232628673315048,
      "learning_rate": 1.2618165289188156e-05,
      "loss": 0.0248,
      "step": 8179
    },
    {
      "epoch": 1.9951219512195122,
      "grad_norm": 0.2247523069381714,
      "learning_rate": 1.2612618050469016e-05,
      "loss": 0.0179,
      "step": 8180
    },
    {
      "epoch": 1.9953658536585366,
      "grad_norm": 0.10368672013282776,
      "learning_rate": 1.2607071619988636e-05,
      "loss": 0.0094,
      "step": 8181
    },
    {
      "epoch": 1.995609756097561,
      "grad_norm": 0.1288413554430008,
      "learning_rate": 1.2601525998108896e-05,
      "loss": 0.0146,
      "step": 8182
    },
    {
      "epoch": 1.9958536585365854,
      "grad_norm": 0.10317950695753098,
      "learning_rate": 1.259598118519163e-05,
      "loss": 0.0112,
      "step": 8183
    },
    {
      "epoch": 1.9960975609756098,
      "grad_norm": 0.23340414464473724,
      "learning_rate": 1.2590437181598607e-05,
      "loss": 0.0215,
      "step": 8184
    },
    {
      "epoch": 1.9963414634146341,
      "grad_norm": 0.06531718373298645,
      "learning_rate": 1.2584893987691577e-05,
      "loss": 0.0083,
      "step": 8185
    },
    {
      "epoch": 1.9965853658536585,
      "grad_norm": 0.15829429030418396,
      "learning_rate": 1.2579351603832212e-05,
      "loss": 0.0315,
      "step": 8186
    },
    {
      "epoch": 1.996829268292683,
      "grad_norm": 0.13867303729057312,
      "learning_rate": 1.2573810030382119e-05,
      "loss": 0.0145,
      "step": 8187
    },
    {
      "epoch": 1.9970731707317073,
      "grad_norm": 0.1466185599565506,
      "learning_rate": 1.2568269267702893e-05,
      "loss": 0.0204,
      "step": 8188
    },
    {
      "epoch": 1.9973170731707317,
      "grad_norm": 0.12416039407253265,
      "learning_rate": 1.2562729316156039e-05,
      "loss": 0.0273,
      "step": 8189
    },
    {
      "epoch": 1.997560975609756,
      "grad_norm": 0.09278309345245361,
      "learning_rate": 1.2557190176103012e-05,
      "loss": 0.0174,
      "step": 8190
    },
    {
      "epoch": 1.9978048780487805,
      "grad_norm": 0.09900110960006714,
      "learning_rate": 1.2551651847905247e-05,
      "loss": 0.0148,
      "step": 8191
    },
    {
      "epoch": 1.9980487804878049,
      "grad_norm": 0.12882599234580994,
      "learning_rate": 1.2546114331924089e-05,
      "loss": 0.0142,
      "step": 8192
    },
    {
      "epoch": 1.9982926829268293,
      "grad_norm": 0.1080879420042038,
      "learning_rate": 1.254057762852084e-05,
      "loss": 0.0297,
      "step": 8193
    },
    {
      "epoch": 1.9985365853658537,
      "grad_norm": 0.13747495412826538,
      "learning_rate": 1.2535041738056763e-05,
      "loss": 0.0112,
      "step": 8194
    },
    {
      "epoch": 1.998780487804878,
      "grad_norm": 0.1855824738740921,
      "learning_rate": 1.2529506660893045e-05,
      "loss": 0.0215,
      "step": 8195
    },
    {
      "epoch": 1.9990243902439024,
      "grad_norm": 0.06582260131835938,
      "learning_rate": 1.2523972397390854e-05,
      "loss": 0.0133,
      "step": 8196
    },
    {
      "epoch": 1.9992682926829268,
      "grad_norm": 0.0715918317437172,
      "learning_rate": 1.251843894791127e-05,
      "loss": 0.014,
      "step": 8197
    },
    {
      "epoch": 1.9995121951219512,
      "grad_norm": 0.13050521910190582,
      "learning_rate": 1.2512906312815334e-05,
      "loss": 0.0206,
      "step": 8198
    },
    {
      "epoch": 1.9997560975609756,
      "grad_norm": 0.08360162377357483,
      "learning_rate": 1.2507374492464036e-05,
      "loss": 0.0232,
      "step": 8199
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.06413902342319489,
      "learning_rate": 1.2501843487218298e-05,
      "loss": 0.0117,
      "step": 8200
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.42902439024390243,
      "eval_f1": 0.6251959101969806,
      "eval_loss": 0.019920118153095245,
      "eval_precision": 0.698606628070492,
      "eval_recall": 0.5944292594038787,
      "eval_roc_auc": NaN,
      "eval_runtime": 22.6241,
      "eval_samples_per_second": 181.223,
      "eval_steps_per_second": 22.675,
      "step": 8200
    },
    {
      "epoch": 2.000243902439024,
      "grad_norm": 0.0754343569278717,
      "learning_rate": 1.2496313297439023e-05,
      "loss": 0.0124,
      "step": 8201
    },
    {
      "epoch": 2.000487804878049,
      "grad_norm": 0.13404978811740875,
      "learning_rate": 1.2490783923487026e-05,
      "loss": 0.0258,
      "step": 8202
    },
    {
      "epoch": 2.000731707317073,
      "grad_norm": 0.08352246135473251,
      "learning_rate": 1.2485255365723076e-05,
      "loss": 0.0289,
      "step": 8203
    },
    {
      "epoch": 2.0009756097560976,
      "grad_norm": 0.13627785444259644,
      "learning_rate": 1.247972762450791e-05,
      "loss": 0.0147,
      "step": 8204
    },
    {
      "epoch": 2.0012195121951217,
      "grad_norm": 0.10842006653547287,
      "learning_rate": 1.2474200700202188e-05,
      "loss": 0.0096,
      "step": 8205
    },
    {
      "epoch": 2.0014634146341463,
      "grad_norm": 0.07278109341859818,
      "learning_rate": 1.2468674593166516e-05,
      "loss": 0.0131,
      "step": 8206
    },
    {
      "epoch": 2.0017073170731705,
      "grad_norm": 0.06435979902744293,
      "learning_rate": 1.2463149303761473e-05,
      "loss": 0.0148,
      "step": 8207
    },
    {
      "epoch": 2.001951219512195,
      "grad_norm": 0.06563552469015121,
      "learning_rate": 1.2457624832347552e-05,
      "loss": 0.009,
      "step": 8208
    },
    {
      "epoch": 2.0021951219512193,
      "grad_norm": 0.0889480784535408,
      "learning_rate": 1.2452101179285225e-05,
      "loss": 0.012,
      "step": 8209
    },
    {
      "epoch": 2.002439024390244,
      "grad_norm": 0.11345523595809937,
      "learning_rate": 1.2446578344934881e-05,
      "loss": 0.0139,
      "step": 8210
    },
    {
      "epoch": 2.002682926829268,
      "grad_norm": 0.07998009026050568,
      "learning_rate": 1.2441056329656861e-05,
      "loss": 0.0069,
      "step": 8211
    },
    {
      "epoch": 2.0029268292682927,
      "grad_norm": 0.150605708360672,
      "learning_rate": 1.2435535133811479e-05,
      "loss": 0.0237,
      "step": 8212
    },
    {
      "epoch": 2.003170731707317,
      "grad_norm": 0.06939156353473663,
      "learning_rate": 1.2430014757758964e-05,
      "loss": 0.0214,
      "step": 8213
    },
    {
      "epoch": 2.0034146341463415,
      "grad_norm": 0.22730635106563568,
      "learning_rate": 1.2424495201859499e-05,
      "loss": 0.0233,
      "step": 8214
    },
    {
      "epoch": 2.0036585365853656,
      "grad_norm": 0.15864430367946625,
      "learning_rate": 1.2418976466473242e-05,
      "loss": 0.0209,
      "step": 8215
    },
    {
      "epoch": 2.0039024390243902,
      "grad_norm": 0.160999596118927,
      "learning_rate": 1.2413458551960239e-05,
      "loss": 0.027,
      "step": 8216
    },
    {
      "epoch": 2.0041463414634144,
      "grad_norm": 0.13420845568180084,
      "learning_rate": 1.2407941458680544e-05,
      "loss": 0.0116,
      "step": 8217
    },
    {
      "epoch": 2.004390243902439,
      "grad_norm": 0.1734374314546585,
      "learning_rate": 1.2402425186994119e-05,
      "loss": 0.0303,
      "step": 8218
    },
    {
      "epoch": 2.004634146341463,
      "grad_norm": 0.07737921923398972,
      "learning_rate": 1.2396909737260875e-05,
      "loss": 0.0209,
      "step": 8219
    },
    {
      "epoch": 2.004878048780488,
      "grad_norm": 0.08514419943094254,
      "learning_rate": 1.2391395109840703e-05,
      "loss": 0.0109,
      "step": 8220
    },
    {
      "epoch": 2.005121951219512,
      "grad_norm": 0.09661643952131271,
      "learning_rate": 1.238588130509339e-05,
      "loss": 0.0261,
      "step": 8221
    },
    {
      "epoch": 2.0053658536585366,
      "grad_norm": 0.057101666927337646,
      "learning_rate": 1.2380368323378714e-05,
      "loss": 0.0124,
      "step": 8222
    },
    {
      "epoch": 2.0056097560975608,
      "grad_norm": 0.11281012743711472,
      "learning_rate": 1.2374856165056373e-05,
      "loss": 0.0135,
      "step": 8223
    },
    {
      "epoch": 2.0058536585365854,
      "grad_norm": 0.1075773537158966,
      "learning_rate": 1.236934483048601e-05,
      "loss": 0.0108,
      "step": 8224
    },
    {
      "epoch": 2.0060975609756095,
      "grad_norm": 0.14025269448757172,
      "learning_rate": 1.2363834320027236e-05,
      "loss": 0.011,
      "step": 8225
    },
    {
      "epoch": 2.006341463414634,
      "grad_norm": 0.0971265509724617,
      "learning_rate": 1.2358324634039595e-05,
      "loss": 0.0139,
      "step": 8226
    },
    {
      "epoch": 2.0065853658536583,
      "grad_norm": 0.1165572926402092,
      "learning_rate": 1.2352815772882559e-05,
      "loss": 0.0202,
      "step": 8227
    },
    {
      "epoch": 2.006829268292683,
      "grad_norm": 0.07765660434961319,
      "learning_rate": 1.234730773691559e-05,
      "loss": 0.015,
      "step": 8228
    },
    {
      "epoch": 2.007073170731707,
      "grad_norm": 0.11675108224153519,
      "learning_rate": 1.2341800526498046e-05,
      "loss": 0.007,
      "step": 8229
    },
    {
      "epoch": 2.0073170731707317,
      "grad_norm": 0.11609283834695816,
      "learning_rate": 1.2336294141989274e-05,
      "loss": 0.0226,
      "step": 8230
    },
    {
      "epoch": 2.007560975609756,
      "grad_norm": 0.09139843285083771,
      "learning_rate": 1.2330788583748554e-05,
      "loss": 0.0108,
      "step": 8231
    },
    {
      "epoch": 2.0078048780487805,
      "grad_norm": 0.1520911157131195,
      "learning_rate": 1.2325283852135074e-05,
      "loss": 0.0101,
      "step": 8232
    },
    {
      "epoch": 2.0080487804878047,
      "grad_norm": 0.0497262105345726,
      "learning_rate": 1.2319779947508034e-05,
      "loss": 0.0146,
      "step": 8233
    },
    {
      "epoch": 2.0082926829268293,
      "grad_norm": 0.08253733068704605,
      "learning_rate": 1.2314276870226523e-05,
      "loss": 0.0137,
      "step": 8234
    },
    {
      "epoch": 2.0085365853658534,
      "grad_norm": 0.10122411698102951,
      "learning_rate": 1.2308774620649625e-05,
      "loss": 0.0196,
      "step": 8235
    },
    {
      "epoch": 2.008780487804878,
      "grad_norm": 0.10797344148159027,
      "learning_rate": 1.2303273199136325e-05,
      "loss": 0.0101,
      "step": 8236
    },
    {
      "epoch": 2.0090243902439022,
      "grad_norm": 0.05186760798096657,
      "learning_rate": 1.2297772606045573e-05,
      "loss": 0.0109,
      "step": 8237
    },
    {
      "epoch": 2.009268292682927,
      "grad_norm": 0.10380443185567856,
      "learning_rate": 1.2292272841736285e-05,
      "loss": 0.0203,
      "step": 8238
    },
    {
      "epoch": 2.009512195121951,
      "grad_norm": 0.22189883887767792,
      "learning_rate": 1.2286773906567292e-05,
      "loss": 0.0168,
      "step": 8239
    },
    {
      "epoch": 2.0097560975609756,
      "grad_norm": 0.088778555393219,
      "learning_rate": 1.228127580089737e-05,
      "loss": 0.0123,
      "step": 8240
    },
    {
      "epoch": 2.01,
      "grad_norm": 0.08543170243501663,
      "learning_rate": 1.227577852508528e-05,
      "loss": 0.0117,
      "step": 8241
    },
    {
      "epoch": 2.0102439024390244,
      "grad_norm": 0.08409186452627182,
      "learning_rate": 1.2270282079489676e-05,
      "loss": 0.0096,
      "step": 8242
    },
    {
      "epoch": 2.0104878048780486,
      "grad_norm": 0.19433970749378204,
      "learning_rate": 1.2264786464469209e-05,
      "loss": 0.02,
      "step": 8243
    },
    {
      "epoch": 2.010731707317073,
      "grad_norm": 0.2416953593492508,
      "learning_rate": 1.2259291680382434e-05,
      "loss": 0.0155,
      "step": 8244
    },
    {
      "epoch": 2.0109756097560973,
      "grad_norm": 0.0963641107082367,
      "learning_rate": 1.2253797727587867e-05,
      "loss": 0.016,
      "step": 8245
    },
    {
      "epoch": 2.011219512195122,
      "grad_norm": 0.05887611582875252,
      "learning_rate": 1.224830460644399e-05,
      "loss": 0.0095,
      "step": 8246
    },
    {
      "epoch": 2.011463414634146,
      "grad_norm": 0.07919380068778992,
      "learning_rate": 1.22428123173092e-05,
      "loss": 0.0152,
      "step": 8247
    },
    {
      "epoch": 2.0117073170731707,
      "grad_norm": 0.08570649474859238,
      "learning_rate": 1.223732086054185e-05,
      "loss": 0.0095,
      "step": 8248
    },
    {
      "epoch": 2.011951219512195,
      "grad_norm": 0.15617157518863678,
      "learning_rate": 1.2231830236500246e-05,
      "loss": 0.0271,
      "step": 8249
    },
    {
      "epoch": 2.0121951219512195,
      "grad_norm": 0.08522532135248184,
      "learning_rate": 1.2226340445542623e-05,
      "loss": 0.009,
      "step": 8250
    },
    {
      "epoch": 2.0124390243902437,
      "grad_norm": 0.09277330338954926,
      "learning_rate": 1.2220851488027188e-05,
      "loss": 0.0188,
      "step": 8251
    },
    {
      "epoch": 2.0126829268292683,
      "grad_norm": 0.11018197983503342,
      "learning_rate": 1.2215363364312076e-05,
      "loss": 0.0258,
      "step": 8252
    },
    {
      "epoch": 2.0129268292682925,
      "grad_norm": 0.09303861111402512,
      "learning_rate": 1.2209876074755356e-05,
      "loss": 0.0159,
      "step": 8253
    },
    {
      "epoch": 2.013170731707317,
      "grad_norm": 0.19056864082813263,
      "learning_rate": 1.2204389619715079e-05,
      "loss": 0.0213,
      "step": 8254
    },
    {
      "epoch": 2.0134146341463413,
      "grad_norm": 0.22098538279533386,
      "learning_rate": 1.2198903999549196e-05,
      "loss": 0.0327,
      "step": 8255
    },
    {
      "epoch": 2.013658536585366,
      "grad_norm": 0.16766996681690216,
      "learning_rate": 1.2193419214615651e-05,
      "loss": 0.0185,
      "step": 8256
    },
    {
      "epoch": 2.01390243902439,
      "grad_norm": 0.11829014122486115,
      "learning_rate": 1.2187935265272296e-05,
      "loss": 0.0149,
      "step": 8257
    },
    {
      "epoch": 2.0141463414634146,
      "grad_norm": 0.11278080195188522,
      "learning_rate": 1.2182452151876935e-05,
      "loss": 0.021,
      "step": 8258
    },
    {
      "epoch": 2.014390243902439,
      "grad_norm": 0.07715068757534027,
      "learning_rate": 1.2176969874787341e-05,
      "loss": 0.017,
      "step": 8259
    },
    {
      "epoch": 2.0146341463414634,
      "grad_norm": 0.06381314247846603,
      "learning_rate": 1.2171488434361208e-05,
      "loss": 0.007,
      "step": 8260
    },
    {
      "epoch": 2.0148780487804876,
      "grad_norm": 0.09823770076036453,
      "learning_rate": 1.2166007830956175e-05,
      "loss": 0.0202,
      "step": 8261
    },
    {
      "epoch": 2.015121951219512,
      "grad_norm": 0.07171215116977692,
      "learning_rate": 1.2160528064929849e-05,
      "loss": 0.0155,
      "step": 8262
    },
    {
      "epoch": 2.0153658536585364,
      "grad_norm": 0.2693392336368561,
      "learning_rate": 1.2155049136639766e-05,
      "loss": 0.028,
      "step": 8263
    },
    {
      "epoch": 2.015609756097561,
      "grad_norm": 0.08565929532051086,
      "learning_rate": 1.21495710464434e-05,
      "loss": 0.0171,
      "step": 8264
    },
    {
      "epoch": 2.015853658536585,
      "grad_norm": 0.09818670153617859,
      "learning_rate": 1.2144093794698185e-05,
      "loss": 0.0213,
      "step": 8265
    },
    {
      "epoch": 2.0160975609756098,
      "grad_norm": 0.12397897243499756,
      "learning_rate": 1.2138617381761489e-05,
      "loss": 0.0138,
      "step": 8266
    },
    {
      "epoch": 2.016341463414634,
      "grad_norm": 0.06687339395284653,
      "learning_rate": 1.2133141807990642e-05,
      "loss": 0.0038,
      "step": 8267
    },
    {
      "epoch": 2.0165853658536586,
      "grad_norm": 0.09999988228082657,
      "learning_rate": 1.2127667073742894e-05,
      "loss": 0.011,
      "step": 8268
    },
    {
      "epoch": 2.0168292682926827,
      "grad_norm": 0.35281163454055786,
      "learning_rate": 1.212219317937547e-05,
      "loss": 0.0282,
      "step": 8269
    },
    {
      "epoch": 2.0170731707317073,
      "grad_norm": 0.1614811271429062,
      "learning_rate": 1.2116720125245524e-05,
      "loss": 0.0311,
      "step": 8270
    },
    {
      "epoch": 2.0173170731707315,
      "grad_norm": 0.1354474425315857,
      "learning_rate": 1.2111247911710136e-05,
      "loss": 0.0281,
      "step": 8271
    },
    {
      "epoch": 2.017560975609756,
      "grad_norm": 0.1284305453300476,
      "learning_rate": 1.2105776539126376e-05,
      "loss": 0.0193,
      "step": 8272
    },
    {
      "epoch": 2.0178048780487803,
      "grad_norm": 0.11560490727424622,
      "learning_rate": 1.2100306007851226e-05,
      "loss": 0.0244,
      "step": 8273
    },
    {
      "epoch": 2.018048780487805,
      "grad_norm": 0.10322486609220505,
      "learning_rate": 1.209483631824161e-05,
      "loss": 0.0181,
      "step": 8274
    },
    {
      "epoch": 2.018292682926829,
      "grad_norm": 0.06280262023210526,
      "learning_rate": 1.2089367470654426e-05,
      "loss": 0.0152,
      "step": 8275
    },
    {
      "epoch": 2.0185365853658537,
      "grad_norm": 0.09863701462745667,
      "learning_rate": 1.2083899465446489e-05,
      "loss": 0.0245,
      "step": 8276
    },
    {
      "epoch": 2.018780487804878,
      "grad_norm": 0.1396627575159073,
      "learning_rate": 1.2078432302974565e-05,
      "loss": 0.0177,
      "step": 8277
    },
    {
      "epoch": 2.0190243902439025,
      "grad_norm": 0.14077766239643097,
      "learning_rate": 1.2072965983595386e-05,
      "loss": 0.0199,
      "step": 8278
    },
    {
      "epoch": 2.0192682926829266,
      "grad_norm": 0.14172986149787903,
      "learning_rate": 1.2067500507665591e-05,
      "loss": 0.0137,
      "step": 8279
    },
    {
      "epoch": 2.0195121951219512,
      "grad_norm": 0.1126471608877182,
      "learning_rate": 1.2062035875541811e-05,
      "loss": 0.017,
      "step": 8280
    },
    {
      "epoch": 2.0197560975609754,
      "grad_norm": 0.07723257690668106,
      "learning_rate": 1.2056572087580582e-05,
      "loss": 0.0127,
      "step": 8281
    },
    {
      "epoch": 2.02,
      "grad_norm": 0.08708933740854263,
      "learning_rate": 1.20511091441384e-05,
      "loss": 0.0114,
      "step": 8282
    },
    {
      "epoch": 2.020243902439024,
      "grad_norm": 0.12811043858528137,
      "learning_rate": 1.2045647045571706e-05,
      "loss": 0.0108,
      "step": 8283
    },
    {
      "epoch": 2.020487804878049,
      "grad_norm": 0.1184699684381485,
      "learning_rate": 1.2040185792236874e-05,
      "loss": 0.0203,
      "step": 8284
    },
    {
      "epoch": 2.020731707317073,
      "grad_norm": 0.09726819396018982,
      "learning_rate": 1.203472538449026e-05,
      "loss": 0.0221,
      "step": 8285
    },
    {
      "epoch": 2.0209756097560976,
      "grad_norm": 0.08024326711893082,
      "learning_rate": 1.2029265822688119e-05,
      "loss": 0.0081,
      "step": 8286
    },
    {
      "epoch": 2.0212195121951217,
      "grad_norm": 0.19973315298557281,
      "learning_rate": 1.2023807107186669e-05,
      "loss": 0.0281,
      "step": 8287
    },
    {
      "epoch": 2.0214634146341464,
      "grad_norm": 0.18553262948989868,
      "learning_rate": 1.2018349238342089e-05,
      "loss": 0.0147,
      "step": 8288
    },
    {
      "epoch": 2.0217073170731705,
      "grad_norm": 0.11595886945724487,
      "learning_rate": 1.201289221651048e-05,
      "loss": 0.0132,
      "step": 8289
    },
    {
      "epoch": 2.021951219512195,
      "grad_norm": 0.11094960570335388,
      "learning_rate": 1.200743604204789e-05,
      "loss": 0.0129,
      "step": 8290
    },
    {
      "epoch": 2.0221951219512193,
      "grad_norm": 0.1110021099448204,
      "learning_rate": 1.2001980715310332e-05,
      "loss": 0.022,
      "step": 8291
    },
    {
      "epoch": 2.022439024390244,
      "grad_norm": 0.14638260006904602,
      "learning_rate": 1.1996526236653732e-05,
      "loss": 0.0219,
      "step": 8292
    },
    {
      "epoch": 2.022682926829268,
      "grad_norm": 0.12646923959255219,
      "learning_rate": 1.1991072606433999e-05,
      "loss": 0.0135,
      "step": 8293
    },
    {
      "epoch": 2.0229268292682927,
      "grad_norm": 0.10246797651052475,
      "learning_rate": 1.198561982500695e-05,
      "loss": 0.0207,
      "step": 8294
    },
    {
      "epoch": 2.023170731707317,
      "grad_norm": 0.17196503281593323,
      "learning_rate": 1.1980167892728361e-05,
      "loss": 0.0214,
      "step": 8295
    },
    {
      "epoch": 2.0234146341463415,
      "grad_norm": 0.1524459719657898,
      "learning_rate": 1.1974716809953968e-05,
      "loss": 0.0145,
      "step": 8296
    },
    {
      "epoch": 2.0236585365853657,
      "grad_norm": 0.09616857767105103,
      "learning_rate": 1.1969266577039428e-05,
      "loss": 0.0129,
      "step": 8297
    },
    {
      "epoch": 2.0239024390243903,
      "grad_norm": 0.21518631279468536,
      "learning_rate": 1.1963817194340355e-05,
      "loss": 0.0221,
      "step": 8298
    },
    {
      "epoch": 2.0241463414634144,
      "grad_norm": 0.1314479261636734,
      "learning_rate": 1.19583686622123e-05,
      "loss": 0.0181,
      "step": 8299
    },
    {
      "epoch": 2.024390243902439,
      "grad_norm": 0.2234189808368683,
      "learning_rate": 1.1952920981010757e-05,
      "loss": 0.0228,
      "step": 8300
    },
    {
      "epoch": 2.024634146341463,
      "grad_norm": 0.08150161057710648,
      "learning_rate": 1.1947474151091187e-05,
      "loss": 0.0123,
      "step": 8301
    },
    {
      "epoch": 2.024878048780488,
      "grad_norm": 0.14445072412490845,
      "learning_rate": 1.1942028172808972e-05,
      "loss": 0.0094,
      "step": 8302
    },
    {
      "epoch": 2.025121951219512,
      "grad_norm": 0.08602599799633026,
      "learning_rate": 1.1936583046519436e-05,
      "loss": 0.0171,
      "step": 8303
    },
    {
      "epoch": 2.0253658536585366,
      "grad_norm": 0.15217742323875427,
      "learning_rate": 1.1931138772577876e-05,
      "loss": 0.0127,
      "step": 8304
    },
    {
      "epoch": 2.0256097560975608,
      "grad_norm": 0.09192255884408951,
      "learning_rate": 1.1925695351339491e-05,
      "loss": 0.0117,
      "step": 8305
    },
    {
      "epoch": 2.0258536585365854,
      "grad_norm": 0.0849718526005745,
      "learning_rate": 1.1920252783159472e-05,
      "loss": 0.0097,
      "step": 8306
    },
    {
      "epoch": 2.0260975609756096,
      "grad_norm": 0.11434415727853775,
      "learning_rate": 1.1914811068392916e-05,
      "loss": 0.0154,
      "step": 8307
    },
    {
      "epoch": 2.026341463414634,
      "grad_norm": 0.12034352123737335,
      "learning_rate": 1.1909370207394871e-05,
      "loss": 0.0231,
      "step": 8308
    },
    {
      "epoch": 2.0265853658536583,
      "grad_norm": 0.1038360744714737,
      "learning_rate": 1.1903930200520354e-05,
      "loss": 0.0211,
      "step": 8309
    },
    {
      "epoch": 2.026829268292683,
      "grad_norm": 0.16624531149864197,
      "learning_rate": 1.1898491048124306e-05,
      "loss": 0.0224,
      "step": 8310
    },
    {
      "epoch": 2.027073170731707,
      "grad_norm": 0.10125725716352463,
      "learning_rate": 1.1893052750561595e-05,
      "loss": 0.0158,
      "step": 8311
    },
    {
      "epoch": 2.0273170731707317,
      "grad_norm": 0.1346963346004486,
      "learning_rate": 1.1887615308187079e-05,
      "loss": 0.0258,
      "step": 8312
    },
    {
      "epoch": 2.027560975609756,
      "grad_norm": 0.1607235074043274,
      "learning_rate": 1.1882178721355514e-05,
      "loss": 0.0194,
      "step": 8313
    },
    {
      "epoch": 2.0278048780487805,
      "grad_norm": 0.18158575892448425,
      "learning_rate": 1.1876742990421647e-05,
      "loss": 0.013,
      "step": 8314
    },
    {
      "epoch": 2.0280487804878047,
      "grad_norm": 0.0799793154001236,
      "learning_rate": 1.1871308115740106e-05,
      "loss": 0.0094,
      "step": 8315
    },
    {
      "epoch": 2.0282926829268293,
      "grad_norm": 0.16135862469673157,
      "learning_rate": 1.1865874097665533e-05,
      "loss": 0.0196,
      "step": 8316
    },
    {
      "epoch": 2.0285365853658535,
      "grad_norm": 0.08256980776786804,
      "learning_rate": 1.1860440936552466e-05,
      "loss": 0.014,
      "step": 8317
    },
    {
      "epoch": 2.028780487804878,
      "grad_norm": 0.07864036411046982,
      "learning_rate": 1.1855008632755393e-05,
      "loss": 0.0059,
      "step": 8318
    },
    {
      "epoch": 2.0290243902439022,
      "grad_norm": 0.10433410108089447,
      "learning_rate": 1.1849577186628776e-05,
      "loss": 0.0176,
      "step": 8319
    },
    {
      "epoch": 2.029268292682927,
      "grad_norm": 0.08107549697160721,
      "learning_rate": 1.1844146598526992e-05,
      "loss": 0.0111,
      "step": 8320
    },
    {
      "epoch": 2.029512195121951,
      "grad_norm": 0.11255152523517609,
      "learning_rate": 1.1838716868804358e-05,
      "loss": 0.0222,
      "step": 8321
    },
    {
      "epoch": 2.0297560975609756,
      "grad_norm": 0.14387273788452148,
      "learning_rate": 1.1833287997815168e-05,
      "loss": 0.0156,
      "step": 8322
    },
    {
      "epoch": 2.03,
      "grad_norm": 0.06454791873693466,
      "learning_rate": 1.1827859985913628e-05,
      "loss": 0.0136,
      "step": 8323
    },
    {
      "epoch": 2.0302439024390244,
      "grad_norm": 0.11600779742002487,
      "learning_rate": 1.1822432833453892e-05,
      "loss": 0.0155,
      "step": 8324
    },
    {
      "epoch": 2.0304878048780486,
      "grad_norm": 0.11503847688436508,
      "learning_rate": 1.181700654079008e-05,
      "loss": 0.0138,
      "step": 8325
    },
    {
      "epoch": 2.030731707317073,
      "grad_norm": 0.1588723361492157,
      "learning_rate": 1.1811581108276227e-05,
      "loss": 0.024,
      "step": 8326
    },
    {
      "epoch": 2.0309756097560974,
      "grad_norm": 0.11818834394216537,
      "learning_rate": 1.180615653626634e-05,
      "loss": 0.0083,
      "step": 8327
    },
    {
      "epoch": 2.031219512195122,
      "grad_norm": 0.17886996269226074,
      "learning_rate": 1.1800732825114351e-05,
      "loss": 0.0364,
      "step": 8328
    },
    {
      "epoch": 2.031463414634146,
      "grad_norm": 0.12605534493923187,
      "learning_rate": 1.179530997517413e-05,
      "loss": 0.0283,
      "step": 8329
    },
    {
      "epoch": 2.0317073170731708,
      "grad_norm": 0.12346276640892029,
      "learning_rate": 1.1789887986799528e-05,
      "loss": 0.0159,
      "step": 8330
    },
    {
      "epoch": 2.031951219512195,
      "grad_norm": 0.19047187268733978,
      "learning_rate": 1.1784466860344272e-05,
      "loss": 0.0368,
      "step": 8331
    },
    {
      "epoch": 2.0321951219512195,
      "grad_norm": 0.09757580608129501,
      "learning_rate": 1.177904659616211e-05,
      "loss": 0.0097,
      "step": 8332
    },
    {
      "epoch": 2.0324390243902437,
      "grad_norm": 0.2579587399959564,
      "learning_rate": 1.1773627194606684e-05,
      "loss": 0.0136,
      "step": 8333
    },
    {
      "epoch": 2.0326829268292683,
      "grad_norm": 0.08846791833639145,
      "learning_rate": 1.1768208656031585e-05,
      "loss": 0.0146,
      "step": 8334
    },
    {
      "epoch": 2.0329268292682925,
      "grad_norm": 0.1532379388809204,
      "learning_rate": 1.1762790980790373e-05,
      "loss": 0.0283,
      "step": 8335
    },
    {
      "epoch": 2.033170731707317,
      "grad_norm": 0.0671776607632637,
      "learning_rate": 1.1757374169236526e-05,
      "loss": 0.0075,
      "step": 8336
    },
    {
      "epoch": 2.0334146341463413,
      "grad_norm": 0.20474179089069366,
      "learning_rate": 1.1751958221723466e-05,
      "loss": 0.0185,
      "step": 8337
    },
    {
      "epoch": 2.033658536585366,
      "grad_norm": 0.05941540375351906,
      "learning_rate": 1.1746543138604585e-05,
      "loss": 0.0112,
      "step": 8338
    },
    {
      "epoch": 2.03390243902439,
      "grad_norm": 0.08768048882484436,
      "learning_rate": 1.1741128920233182e-05,
      "loss": 0.0177,
      "step": 8339
    },
    {
      "epoch": 2.0341463414634147,
      "grad_norm": 0.266572505235672,
      "learning_rate": 1.1735715566962538e-05,
      "loss": 0.0341,
      "step": 8340
    },
    {
      "epoch": 2.034390243902439,
      "grad_norm": 0.1128612831234932,
      "learning_rate": 1.1730303079145846e-05,
      "loss": 0.0111,
      "step": 8341
    },
    {
      "epoch": 2.0346341463414634,
      "grad_norm": 0.05846455693244934,
      "learning_rate": 1.1724891457136247e-05,
      "loss": 0.0081,
      "step": 8342
    },
    {
      "epoch": 2.0348780487804876,
      "grad_norm": 0.2662982642650604,
      "learning_rate": 1.1719480701286853e-05,
      "loss": 0.014,
      "step": 8343
    },
    {
      "epoch": 2.0351219512195122,
      "grad_norm": 0.14598777890205383,
      "learning_rate": 1.1714070811950684e-05,
      "loss": 0.0305,
      "step": 8344
    },
    {
      "epoch": 2.0353658536585364,
      "grad_norm": 0.09790240973234177,
      "learning_rate": 1.1708661789480716e-05,
      "loss": 0.0164,
      "step": 8345
    },
    {
      "epoch": 2.035609756097561,
      "grad_norm": 0.18020853400230408,
      "learning_rate": 1.1703253634229885e-05,
      "loss": 0.0318,
      "step": 8346
    },
    {
      "epoch": 2.035853658536585,
      "grad_norm": 0.09634200483560562,
      "learning_rate": 1.169784634655105e-05,
      "loss": 0.0166,
      "step": 8347
    },
    {
      "epoch": 2.03609756097561,
      "grad_norm": 0.09438249468803406,
      "learning_rate": 1.1692439926797018e-05,
      "loss": 0.0099,
      "step": 8348
    },
    {
      "epoch": 2.036341463414634,
      "grad_norm": 0.13175572454929352,
      "learning_rate": 1.1687034375320544e-05,
      "loss": 0.0338,
      "step": 8349
    },
    {
      "epoch": 2.0365853658536586,
      "grad_norm": 0.18282544612884521,
      "learning_rate": 1.1681629692474311e-05,
      "loss": 0.0171,
      "step": 8350
    },
    {
      "epoch": 2.0368292682926827,
      "grad_norm": 0.1424769014120102,
      "learning_rate": 1.167622587861098e-05,
      "loss": 0.0121,
      "step": 8351
    },
    {
      "epoch": 2.0370731707317074,
      "grad_norm": 0.07636482268571854,
      "learning_rate": 1.1670822934083111e-05,
      "loss": 0.0171,
      "step": 8352
    },
    {
      "epoch": 2.0373170731707315,
      "grad_norm": 0.13165880739688873,
      "learning_rate": 1.1665420859243254e-05,
      "loss": 0.0213,
      "step": 8353
    },
    {
      "epoch": 2.037560975609756,
      "grad_norm": 0.13337337970733643,
      "learning_rate": 1.166001965444386e-05,
      "loss": 0.0174,
      "step": 8354
    },
    {
      "epoch": 2.0378048780487803,
      "grad_norm": 0.08772767335176468,
      "learning_rate": 1.165461932003734e-05,
      "loss": 0.015,
      "step": 8355
    },
    {
      "epoch": 2.038048780487805,
      "grad_norm": 0.11671961843967438,
      "learning_rate": 1.1649219856376065e-05,
      "loss": 0.0135,
      "step": 8356
    },
    {
      "epoch": 2.038292682926829,
      "grad_norm": 0.07324910908937454,
      "learning_rate": 1.164382126381232e-05,
      "loss": 0.0116,
      "step": 8357
    },
    {
      "epoch": 2.0385365853658537,
      "grad_norm": 0.07285765558481216,
      "learning_rate": 1.1638423542698346e-05,
      "loss": 0.0109,
      "step": 8358
    },
    {
      "epoch": 2.038780487804878,
      "grad_norm": 0.10811895877122879,
      "learning_rate": 1.1633026693386339e-05,
      "loss": 0.018,
      "step": 8359
    },
    {
      "epoch": 2.0390243902439025,
      "grad_norm": 0.10755583643913269,
      "learning_rate": 1.1627630716228411e-05,
      "loss": 0.0248,
      "step": 8360
    },
    {
      "epoch": 2.0392682926829266,
      "grad_norm": 0.1697467863559723,
      "learning_rate": 1.162223561157665e-05,
      "loss": 0.0205,
      "step": 8361
    },
    {
      "epoch": 2.0395121951219513,
      "grad_norm": 0.29078409075737,
      "learning_rate": 1.1616841379783064e-05,
      "loss": 0.0269,
      "step": 8362
    },
    {
      "epoch": 2.0397560975609754,
      "grad_norm": 0.23235562443733215,
      "learning_rate": 1.1611448021199608e-05,
      "loss": 0.0346,
      "step": 8363
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.05610588565468788,
      "learning_rate": 1.1606055536178182e-05,
      "loss": 0.0085,
      "step": 8364
    },
    {
      "epoch": 2.040243902439024,
      "grad_norm": 0.08301109820604324,
      "learning_rate": 1.1600663925070617e-05,
      "loss": 0.0127,
      "step": 8365
    },
    {
      "epoch": 2.040487804878049,
      "grad_norm": 0.1433698534965515,
      "learning_rate": 1.1595273188228725e-05,
      "loss": 0.0342,
      "step": 8366
    },
    {
      "epoch": 2.040731707317073,
      "grad_norm": 0.1348993182182312,
      "learning_rate": 1.1589883326004222e-05,
      "loss": 0.021,
      "step": 8367
    },
    {
      "epoch": 2.0409756097560976,
      "grad_norm": 0.2152598649263382,
      "learning_rate": 1.158449433874877e-05,
      "loss": 0.032,
      "step": 8368
    },
    {
      "epoch": 2.0412195121951218,
      "grad_norm": 0.1414482444524765,
      "learning_rate": 1.1579106226813999e-05,
      "loss": 0.0146,
      "step": 8369
    },
    {
      "epoch": 2.0414634146341464,
      "grad_norm": 0.15892794728279114,
      "learning_rate": 1.1573718990551465e-05,
      "loss": 0.0283,
      "step": 8370
    },
    {
      "epoch": 2.0417073170731705,
      "grad_norm": 0.22449736297130585,
      "learning_rate": 1.1568332630312652e-05,
      "loss": 0.013,
      "step": 8371
    },
    {
      "epoch": 2.041951219512195,
      "grad_norm": 0.09811970591545105,
      "learning_rate": 1.1562947146449033e-05,
      "loss": 0.019,
      "step": 8372
    },
    {
      "epoch": 2.0421951219512193,
      "grad_norm": 0.11614641547203064,
      "learning_rate": 1.1557562539311961e-05,
      "loss": 0.0217,
      "step": 8373
    },
    {
      "epoch": 2.042439024390244,
      "grad_norm": 0.10384351015090942,
      "learning_rate": 1.1552178809252798e-05,
      "loss": 0.0239,
      "step": 8374
    },
    {
      "epoch": 2.042682926829268,
      "grad_norm": 0.08631129562854767,
      "learning_rate": 1.1546795956622794e-05,
      "loss": 0.014,
      "step": 8375
    },
    {
      "epoch": 2.0429268292682927,
      "grad_norm": 0.14141137897968292,
      "learning_rate": 1.1541413981773163e-05,
      "loss": 0.0269,
      "step": 8376
    },
    {
      "epoch": 2.043170731707317,
      "grad_norm": 0.06710173189640045,
      "learning_rate": 1.1536032885055079e-05,
      "loss": 0.0102,
      "step": 8377
    },
    {
      "epoch": 2.0434146341463415,
      "grad_norm": 0.1781751960515976,
      "learning_rate": 1.1530652666819634e-05,
      "loss": 0.0215,
      "step": 8378
    },
    {
      "epoch": 2.0436585365853657,
      "grad_norm": 0.11211257427930832,
      "learning_rate": 1.1525273327417854e-05,
      "loss": 0.0165,
      "step": 8379
    },
    {
      "epoch": 2.0439024390243903,
      "grad_norm": 0.14489911496639252,
      "learning_rate": 1.1519894867200761e-05,
      "loss": 0.0232,
      "step": 8380
    },
    {
      "epoch": 2.0441463414634145,
      "grad_norm": 0.11303363740444183,
      "learning_rate": 1.1514517286519245e-05,
      "loss": 0.0235,
      "step": 8381
    },
    {
      "epoch": 2.044390243902439,
      "grad_norm": 0.16061155498027802,
      "learning_rate": 1.1509140585724199e-05,
      "loss": 0.0232,
      "step": 8382
    },
    {
      "epoch": 2.0446341463414632,
      "grad_norm": 0.09071461856365204,
      "learning_rate": 1.150376476516643e-05,
      "loss": 0.0205,
      "step": 8383
    },
    {
      "epoch": 2.044878048780488,
      "grad_norm": 0.09853950142860413,
      "learning_rate": 1.1498389825196687e-05,
      "loss": 0.011,
      "step": 8384
    },
    {
      "epoch": 2.045121951219512,
      "grad_norm": 0.1642674058675766,
      "learning_rate": 1.1493015766165683e-05,
      "loss": 0.0104,
      "step": 8385
    },
    {
      "epoch": 2.0453658536585366,
      "grad_norm": 0.0843258872628212,
      "learning_rate": 1.1487642588424044e-05,
      "loss": 0.0072,
      "step": 8386
    },
    {
      "epoch": 2.045609756097561,
      "grad_norm": 0.11859983950853348,
      "learning_rate": 1.148227029232237e-05,
      "loss": 0.0165,
      "step": 8387
    },
    {
      "epoch": 2.0458536585365854,
      "grad_norm": 0.1504235714673996,
      "learning_rate": 1.1476898878211176e-05,
      "loss": 0.0267,
      "step": 8388
    },
    {
      "epoch": 2.0460975609756096,
      "grad_norm": 0.10717649012804031,
      "learning_rate": 1.1471528346440924e-05,
      "loss": 0.0097,
      "step": 8389
    },
    {
      "epoch": 2.046341463414634,
      "grad_norm": 0.10740746557712555,
      "learning_rate": 1.146615869736204e-05,
      "loss": 0.01,
      "step": 8390
    },
    {
      "epoch": 2.0465853658536584,
      "grad_norm": 0.13021007180213928,
      "learning_rate": 1.1460789931324872e-05,
      "loss": 0.0135,
      "step": 8391
    },
    {
      "epoch": 2.046829268292683,
      "grad_norm": 0.18454401195049286,
      "learning_rate": 1.1455422048679704e-05,
      "loss": 0.0201,
      "step": 8392
    },
    {
      "epoch": 2.047073170731707,
      "grad_norm": 0.06960148364305496,
      "learning_rate": 1.1450055049776792e-05,
      "loss": 0.0098,
      "step": 8393
    },
    {
      "epoch": 2.0473170731707317,
      "grad_norm": 0.12763430178165436,
      "learning_rate": 1.1444688934966308e-05,
      "loss": 0.0177,
      "step": 8394
    },
    {
      "epoch": 2.047560975609756,
      "grad_norm": 0.12204191088676453,
      "learning_rate": 1.1439323704598366e-05,
      "loss": 0.02,
      "step": 8395
    },
    {
      "epoch": 2.0478048780487805,
      "grad_norm": 0.1539892852306366,
      "learning_rate": 1.1433959359023055e-05,
      "loss": 0.0221,
      "step": 8396
    },
    {
      "epoch": 2.0480487804878047,
      "grad_norm": 0.05744915083050728,
      "learning_rate": 1.142859589859035e-05,
      "loss": 0.0138,
      "step": 8397
    },
    {
      "epoch": 2.0482926829268293,
      "grad_norm": 0.05501430481672287,
      "learning_rate": 1.1423233323650227e-05,
      "loss": 0.0074,
      "step": 8398
    },
    {
      "epoch": 2.0485365853658535,
      "grad_norm": 0.2830021381378174,
      "learning_rate": 1.1417871634552554e-05,
      "loss": 0.0225,
      "step": 8399
    },
    {
      "epoch": 2.048780487804878,
      "grad_norm": 0.1564408242702484,
      "learning_rate": 1.1412510831647189e-05,
      "loss": 0.0253,
      "step": 8400
    },
    {
      "epoch": 2.0490243902439023,
      "grad_norm": 0.09880216419696808,
      "learning_rate": 1.1407150915283899e-05,
      "loss": 0.0175,
      "step": 8401
    },
    {
      "epoch": 2.049268292682927,
      "grad_norm": 0.16963423788547516,
      "learning_rate": 1.1401791885812388e-05,
      "loss": 0.0213,
      "step": 8402
    },
    {
      "epoch": 2.049512195121951,
      "grad_norm": 0.0812411829829216,
      "learning_rate": 1.1396433743582339e-05,
      "loss": 0.0146,
      "step": 8403
    },
    {
      "epoch": 2.0497560975609757,
      "grad_norm": 0.1375686079263687,
      "learning_rate": 1.1391076488943343e-05,
      "loss": 0.02,
      "step": 8404
    },
    {
      "epoch": 2.05,
      "grad_norm": 0.11825292557477951,
      "learning_rate": 1.1385720122244937e-05,
      "loss": 0.0187,
      "step": 8405
    },
    {
      "epoch": 2.0502439024390244,
      "grad_norm": 0.08214585483074188,
      "learning_rate": 1.1380364643836621e-05,
      "loss": 0.0066,
      "step": 8406
    },
    {
      "epoch": 2.0504878048780486,
      "grad_norm": 0.10970378667116165,
      "learning_rate": 1.1375010054067822e-05,
      "loss": 0.0143,
      "step": 8407
    },
    {
      "epoch": 2.050731707317073,
      "grad_norm": 0.14681226015090942,
      "learning_rate": 1.1369656353287896e-05,
      "loss": 0.01,
      "step": 8408
    },
    {
      "epoch": 2.0509756097560974,
      "grad_norm": 0.18610337376594543,
      "learning_rate": 1.1364303541846177e-05,
      "loss": 0.0195,
      "step": 8409
    },
    {
      "epoch": 2.051219512195122,
      "grad_norm": 0.14418712258338928,
      "learning_rate": 1.13589516200919e-05,
      "loss": 0.0119,
      "step": 8410
    },
    {
      "epoch": 2.051463414634146,
      "grad_norm": 0.09962161630392075,
      "learning_rate": 1.135360058837428e-05,
      "loss": 0.0203,
      "step": 8411
    },
    {
      "epoch": 2.0517073170731708,
      "grad_norm": 0.12748587131500244,
      "learning_rate": 1.1348250447042447e-05,
      "loss": 0.0111,
      "step": 8412
    },
    {
      "epoch": 2.051951219512195,
      "grad_norm": 0.11115634441375732,
      "learning_rate": 1.1342901196445477e-05,
      "loss": 0.0208,
      "step": 8413
    },
    {
      "epoch": 2.0521951219512196,
      "grad_norm": 0.09606791287660599,
      "learning_rate": 1.1337552836932396e-05,
      "loss": 0.0161,
      "step": 8414
    },
    {
      "epoch": 2.0524390243902437,
      "grad_norm": 0.1336493045091629,
      "learning_rate": 1.133220536885216e-05,
      "loss": 0.0204,
      "step": 8415
    },
    {
      "epoch": 2.0526829268292683,
      "grad_norm": 0.19287988543510437,
      "learning_rate": 1.1326858792553694e-05,
      "loss": 0.0212,
      "step": 8416
    },
    {
      "epoch": 2.0529268292682925,
      "grad_norm": 0.14584317803382874,
      "learning_rate": 1.1321513108385832e-05,
      "loss": 0.0159,
      "step": 8417
    },
    {
      "epoch": 2.053170731707317,
      "grad_norm": 0.13913236558437347,
      "learning_rate": 1.1316168316697353e-05,
      "loss": 0.0178,
      "step": 8418
    },
    {
      "epoch": 2.0534146341463413,
      "grad_norm": 0.17984674870967865,
      "learning_rate": 1.1310824417837015e-05,
      "loss": 0.0186,
      "step": 8419
    },
    {
      "epoch": 2.053658536585366,
      "grad_norm": 0.187668114900589,
      "learning_rate": 1.1305481412153476e-05,
      "loss": 0.016,
      "step": 8420
    },
    {
      "epoch": 2.05390243902439,
      "grad_norm": 0.14722009003162384,
      "learning_rate": 1.1300139299995342e-05,
      "loss": 0.011,
      "step": 8421
    },
    {
      "epoch": 2.0541463414634147,
      "grad_norm": 0.11618863791227341,
      "learning_rate": 1.1294798081711188e-05,
      "loss": 0.0201,
      "step": 8422
    },
    {
      "epoch": 2.054390243902439,
      "grad_norm": 0.11906561255455017,
      "learning_rate": 1.1289457757649496e-05,
      "loss": 0.024,
      "step": 8423
    },
    {
      "epoch": 2.0546341463414635,
      "grad_norm": 0.08902434259653091,
      "learning_rate": 1.1284118328158721e-05,
      "loss": 0.0087,
      "step": 8424
    },
    {
      "epoch": 2.0548780487804876,
      "grad_norm": 0.14530721306800842,
      "learning_rate": 1.1278779793587239e-05,
      "loss": 0.0235,
      "step": 8425
    },
    {
      "epoch": 2.0551219512195122,
      "grad_norm": 0.17411473393440247,
      "learning_rate": 1.127344215428336e-05,
      "loss": 0.0227,
      "step": 8426
    },
    {
      "epoch": 2.0553658536585364,
      "grad_norm": 0.05890117585659027,
      "learning_rate": 1.1268105410595367e-05,
      "loss": 0.0118,
      "step": 8427
    },
    {
      "epoch": 2.055609756097561,
      "grad_norm": 0.11495275050401688,
      "learning_rate": 1.126276956287146e-05,
      "loss": 0.0098,
      "step": 8428
    },
    {
      "epoch": 2.055853658536585,
      "grad_norm": 0.07366856187582016,
      "learning_rate": 1.1257434611459786e-05,
      "loss": 0.0187,
      "step": 8429
    },
    {
      "epoch": 2.05609756097561,
      "grad_norm": 0.08398552238941193,
      "learning_rate": 1.1252100556708434e-05,
      "loss": 0.0159,
      "step": 8430
    },
    {
      "epoch": 2.056341463414634,
      "grad_norm": 0.06678805500268936,
      "learning_rate": 1.1246767398965424e-05,
      "loss": 0.0143,
      "step": 8431
    },
    {
      "epoch": 2.0565853658536586,
      "grad_norm": 0.07782754302024841,
      "learning_rate": 1.1241435138578749e-05,
      "loss": 0.0131,
      "step": 8432
    },
    {
      "epoch": 2.0568292682926828,
      "grad_norm": 0.17850278317928314,
      "learning_rate": 1.1236103775896312e-05,
      "loss": 0.0215,
      "step": 8433
    },
    {
      "epoch": 2.0570731707317074,
      "grad_norm": 0.13655483722686768,
      "learning_rate": 1.1230773311265957e-05,
      "loss": 0.0247,
      "step": 8434
    },
    {
      "epoch": 2.0573170731707315,
      "grad_norm": 0.11209848523139954,
      "learning_rate": 1.1225443745035502e-05,
      "loss": 0.019,
      "step": 8435
    },
    {
      "epoch": 2.057560975609756,
      "grad_norm": 0.07096441835165024,
      "learning_rate": 1.1220115077552667e-05,
      "loss": 0.0132,
      "step": 8436
    },
    {
      "epoch": 2.0578048780487803,
      "grad_norm": 0.05429304763674736,
      "learning_rate": 1.1214787309165145e-05,
      "loss": 0.0087,
      "step": 8437
    },
    {
      "epoch": 2.058048780487805,
      "grad_norm": 0.10325168073177338,
      "learning_rate": 1.1209460440220553e-05,
      "loss": 0.0212,
      "step": 8438
    },
    {
      "epoch": 2.058292682926829,
      "grad_norm": 0.15296319127082825,
      "learning_rate": 1.1204134471066441e-05,
      "loss": 0.0256,
      "step": 8439
    },
    {
      "epoch": 2.0585365853658537,
      "grad_norm": 0.17184937000274658,
      "learning_rate": 1.1198809402050331e-05,
      "loss": 0.0093,
      "step": 8440
    },
    {
      "epoch": 2.058780487804878,
      "grad_norm": 0.14929360151290894,
      "learning_rate": 1.1193485233519658e-05,
      "loss": 0.0183,
      "step": 8441
    },
    {
      "epoch": 2.0590243902439025,
      "grad_norm": 0.10151831805706024,
      "learning_rate": 1.1188161965821797e-05,
      "loss": 0.0244,
      "step": 8442
    },
    {
      "epoch": 2.0592682926829267,
      "grad_norm": 0.10456430166959763,
      "learning_rate": 1.1182839599304095e-05,
      "loss": 0.0177,
      "step": 8443
    },
    {
      "epoch": 2.0595121951219513,
      "grad_norm": 0.09790217131376266,
      "learning_rate": 1.1177518134313803e-05,
      "loss": 0.0262,
      "step": 8444
    },
    {
      "epoch": 2.0597560975609754,
      "grad_norm": 0.11488626152276993,
      "learning_rate": 1.1172197571198145e-05,
      "loss": 0.0182,
      "step": 8445
    },
    {
      "epoch": 2.06,
      "grad_norm": 0.11331258714199066,
      "learning_rate": 1.1166877910304269e-05,
      "loss": 0.0285,
      "step": 8446
    },
    {
      "epoch": 2.060243902439024,
      "grad_norm": 0.11004647612571716,
      "learning_rate": 1.1161559151979259e-05,
      "loss": 0.0133,
      "step": 8447
    },
    {
      "epoch": 2.060487804878049,
      "grad_norm": 0.14849907159805298,
      "learning_rate": 1.1156241296570149e-05,
      "loss": 0.0184,
      "step": 8448
    },
    {
      "epoch": 2.060731707317073,
      "grad_norm": 0.07547447830438614,
      "learning_rate": 1.1150924344423907e-05,
      "loss": 0.019,
      "step": 8449
    },
    {
      "epoch": 2.0609756097560976,
      "grad_norm": 0.10759945213794708,
      "learning_rate": 1.1145608295887466e-05,
      "loss": 0.0152,
      "step": 8450
    },
    {
      "epoch": 2.061219512195122,
      "grad_norm": 0.0793793797492981,
      "learning_rate": 1.114029315130767e-05,
      "loss": 0.0149,
      "step": 8451
    },
    {
      "epoch": 2.0614634146341464,
      "grad_norm": 0.11522089689970016,
      "learning_rate": 1.113497891103131e-05,
      "loss": 0.0181,
      "step": 8452
    },
    {
      "epoch": 2.0617073170731706,
      "grad_norm": 0.12455655634403229,
      "learning_rate": 1.1129665575405138e-05,
      "loss": 0.022,
      "step": 8453
    },
    {
      "epoch": 2.061951219512195,
      "grad_norm": 0.0784517377614975,
      "learning_rate": 1.1124353144775826e-05,
      "loss": 0.0073,
      "step": 8454
    },
    {
      "epoch": 2.0621951219512193,
      "grad_norm": 0.14957775175571442,
      "learning_rate": 1.1119041619489986e-05,
      "loss": 0.0166,
      "step": 8455
    },
    {
      "epoch": 2.062439024390244,
      "grad_norm": 0.07751422375440598,
      "learning_rate": 1.1113730999894195e-05,
      "loss": 0.0163,
      "step": 8456
    },
    {
      "epoch": 2.062682926829268,
      "grad_norm": 0.24099986255168915,
      "learning_rate": 1.1108421286334939e-05,
      "loss": 0.0167,
      "step": 8457
    },
    {
      "epoch": 2.0629268292682927,
      "grad_norm": 0.10787312686443329,
      "learning_rate": 1.1103112479158673e-05,
      "loss": 0.0101,
      "step": 8458
    },
    {
      "epoch": 2.063170731707317,
      "grad_norm": 0.1013679951429367,
      "learning_rate": 1.109780457871178e-05,
      "loss": 0.0159,
      "step": 8459
    },
    {
      "epoch": 2.0634146341463415,
      "grad_norm": 0.11464428156614304,
      "learning_rate": 1.1092497585340566e-05,
      "loss": 0.0303,
      "step": 8460
    },
    {
      "epoch": 2.0636585365853657,
      "grad_norm": 0.16703525185585022,
      "learning_rate": 1.1087191499391322e-05,
      "loss": 0.0243,
      "step": 8461
    },
    {
      "epoch": 2.0639024390243903,
      "grad_norm": 0.13998477160930634,
      "learning_rate": 1.1081886321210242e-05,
      "loss": 0.017,
      "step": 8462
    },
    {
      "epoch": 2.0641463414634145,
      "grad_norm": 0.14671297371387482,
      "learning_rate": 1.1076582051143469e-05,
      "loss": 0.0269,
      "step": 8463
    },
    {
      "epoch": 2.064390243902439,
      "grad_norm": 0.13173088431358337,
      "learning_rate": 1.1071278689537096e-05,
      "loss": 0.0172,
      "step": 8464
    },
    {
      "epoch": 2.0646341463414632,
      "grad_norm": 0.18716168403625488,
      "learning_rate": 1.1065976236737135e-05,
      "loss": 0.014,
      "step": 8465
    },
    {
      "epoch": 2.064878048780488,
      "grad_norm": 0.31323760747909546,
      "learning_rate": 1.106067469308958e-05,
      "loss": 0.024,
      "step": 8466
    },
    {
      "epoch": 2.065121951219512,
      "grad_norm": 0.1418239325284958,
      "learning_rate": 1.1055374058940332e-05,
      "loss": 0.0161,
      "step": 8467
    },
    {
      "epoch": 2.0653658536585366,
      "grad_norm": 0.1472962498664856,
      "learning_rate": 1.1050074334635226e-05,
      "loss": 0.014,
      "step": 8468
    },
    {
      "epoch": 2.065609756097561,
      "grad_norm": 0.1438789665699005,
      "learning_rate": 1.1044775520520076e-05,
      "loss": 0.0243,
      "step": 8469
    },
    {
      "epoch": 2.0658536585365854,
      "grad_norm": 0.09760022908449173,
      "learning_rate": 1.1039477616940591e-05,
      "loss": 0.0142,
      "step": 8470
    },
    {
      "epoch": 2.0660975609756096,
      "grad_norm": 0.13513751327991486,
      "learning_rate": 1.1034180624242466e-05,
      "loss": 0.0152,
      "step": 8471
    },
    {
      "epoch": 2.066341463414634,
      "grad_norm": 0.11609353125095367,
      "learning_rate": 1.1028884542771301e-05,
      "loss": 0.0211,
      "step": 8472
    },
    {
      "epoch": 2.0665853658536584,
      "grad_norm": 0.12098179012537003,
      "learning_rate": 1.1023589372872639e-05,
      "loss": 0.0162,
      "step": 8473
    },
    {
      "epoch": 2.066829268292683,
      "grad_norm": 0.12417907267808914,
      "learning_rate": 1.1018295114891993e-05,
      "loss": 0.0104,
      "step": 8474
    },
    {
      "epoch": 2.067073170731707,
      "grad_norm": 0.10915721207857132,
      "learning_rate": 1.101300176917479e-05,
      "loss": 0.0196,
      "step": 8475
    },
    {
      "epoch": 2.0673170731707318,
      "grad_norm": 0.0744127482175827,
      "learning_rate": 1.1007709336066394e-05,
      "loss": 0.0148,
      "step": 8476
    },
    {
      "epoch": 2.067560975609756,
      "grad_norm": 0.08519947528839111,
      "learning_rate": 1.1002417815912138e-05,
      "loss": 0.0095,
      "step": 8477
    },
    {
      "epoch": 2.0678048780487805,
      "grad_norm": 0.0853106826543808,
      "learning_rate": 1.0997127209057261e-05,
      "loss": 0.0091,
      "step": 8478
    },
    {
      "epoch": 2.0680487804878047,
      "grad_norm": 0.15904276072978973,
      "learning_rate": 1.0991837515846983e-05,
      "loss": 0.0185,
      "step": 8479
    },
    {
      "epoch": 2.0682926829268293,
      "grad_norm": 0.09272259473800659,
      "learning_rate": 1.0986548736626418e-05,
      "loss": 0.0115,
      "step": 8480
    },
    {
      "epoch": 2.0685365853658535,
      "grad_norm": 0.10500477254390717,
      "learning_rate": 1.0981260871740637e-05,
      "loss": 0.0244,
      "step": 8481
    },
    {
      "epoch": 2.068780487804878,
      "grad_norm": 0.05730203911662102,
      "learning_rate": 1.097597392153468e-05,
      "loss": 0.01,
      "step": 8482
    },
    {
      "epoch": 2.0690243902439023,
      "grad_norm": 0.09354301542043686,
      "learning_rate": 1.0970687886353481e-05,
      "loss": 0.025,
      "step": 8483
    },
    {
      "epoch": 2.069268292682927,
      "grad_norm": 0.12072433531284332,
      "learning_rate": 1.096540276654196e-05,
      "loss": 0.0228,
      "step": 8484
    },
    {
      "epoch": 2.069512195121951,
      "grad_norm": 0.1280040740966797,
      "learning_rate": 1.0960118562444945e-05,
      "loss": 0.0136,
      "step": 8485
    },
    {
      "epoch": 2.0697560975609757,
      "grad_norm": 0.19919539988040924,
      "learning_rate": 1.0954835274407204e-05,
      "loss": 0.0248,
      "step": 8486
    },
    {
      "epoch": 2.07,
      "grad_norm": 0.1903870403766632,
      "learning_rate": 1.0949552902773475e-05,
      "loss": 0.0378,
      "step": 8487
    },
    {
      "epoch": 2.0702439024390245,
      "grad_norm": 0.17819297313690186,
      "learning_rate": 1.0944271447888405e-05,
      "loss": 0.0227,
      "step": 8488
    },
    {
      "epoch": 2.0704878048780486,
      "grad_norm": 0.07655435055494308,
      "learning_rate": 1.0938990910096586e-05,
      "loss": 0.0116,
      "step": 8489
    },
    {
      "epoch": 2.0707317073170732,
      "grad_norm": 0.13181862235069275,
      "learning_rate": 1.0933711289742573e-05,
      "loss": 0.0215,
      "step": 8490
    },
    {
      "epoch": 2.0709756097560974,
      "grad_norm": 0.07548028230667114,
      "learning_rate": 1.0928432587170828e-05,
      "loss": 0.016,
      "step": 8491
    },
    {
      "epoch": 2.071219512195122,
      "grad_norm": 0.11522158980369568,
      "learning_rate": 1.092315480272579e-05,
      "loss": 0.0165,
      "step": 8492
    },
    {
      "epoch": 2.071463414634146,
      "grad_norm": 0.11462653428316116,
      "learning_rate": 1.0917877936751808e-05,
      "loss": 0.0244,
      "step": 8493
    },
    {
      "epoch": 2.071707317073171,
      "grad_norm": 0.15628638863563538,
      "learning_rate": 1.0912601989593172e-05,
      "loss": 0.026,
      "step": 8494
    },
    {
      "epoch": 2.071951219512195,
      "grad_norm": 0.09265171736478806,
      "learning_rate": 1.090732696159415e-05,
      "loss": 0.0177,
      "step": 8495
    },
    {
      "epoch": 2.0721951219512196,
      "grad_norm": 0.16235263645648956,
      "learning_rate": 1.0902052853098881e-05,
      "loss": 0.0238,
      "step": 8496
    },
    {
      "epoch": 2.0724390243902437,
      "grad_norm": 0.1042056456208229,
      "learning_rate": 1.0896779664451516e-05,
      "loss": 0.0065,
      "step": 8497
    },
    {
      "epoch": 2.0726829268292684,
      "grad_norm": 0.1270371526479721,
      "learning_rate": 1.0891507395996104e-05,
      "loss": 0.0178,
      "step": 8498
    },
    {
      "epoch": 2.0729268292682925,
      "grad_norm": 0.13831648230552673,
      "learning_rate": 1.0886236048076634e-05,
      "loss": 0.0233,
      "step": 8499
    },
    {
      "epoch": 2.073170731707317,
      "grad_norm": 0.18632599711418152,
      "learning_rate": 1.0880965621037065e-05,
      "loss": 0.0153,
      "step": 8500
    },
    {
      "epoch": 2.0734146341463413,
      "grad_norm": 0.21952000260353088,
      "learning_rate": 1.0875696115221266e-05,
      "loss": 0.014,
      "step": 8501
    },
    {
      "epoch": 2.073658536585366,
      "grad_norm": 0.08047402650117874,
      "learning_rate": 1.087042753097305e-05,
      "loss": 0.0116,
      "step": 8502
    },
    {
      "epoch": 2.07390243902439,
      "grad_norm": 0.12382055819034576,
      "learning_rate": 1.0865159868636187e-05,
      "loss": 0.0298,
      "step": 8503
    },
    {
      "epoch": 2.0741463414634147,
      "grad_norm": 0.056844454258680344,
      "learning_rate": 1.0859893128554368e-05,
      "loss": 0.0098,
      "step": 8504
    },
    {
      "epoch": 2.074390243902439,
      "grad_norm": 0.09003574401140213,
      "learning_rate": 1.0854627311071242e-05,
      "loss": 0.0152,
      "step": 8505
    },
    {
      "epoch": 2.0746341463414635,
      "grad_norm": 0.11210581660270691,
      "learning_rate": 1.0849362416530382e-05,
      "loss": 0.0155,
      "step": 8506
    },
    {
      "epoch": 2.0748780487804876,
      "grad_norm": 0.07251770794391632,
      "learning_rate": 1.0844098445275294e-05,
      "loss": 0.0079,
      "step": 8507
    },
    {
      "epoch": 2.0751219512195123,
      "grad_norm": 0.07135399430990219,
      "learning_rate": 1.0838835397649457e-05,
      "loss": 0.0179,
      "step": 8508
    },
    {
      "epoch": 2.0753658536585364,
      "grad_norm": 0.14096975326538086,
      "learning_rate": 1.0833573273996261e-05,
      "loss": 0.0239,
      "step": 8509
    },
    {
      "epoch": 2.075609756097561,
      "grad_norm": 0.0740564614534378,
      "learning_rate": 1.0828312074659031e-05,
      "loss": 0.0127,
      "step": 8510
    },
    {
      "epoch": 2.075853658536585,
      "grad_norm": 0.09835540503263474,
      "learning_rate": 1.0823051799981062e-05,
      "loss": 0.0159,
      "step": 8511
    },
    {
      "epoch": 2.07609756097561,
      "grad_norm": 0.1287306398153305,
      "learning_rate": 1.0817792450305564e-05,
      "loss": 0.025,
      "step": 8512
    },
    {
      "epoch": 2.076341463414634,
      "grad_norm": 0.1075187399983406,
      "learning_rate": 1.0812534025975693e-05,
      "loss": 0.0099,
      "step": 8513
    },
    {
      "epoch": 2.0765853658536586,
      "grad_norm": 0.10580004006624222,
      "learning_rate": 1.0807276527334546e-05,
      "loss": 0.0142,
      "step": 8514
    },
    {
      "epoch": 2.0768292682926828,
      "grad_norm": 0.12465479224920273,
      "learning_rate": 1.0802019954725146e-05,
      "loss": 0.017,
      "step": 8515
    },
    {
      "epoch": 2.0770731707317074,
      "grad_norm": 0.06414623558521271,
      "learning_rate": 1.079676430849049e-05,
      "loss": 0.0057,
      "step": 8516
    },
    {
      "epoch": 2.0773170731707316,
      "grad_norm": 0.20901119709014893,
      "learning_rate": 1.0791509588973472e-05,
      "loss": 0.0301,
      "step": 8517
    },
    {
      "epoch": 2.077560975609756,
      "grad_norm": 0.08261643350124359,
      "learning_rate": 1.0786255796516967e-05,
      "loss": 0.0098,
      "step": 8518
    },
    {
      "epoch": 2.0778048780487803,
      "grad_norm": 0.1769789308309555,
      "learning_rate": 1.078100293146376e-05,
      "loss": 0.0168,
      "step": 8519
    },
    {
      "epoch": 2.078048780487805,
      "grad_norm": 0.0863456279039383,
      "learning_rate": 1.0775750994156574e-05,
      "loss": 0.0119,
      "step": 8520
    },
    {
      "epoch": 2.078292682926829,
      "grad_norm": 0.09188985824584961,
      "learning_rate": 1.0770499984938099e-05,
      "loss": 0.0142,
      "step": 8521
    },
    {
      "epoch": 2.0785365853658537,
      "grad_norm": 0.08825512230396271,
      "learning_rate": 1.076524990415094e-05,
      "loss": 0.0105,
      "step": 8522
    },
    {
      "epoch": 2.078780487804878,
      "grad_norm": 0.07325462251901627,
      "learning_rate": 1.0760000752137639e-05,
      "loss": 0.0114,
      "step": 8523
    },
    {
      "epoch": 2.0790243902439025,
      "grad_norm": 0.14345155656337738,
      "learning_rate": 1.0754752529240705e-05,
      "loss": 0.017,
      "step": 8524
    },
    {
      "epoch": 2.0792682926829267,
      "grad_norm": 0.08282625675201416,
      "learning_rate": 1.074950523580256e-05,
      "loss": 0.0222,
      "step": 8525
    },
    {
      "epoch": 2.0795121951219513,
      "grad_norm": 0.08739742636680603,
      "learning_rate": 1.0744258872165563e-05,
      "loss": 0.0079,
      "step": 8526
    },
    {
      "epoch": 2.0797560975609755,
      "grad_norm": 0.0753854513168335,
      "learning_rate": 1.0739013438672043e-05,
      "loss": 0.0117,
      "step": 8527
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.08967389911413193,
      "learning_rate": 1.073376893566424e-05,
      "loss": 0.0137,
      "step": 8528
    },
    {
      "epoch": 2.0802439024390242,
      "grad_norm": 0.15763410925865173,
      "learning_rate": 1.072852536348434e-05,
      "loss": 0.016,
      "step": 8529
    },
    {
      "epoch": 2.080487804878049,
      "grad_norm": 0.17905712127685547,
      "learning_rate": 1.0723282722474462e-05,
      "loss": 0.0197,
      "step": 8530
    },
    {
      "epoch": 2.080731707317073,
      "grad_norm": 0.1459358036518097,
      "learning_rate": 1.071804101297669e-05,
      "loss": 0.0187,
      "step": 8531
    },
    {
      "epoch": 2.0809756097560976,
      "grad_norm": 0.08477365225553513,
      "learning_rate": 1.0712800235333022e-05,
      "loss": 0.021,
      "step": 8532
    },
    {
      "epoch": 2.081219512195122,
      "grad_norm": 0.13922199606895447,
      "learning_rate": 1.070756038988539e-05,
      "loss": 0.0121,
      "step": 8533
    },
    {
      "epoch": 2.0814634146341464,
      "grad_norm": 0.1512182652950287,
      "learning_rate": 1.0702321476975701e-05,
      "loss": 0.0253,
      "step": 8534
    },
    {
      "epoch": 2.0817073170731706,
      "grad_norm": 0.12128577381372452,
      "learning_rate": 1.0697083496945765e-05,
      "loss": 0.0225,
      "step": 8535
    },
    {
      "epoch": 2.081951219512195,
      "grad_norm": 0.1384338140487671,
      "learning_rate": 1.0691846450137336e-05,
      "loss": 0.0174,
      "step": 8536
    },
    {
      "epoch": 2.0821951219512194,
      "grad_norm": 0.06795279681682587,
      "learning_rate": 1.0686610336892133e-05,
      "loss": 0.012,
      "step": 8537
    },
    {
      "epoch": 2.082439024390244,
      "grad_norm": 0.08761734515428543,
      "learning_rate": 1.068137515755179e-05,
      "loss": 0.0095,
      "step": 8538
    },
    {
      "epoch": 2.082682926829268,
      "grad_norm": 0.10698534548282623,
      "learning_rate": 1.0676140912457874e-05,
      "loss": 0.0148,
      "step": 8539
    },
    {
      "epoch": 2.0829268292682928,
      "grad_norm": 0.12868288159370422,
      "learning_rate": 1.0670907601951923e-05,
      "loss": 0.0212,
      "step": 8540
    },
    {
      "epoch": 2.083170731707317,
      "grad_norm": 0.06794419139623642,
      "learning_rate": 1.0665675226375376e-05,
      "loss": 0.018,
      "step": 8541
    },
    {
      "epoch": 2.0834146341463415,
      "grad_norm": 0.09558883309364319,
      "learning_rate": 1.0660443786069648e-05,
      "loss": 0.0143,
      "step": 8542
    },
    {
      "epoch": 2.0836585365853657,
      "grad_norm": 0.09421420097351074,
      "learning_rate": 1.0655213281376068e-05,
      "loss": 0.0132,
      "step": 8543
    },
    {
      "epoch": 2.0839024390243903,
      "grad_norm": 0.08375506103038788,
      "learning_rate": 1.0649983712635896e-05,
      "loss": 0.0163,
      "step": 8544
    },
    {
      "epoch": 2.0841463414634145,
      "grad_norm": 0.2172333300113678,
      "learning_rate": 1.0644755080190378e-05,
      "loss": 0.0227,
      "step": 8545
    },
    {
      "epoch": 2.084390243902439,
      "grad_norm": 0.11920086294412613,
      "learning_rate": 1.0639527384380623e-05,
      "loss": 0.0191,
      "step": 8546
    },
    {
      "epoch": 2.0846341463414633,
      "grad_norm": 0.16702039539813995,
      "learning_rate": 1.0634300625547756e-05,
      "loss": 0.0228,
      "step": 8547
    },
    {
      "epoch": 2.084878048780488,
      "grad_norm": 0.09378302842378616,
      "learning_rate": 1.0629074804032796e-05,
      "loss": 0.0181,
      "step": 8548
    },
    {
      "epoch": 2.085121951219512,
      "grad_norm": 0.08839475363492966,
      "learning_rate": 1.0623849920176701e-05,
      "loss": 0.0148,
      "step": 8549
    },
    {
      "epoch": 2.0853658536585367,
      "grad_norm": 0.10341997444629669,
      "learning_rate": 1.0618625974320399e-05,
      "loss": 0.0142,
      "step": 8550
    },
    {
      "epoch": 2.085609756097561,
      "grad_norm": 0.10472293943166733,
      "learning_rate": 1.0613402966804725e-05,
      "loss": 0.016,
      "step": 8551
    },
    {
      "epoch": 2.0858536585365854,
      "grad_norm": 0.12929567694664001,
      "learning_rate": 1.060818089797046e-05,
      "loss": 0.0274,
      "step": 8552
    },
    {
      "epoch": 2.0860975609756096,
      "grad_norm": 0.165876105427742,
      "learning_rate": 1.0602959768158342e-05,
      "loss": 0.016,
      "step": 8553
    },
    {
      "epoch": 2.0863414634146342,
      "grad_norm": 0.15313395857810974,
      "learning_rate": 1.0597739577709015e-05,
      "loss": 0.0258,
      "step": 8554
    },
    {
      "epoch": 2.0865853658536584,
      "grad_norm": 0.05521753802895546,
      "learning_rate": 1.0592520326963099e-05,
      "loss": 0.0162,
      "step": 8555
    },
    {
      "epoch": 2.086829268292683,
      "grad_norm": 0.1001216322183609,
      "learning_rate": 1.0587302016261127e-05,
      "loss": 0.013,
      "step": 8556
    },
    {
      "epoch": 2.087073170731707,
      "grad_norm": 0.14584128558635712,
      "learning_rate": 1.058208464594357e-05,
      "loss": 0.0115,
      "step": 8557
    },
    {
      "epoch": 2.087317073170732,
      "grad_norm": 0.1729348599910736,
      "learning_rate": 1.057686821635086e-05,
      "loss": 0.0308,
      "step": 8558
    },
    {
      "epoch": 2.087560975609756,
      "grad_norm": 0.26600000262260437,
      "learning_rate": 1.0571652727823344e-05,
      "loss": 0.0121,
      "step": 8559
    },
    {
      "epoch": 2.0878048780487806,
      "grad_norm": 0.13348513841629028,
      "learning_rate": 1.0566438180701313e-05,
      "loss": 0.0333,
      "step": 8560
    },
    {
      "epoch": 2.0880487804878047,
      "grad_norm": 0.14667245745658875,
      "learning_rate": 1.056122457532502e-05,
      "loss": 0.0109,
      "step": 8561
    },
    {
      "epoch": 2.0882926829268293,
      "grad_norm": 0.08949030190706253,
      "learning_rate": 1.0556011912034605e-05,
      "loss": 0.024,
      "step": 8562
    },
    {
      "epoch": 2.0885365853658535,
      "grad_norm": 0.09457484632730484,
      "learning_rate": 1.0550800191170204e-05,
      "loss": 0.0208,
      "step": 8563
    },
    {
      "epoch": 2.088780487804878,
      "grad_norm": 0.12154752016067505,
      "learning_rate": 1.0545589413071857e-05,
      "loss": 0.0093,
      "step": 8564
    },
    {
      "epoch": 2.0890243902439023,
      "grad_norm": 0.19380241632461548,
      "learning_rate": 1.0540379578079545e-05,
      "loss": 0.029,
      "step": 8565
    },
    {
      "epoch": 2.089268292682927,
      "grad_norm": 0.08329708874225616,
      "learning_rate": 1.0535170686533207e-05,
      "loss": 0.0106,
      "step": 8566
    },
    {
      "epoch": 2.089512195121951,
      "grad_norm": 0.10875437408685684,
      "learning_rate": 1.0529962738772692e-05,
      "loss": 0.0125,
      "step": 8567
    },
    {
      "epoch": 2.0897560975609757,
      "grad_norm": 0.09111461043357849,
      "learning_rate": 1.052475573513782e-05,
      "loss": 0.0162,
      "step": 8568
    },
    {
      "epoch": 2.09,
      "grad_norm": 0.10049090534448624,
      "learning_rate": 1.0519549675968327e-05,
      "loss": 0.0086,
      "step": 8569
    },
    {
      "epoch": 2.0902439024390245,
      "grad_norm": 0.0759999230504036,
      "learning_rate": 1.0514344561603875e-05,
      "loss": 0.0071,
      "step": 8570
    },
    {
      "epoch": 2.0904878048780486,
      "grad_norm": 0.122614786028862,
      "learning_rate": 1.0509140392384106e-05,
      "loss": 0.0144,
      "step": 8571
    },
    {
      "epoch": 2.0907317073170733,
      "grad_norm": 0.1000991016626358,
      "learning_rate": 1.0503937168648567e-05,
      "loss": 0.0204,
      "step": 8572
    },
    {
      "epoch": 2.0909756097560974,
      "grad_norm": 0.10873778164386749,
      "learning_rate": 1.0498734890736738e-05,
      "loss": 0.0242,
      "step": 8573
    },
    {
      "epoch": 2.091219512195122,
      "grad_norm": 0.17246443033218384,
      "learning_rate": 1.0493533558988078e-05,
      "loss": 0.0121,
      "step": 8574
    },
    {
      "epoch": 2.091463414634146,
      "grad_norm": 0.09736497700214386,
      "learning_rate": 1.0488333173741936e-05,
      "loss": 0.0109,
      "step": 8575
    },
    {
      "epoch": 2.091707317073171,
      "grad_norm": 0.0990142896771431,
      "learning_rate": 1.0483133735337636e-05,
      "loss": 0.0196,
      "step": 8576
    },
    {
      "epoch": 2.091951219512195,
      "grad_norm": 0.15596064925193787,
      "learning_rate": 1.047793524411442e-05,
      "loss": 0.0211,
      "step": 8577
    },
    {
      "epoch": 2.0921951219512196,
      "grad_norm": 0.11350682377815247,
      "learning_rate": 1.0472737700411475e-05,
      "loss": 0.0124,
      "step": 8578
    },
    {
      "epoch": 2.0924390243902438,
      "grad_norm": 0.06414935737848282,
      "learning_rate": 1.0467541104567921e-05,
      "loss": 0.0124,
      "step": 8579
    },
    {
      "epoch": 2.0926829268292684,
      "grad_norm": 0.19663292169570923,
      "learning_rate": 1.0462345456922812e-05,
      "loss": 0.0169,
      "step": 8580
    },
    {
      "epoch": 2.0929268292682925,
      "grad_norm": 0.11504774540662766,
      "learning_rate": 1.045715075781517e-05,
      "loss": 0.0164,
      "step": 8581
    },
    {
      "epoch": 2.093170731707317,
      "grad_norm": 0.12850138545036316,
      "learning_rate": 1.0451957007583918e-05,
      "loss": 0.0119,
      "step": 8582
    },
    {
      "epoch": 2.0934146341463413,
      "grad_norm": 0.10584525763988495,
      "learning_rate": 1.0446764206567927e-05,
      "loss": 0.0149,
      "step": 8583
    },
    {
      "epoch": 2.093658536585366,
      "grad_norm": 0.08355621993541718,
      "learning_rate": 1.044157235510603e-05,
      "loss": 0.0142,
      "step": 8584
    },
    {
      "epoch": 2.09390243902439,
      "grad_norm": 0.12123560160398483,
      "learning_rate": 1.043638145353697e-05,
      "loss": 0.0141,
      "step": 8585
    },
    {
      "epoch": 2.0941463414634147,
      "grad_norm": 0.12904539704322815,
      "learning_rate": 1.0431191502199425e-05,
      "loss": 0.0169,
      "step": 8586
    },
    {
      "epoch": 2.094390243902439,
      "grad_norm": 0.15237756073474884,
      "learning_rate": 1.0426002501432048e-05,
      "loss": 0.0217,
      "step": 8587
    },
    {
      "epoch": 2.0946341463414635,
      "grad_norm": 0.13164100050926208,
      "learning_rate": 1.042081445157338e-05,
      "loss": 0.0153,
      "step": 8588
    },
    {
      "epoch": 2.0948780487804877,
      "grad_norm": 0.12152640521526337,
      "learning_rate": 1.0415627352961946e-05,
      "loss": 0.0148,
      "step": 8589
    },
    {
      "epoch": 2.0951219512195123,
      "grad_norm": 0.1390538066625595,
      "learning_rate": 1.0410441205936183e-05,
      "loss": 0.025,
      "step": 8590
    },
    {
      "epoch": 2.0953658536585364,
      "grad_norm": 0.1364479660987854,
      "learning_rate": 1.0405256010834457e-05,
      "loss": 0.0135,
      "step": 8591
    },
    {
      "epoch": 2.095609756097561,
      "grad_norm": 0.09243956208229065,
      "learning_rate": 1.040007176799511e-05,
      "loss": 0.0096,
      "step": 8592
    },
    {
      "epoch": 2.0958536585365852,
      "grad_norm": 0.0958237573504448,
      "learning_rate": 1.039488847775638e-05,
      "loss": 0.0114,
      "step": 8593
    },
    {
      "epoch": 2.09609756097561,
      "grad_norm": 0.10543002188205719,
      "learning_rate": 1.0389706140456473e-05,
      "loss": 0.0207,
      "step": 8594
    },
    {
      "epoch": 2.096341463414634,
      "grad_norm": 0.1092691645026207,
      "learning_rate": 1.0384524756433508e-05,
      "loss": 0.0179,
      "step": 8595
    },
    {
      "epoch": 2.0965853658536586,
      "grad_norm": 0.1737116575241089,
      "learning_rate": 1.0379344326025556e-05,
      "loss": 0.0161,
      "step": 8596
    },
    {
      "epoch": 2.096829268292683,
      "grad_norm": 0.12240041047334671,
      "learning_rate": 1.0374164849570636e-05,
      "loss": 0.0239,
      "step": 8597
    },
    {
      "epoch": 2.0970731707317074,
      "grad_norm": 0.11113058775663376,
      "learning_rate": 1.0368986327406684e-05,
      "loss": 0.0105,
      "step": 8598
    },
    {
      "epoch": 2.0973170731707316,
      "grad_norm": 0.12755148112773895,
      "learning_rate": 1.0363808759871577e-05,
      "loss": 0.014,
      "step": 8599
    },
    {
      "epoch": 2.097560975609756,
      "grad_norm": 0.05009642243385315,
      "learning_rate": 1.0358632147303151e-05,
      "loss": 0.0052,
      "step": 8600
    },
    {
      "epoch": 2.0978048780487804,
      "grad_norm": 0.1183539628982544,
      "learning_rate": 1.0353456490039148e-05,
      "loss": 0.0103,
      "step": 8601
    },
    {
      "epoch": 2.098048780487805,
      "grad_norm": 0.04242946580052376,
      "learning_rate": 1.034828178841728e-05,
      "loss": 0.0099,
      "step": 8602
    },
    {
      "epoch": 2.098292682926829,
      "grad_norm": 0.14549310505390167,
      "learning_rate": 1.034310804277517e-05,
      "loss": 0.022,
      "step": 8603
    },
    {
      "epoch": 2.0985365853658537,
      "grad_norm": 0.06904097646474838,
      "learning_rate": 1.0337935253450385e-05,
      "loss": 0.0129,
      "step": 8604
    },
    {
      "epoch": 2.098780487804878,
      "grad_norm": 0.2115691751241684,
      "learning_rate": 1.0332763420780448e-05,
      "loss": 0.036,
      "step": 8605
    },
    {
      "epoch": 2.0990243902439025,
      "grad_norm": 0.08680788427591324,
      "learning_rate": 1.0327592545102795e-05,
      "loss": 0.0191,
      "step": 8606
    },
    {
      "epoch": 2.0992682926829267,
      "grad_norm": 0.07588999718427658,
      "learning_rate": 1.0322422626754804e-05,
      "loss": 0.01,
      "step": 8607
    },
    {
      "epoch": 2.0995121951219513,
      "grad_norm": 0.19781726598739624,
      "learning_rate": 1.0317253666073814e-05,
      "loss": 0.0208,
      "step": 8608
    },
    {
      "epoch": 2.0997560975609755,
      "grad_norm": 0.1500519961118698,
      "learning_rate": 1.0312085663397066e-05,
      "loss": 0.0244,
      "step": 8609
    },
    {
      "epoch": 2.1,
      "grad_norm": 0.07828958332538605,
      "learning_rate": 1.0306918619061774e-05,
      "loss": 0.0215,
      "step": 8610
    },
    {
      "epoch": 2.1002439024390243,
      "grad_norm": 0.13705088198184967,
      "learning_rate": 1.0301752533405069e-05,
      "loss": 0.0124,
      "step": 8611
    },
    {
      "epoch": 2.100487804878049,
      "grad_norm": 0.13161402940750122,
      "learning_rate": 1.0296587406764e-05,
      "loss": 0.0233,
      "step": 8612
    },
    {
      "epoch": 2.100731707317073,
      "grad_norm": 0.23974525928497314,
      "learning_rate": 1.0291423239475598e-05,
      "loss": 0.0079,
      "step": 8613
    },
    {
      "epoch": 2.1009756097560977,
      "grad_norm": 0.1429370641708374,
      "learning_rate": 1.0286260031876793e-05,
      "loss": 0.0185,
      "step": 8614
    },
    {
      "epoch": 2.101219512195122,
      "grad_norm": 0.08831188827753067,
      "learning_rate": 1.0281097784304488e-05,
      "loss": 0.0152,
      "step": 8615
    },
    {
      "epoch": 2.1014634146341464,
      "grad_norm": 0.10302034020423889,
      "learning_rate": 1.0275936497095493e-05,
      "loss": 0.0138,
      "step": 8616
    },
    {
      "epoch": 2.1017073170731706,
      "grad_norm": 0.1261437088251114,
      "learning_rate": 1.0270776170586558e-05,
      "loss": 0.0184,
      "step": 8617
    },
    {
      "epoch": 2.101951219512195,
      "grad_norm": 0.14012573659420013,
      "learning_rate": 1.0265616805114397e-05,
      "loss": 0.0177,
      "step": 8618
    },
    {
      "epoch": 2.1021951219512194,
      "grad_norm": 0.06705991178750992,
      "learning_rate": 1.0260458401015632e-05,
      "loss": 0.0139,
      "step": 8619
    },
    {
      "epoch": 2.102439024390244,
      "grad_norm": 0.06644223630428314,
      "learning_rate": 1.0255300958626826e-05,
      "loss": 0.0125,
      "step": 8620
    },
    {
      "epoch": 2.102682926829268,
      "grad_norm": 0.10586201399564743,
      "learning_rate": 1.0250144478284505e-05,
      "loss": 0.0162,
      "step": 8621
    },
    {
      "epoch": 2.1029268292682928,
      "grad_norm": 0.0889517068862915,
      "learning_rate": 1.0244988960325091e-05,
      "loss": 0.0118,
      "step": 8622
    },
    {
      "epoch": 2.103170731707317,
      "grad_norm": 0.09638392925262451,
      "learning_rate": 1.0239834405084992e-05,
      "loss": 0.0141,
      "step": 8623
    },
    {
      "epoch": 2.1034146341463416,
      "grad_norm": 0.05591202154755592,
      "learning_rate": 1.0234680812900507e-05,
      "loss": 0.0091,
      "step": 8624
    },
    {
      "epoch": 2.1036585365853657,
      "grad_norm": 0.1331881284713745,
      "learning_rate": 1.0229528184107892e-05,
      "loss": 0.0269,
      "step": 8625
    },
    {
      "epoch": 2.1039024390243903,
      "grad_norm": 0.11931470781564713,
      "learning_rate": 1.0224376519043354e-05,
      "loss": 0.03,
      "step": 8626
    },
    {
      "epoch": 2.1041463414634145,
      "grad_norm": 0.10530375689268112,
      "learning_rate": 1.0219225818043018e-05,
      "loss": 0.0212,
      "step": 8627
    },
    {
      "epoch": 2.104390243902439,
      "grad_norm": 0.12296150624752045,
      "learning_rate": 1.0214076081442947e-05,
      "loss": 0.0208,
      "step": 8628
    },
    {
      "epoch": 2.1046341463414633,
      "grad_norm": 0.08549429476261139,
      "learning_rate": 1.020892730957915e-05,
      "loss": 0.0151,
      "step": 8629
    },
    {
      "epoch": 2.104878048780488,
      "grad_norm": 0.0981382206082344,
      "learning_rate": 1.0203779502787556e-05,
      "loss": 0.0109,
      "step": 8630
    },
    {
      "epoch": 2.105121951219512,
      "grad_norm": 0.1257721334695816,
      "learning_rate": 1.0198632661404063e-05,
      "loss": 0.0169,
      "step": 8631
    },
    {
      "epoch": 2.1053658536585367,
      "grad_norm": 0.22445537149906158,
      "learning_rate": 1.0193486785764476e-05,
      "loss": 0.0393,
      "step": 8632
    },
    {
      "epoch": 2.105609756097561,
      "grad_norm": 0.16226498782634735,
      "learning_rate": 1.0188341876204537e-05,
      "loss": 0.0214,
      "step": 8633
    },
    {
      "epoch": 2.1058536585365855,
      "grad_norm": 0.11909732222557068,
      "learning_rate": 1.0183197933059963e-05,
      "loss": 0.0096,
      "step": 8634
    },
    {
      "epoch": 2.1060975609756096,
      "grad_norm": 0.13152232766151428,
      "learning_rate": 1.0178054956666355e-05,
      "loss": 0.0211,
      "step": 8635
    },
    {
      "epoch": 2.1063414634146342,
      "grad_norm": 0.1801990270614624,
      "learning_rate": 1.0172912947359295e-05,
      "loss": 0.0266,
      "step": 8636
    },
    {
      "epoch": 2.1065853658536584,
      "grad_norm": 0.11838638782501221,
      "learning_rate": 1.0167771905474274e-05,
      "loss": 0.0169,
      "step": 8637
    },
    {
      "epoch": 2.106829268292683,
      "grad_norm": 0.23013198375701904,
      "learning_rate": 1.0162631831346722e-05,
      "loss": 0.0132,
      "step": 8638
    },
    {
      "epoch": 2.107073170731707,
      "grad_norm": 0.10410768538713455,
      "learning_rate": 1.0157492725312031e-05,
      "loss": 0.0162,
      "step": 8639
    },
    {
      "epoch": 2.107317073170732,
      "grad_norm": 0.47920170426368713,
      "learning_rate": 1.0152354587705501e-05,
      "loss": 0.0273,
      "step": 8640
    },
    {
      "epoch": 2.107560975609756,
      "grad_norm": 0.10603947937488556,
      "learning_rate": 1.0147217418862373e-05,
      "loss": 0.0045,
      "step": 8641
    },
    {
      "epoch": 2.1078048780487806,
      "grad_norm": 0.15151503682136536,
      "learning_rate": 1.0142081219117849e-05,
      "loss": 0.0209,
      "step": 8642
    },
    {
      "epoch": 2.1080487804878048,
      "grad_norm": 0.10845113545656204,
      "learning_rate": 1.0136945988807043e-05,
      "loss": 0.0185,
      "step": 8643
    },
    {
      "epoch": 2.1082926829268294,
      "grad_norm": 0.07474680244922638,
      "learning_rate": 1.0131811728265008e-05,
      "loss": 0.0046,
      "step": 8644
    },
    {
      "epoch": 2.1085365853658535,
      "grad_norm": 0.05772049352526665,
      "learning_rate": 1.0126678437826742e-05,
      "loss": 0.0057,
      "step": 8645
    },
    {
      "epoch": 2.108780487804878,
      "grad_norm": 0.1067754253745079,
      "learning_rate": 1.0121546117827169e-05,
      "loss": 0.0126,
      "step": 8646
    },
    {
      "epoch": 2.1090243902439023,
      "grad_norm": 0.1565428227186203,
      "learning_rate": 1.0116414768601171e-05,
      "loss": 0.0266,
      "step": 8647
    },
    {
      "epoch": 2.109268292682927,
      "grad_norm": 0.15814580023288727,
      "learning_rate": 1.0111284390483541e-05,
      "loss": 0.0243,
      "step": 8648
    },
    {
      "epoch": 2.109512195121951,
      "grad_norm": 0.09404517710208893,
      "learning_rate": 1.0106154983809033e-05,
      "loss": 0.0167,
      "step": 8649
    },
    {
      "epoch": 2.1097560975609757,
      "grad_norm": 0.1318933069705963,
      "learning_rate": 1.0101026548912315e-05,
      "loss": 0.019,
      "step": 8650
    },
    {
      "epoch": 2.11,
      "grad_norm": 0.11312102526426315,
      "learning_rate": 1.0095899086127999e-05,
      "loss": 0.0399,
      "step": 8651
    },
    {
      "epoch": 2.1102439024390245,
      "grad_norm": 0.0649651437997818,
      "learning_rate": 1.0090772595790651e-05,
      "loss": 0.0102,
      "step": 8652
    },
    {
      "epoch": 2.1104878048780487,
      "grad_norm": 0.13679523766040802,
      "learning_rate": 1.0085647078234747e-05,
      "loss": 0.019,
      "step": 8653
    },
    {
      "epoch": 2.1107317073170733,
      "grad_norm": 0.10489880293607712,
      "learning_rate": 1.0080522533794706e-05,
      "loss": 0.0197,
      "step": 8654
    },
    {
      "epoch": 2.1109756097560974,
      "grad_norm": 0.12707826495170593,
      "learning_rate": 1.0075398962804907e-05,
      "loss": 0.0349,
      "step": 8655
    },
    {
      "epoch": 2.111219512195122,
      "grad_norm": 0.14825566112995148,
      "learning_rate": 1.0070276365599637e-05,
      "loss": 0.0118,
      "step": 8656
    },
    {
      "epoch": 2.111463414634146,
      "grad_norm": 0.0953698605298996,
      "learning_rate": 1.006515474251312e-05,
      "loss": 0.0126,
      "step": 8657
    },
    {
      "epoch": 2.111707317073171,
      "grad_norm": 0.17366555333137512,
      "learning_rate": 1.0060034093879545e-05,
      "loss": 0.0185,
      "step": 8658
    },
    {
      "epoch": 2.111951219512195,
      "grad_norm": 0.0812801644206047,
      "learning_rate": 1.0054914420033e-05,
      "loss": 0.0116,
      "step": 8659
    },
    {
      "epoch": 2.1121951219512196,
      "grad_norm": 0.19344362616539001,
      "learning_rate": 1.0049795721307558e-05,
      "loss": 0.0257,
      "step": 8660
    },
    {
      "epoch": 2.112439024390244,
      "grad_norm": 0.12090317159891129,
      "learning_rate": 1.004467799803716e-05,
      "loss": 0.0123,
      "step": 8661
    },
    {
      "epoch": 2.1126829268292684,
      "grad_norm": 0.20574836432933807,
      "learning_rate": 1.0039561250555749e-05,
      "loss": 0.0156,
      "step": 8662
    },
    {
      "epoch": 2.1129268292682926,
      "grad_norm": 0.0869661420583725,
      "learning_rate": 1.0034445479197167e-05,
      "loss": 0.0209,
      "step": 8663
    },
    {
      "epoch": 2.113170731707317,
      "grad_norm": 0.1572262942790985,
      "learning_rate": 1.0029330684295193e-05,
      "loss": 0.023,
      "step": 8664
    },
    {
      "epoch": 2.1134146341463413,
      "grad_norm": 0.14241014420986176,
      "learning_rate": 1.0024216866183573e-05,
      "loss": 0.018,
      "step": 8665
    },
    {
      "epoch": 2.113658536585366,
      "grad_norm": 0.08381733298301697,
      "learning_rate": 1.0019104025195959e-05,
      "loss": 0.0129,
      "step": 8666
    },
    {
      "epoch": 2.11390243902439,
      "grad_norm": 0.13711516559123993,
      "learning_rate": 1.0013992161665934e-05,
      "loss": 0.0224,
      "step": 8667
    },
    {
      "epoch": 2.1141463414634147,
      "grad_norm": 0.10479168593883514,
      "learning_rate": 1.0008881275927054e-05,
      "loss": 0.0119,
      "step": 8668
    },
    {
      "epoch": 2.114390243902439,
      "grad_norm": 0.13694292306900024,
      "learning_rate": 1.0003771368312781e-05,
      "loss": 0.0189,
      "step": 8669
    },
    {
      "epoch": 2.1146341463414635,
      "grad_norm": 0.08455769717693329,
      "learning_rate": 9.998662439156506e-06,
      "loss": 0.0248,
      "step": 8670
    },
    {
      "epoch": 2.1148780487804877,
      "grad_norm": 0.11148799955844879,
      "learning_rate": 9.993554488791593e-06,
      "loss": 0.0175,
      "step": 8671
    },
    {
      "epoch": 2.1151219512195123,
      "grad_norm": 0.07134751975536346,
      "learning_rate": 9.988447517551303e-06,
      "loss": 0.0082,
      "step": 8672
    },
    {
      "epoch": 2.1153658536585365,
      "grad_norm": 0.19101868569850922,
      "learning_rate": 9.983341525768867e-06,
      "loss": 0.0204,
      "step": 8673
    },
    {
      "epoch": 2.115609756097561,
      "grad_norm": 0.11749836802482605,
      "learning_rate": 9.978236513777423e-06,
      "loss": 0.0101,
      "step": 8674
    },
    {
      "epoch": 2.1158536585365852,
      "grad_norm": 0.11832483857870102,
      "learning_rate": 9.973132481910055e-06,
      "loss": 0.0078,
      "step": 8675
    },
    {
      "epoch": 2.11609756097561,
      "grad_norm": 0.05948634445667267,
      "learning_rate": 9.968029430499797e-06,
      "loss": 0.0081,
      "step": 8676
    },
    {
      "epoch": 2.116341463414634,
      "grad_norm": 0.1662699282169342,
      "learning_rate": 9.962927359879603e-06,
      "loss": 0.0162,
      "step": 8677
    },
    {
      "epoch": 2.1165853658536586,
      "grad_norm": 0.1230008602142334,
      "learning_rate": 9.957826270382364e-06,
      "loss": 0.0205,
      "step": 8678
    },
    {
      "epoch": 2.116829268292683,
      "grad_norm": 0.0906670019030571,
      "learning_rate": 9.952726162340913e-06,
      "loss": 0.0131,
      "step": 8679
    },
    {
      "epoch": 2.1170731707317074,
      "grad_norm": 0.1232997477054596,
      "learning_rate": 9.947627036088008e-06,
      "loss": 0.0227,
      "step": 8680
    },
    {
      "epoch": 2.1173170731707316,
      "grad_norm": 0.0966453105211258,
      "learning_rate": 9.942528891956371e-06,
      "loss": 0.0132,
      "step": 8681
    },
    {
      "epoch": 2.117560975609756,
      "grad_norm": 0.2324530929327011,
      "learning_rate": 9.937431730278624e-06,
      "loss": 0.0222,
      "step": 8682
    },
    {
      "epoch": 2.1178048780487804,
      "grad_norm": 0.1476486474275589,
      "learning_rate": 9.932335551387336e-06,
      "loss": 0.0205,
      "step": 8683
    },
    {
      "epoch": 2.118048780487805,
      "grad_norm": 0.05424924194812775,
      "learning_rate": 9.92724035561504e-06,
      "loss": 0.0106,
      "step": 8684
    },
    {
      "epoch": 2.118292682926829,
      "grad_norm": 0.08186474442481995,
      "learning_rate": 9.922146143294156e-06,
      "loss": 0.0201,
      "step": 8685
    },
    {
      "epoch": 2.1185365853658538,
      "grad_norm": 0.0695989578962326,
      "learning_rate": 9.917052914757088e-06,
      "loss": 0.0123,
      "step": 8686
    },
    {
      "epoch": 2.118780487804878,
      "grad_norm": 0.11621386557817459,
      "learning_rate": 9.911960670336145e-06,
      "loss": 0.0148,
      "step": 8687
    },
    {
      "epoch": 2.1190243902439025,
      "grad_norm": 0.11148153990507126,
      "learning_rate": 9.90686941036357e-06,
      "loss": 0.0124,
      "step": 8688
    },
    {
      "epoch": 2.1192682926829267,
      "grad_norm": 0.1373412013053894,
      "learning_rate": 9.90177913517157e-06,
      "loss": 0.0139,
      "step": 8689
    },
    {
      "epoch": 2.1195121951219513,
      "grad_norm": 0.10049781948328018,
      "learning_rate": 9.896689845092264e-06,
      "loss": 0.0106,
      "step": 8690
    },
    {
      "epoch": 2.1197560975609755,
      "grad_norm": 0.09764807671308517,
      "learning_rate": 9.891601540457698e-06,
      "loss": 0.0149,
      "step": 8691
    },
    {
      "epoch": 2.12,
      "grad_norm": 0.13887432217597961,
      "learning_rate": 9.886514221599892e-06,
      "loss": 0.0137,
      "step": 8692
    },
    {
      "epoch": 2.1202439024390243,
      "grad_norm": 0.10151087492704391,
      "learning_rate": 9.881427888850764e-06,
      "loss": 0.0161,
      "step": 8693
    },
    {
      "epoch": 2.120487804878049,
      "grad_norm": 0.15462300181388855,
      "learning_rate": 9.876342542542183e-06,
      "loss": 0.0215,
      "step": 8694
    },
    {
      "epoch": 2.120731707317073,
      "grad_norm": 0.1622929871082306,
      "learning_rate": 9.871258183005954e-06,
      "loss": 0.0343,
      "step": 8695
    },
    {
      "epoch": 2.1209756097560977,
      "grad_norm": 0.15419328212738037,
      "learning_rate": 9.866174810573806e-06,
      "loss": 0.0124,
      "step": 8696
    },
    {
      "epoch": 2.121219512195122,
      "grad_norm": 0.12271302193403244,
      "learning_rate": 9.861092425577431e-06,
      "loss": 0.0168,
      "step": 8697
    },
    {
      "epoch": 2.1214634146341464,
      "grad_norm": 0.06775876879692078,
      "learning_rate": 9.856011028348424e-06,
      "loss": 0.0167,
      "step": 8698
    },
    {
      "epoch": 2.1217073170731706,
      "grad_norm": 0.08602989464998245,
      "learning_rate": 9.850930619218343e-06,
      "loss": 0.0248,
      "step": 8699
    },
    {
      "epoch": 2.1219512195121952,
      "grad_norm": 0.05539269000291824,
      "learning_rate": 9.845851198518666e-06,
      "loss": 0.0136,
      "step": 8700
    },
    {
      "epoch": 2.1221951219512194,
      "grad_norm": 0.11782719194889069,
      "learning_rate": 9.840772766580796e-06,
      "loss": 0.0151,
      "step": 8701
    },
    {
      "epoch": 2.122439024390244,
      "grad_norm": 0.20056870579719543,
      "learning_rate": 9.835695323736107e-06,
      "loss": 0.0138,
      "step": 8702
    },
    {
      "epoch": 2.122682926829268,
      "grad_norm": 0.06248609721660614,
      "learning_rate": 9.830618870315875e-06,
      "loss": 0.0111,
      "step": 8703
    },
    {
      "epoch": 2.122926829268293,
      "grad_norm": 0.12894445657730103,
      "learning_rate": 9.825543406651313e-06,
      "loss": 0.0172,
      "step": 8704
    },
    {
      "epoch": 2.123170731707317,
      "grad_norm": 0.11061181128025055,
      "learning_rate": 9.820468933073601e-06,
      "loss": 0.0099,
      "step": 8705
    },
    {
      "epoch": 2.1234146341463416,
      "grad_norm": 0.092572420835495,
      "learning_rate": 9.815395449913814e-06,
      "loss": 0.0185,
      "step": 8706
    },
    {
      "epoch": 2.1236585365853657,
      "grad_norm": 0.22183826565742493,
      "learning_rate": 9.810322957502996e-06,
      "loss": 0.0124,
      "step": 8707
    },
    {
      "epoch": 2.1239024390243904,
      "grad_norm": 0.06806153059005737,
      "learning_rate": 9.805251456172108e-06,
      "loss": 0.0054,
      "step": 8708
    },
    {
      "epoch": 2.1241463414634145,
      "grad_norm": 0.06988225877285004,
      "learning_rate": 9.800180946252035e-06,
      "loss": 0.0145,
      "step": 8709
    },
    {
      "epoch": 2.124390243902439,
      "grad_norm": 0.06740612536668777,
      "learning_rate": 9.795111428073645e-06,
      "loss": 0.0099,
      "step": 8710
    },
    {
      "epoch": 2.1246341463414633,
      "grad_norm": 0.13196389377117157,
      "learning_rate": 9.790042901967666e-06,
      "loss": 0.0158,
      "step": 8711
    },
    {
      "epoch": 2.124878048780488,
      "grad_norm": 0.1615900844335556,
      "learning_rate": 9.784975368264837e-06,
      "loss": 0.0155,
      "step": 8712
    },
    {
      "epoch": 2.125121951219512,
      "grad_norm": 0.16617345809936523,
      "learning_rate": 9.779908827295787e-06,
      "loss": 0.0174,
      "step": 8713
    },
    {
      "epoch": 2.1253658536585367,
      "grad_norm": 0.08742024749517441,
      "learning_rate": 9.774843279391086e-06,
      "loss": 0.0078,
      "step": 8714
    },
    {
      "epoch": 2.125609756097561,
      "grad_norm": 0.0633724182844162,
      "learning_rate": 9.769778724881256e-06,
      "loss": 0.0149,
      "step": 8715
    },
    {
      "epoch": 2.1258536585365855,
      "grad_norm": 0.12580275535583496,
      "learning_rate": 9.764715164096744e-06,
      "loss": 0.0244,
      "step": 8716
    },
    {
      "epoch": 2.1260975609756096,
      "grad_norm": 0.170425683259964,
      "learning_rate": 9.759652597367919e-06,
      "loss": 0.0214,
      "step": 8717
    },
    {
      "epoch": 2.1263414634146343,
      "grad_norm": 0.08566675335168839,
      "learning_rate": 9.754591025025114e-06,
      "loss": 0.0084,
      "step": 8718
    },
    {
      "epoch": 2.1265853658536584,
      "grad_norm": 0.09119120985269547,
      "learning_rate": 9.749530447398567e-06,
      "loss": 0.01,
      "step": 8719
    },
    {
      "epoch": 2.126829268292683,
      "grad_norm": 0.14433394372463226,
      "learning_rate": 9.74447086481848e-06,
      "loss": 0.016,
      "step": 8720
    },
    {
      "epoch": 2.127073170731707,
      "grad_norm": 0.08486876636743546,
      "learning_rate": 9.739412277614967e-06,
      "loss": 0.0144,
      "step": 8721
    },
    {
      "epoch": 2.127317073170732,
      "grad_norm": 0.06386583298444748,
      "learning_rate": 9.734354686118077e-06,
      "loss": 0.0101,
      "step": 8722
    },
    {
      "epoch": 2.127560975609756,
      "grad_norm": 0.057685840874910355,
      "learning_rate": 9.72929809065782e-06,
      "loss": 0.009,
      "step": 8723
    },
    {
      "epoch": 2.1278048780487806,
      "grad_norm": 0.09390756487846375,
      "learning_rate": 9.724242491564115e-06,
      "loss": 0.0163,
      "step": 8724
    },
    {
      "epoch": 2.1280487804878048,
      "grad_norm": 0.10238135606050491,
      "learning_rate": 9.719187889166813e-06,
      "loss": 0.0183,
      "step": 8725
    },
    {
      "epoch": 2.1282926829268294,
      "grad_norm": 0.11516198515892029,
      "learning_rate": 9.714134283795745e-06,
      "loss": 0.0141,
      "step": 8726
    },
    {
      "epoch": 2.1285365853658536,
      "grad_norm": 0.1947607696056366,
      "learning_rate": 9.709081675780598e-06,
      "loss": 0.0305,
      "step": 8727
    },
    {
      "epoch": 2.128780487804878,
      "grad_norm": 0.11223536729812622,
      "learning_rate": 9.704030065451072e-06,
      "loss": 0.0246,
      "step": 8728
    },
    {
      "epoch": 2.1290243902439023,
      "grad_norm": 0.08612097799777985,
      "learning_rate": 9.698979453136759e-06,
      "loss": 0.005,
      "step": 8729
    },
    {
      "epoch": 2.129268292682927,
      "grad_norm": 0.1396549791097641,
      "learning_rate": 9.693929839167187e-06,
      "loss": 0.0144,
      "step": 8730
    },
    {
      "epoch": 2.129512195121951,
      "grad_norm": 0.06787741184234619,
      "learning_rate": 9.688881223871846e-06,
      "loss": 0.0078,
      "step": 8731
    },
    {
      "epoch": 2.1297560975609757,
      "grad_norm": 0.16568365693092346,
      "learning_rate": 9.683833607580124e-06,
      "loss": 0.0238,
      "step": 8732
    },
    {
      "epoch": 2.13,
      "grad_norm": 0.09320342540740967,
      "learning_rate": 9.678786990621377e-06,
      "loss": 0.0119,
      "step": 8733
    },
    {
      "epoch": 2.1302439024390245,
      "grad_norm": 0.10512956976890564,
      "learning_rate": 9.67374137332488e-06,
      "loss": 0.019,
      "step": 8734
    },
    {
      "epoch": 2.1304878048780487,
      "grad_norm": 0.2376154065132141,
      "learning_rate": 9.668696756019832e-06,
      "loss": 0.0185,
      "step": 8735
    },
    {
      "epoch": 2.1307317073170733,
      "grad_norm": 0.07292629778385162,
      "learning_rate": 9.663653139035395e-06,
      "loss": 0.0234,
      "step": 8736
    },
    {
      "epoch": 2.1309756097560975,
      "grad_norm": 0.21797364950180054,
      "learning_rate": 9.65861052270064e-06,
      "loss": 0.0454,
      "step": 8737
    },
    {
      "epoch": 2.131219512195122,
      "grad_norm": 0.11762750148773193,
      "learning_rate": 9.653568907344576e-06,
      "loss": 0.0173,
      "step": 8738
    },
    {
      "epoch": 2.1314634146341462,
      "grad_norm": 0.06647967547178268,
      "learning_rate": 9.648528293296169e-06,
      "loss": 0.0082,
      "step": 8739
    },
    {
      "epoch": 2.131707317073171,
      "grad_norm": 0.11593639850616455,
      "learning_rate": 9.643488680884288e-06,
      "loss": 0.011,
      "step": 8740
    },
    {
      "epoch": 2.131951219512195,
      "grad_norm": 0.09696371853351593,
      "learning_rate": 9.638450070437768e-06,
      "loss": 0.0131,
      "step": 8741
    },
    {
      "epoch": 2.1321951219512196,
      "grad_norm": 0.05132799223065376,
      "learning_rate": 9.633412462285354e-06,
      "loss": 0.0098,
      "step": 8742
    },
    {
      "epoch": 2.132439024390244,
      "grad_norm": 0.06770481914281845,
      "learning_rate": 9.628375856755736e-06,
      "loss": 0.0094,
      "step": 8743
    },
    {
      "epoch": 2.1326829268292684,
      "grad_norm": 0.17343677580356598,
      "learning_rate": 9.623340254177538e-06,
      "loss": 0.0367,
      "step": 8744
    },
    {
      "epoch": 2.1329268292682926,
      "grad_norm": 0.05777456983923912,
      "learning_rate": 9.618305654879306e-06,
      "loss": 0.0071,
      "step": 8745
    },
    {
      "epoch": 2.133170731707317,
      "grad_norm": 0.12029559910297394,
      "learning_rate": 9.61327205918955e-06,
      "loss": 0.0119,
      "step": 8746
    },
    {
      "epoch": 2.1334146341463414,
      "grad_norm": 0.12642094492912292,
      "learning_rate": 9.60823946743669e-06,
      "loss": 0.0142,
      "step": 8747
    },
    {
      "epoch": 2.133658536585366,
      "grad_norm": 0.14981532096862793,
      "learning_rate": 9.603207879949075e-06,
      "loss": 0.033,
      "step": 8748
    },
    {
      "epoch": 2.13390243902439,
      "grad_norm": 0.11779554933309555,
      "learning_rate": 9.598177297055022e-06,
      "loss": 0.0326,
      "step": 8749
    },
    {
      "epoch": 2.1341463414634148,
      "grad_norm": 0.12646345794200897,
      "learning_rate": 9.59314771908275e-06,
      "loss": 0.0221,
      "step": 8750
    },
    {
      "epoch": 2.134390243902439,
      "grad_norm": 0.10847297310829163,
      "learning_rate": 9.58811914636042e-06,
      "loss": 0.0129,
      "step": 8751
    },
    {
      "epoch": 2.1346341463414635,
      "grad_norm": 0.07702083140611649,
      "learning_rate": 9.583091579216139e-06,
      "loss": 0.0085,
      "step": 8752
    },
    {
      "epoch": 2.1348780487804877,
      "grad_norm": 0.14428803324699402,
      "learning_rate": 9.57806501797793e-06,
      "loss": 0.0192,
      "step": 8753
    },
    {
      "epoch": 2.1351219512195123,
      "grad_norm": 0.0937100499868393,
      "learning_rate": 9.573039462973777e-06,
      "loss": 0.0189,
      "step": 8754
    },
    {
      "epoch": 2.1353658536585365,
      "grad_norm": 0.09882843494415283,
      "learning_rate": 9.56801491453157e-06,
      "loss": 0.0123,
      "step": 8755
    },
    {
      "epoch": 2.135609756097561,
      "grad_norm": 0.14844250679016113,
      "learning_rate": 9.56299137297914e-06,
      "loss": 0.0175,
      "step": 8756
    },
    {
      "epoch": 2.1358536585365853,
      "grad_norm": 0.08031979948282242,
      "learning_rate": 9.557968838644277e-06,
      "loss": 0.0146,
      "step": 8757
    },
    {
      "epoch": 2.13609756097561,
      "grad_norm": 0.10366586595773697,
      "learning_rate": 9.552947311854673e-06,
      "loss": 0.0173,
      "step": 8758
    },
    {
      "epoch": 2.136341463414634,
      "grad_norm": 0.0739946961402893,
      "learning_rate": 9.547926792937969e-06,
      "loss": 0.012,
      "step": 8759
    },
    {
      "epoch": 2.1365853658536587,
      "grad_norm": 0.16094329953193665,
      "learning_rate": 9.54290728222174e-06,
      "loss": 0.0247,
      "step": 8760
    },
    {
      "epoch": 2.136829268292683,
      "grad_norm": 0.14590999484062195,
      "learning_rate": 9.537888780033479e-06,
      "loss": 0.018,
      "step": 8761
    },
    {
      "epoch": 2.1370731707317074,
      "grad_norm": 0.0821959599852562,
      "learning_rate": 9.532871286700653e-06,
      "loss": 0.0161,
      "step": 8762
    },
    {
      "epoch": 2.1373170731707316,
      "grad_norm": 0.12681056559085846,
      "learning_rate": 9.527854802550626e-06,
      "loss": 0.0244,
      "step": 8763
    },
    {
      "epoch": 2.137560975609756,
      "grad_norm": 0.10079125314950943,
      "learning_rate": 9.522839327910699e-06,
      "loss": 0.0097,
      "step": 8764
    },
    {
      "epoch": 2.1378048780487804,
      "grad_norm": 0.12236915528774261,
      "learning_rate": 9.517824863108135e-06,
      "loss": 0.0262,
      "step": 8765
    },
    {
      "epoch": 2.138048780487805,
      "grad_norm": 0.14173036813735962,
      "learning_rate": 9.512811408470094e-06,
      "loss": 0.0246,
      "step": 8766
    },
    {
      "epoch": 2.138292682926829,
      "grad_norm": 0.16131816804409027,
      "learning_rate": 9.507798964323705e-06,
      "loss": 0.0063,
      "step": 8767
    },
    {
      "epoch": 2.138536585365854,
      "grad_norm": 0.10915251076221466,
      "learning_rate": 9.502787530996011e-06,
      "loss": 0.015,
      "step": 8768
    },
    {
      "epoch": 2.138780487804878,
      "grad_norm": 0.1128152534365654,
      "learning_rate": 9.497777108813977e-06,
      "loss": 0.0239,
      "step": 8769
    },
    {
      "epoch": 2.1390243902439026,
      "grad_norm": 0.0848076120018959,
      "learning_rate": 9.492767698104541e-06,
      "loss": 0.0223,
      "step": 8770
    },
    {
      "epoch": 2.1392682926829267,
      "grad_norm": 0.15247870981693268,
      "learning_rate": 9.48775929919454e-06,
      "loss": 0.0242,
      "step": 8771
    },
    {
      "epoch": 2.1395121951219513,
      "grad_norm": 0.07350750267505646,
      "learning_rate": 9.482751912410748e-06,
      "loss": 0.0129,
      "step": 8772
    },
    {
      "epoch": 2.1397560975609755,
      "grad_norm": 0.11428552865982056,
      "learning_rate": 9.4777455380799e-06,
      "loss": 0.0194,
      "step": 8773
    },
    {
      "epoch": 2.14,
      "grad_norm": 0.12200773507356644,
      "learning_rate": 9.472740176528635e-06,
      "loss": 0.0216,
      "step": 8774
    },
    {
      "epoch": 2.1402439024390243,
      "grad_norm": 0.07994253933429718,
      "learning_rate": 9.467735828083534e-06,
      "loss": 0.0149,
      "step": 8775
    },
    {
      "epoch": 2.140487804878049,
      "grad_norm": 0.11030402034521103,
      "learning_rate": 9.462732493071139e-06,
      "loss": 0.0185,
      "step": 8776
    },
    {
      "epoch": 2.140731707317073,
      "grad_norm": 0.13175764679908752,
      "learning_rate": 9.457730171817867e-06,
      "loss": 0.0099,
      "step": 8777
    },
    {
      "epoch": 2.1409756097560977,
      "grad_norm": 0.08557973057031631,
      "learning_rate": 9.452728864650128e-06,
      "loss": 0.0168,
      "step": 8778
    },
    {
      "epoch": 2.141219512195122,
      "grad_norm": 0.07114949822425842,
      "learning_rate": 9.44772857189423e-06,
      "loss": 0.0083,
      "step": 8779
    },
    {
      "epoch": 2.1414634146341465,
      "grad_norm": 0.14671270549297333,
      "learning_rate": 9.44272929387644e-06,
      "loss": 0.0179,
      "step": 8780
    },
    {
      "epoch": 2.1417073170731706,
      "grad_norm": 0.1290215402841568,
      "learning_rate": 9.43773103092294e-06,
      "loss": 0.0173,
      "step": 8781
    },
    {
      "epoch": 2.1419512195121952,
      "grad_norm": 0.16218191385269165,
      "learning_rate": 9.432733783359838e-06,
      "loss": 0.0109,
      "step": 8782
    },
    {
      "epoch": 2.1421951219512194,
      "grad_norm": 0.05841510370373726,
      "learning_rate": 9.42773755151321e-06,
      "loss": 0.0051,
      "step": 8783
    },
    {
      "epoch": 2.142439024390244,
      "grad_norm": 0.1817552000284195,
      "learning_rate": 9.422742335709037e-06,
      "loss": 0.0218,
      "step": 8784
    },
    {
      "epoch": 2.142682926829268,
      "grad_norm": 0.09414135664701462,
      "learning_rate": 9.41774813627323e-06,
      "loss": 0.0199,
      "step": 8785
    },
    {
      "epoch": 2.142926829268293,
      "grad_norm": 0.1478199064731598,
      "learning_rate": 9.412754953531663e-06,
      "loss": 0.0162,
      "step": 8786
    },
    {
      "epoch": 2.143170731707317,
      "grad_norm": 0.0809248685836792,
      "learning_rate": 9.40776278781012e-06,
      "loss": 0.0272,
      "step": 8787
    },
    {
      "epoch": 2.1434146341463416,
      "grad_norm": 0.14812378585338593,
      "learning_rate": 9.402771639434313e-06,
      "loss": 0.0137,
      "step": 8788
    },
    {
      "epoch": 2.1436585365853658,
      "grad_norm": 0.08100148290395737,
      "learning_rate": 9.397781508729919e-06,
      "loss": 0.0122,
      "step": 8789
    },
    {
      "epoch": 2.1439024390243904,
      "grad_norm": 0.12496204674243927,
      "learning_rate": 9.39279239602251e-06,
      "loss": 0.0289,
      "step": 8790
    },
    {
      "epoch": 2.1441463414634145,
      "grad_norm": 0.12803921103477478,
      "learning_rate": 9.387804301637626e-06,
      "loss": 0.0185,
      "step": 8791
    },
    {
      "epoch": 2.144390243902439,
      "grad_norm": 0.08866224437952042,
      "learning_rate": 9.38281722590072e-06,
      "loss": 0.0134,
      "step": 8792
    },
    {
      "epoch": 2.1446341463414633,
      "grad_norm": 0.24836188554763794,
      "learning_rate": 9.377831169137177e-06,
      "loss": 0.0247,
      "step": 8793
    },
    {
      "epoch": 2.144878048780488,
      "grad_norm": 0.1090719923377037,
      "learning_rate": 9.37284613167233e-06,
      "loss": 0.0182,
      "step": 8794
    },
    {
      "epoch": 2.145121951219512,
      "grad_norm": 0.1258758157491684,
      "learning_rate": 9.367862113831424e-06,
      "loss": 0.0208,
      "step": 8795
    },
    {
      "epoch": 2.1453658536585367,
      "grad_norm": 0.06375709176063538,
      "learning_rate": 9.362879115939668e-06,
      "loss": 0.0111,
      "step": 8796
    },
    {
      "epoch": 2.145609756097561,
      "grad_norm": 0.1039741113781929,
      "learning_rate": 9.35789713832218e-06,
      "loss": 0.0111,
      "step": 8797
    },
    {
      "epoch": 2.1458536585365855,
      "grad_norm": 0.18326233327388763,
      "learning_rate": 9.352916181304011e-06,
      "loss": 0.0124,
      "step": 8798
    },
    {
      "epoch": 2.1460975609756097,
      "grad_norm": 0.11234442889690399,
      "learning_rate": 9.347936245210171e-06,
      "loss": 0.0155,
      "step": 8799
    },
    {
      "epoch": 2.1463414634146343,
      "grad_norm": 0.16340969502925873,
      "learning_rate": 9.342957330365573e-06,
      "loss": 0.0117,
      "step": 8800
    },
    {
      "epoch": 2.1465853658536584,
      "grad_norm": 0.09316661953926086,
      "learning_rate": 9.337979437095071e-06,
      "loss": 0.015,
      "step": 8801
    },
    {
      "epoch": 2.146829268292683,
      "grad_norm": 0.1076723113656044,
      "learning_rate": 9.333002565723472e-06,
      "loss": 0.0252,
      "step": 8802
    },
    {
      "epoch": 2.1470731707317072,
      "grad_norm": 0.10916144400835037,
      "learning_rate": 9.328026716575486e-06,
      "loss": 0.018,
      "step": 8803
    },
    {
      "epoch": 2.147317073170732,
      "grad_norm": 0.24548012018203735,
      "learning_rate": 9.32305188997579e-06,
      "loss": 0.02,
      "step": 8804
    },
    {
      "epoch": 2.147560975609756,
      "grad_norm": 0.06598459184169769,
      "learning_rate": 9.318078086248965e-06,
      "loss": 0.0093,
      "step": 8805
    },
    {
      "epoch": 2.1478048780487806,
      "grad_norm": 0.1840680092573166,
      "learning_rate": 9.31310530571953e-06,
      "loss": 0.0273,
      "step": 8806
    },
    {
      "epoch": 2.148048780487805,
      "grad_norm": 0.048538949340581894,
      "learning_rate": 9.308133548711962e-06,
      "loss": 0.0089,
      "step": 8807
    },
    {
      "epoch": 2.1482926829268294,
      "grad_norm": 0.11113471537828445,
      "learning_rate": 9.303162815550639e-06,
      "loss": 0.0213,
      "step": 8808
    },
    {
      "epoch": 2.1485365853658536,
      "grad_norm": 0.07299843430519104,
      "learning_rate": 9.298193106559892e-06,
      "loss": 0.014,
      "step": 8809
    },
    {
      "epoch": 2.148780487804878,
      "grad_norm": 0.0654052123427391,
      "learning_rate": 9.293224422063976e-06,
      "loss": 0.0172,
      "step": 8810
    },
    {
      "epoch": 2.1490243902439023,
      "grad_norm": 0.08802129328250885,
      "learning_rate": 9.288256762387076e-06,
      "loss": 0.0153,
      "step": 8811
    },
    {
      "epoch": 2.149268292682927,
      "grad_norm": 0.08283710479736328,
      "learning_rate": 9.28329012785333e-06,
      "loss": 0.0074,
      "step": 8812
    },
    {
      "epoch": 2.149512195121951,
      "grad_norm": 0.20843595266342163,
      "learning_rate": 9.278324518786794e-06,
      "loss": 0.0186,
      "step": 8813
    },
    {
      "epoch": 2.1497560975609757,
      "grad_norm": 0.14739057421684265,
      "learning_rate": 9.273359935511442e-06,
      "loss": 0.0267,
      "step": 8814
    },
    {
      "epoch": 2.15,
      "grad_norm": 0.08991672098636627,
      "learning_rate": 9.268396378351222e-06,
      "loss": 0.0204,
      "step": 8815
    },
    {
      "epoch": 2.1502439024390245,
      "grad_norm": 0.09062722325325012,
      "learning_rate": 9.263433847629969e-06,
      "loss": 0.0178,
      "step": 8816
    },
    {
      "epoch": 2.1504878048780487,
      "grad_norm": 0.1309940218925476,
      "learning_rate": 9.25847234367149e-06,
      "loss": 0.0195,
      "step": 8817
    },
    {
      "epoch": 2.1507317073170733,
      "grad_norm": 0.09130999445915222,
      "learning_rate": 9.253511866799502e-06,
      "loss": 0.0112,
      "step": 8818
    },
    {
      "epoch": 2.1509756097560975,
      "grad_norm": 0.15920089185237885,
      "learning_rate": 9.248552417337653e-06,
      "loss": 0.0143,
      "step": 8819
    },
    {
      "epoch": 2.151219512195122,
      "grad_norm": 0.18152613937854767,
      "learning_rate": 9.243593995609548e-06,
      "loss": 0.0161,
      "step": 8820
    },
    {
      "epoch": 2.1514634146341463,
      "grad_norm": 0.15570081770420074,
      "learning_rate": 9.238636601938699e-06,
      "loss": 0.0286,
      "step": 8821
    },
    {
      "epoch": 2.151707317073171,
      "grad_norm": 0.10709388554096222,
      "learning_rate": 9.23368023664855e-06,
      "loss": 0.018,
      "step": 8822
    },
    {
      "epoch": 2.151951219512195,
      "grad_norm": 0.17548395693302155,
      "learning_rate": 9.228724900062513e-06,
      "loss": 0.0151,
      "step": 8823
    },
    {
      "epoch": 2.1521951219512196,
      "grad_norm": 0.12301119416952133,
      "learning_rate": 9.223770592503886e-06,
      "loss": 0.0228,
      "step": 8824
    },
    {
      "epoch": 2.152439024390244,
      "grad_norm": 0.07329126447439194,
      "learning_rate": 9.21881731429595e-06,
      "loss": 0.011,
      "step": 8825
    },
    {
      "epoch": 2.1526829268292684,
      "grad_norm": 0.10826098173856735,
      "learning_rate": 9.213865065761862e-06,
      "loss": 0.0315,
      "step": 8826
    },
    {
      "epoch": 2.1529268292682926,
      "grad_norm": 0.07609539479017258,
      "learning_rate": 9.208913847224746e-06,
      "loss": 0.0151,
      "step": 8827
    },
    {
      "epoch": 2.153170731707317,
      "grad_norm": 0.06500767916440964,
      "learning_rate": 9.20396365900767e-06,
      "loss": 0.0075,
      "step": 8828
    },
    {
      "epoch": 2.1534146341463414,
      "grad_norm": 0.10407458990812302,
      "learning_rate": 9.199014501433597e-06,
      "loss": 0.0146,
      "step": 8829
    },
    {
      "epoch": 2.153658536585366,
      "grad_norm": 0.12752927839756012,
      "learning_rate": 9.194066374825468e-06,
      "loss": 0.0154,
      "step": 8830
    },
    {
      "epoch": 2.15390243902439,
      "grad_norm": 0.11722450703382492,
      "learning_rate": 9.189119279506119e-06,
      "loss": 0.0145,
      "step": 8831
    },
    {
      "epoch": 2.1541463414634148,
      "grad_norm": 0.08528798818588257,
      "learning_rate": 9.18417321579833e-06,
      "loss": 0.0135,
      "step": 8832
    },
    {
      "epoch": 2.154390243902439,
      "grad_norm": 0.14699654281139374,
      "learning_rate": 9.179228184024826e-06,
      "loss": 0.0077,
      "step": 8833
    },
    {
      "epoch": 2.1546341463414636,
      "grad_norm": 0.07542812824249268,
      "learning_rate": 9.174284184508256e-06,
      "loss": 0.0216,
      "step": 8834
    },
    {
      "epoch": 2.1548780487804877,
      "grad_norm": 0.08216528594493866,
      "learning_rate": 9.169341217571184e-06,
      "loss": 0.0146,
      "step": 8835
    },
    {
      "epoch": 2.1551219512195123,
      "grad_norm": 0.08245214819908142,
      "learning_rate": 9.164399283536143e-06,
      "loss": 0.023,
      "step": 8836
    },
    {
      "epoch": 2.1553658536585365,
      "grad_norm": 0.08378217369318008,
      "learning_rate": 9.159458382725567e-06,
      "loss": 0.0118,
      "step": 8837
    },
    {
      "epoch": 2.155609756097561,
      "grad_norm": 0.13883136212825775,
      "learning_rate": 9.154518515461844e-06,
      "loss": 0.0254,
      "step": 8838
    },
    {
      "epoch": 2.1558536585365853,
      "grad_norm": 0.07949512451887131,
      "learning_rate": 9.149579682067281e-06,
      "loss": 0.0107,
      "step": 8839
    },
    {
      "epoch": 2.15609756097561,
      "grad_norm": 0.16310958564281464,
      "learning_rate": 9.14464188286411e-06,
      "loss": 0.029,
      "step": 8840
    },
    {
      "epoch": 2.156341463414634,
      "grad_norm": 0.09729553014039993,
      "learning_rate": 9.13970511817453e-06,
      "loss": 0.0139,
      "step": 8841
    },
    {
      "epoch": 2.1565853658536587,
      "grad_norm": 0.0896703228354454,
      "learning_rate": 9.134769388320638e-06,
      "loss": 0.0215,
      "step": 8842
    },
    {
      "epoch": 2.156829268292683,
      "grad_norm": 0.1000286266207695,
      "learning_rate": 9.129834693624472e-06,
      "loss": 0.0179,
      "step": 8843
    },
    {
      "epoch": 2.1570731707317075,
      "grad_norm": 0.10406560450792313,
      "learning_rate": 9.124901034408012e-06,
      "loss": 0.0171,
      "step": 8844
    },
    {
      "epoch": 2.1573170731707316,
      "grad_norm": 0.09576211869716644,
      "learning_rate": 9.119968410993152e-06,
      "loss": 0.0215,
      "step": 8845
    },
    {
      "epoch": 2.1575609756097562,
      "grad_norm": 0.14169420301914215,
      "learning_rate": 9.11503682370175e-06,
      "loss": 0.015,
      "step": 8846
    },
    {
      "epoch": 2.1578048780487804,
      "grad_norm": 0.08262292295694351,
      "learning_rate": 9.110106272855564e-06,
      "loss": 0.0146,
      "step": 8847
    },
    {
      "epoch": 2.158048780487805,
      "grad_norm": 0.15971089899539948,
      "learning_rate": 9.105176758776293e-06,
      "loss": 0.0191,
      "step": 8848
    },
    {
      "epoch": 2.158292682926829,
      "grad_norm": 0.1115761548280716,
      "learning_rate": 9.100248281785587e-06,
      "loss": 0.0181,
      "step": 8849
    },
    {
      "epoch": 2.158536585365854,
      "grad_norm": 0.06900753080844879,
      "learning_rate": 9.095320842204999e-06,
      "loss": 0.0148,
      "step": 8850
    },
    {
      "epoch": 2.158780487804878,
      "grad_norm": 0.13408592343330383,
      "learning_rate": 9.090394440356045e-06,
      "loss": 0.0125,
      "step": 8851
    },
    {
      "epoch": 2.1590243902439026,
      "grad_norm": 0.28214147686958313,
      "learning_rate": 9.085469076560152e-06,
      "loss": 0.0177,
      "step": 8852
    },
    {
      "epoch": 2.1592682926829267,
      "grad_norm": 0.09190182387828827,
      "learning_rate": 9.080544751138673e-06,
      "loss": 0.015,
      "step": 8853
    },
    {
      "epoch": 2.1595121951219514,
      "grad_norm": 0.1445937305688858,
      "learning_rate": 9.075621464412923e-06,
      "loss": 0.0217,
      "step": 8854
    },
    {
      "epoch": 2.1597560975609755,
      "grad_norm": 0.08579380810260773,
      "learning_rate": 9.070699216704123e-06,
      "loss": 0.0181,
      "step": 8855
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.2645725607872009,
      "learning_rate": 9.065778008333426e-06,
      "loss": 0.0243,
      "step": 8856
    },
    {
      "epoch": 2.1602439024390243,
      "grad_norm": 0.08977071940898895,
      "learning_rate": 9.060857839621945e-06,
      "loss": 0.0127,
      "step": 8857
    },
    {
      "epoch": 2.160487804878049,
      "grad_norm": 0.15144449472427368,
      "learning_rate": 9.055938710890693e-06,
      "loss": 0.0181,
      "step": 8858
    },
    {
      "epoch": 2.160731707317073,
      "grad_norm": 0.10669649392366409,
      "learning_rate": 9.051020622460632e-06,
      "loss": 0.0119,
      "step": 8859
    },
    {
      "epoch": 2.1609756097560977,
      "grad_norm": 0.10182991623878479,
      "learning_rate": 9.04610357465265e-06,
      "loss": 0.013,
      "step": 8860
    },
    {
      "epoch": 2.161219512195122,
      "grad_norm": 0.06444964557886124,
      "learning_rate": 9.041187567787566e-06,
      "loss": 0.0107,
      "step": 8861
    },
    {
      "epoch": 2.1614634146341465,
      "grad_norm": 0.23282454907894135,
      "learning_rate": 9.036272602186141e-06,
      "loss": 0.0312,
      "step": 8862
    },
    {
      "epoch": 2.1617073170731707,
      "grad_norm": 0.2097407877445221,
      "learning_rate": 9.031358678169055e-06,
      "loss": 0.0269,
      "step": 8863
    },
    {
      "epoch": 2.1619512195121953,
      "grad_norm": 0.11314725130796432,
      "learning_rate": 9.026445796056938e-06,
      "loss": 0.0145,
      "step": 8864
    },
    {
      "epoch": 2.1621951219512194,
      "grad_norm": 0.07545402646064758,
      "learning_rate": 9.02153395617033e-06,
      "loss": 0.0141,
      "step": 8865
    },
    {
      "epoch": 2.162439024390244,
      "grad_norm": 0.2353215515613556,
      "learning_rate": 9.016623158829712e-06,
      "loss": 0.0237,
      "step": 8866
    },
    {
      "epoch": 2.162682926829268,
      "grad_norm": 0.1387997567653656,
      "learning_rate": 9.01171340435551e-06,
      "loss": 0.0218,
      "step": 8867
    },
    {
      "epoch": 2.162926829268293,
      "grad_norm": 0.127463698387146,
      "learning_rate": 9.006804693068064e-06,
      "loss": 0.0148,
      "step": 8868
    },
    {
      "epoch": 2.163170731707317,
      "grad_norm": 0.07166442275047302,
      "learning_rate": 9.001897025287643e-06,
      "loss": 0.0153,
      "step": 8869
    },
    {
      "epoch": 2.1634146341463416,
      "grad_norm": 0.10527144372463226,
      "learning_rate": 8.996990401334473e-06,
      "loss": 0.0203,
      "step": 8870
    },
    {
      "epoch": 2.1636585365853658,
      "grad_norm": 0.18942025303840637,
      "learning_rate": 8.992084821528681e-06,
      "loss": 0.0206,
      "step": 8871
    },
    {
      "epoch": 2.1639024390243904,
      "grad_norm": 0.14011865854263306,
      "learning_rate": 8.98718028619036e-06,
      "loss": 0.0203,
      "step": 8872
    },
    {
      "epoch": 2.1641463414634146,
      "grad_norm": 0.10880602896213531,
      "learning_rate": 8.982276795639504e-06,
      "loss": 0.0201,
      "step": 8873
    },
    {
      "epoch": 2.164390243902439,
      "grad_norm": 0.15960204601287842,
      "learning_rate": 8.977374350196047e-06,
      "loss": 0.0117,
      "step": 8874
    },
    {
      "epoch": 2.1646341463414633,
      "grad_norm": 0.09184607863426208,
      "learning_rate": 8.972472950179877e-06,
      "loss": 0.01,
      "step": 8875
    },
    {
      "epoch": 2.164878048780488,
      "grad_norm": 0.14287947118282318,
      "learning_rate": 8.967572595910767e-06,
      "loss": 0.0105,
      "step": 8876
    },
    {
      "epoch": 2.165121951219512,
      "grad_norm": 0.1916048228740692,
      "learning_rate": 8.962673287708474e-06,
      "loss": 0.0199,
      "step": 8877
    },
    {
      "epoch": 2.1653658536585367,
      "grad_norm": 0.12457029521465302,
      "learning_rate": 8.957775025892651e-06,
      "loss": 0.01,
      "step": 8878
    },
    {
      "epoch": 2.165609756097561,
      "grad_norm": 0.19750438630580902,
      "learning_rate": 8.952877810782892e-06,
      "loss": 0.0208,
      "step": 8879
    },
    {
      "epoch": 2.1658536585365855,
      "grad_norm": 0.2344026416540146,
      "learning_rate": 8.947981642698738e-06,
      "loss": 0.023,
      "step": 8880
    },
    {
      "epoch": 2.1660975609756097,
      "grad_norm": 0.14861749112606049,
      "learning_rate": 8.943086521959643e-06,
      "loss": 0.0315,
      "step": 8881
    },
    {
      "epoch": 2.1663414634146343,
      "grad_norm": 0.13292649388313293,
      "learning_rate": 8.938192448884988e-06,
      "loss": 0.016,
      "step": 8882
    },
    {
      "epoch": 2.1665853658536585,
      "grad_norm": 0.13353055715560913,
      "learning_rate": 8.933299423794114e-06,
      "loss": 0.0143,
      "step": 8883
    },
    {
      "epoch": 2.166829268292683,
      "grad_norm": 0.13430732488632202,
      "learning_rate": 8.928407447006259e-06,
      "loss": 0.0218,
      "step": 8884
    },
    {
      "epoch": 2.1670731707317072,
      "grad_norm": 0.14897102117538452,
      "learning_rate": 8.92351651884063e-06,
      "loss": 0.0228,
      "step": 8885
    },
    {
      "epoch": 2.167317073170732,
      "grad_norm": 0.16454043984413147,
      "learning_rate": 8.91862663961633e-06,
      "loss": 0.0341,
      "step": 8886
    },
    {
      "epoch": 2.167560975609756,
      "grad_norm": 0.04964778944849968,
      "learning_rate": 8.913737809652403e-06,
      "loss": 0.015,
      "step": 8887
    },
    {
      "epoch": 2.1678048780487806,
      "grad_norm": 0.08780599385499954,
      "learning_rate": 8.908850029267848e-06,
      "loss": 0.0121,
      "step": 8888
    },
    {
      "epoch": 2.168048780487805,
      "grad_norm": 0.15408477187156677,
      "learning_rate": 8.90396329878157e-06,
      "loss": 0.0099,
      "step": 8889
    },
    {
      "epoch": 2.1682926829268294,
      "grad_norm": 0.13792000710964203,
      "learning_rate": 8.899077618512406e-06,
      "loss": 0.0142,
      "step": 8890
    },
    {
      "epoch": 2.1685365853658536,
      "grad_norm": 0.10444004088640213,
      "learning_rate": 8.89419298877915e-06,
      "loss": 0.0231,
      "step": 8891
    },
    {
      "epoch": 2.168780487804878,
      "grad_norm": 0.13656431436538696,
      "learning_rate": 8.889309409900481e-06,
      "loss": 0.0237,
      "step": 8892
    },
    {
      "epoch": 2.1690243902439024,
      "grad_norm": 0.13073234260082245,
      "learning_rate": 8.884426882195066e-06,
      "loss": 0.0144,
      "step": 8893
    },
    {
      "epoch": 2.169268292682927,
      "grad_norm": 0.08177392929792404,
      "learning_rate": 8.879545405981457e-06,
      "loss": 0.0148,
      "step": 8894
    },
    {
      "epoch": 2.169512195121951,
      "grad_norm": 0.10413366556167603,
      "learning_rate": 8.874664981578154e-06,
      "loss": 0.0264,
      "step": 8895
    },
    {
      "epoch": 2.1697560975609758,
      "grad_norm": 0.11437945812940598,
      "learning_rate": 8.869785609303608e-06,
      "loss": 0.0169,
      "step": 8896
    },
    {
      "epoch": 2.17,
      "grad_norm": 0.15448756515979767,
      "learning_rate": 8.86490728947616e-06,
      "loss": 0.015,
      "step": 8897
    },
    {
      "epoch": 2.1702439024390245,
      "grad_norm": 0.09901873767375946,
      "learning_rate": 8.860030022414125e-06,
      "loss": 0.0225,
      "step": 8898
    },
    {
      "epoch": 2.1704878048780487,
      "grad_norm": 0.1014951765537262,
      "learning_rate": 8.855153808435725e-06,
      "loss": 0.016,
      "step": 8899
    },
    {
      "epoch": 2.1707317073170733,
      "grad_norm": 0.14746421575546265,
      "learning_rate": 8.850278647859103e-06,
      "loss": 0.0199,
      "step": 8900
    },
    {
      "epoch": 2.1709756097560975,
      "grad_norm": 0.14255516231060028,
      "learning_rate": 8.845404541002372e-06,
      "loss": 0.0137,
      "step": 8901
    },
    {
      "epoch": 2.171219512195122,
      "grad_norm": 0.13971653580665588,
      "learning_rate": 8.84053148818354e-06,
      "loss": 0.0185,
      "step": 8902
    },
    {
      "epoch": 2.1714634146341463,
      "grad_norm": 0.20761704444885254,
      "learning_rate": 8.835659489720549e-06,
      "loss": 0.0363,
      "step": 8903
    },
    {
      "epoch": 2.171707317073171,
      "grad_norm": 0.06292801350355148,
      "learning_rate": 8.8307885459313e-06,
      "loss": 0.0038,
      "step": 8904
    },
    {
      "epoch": 2.171951219512195,
      "grad_norm": 0.13318049907684326,
      "learning_rate": 8.825918657133602e-06,
      "loss": 0.0182,
      "step": 8905
    },
    {
      "epoch": 2.1721951219512197,
      "grad_norm": 0.11290327459573746,
      "learning_rate": 8.821049823645192e-06,
      "loss": 0.0135,
      "step": 8906
    },
    {
      "epoch": 2.172439024390244,
      "grad_norm": 0.11010559648275375,
      "learning_rate": 8.816182045783758e-06,
      "loss": 0.0169,
      "step": 8907
    },
    {
      "epoch": 2.1726829268292684,
      "grad_norm": 0.15314173698425293,
      "learning_rate": 8.811315323866904e-06,
      "loss": 0.022,
      "step": 8908
    },
    {
      "epoch": 2.1729268292682926,
      "grad_norm": 0.08644494414329529,
      "learning_rate": 8.806449658212163e-06,
      "loss": 0.0095,
      "step": 8909
    },
    {
      "epoch": 2.1731707317073172,
      "grad_norm": 0.13640843331813812,
      "learning_rate": 8.801585049137e-06,
      "loss": 0.0103,
      "step": 8910
    },
    {
      "epoch": 2.1734146341463414,
      "grad_norm": 0.15081334114074707,
      "learning_rate": 8.796721496958837e-06,
      "loss": 0.0237,
      "step": 8911
    },
    {
      "epoch": 2.173658536585366,
      "grad_norm": 0.12694966793060303,
      "learning_rate": 8.791859001994992e-06,
      "loss": 0.0231,
      "step": 8912
    },
    {
      "epoch": 2.17390243902439,
      "grad_norm": 0.11996516585350037,
      "learning_rate": 8.78699756456272e-06,
      "loss": 0.0251,
      "step": 8913
    },
    {
      "epoch": 2.174146341463415,
      "grad_norm": 0.07945192605257034,
      "learning_rate": 8.782137184979234e-06,
      "loss": 0.0155,
      "step": 8914
    },
    {
      "epoch": 2.174390243902439,
      "grad_norm": 0.12135737389326096,
      "learning_rate": 8.777277863561647e-06,
      "loss": 0.0123,
      "step": 8915
    },
    {
      "epoch": 2.1746341463414636,
      "grad_norm": 0.08185102045536041,
      "learning_rate": 8.77241960062701e-06,
      "loss": 0.0108,
      "step": 8916
    },
    {
      "epoch": 2.1748780487804877,
      "grad_norm": 0.15233497321605682,
      "learning_rate": 8.767562396492324e-06,
      "loss": 0.0106,
      "step": 8917
    },
    {
      "epoch": 2.1751219512195124,
      "grad_norm": 0.17158156633377075,
      "learning_rate": 8.7627062514745e-06,
      "loss": 0.0165,
      "step": 8918
    },
    {
      "epoch": 2.1753658536585365,
      "grad_norm": 0.0808243602514267,
      "learning_rate": 8.757851165890377e-06,
      "loss": 0.0128,
      "step": 8919
    },
    {
      "epoch": 2.175609756097561,
      "grad_norm": 0.09869121760129929,
      "learning_rate": 8.752997140056755e-06,
      "loss": 0.0186,
      "step": 8920
    },
    {
      "epoch": 2.1758536585365853,
      "grad_norm": 0.09773529320955276,
      "learning_rate": 8.748144174290321e-06,
      "loss": 0.0226,
      "step": 8921
    },
    {
      "epoch": 2.17609756097561,
      "grad_norm": 0.12419135123491287,
      "learning_rate": 8.743292268907736e-06,
      "loss": 0.0108,
      "step": 8922
    },
    {
      "epoch": 2.176341463414634,
      "grad_norm": 0.05531207099556923,
      "learning_rate": 8.738441424225564e-06,
      "loss": 0.0087,
      "step": 8923
    },
    {
      "epoch": 2.1765853658536587,
      "grad_norm": 0.06120287626981735,
      "learning_rate": 8.73359164056031e-06,
      "loss": 0.0102,
      "step": 8924
    },
    {
      "epoch": 2.176829268292683,
      "grad_norm": 0.1508672684431076,
      "learning_rate": 8.728742918228402e-06,
      "loss": 0.0198,
      "step": 8925
    },
    {
      "epoch": 2.1770731707317075,
      "grad_norm": 0.15865027904510498,
      "learning_rate": 8.723895257546203e-06,
      "loss": 0.0225,
      "step": 8926
    },
    {
      "epoch": 2.1773170731707316,
      "grad_norm": 0.10255502909421921,
      "learning_rate": 8.719048658830018e-06,
      "loss": 0.0242,
      "step": 8927
    },
    {
      "epoch": 2.1775609756097563,
      "grad_norm": 0.2122853696346283,
      "learning_rate": 8.714203122396066e-06,
      "loss": 0.0214,
      "step": 8928
    },
    {
      "epoch": 2.1778048780487804,
      "grad_norm": 0.13970819115638733,
      "learning_rate": 8.7093586485605e-06,
      "loss": 0.02,
      "step": 8929
    },
    {
      "epoch": 2.178048780487805,
      "grad_norm": 0.14312924444675446,
      "learning_rate": 8.704515237639419e-06,
      "loss": 0.0306,
      "step": 8930
    },
    {
      "epoch": 2.178292682926829,
      "grad_norm": 0.11446402221918106,
      "learning_rate": 8.699672889948831e-06,
      "loss": 0.0183,
      "step": 8931
    },
    {
      "epoch": 2.178536585365854,
      "grad_norm": 0.15725140273571014,
      "learning_rate": 8.694831605804683e-06,
      "loss": 0.0222,
      "step": 8932
    },
    {
      "epoch": 2.178780487804878,
      "grad_norm": 0.09986428916454315,
      "learning_rate": 8.689991385522862e-06,
      "loss": 0.0214,
      "step": 8933
    },
    {
      "epoch": 2.1790243902439026,
      "grad_norm": 0.1481466144323349,
      "learning_rate": 8.685152229419167e-06,
      "loss": 0.0101,
      "step": 8934
    },
    {
      "epoch": 2.1792682926829268,
      "grad_norm": 0.06197560578584671,
      "learning_rate": 8.68031413780935e-06,
      "loss": 0.0055,
      "step": 8935
    },
    {
      "epoch": 2.1795121951219514,
      "grad_norm": 0.08205696195363998,
      "learning_rate": 8.675477111009078e-06,
      "loss": 0.0098,
      "step": 8936
    },
    {
      "epoch": 2.1797560975609755,
      "grad_norm": 0.09040416777133942,
      "learning_rate": 8.670641149333943e-06,
      "loss": 0.0069,
      "step": 8937
    },
    {
      "epoch": 2.18,
      "grad_norm": 0.12707987427711487,
      "learning_rate": 8.665806253099493e-06,
      "loss": 0.0156,
      "step": 8938
    },
    {
      "epoch": 2.1802439024390243,
      "grad_norm": 0.16515381634235382,
      "learning_rate": 8.66097242262118e-06,
      "loss": 0.0157,
      "step": 8939
    },
    {
      "epoch": 2.180487804878049,
      "grad_norm": 0.20880401134490967,
      "learning_rate": 8.65613965821439e-06,
      "loss": 0.0302,
      "step": 8940
    },
    {
      "epoch": 2.180731707317073,
      "grad_norm": 0.1328883022069931,
      "learning_rate": 8.65130796019447e-06,
      "loss": 0.0175,
      "step": 8941
    },
    {
      "epoch": 2.1809756097560977,
      "grad_norm": 0.06695239990949631,
      "learning_rate": 8.64647732887664e-06,
      "loss": 0.0096,
      "step": 8942
    },
    {
      "epoch": 2.181219512195122,
      "grad_norm": 0.24079027771949768,
      "learning_rate": 8.64164776457611e-06,
      "loss": 0.0273,
      "step": 8943
    },
    {
      "epoch": 2.1814634146341465,
      "grad_norm": 0.11449983716011047,
      "learning_rate": 8.636819267607988e-06,
      "loss": 0.0159,
      "step": 8944
    },
    {
      "epoch": 2.1817073170731707,
      "grad_norm": 0.15647180378437042,
      "learning_rate": 8.631991838287307e-06,
      "loss": 0.0184,
      "step": 8945
    },
    {
      "epoch": 2.1819512195121953,
      "grad_norm": 0.12781329452991486,
      "learning_rate": 8.627165476929059e-06,
      "loss": 0.0175,
      "step": 8946
    },
    {
      "epoch": 2.1821951219512195,
      "grad_norm": 0.15527307987213135,
      "learning_rate": 8.622340183848132e-06,
      "loss": 0.0163,
      "step": 8947
    },
    {
      "epoch": 2.182439024390244,
      "grad_norm": 0.055018775165081024,
      "learning_rate": 8.617515959359382e-06,
      "loss": 0.0066,
      "step": 8948
    },
    {
      "epoch": 2.1826829268292682,
      "grad_norm": 0.10978202521800995,
      "learning_rate": 8.612692803777561e-06,
      "loss": 0.0207,
      "step": 8949
    },
    {
      "epoch": 2.182926829268293,
      "grad_norm": 0.10175655782222748,
      "learning_rate": 8.607870717417363e-06,
      "loss": 0.0125,
      "step": 8950
    },
    {
      "epoch": 2.183170731707317,
      "grad_norm": 0.15128348767757416,
      "learning_rate": 8.603049700593424e-06,
      "loss": 0.0163,
      "step": 8951
    },
    {
      "epoch": 2.1834146341463416,
      "grad_norm": 0.1335122287273407,
      "learning_rate": 8.598229753620298e-06,
      "loss": 0.0172,
      "step": 8952
    },
    {
      "epoch": 2.183658536585366,
      "grad_norm": 0.21293017268180847,
      "learning_rate": 8.593410876812463e-06,
      "loss": 0.0101,
      "step": 8953
    },
    {
      "epoch": 2.1839024390243904,
      "grad_norm": 0.1457042396068573,
      "learning_rate": 8.588593070484353e-06,
      "loss": 0.0247,
      "step": 8954
    },
    {
      "epoch": 2.1841463414634146,
      "grad_norm": 0.143475741147995,
      "learning_rate": 8.583776334950292e-06,
      "loss": 0.0128,
      "step": 8955
    },
    {
      "epoch": 2.184390243902439,
      "grad_norm": 0.1306144893169403,
      "learning_rate": 8.57896067052458e-06,
      "loss": 0.0243,
      "step": 8956
    },
    {
      "epoch": 2.1846341463414634,
      "grad_norm": 0.12246596068143845,
      "learning_rate": 8.574146077521425e-06,
      "loss": 0.0122,
      "step": 8957
    },
    {
      "epoch": 2.184878048780488,
      "grad_norm": 0.30671435594558716,
      "learning_rate": 8.569332556254936e-06,
      "loss": 0.0243,
      "step": 8958
    },
    {
      "epoch": 2.185121951219512,
      "grad_norm": 0.07036342471837997,
      "learning_rate": 8.564520107039207e-06,
      "loss": 0.012,
      "step": 8959
    },
    {
      "epoch": 2.1853658536585368,
      "grad_norm": 0.1361462026834488,
      "learning_rate": 8.559708730188217e-06,
      "loss": 0.015,
      "step": 8960
    },
    {
      "epoch": 2.185609756097561,
      "grad_norm": 0.26255691051483154,
      "learning_rate": 8.554898426015917e-06,
      "loss": 0.0123,
      "step": 8961
    },
    {
      "epoch": 2.1858536585365855,
      "grad_norm": 0.14570064842700958,
      "learning_rate": 8.550089194836148e-06,
      "loss": 0.0282,
      "step": 8962
    },
    {
      "epoch": 2.1860975609756097,
      "grad_norm": 0.18844564259052277,
      "learning_rate": 8.545281036962694e-06,
      "loss": 0.0124,
      "step": 8963
    },
    {
      "epoch": 2.1863414634146343,
      "grad_norm": 0.06519190222024918,
      "learning_rate": 8.540473952709286e-06,
      "loss": 0.0124,
      "step": 8964
    },
    {
      "epoch": 2.1865853658536585,
      "grad_norm": 0.15219098329544067,
      "learning_rate": 8.535667942389567e-06,
      "loss": 0.0358,
      "step": 8965
    },
    {
      "epoch": 2.186829268292683,
      "grad_norm": 0.26064637303352356,
      "learning_rate": 8.530863006317103e-06,
      "loss": 0.0197,
      "step": 8966
    },
    {
      "epoch": 2.1870731707317073,
      "grad_norm": 0.15257063508033752,
      "learning_rate": 8.526059144805418e-06,
      "loss": 0.0261,
      "step": 8967
    },
    {
      "epoch": 2.187317073170732,
      "grad_norm": 0.1149083748459816,
      "learning_rate": 8.521256358167936e-06,
      "loss": 0.0144,
      "step": 8968
    },
    {
      "epoch": 2.187560975609756,
      "grad_norm": 0.0999949723482132,
      "learning_rate": 8.516454646718039e-06,
      "loss": 0.0169,
      "step": 8969
    },
    {
      "epoch": 2.1878048780487807,
      "grad_norm": 0.08127465099096298,
      "learning_rate": 8.511654010769015e-06,
      "loss": 0.0126,
      "step": 8970
    },
    {
      "epoch": 2.188048780487805,
      "grad_norm": 0.10045257955789566,
      "learning_rate": 8.506854450634083e-06,
      "loss": 0.0097,
      "step": 8971
    },
    {
      "epoch": 2.1882926829268294,
      "grad_norm": 0.08810635656118393,
      "learning_rate": 8.502055966626415e-06,
      "loss": 0.0111,
      "step": 8972
    },
    {
      "epoch": 2.1885365853658536,
      "grad_norm": 0.11952510476112366,
      "learning_rate": 8.49725855905909e-06,
      "loss": 0.0134,
      "step": 8973
    },
    {
      "epoch": 2.188780487804878,
      "grad_norm": 0.06290862709283829,
      "learning_rate": 8.492462228245126e-06,
      "loss": 0.0075,
      "step": 8974
    },
    {
      "epoch": 2.1890243902439024,
      "grad_norm": 0.16306999325752258,
      "learning_rate": 8.487666974497466e-06,
      "loss": 0.0197,
      "step": 8975
    },
    {
      "epoch": 2.189268292682927,
      "grad_norm": 0.1577170342206955,
      "learning_rate": 8.482872798128976e-06,
      "loss": 0.0203,
      "step": 8976
    },
    {
      "epoch": 2.189512195121951,
      "grad_norm": 0.08390478044748306,
      "learning_rate": 8.478079699452485e-06,
      "loss": 0.0148,
      "step": 8977
    },
    {
      "epoch": 2.189756097560976,
      "grad_norm": 0.11226921528577805,
      "learning_rate": 8.473287678780715e-06,
      "loss": 0.0206,
      "step": 8978
    },
    {
      "epoch": 2.19,
      "grad_norm": 0.07471028715372086,
      "learning_rate": 8.468496736426317e-06,
      "loss": 0.0152,
      "step": 8979
    },
    {
      "epoch": 2.1902439024390246,
      "grad_norm": 0.14110812544822693,
      "learning_rate": 8.463706872701913e-06,
      "loss": 0.0196,
      "step": 8980
    },
    {
      "epoch": 2.1904878048780487,
      "grad_norm": 0.18966929614543915,
      "learning_rate": 8.458918087920003e-06,
      "loss": 0.0266,
      "step": 8981
    },
    {
      "epoch": 2.1907317073170733,
      "grad_norm": 0.09990950673818588,
      "learning_rate": 8.454130382393061e-06,
      "loss": 0.0163,
      "step": 8982
    },
    {
      "epoch": 2.1909756097560975,
      "grad_norm": 0.10962667316198349,
      "learning_rate": 8.449343756433456e-06,
      "loss": 0.0137,
      "step": 8983
    },
    {
      "epoch": 2.191219512195122,
      "grad_norm": 0.1864822655916214,
      "learning_rate": 8.444558210353498e-06,
      "loss": 0.0146,
      "step": 8984
    },
    {
      "epoch": 2.1914634146341463,
      "grad_norm": 0.10944563150405884,
      "learning_rate": 8.439773744465443e-06,
      "loss": 0.018,
      "step": 8985
    },
    {
      "epoch": 2.191707317073171,
      "grad_norm": 0.09216968715190887,
      "learning_rate": 8.434990359081457e-06,
      "loss": 0.0175,
      "step": 8986
    },
    {
      "epoch": 2.191951219512195,
      "grad_norm": 0.04924225062131882,
      "learning_rate": 8.43020805451363e-06,
      "loss": 0.0118,
      "step": 8987
    },
    {
      "epoch": 2.1921951219512197,
      "grad_norm": 0.1904170960187912,
      "learning_rate": 8.425426831074013e-06,
      "loss": 0.0186,
      "step": 8988
    },
    {
      "epoch": 2.192439024390244,
      "grad_norm": 0.15865646302700043,
      "learning_rate": 8.420646689074546e-06,
      "loss": 0.0198,
      "step": 8989
    },
    {
      "epoch": 2.1926829268292685,
      "grad_norm": 0.10317801684141159,
      "learning_rate": 8.415867628827138e-06,
      "loss": 0.0188,
      "step": 8990
    },
    {
      "epoch": 2.1929268292682926,
      "grad_norm": 0.18790501356124878,
      "learning_rate": 8.411089650643603e-06,
      "loss": 0.0173,
      "step": 8991
    },
    {
      "epoch": 2.1931707317073172,
      "grad_norm": 0.14388567209243774,
      "learning_rate": 8.406312754835671e-06,
      "loss": 0.0195,
      "step": 8992
    },
    {
      "epoch": 2.1934146341463414,
      "grad_norm": 0.1457253396511078,
      "learning_rate": 8.401536941715042e-06,
      "loss": 0.0271,
      "step": 8993
    },
    {
      "epoch": 2.193658536585366,
      "grad_norm": 0.14836756885051727,
      "learning_rate": 8.396762211593304e-06,
      "loss": 0.0299,
      "step": 8994
    },
    {
      "epoch": 2.19390243902439,
      "grad_norm": 0.12194154411554337,
      "learning_rate": 8.391988564782014e-06,
      "loss": 0.0163,
      "step": 8995
    },
    {
      "epoch": 2.194146341463415,
      "grad_norm": 0.07548519968986511,
      "learning_rate": 8.387216001592629e-06,
      "loss": 0.0152,
      "step": 8996
    },
    {
      "epoch": 2.194390243902439,
      "grad_norm": 0.12989214062690735,
      "learning_rate": 8.382444522336536e-06,
      "loss": 0.023,
      "step": 8997
    },
    {
      "epoch": 2.1946341463414636,
      "grad_norm": 0.14514237642288208,
      "learning_rate": 8.377674127325073e-06,
      "loss": 0.0137,
      "step": 8998
    },
    {
      "epoch": 2.1948780487804878,
      "grad_norm": 0.0958789736032486,
      "learning_rate": 8.372904816869487e-06,
      "loss": 0.0157,
      "step": 8999
    },
    {
      "epoch": 2.1951219512195124,
      "grad_norm": 0.18467563390731812,
      "learning_rate": 8.368136591280953e-06,
      "loss": 0.0207,
      "step": 9000
    },
    {
      "epoch": 2.1953658536585365,
      "grad_norm": 0.09073612093925476,
      "learning_rate": 8.3633694508706e-06,
      "loss": 0.018,
      "step": 9001
    },
    {
      "epoch": 2.195609756097561,
      "grad_norm": 0.038150887936353683,
      "learning_rate": 8.358603395949454e-06,
      "loss": 0.003,
      "step": 9002
    },
    {
      "epoch": 2.1958536585365853,
      "grad_norm": 0.10257718712091446,
      "learning_rate": 8.3538384268285e-06,
      "loss": 0.0194,
      "step": 9003
    },
    {
      "epoch": 2.19609756097561,
      "grad_norm": 0.12442291527986526,
      "learning_rate": 8.349074543818627e-06,
      "loss": 0.0147,
      "step": 9004
    },
    {
      "epoch": 2.196341463414634,
      "grad_norm": 0.12989675998687744,
      "learning_rate": 8.344311747230663e-06,
      "loss": 0.0132,
      "step": 9005
    },
    {
      "epoch": 2.1965853658536587,
      "grad_norm": 0.1709044873714447,
      "learning_rate": 8.339550037375373e-06,
      "loss": 0.023,
      "step": 9006
    },
    {
      "epoch": 2.196829268292683,
      "grad_norm": 0.16276566684246063,
      "learning_rate": 8.334789414563444e-06,
      "loss": 0.0166,
      "step": 9007
    },
    {
      "epoch": 2.1970731707317075,
      "grad_norm": 0.0995776355266571,
      "learning_rate": 8.330029879105488e-06,
      "loss": 0.0163,
      "step": 9008
    },
    {
      "epoch": 2.1973170731707317,
      "grad_norm": 0.08999905735254288,
      "learning_rate": 8.32527143131205e-06,
      "loss": 0.0319,
      "step": 9009
    },
    {
      "epoch": 2.1975609756097563,
      "grad_norm": 0.1652754694223404,
      "learning_rate": 8.320514071493595e-06,
      "loss": 0.0136,
      "step": 9010
    },
    {
      "epoch": 2.1978048780487804,
      "grad_norm": 0.09771102666854858,
      "learning_rate": 8.315757799960546e-06,
      "loss": 0.0207,
      "step": 9011
    },
    {
      "epoch": 2.198048780487805,
      "grad_norm": 0.20560866594314575,
      "learning_rate": 8.311002617023228e-06,
      "loss": 0.0165,
      "step": 9012
    },
    {
      "epoch": 2.1982926829268292,
      "grad_norm": 0.13523560762405396,
      "learning_rate": 8.306248522991889e-06,
      "loss": 0.0087,
      "step": 9013
    },
    {
      "epoch": 2.198536585365854,
      "grad_norm": 0.09279965609312057,
      "learning_rate": 8.301495518176735e-06,
      "loss": 0.0095,
      "step": 9014
    },
    {
      "epoch": 2.198780487804878,
      "grad_norm": 0.1189526841044426,
      "learning_rate": 8.296743602887874e-06,
      "loss": 0.0201,
      "step": 9015
    },
    {
      "epoch": 2.1990243902439026,
      "grad_norm": 0.08344829827547073,
      "learning_rate": 8.291992777435368e-06,
      "loss": 0.0105,
      "step": 9016
    },
    {
      "epoch": 2.199268292682927,
      "grad_norm": 0.1913939267396927,
      "learning_rate": 8.287243042129183e-06,
      "loss": 0.0187,
      "step": 9017
    },
    {
      "epoch": 2.1995121951219514,
      "grad_norm": 0.11973854154348373,
      "learning_rate": 8.282494397279219e-06,
      "loss": 0.0156,
      "step": 9018
    },
    {
      "epoch": 2.1997560975609756,
      "grad_norm": 0.07311446219682693,
      "learning_rate": 8.277746843195327e-06,
      "loss": 0.0113,
      "step": 9019
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.06322485953569412,
      "learning_rate": 8.273000380187262e-06,
      "loss": 0.0142,
      "step": 9020
    },
    {
      "epoch": 2.2002439024390243,
      "grad_norm": 0.18425841629505157,
      "learning_rate": 8.268255008564706e-06,
      "loss": 0.0125,
      "step": 9021
    },
    {
      "epoch": 2.200487804878049,
      "grad_norm": 0.0936928316950798,
      "learning_rate": 8.2635107286373e-06,
      "loss": 0.0151,
      "step": 9022
    },
    {
      "epoch": 2.200731707317073,
      "grad_norm": 0.1243642270565033,
      "learning_rate": 8.258767540714584e-06,
      "loss": 0.0079,
      "step": 9023
    },
    {
      "epoch": 2.2009756097560977,
      "grad_norm": 0.13562063872814178,
      "learning_rate": 8.254025445106037e-06,
      "loss": 0.0202,
      "step": 9024
    },
    {
      "epoch": 2.201219512195122,
      "grad_norm": 0.10656549781560898,
      "learning_rate": 8.249284442121063e-06,
      "loss": 0.0123,
      "step": 9025
    },
    {
      "epoch": 2.2014634146341465,
      "grad_norm": 0.09325825423002243,
      "learning_rate": 8.244544532068993e-06,
      "loss": 0.0158,
      "step": 9026
    },
    {
      "epoch": 2.2017073170731707,
      "grad_norm": 0.17397643625736237,
      "learning_rate": 8.239805715259107e-06,
      "loss": 0.0245,
      "step": 9027
    },
    {
      "epoch": 2.2019512195121953,
      "grad_norm": 0.1427137702703476,
      "learning_rate": 8.235067992000581e-06,
      "loss": 0.0331,
      "step": 9028
    },
    {
      "epoch": 2.2021951219512195,
      "grad_norm": 0.17265684902668,
      "learning_rate": 8.230331362602556e-06,
      "loss": 0.0254,
      "step": 9029
    },
    {
      "epoch": 2.202439024390244,
      "grad_norm": 0.1465722620487213,
      "learning_rate": 8.225595827374072e-06,
      "loss": 0.0222,
      "step": 9030
    },
    {
      "epoch": 2.2026829268292683,
      "grad_norm": 0.16709363460540771,
      "learning_rate": 8.2208613866241e-06,
      "loss": 0.0256,
      "step": 9031
    },
    {
      "epoch": 2.202926829268293,
      "grad_norm": 0.07408705353736877,
      "learning_rate": 8.216128040661567e-06,
      "loss": 0.0045,
      "step": 9032
    },
    {
      "epoch": 2.203170731707317,
      "grad_norm": 0.11022206395864487,
      "learning_rate": 8.211395789795298e-06,
      "loss": 0.0091,
      "step": 9033
    },
    {
      "epoch": 2.2034146341463416,
      "grad_norm": 0.1364457607269287,
      "learning_rate": 8.206664634334049e-06,
      "loss": 0.0142,
      "step": 9034
    },
    {
      "epoch": 2.203658536585366,
      "grad_norm": 0.06752679497003555,
      "learning_rate": 8.201934574586534e-06,
      "loss": 0.0107,
      "step": 9035
    },
    {
      "epoch": 2.2039024390243904,
      "grad_norm": 0.10118722170591354,
      "learning_rate": 8.197205610861363e-06,
      "loss": 0.0104,
      "step": 9036
    },
    {
      "epoch": 2.2041463414634146,
      "grad_norm": 0.11588676273822784,
      "learning_rate": 8.192477743467078e-06,
      "loss": 0.0145,
      "step": 9037
    },
    {
      "epoch": 2.204390243902439,
      "grad_norm": 0.1147448867559433,
      "learning_rate": 8.187750972712177e-06,
      "loss": 0.0167,
      "step": 9038
    },
    {
      "epoch": 2.2046341463414634,
      "grad_norm": 0.1230829507112503,
      "learning_rate": 8.18302529890505e-06,
      "loss": 0.0191,
      "step": 9039
    },
    {
      "epoch": 2.204878048780488,
      "grad_norm": 0.15214964747428894,
      "learning_rate": 8.178300722354057e-06,
      "loss": 0.0197,
      "step": 9040
    },
    {
      "epoch": 2.205121951219512,
      "grad_norm": 0.13007447123527527,
      "learning_rate": 8.173577243367427e-06,
      "loss": 0.0126,
      "step": 9041
    },
    {
      "epoch": 2.2053658536585368,
      "grad_norm": 0.1125655248761177,
      "learning_rate": 8.168854862253384e-06,
      "loss": 0.0322,
      "step": 9042
    },
    {
      "epoch": 2.205609756097561,
      "grad_norm": 0.24767477810382843,
      "learning_rate": 8.164133579320033e-06,
      "loss": 0.0368,
      "step": 9043
    },
    {
      "epoch": 2.2058536585365855,
      "grad_norm": 0.14160987734794617,
      "learning_rate": 8.159413394875417e-06,
      "loss": 0.0225,
      "step": 9044
    },
    {
      "epoch": 2.2060975609756097,
      "grad_norm": 0.1628715842962265,
      "learning_rate": 8.154694309227535e-06,
      "loss": 0.0105,
      "step": 9045
    },
    {
      "epoch": 2.2063414634146343,
      "grad_norm": 0.10378025472164154,
      "learning_rate": 8.149976322684277e-06,
      "loss": 0.014,
      "step": 9046
    },
    {
      "epoch": 2.2065853658536585,
      "grad_norm": 0.1242770254611969,
      "learning_rate": 8.145259435553477e-06,
      "loss": 0.0134,
      "step": 9047
    },
    {
      "epoch": 2.206829268292683,
      "grad_norm": 0.05886450782418251,
      "learning_rate": 8.140543648142906e-06,
      "loss": 0.0133,
      "step": 9048
    },
    {
      "epoch": 2.2070731707317073,
      "grad_norm": 0.10078221559524536,
      "learning_rate": 8.135828960760252e-06,
      "loss": 0.03,
      "step": 9049
    },
    {
      "epoch": 2.207317073170732,
      "grad_norm": 0.13876885175704956,
      "learning_rate": 8.131115373713125e-06,
      "loss": 0.0137,
      "step": 9050
    },
    {
      "epoch": 2.207560975609756,
      "grad_norm": 0.09652213007211685,
      "learning_rate": 8.126402887309084e-06,
      "loss": 0.0179,
      "step": 9051
    },
    {
      "epoch": 2.2078048780487807,
      "grad_norm": 0.1168280690908432,
      "learning_rate": 8.121691501855594e-06,
      "loss": 0.0094,
      "step": 9052
    },
    {
      "epoch": 2.208048780487805,
      "grad_norm": 0.13307958841323853,
      "learning_rate": 8.116981217660071e-06,
      "loss": 0.0146,
      "step": 9053
    },
    {
      "epoch": 2.2082926829268295,
      "grad_norm": 0.1404031366109848,
      "learning_rate": 8.11227203502984e-06,
      "loss": 0.0119,
      "step": 9054
    },
    {
      "epoch": 2.2085365853658536,
      "grad_norm": 0.06060539558529854,
      "learning_rate": 8.107563954272154e-06,
      "loss": 0.0054,
      "step": 9055
    },
    {
      "epoch": 2.2087804878048782,
      "grad_norm": 0.0842752531170845,
      "learning_rate": 8.102856975694214e-06,
      "loss": 0.0094,
      "step": 9056
    },
    {
      "epoch": 2.2090243902439024,
      "grad_norm": 0.12724284827709198,
      "learning_rate": 8.09815109960313e-06,
      "loss": 0.0139,
      "step": 9057
    },
    {
      "epoch": 2.209268292682927,
      "grad_norm": 0.20771445333957672,
      "learning_rate": 8.093446326305942e-06,
      "loss": 0.0348,
      "step": 9058
    },
    {
      "epoch": 2.209512195121951,
      "grad_norm": 0.06791459023952484,
      "learning_rate": 8.08874265610963e-06,
      "loss": 0.0111,
      "step": 9059
    },
    {
      "epoch": 2.209756097560976,
      "grad_norm": 0.22137537598609924,
      "learning_rate": 8.084040089321076e-06,
      "loss": 0.0142,
      "step": 9060
    },
    {
      "epoch": 2.21,
      "grad_norm": 0.06917613744735718,
      "learning_rate": 8.079338626247135e-06,
      "loss": 0.006,
      "step": 9061
    },
    {
      "epoch": 2.2102439024390246,
      "grad_norm": 0.09584382176399231,
      "learning_rate": 8.074638267194545e-06,
      "loss": 0.0121,
      "step": 9062
    },
    {
      "epoch": 2.2104878048780487,
      "grad_norm": 0.13989073038101196,
      "learning_rate": 8.06993901246999e-06,
      "loss": 0.0282,
      "step": 9063
    },
    {
      "epoch": 2.2107317073170734,
      "grad_norm": 0.10634051263332367,
      "learning_rate": 8.065240862380095e-06,
      "loss": 0.0129,
      "step": 9064
    },
    {
      "epoch": 2.2109756097560975,
      "grad_norm": 0.14706604182720184,
      "learning_rate": 8.060543817231378e-06,
      "loss": 0.0209,
      "step": 9065
    },
    {
      "epoch": 2.211219512195122,
      "grad_norm": 0.12320998311042786,
      "learning_rate": 8.055847877330333e-06,
      "loss": 0.0088,
      "step": 9066
    },
    {
      "epoch": 2.2114634146341463,
      "grad_norm": 0.13151900470256805,
      "learning_rate": 8.051153042983342e-06,
      "loss": 0.0131,
      "step": 9067
    },
    {
      "epoch": 2.211707317073171,
      "grad_norm": 0.0741107240319252,
      "learning_rate": 8.04645931449672e-06,
      "loss": 0.0069,
      "step": 9068
    },
    {
      "epoch": 2.211951219512195,
      "grad_norm": 0.10140117257833481,
      "learning_rate": 8.041766692176739e-06,
      "loss": 0.0185,
      "step": 9069
    },
    {
      "epoch": 2.2121951219512197,
      "grad_norm": 0.24688194692134857,
      "learning_rate": 8.037075176329564e-06,
      "loss": 0.0178,
      "step": 9070
    },
    {
      "epoch": 2.212439024390244,
      "grad_norm": 0.10427605360746384,
      "learning_rate": 8.0323847672613e-06,
      "loss": 0.013,
      "step": 9071
    },
    {
      "epoch": 2.2126829268292685,
      "grad_norm": 0.0938025563955307,
      "learning_rate": 8.027695465277992e-06,
      "loss": 0.0186,
      "step": 9072
    },
    {
      "epoch": 2.2129268292682926,
      "grad_norm": 0.095157690346241,
      "learning_rate": 8.0230072706856e-06,
      "loss": 0.0134,
      "step": 9073
    },
    {
      "epoch": 2.2131707317073173,
      "grad_norm": 0.1733129918575287,
      "learning_rate": 8.018320183790012e-06,
      "loss": 0.0171,
      "step": 9074
    },
    {
      "epoch": 2.2134146341463414,
      "grad_norm": 0.17023812234401703,
      "learning_rate": 8.013634204897047e-06,
      "loss": 0.0163,
      "step": 9075
    },
    {
      "epoch": 2.213658536585366,
      "grad_norm": 0.1834871619939804,
      "learning_rate": 8.00894933431244e-06,
      "loss": 0.0146,
      "step": 9076
    },
    {
      "epoch": 2.21390243902439,
      "grad_norm": 0.07129272818565369,
      "learning_rate": 8.004265572341882e-06,
      "loss": 0.0102,
      "step": 9077
    },
    {
      "epoch": 2.214146341463415,
      "grad_norm": 0.11748917400836945,
      "learning_rate": 7.99958291929096e-06,
      "loss": 0.0191,
      "step": 9078
    },
    {
      "epoch": 2.214390243902439,
      "grad_norm": 0.08318320661783218,
      "learning_rate": 7.994901375465216e-06,
      "loss": 0.0114,
      "step": 9079
    },
    {
      "epoch": 2.2146341463414636,
      "grad_norm": 0.10525830090045929,
      "learning_rate": 7.990220941170102e-06,
      "loss": 0.0215,
      "step": 9080
    },
    {
      "epoch": 2.2148780487804878,
      "grad_norm": 0.13416092097759247,
      "learning_rate": 7.98554161671099e-06,
      "loss": 0.0197,
      "step": 9081
    },
    {
      "epoch": 2.2151219512195124,
      "grad_norm": 0.05747583135962486,
      "learning_rate": 7.980863402393207e-06,
      "loss": 0.0134,
      "step": 9082
    },
    {
      "epoch": 2.2153658536585366,
      "grad_norm": 0.08856310695409775,
      "learning_rate": 7.97618629852199e-06,
      "loss": 0.0088,
      "step": 9083
    },
    {
      "epoch": 2.215609756097561,
      "grad_norm": 0.20471103489398956,
      "learning_rate": 7.971510305402492e-06,
      "loss": 0.02,
      "step": 9084
    },
    {
      "epoch": 2.2158536585365853,
      "grad_norm": 0.1509629189968109,
      "learning_rate": 7.966835423339825e-06,
      "loss": 0.0185,
      "step": 9085
    },
    {
      "epoch": 2.21609756097561,
      "grad_norm": 0.17001783847808838,
      "learning_rate": 7.962161652638997e-06,
      "loss": 0.0134,
      "step": 9086
    },
    {
      "epoch": 2.216341463414634,
      "grad_norm": 0.2156124860048294,
      "learning_rate": 7.95748899360497e-06,
      "loss": 0.0292,
      "step": 9087
    },
    {
      "epoch": 2.2165853658536587,
      "grad_norm": 0.10745919495820999,
      "learning_rate": 7.952817446542615e-06,
      "loss": 0.0092,
      "step": 9088
    },
    {
      "epoch": 2.216829268292683,
      "grad_norm": 0.08599862456321716,
      "learning_rate": 7.948147011756727e-06,
      "loss": 0.0182,
      "step": 9089
    },
    {
      "epoch": 2.2170731707317075,
      "grad_norm": 0.14590701460838318,
      "learning_rate": 7.943477689552061e-06,
      "loss": 0.0198,
      "step": 9090
    },
    {
      "epoch": 2.2173170731707317,
      "grad_norm": 0.09155325591564178,
      "learning_rate": 7.938809480233245e-06,
      "loss": 0.0153,
      "step": 9091
    },
    {
      "epoch": 2.2175609756097563,
      "grad_norm": 0.07985147833824158,
      "learning_rate": 7.93414238410489e-06,
      "loss": 0.0128,
      "step": 9092
    },
    {
      "epoch": 2.2178048780487805,
      "grad_norm": 0.09968104213476181,
      "learning_rate": 7.9294764014715e-06,
      "loss": 0.0066,
      "step": 9093
    },
    {
      "epoch": 2.218048780487805,
      "grad_norm": 0.07826104015111923,
      "learning_rate": 7.924811532637508e-06,
      "loss": 0.0153,
      "step": 9094
    },
    {
      "epoch": 2.2182926829268292,
      "grad_norm": 0.0997174009680748,
      "learning_rate": 7.920147777907302e-06,
      "loss": 0.0168,
      "step": 9095
    },
    {
      "epoch": 2.218536585365854,
      "grad_norm": 0.09031016379594803,
      "learning_rate": 7.915485137585166e-06,
      "loss": 0.0129,
      "step": 9096
    },
    {
      "epoch": 2.218780487804878,
      "grad_norm": 0.19727198779582977,
      "learning_rate": 7.910823611975316e-06,
      "loss": 0.0185,
      "step": 9097
    },
    {
      "epoch": 2.2190243902439026,
      "grad_norm": 0.10536083579063416,
      "learning_rate": 7.906163201381917e-06,
      "loss": 0.0216,
      "step": 9098
    },
    {
      "epoch": 2.219268292682927,
      "grad_norm": 0.19264943897724152,
      "learning_rate": 7.901503906109032e-06,
      "loss": 0.0192,
      "step": 9099
    },
    {
      "epoch": 2.2195121951219514,
      "grad_norm": 0.10214710980653763,
      "learning_rate": 7.89684572646068e-06,
      "loss": 0.02,
      "step": 9100
    },
    {
      "epoch": 2.2197560975609756,
      "grad_norm": 0.1633559912443161,
      "learning_rate": 7.89218866274079e-06,
      "loss": 0.019,
      "step": 9101
    },
    {
      "epoch": 2.22,
      "grad_norm": 0.10351257771253586,
      "learning_rate": 7.887532715253209e-06,
      "loss": 0.0156,
      "step": 9102
    },
    {
      "epoch": 2.2202439024390244,
      "grad_norm": 0.08480382710695267,
      "learning_rate": 7.88287788430174e-06,
      "loss": 0.017,
      "step": 9103
    },
    {
      "epoch": 2.220487804878049,
      "grad_norm": 0.1247243657708168,
      "learning_rate": 7.878224170190087e-06,
      "loss": 0.0204,
      "step": 9104
    },
    {
      "epoch": 2.220731707317073,
      "grad_norm": 0.14015650749206543,
      "learning_rate": 7.873571573221886e-06,
      "loss": 0.0204,
      "step": 9105
    },
    {
      "epoch": 2.2209756097560978,
      "grad_norm": 0.10771878808736801,
      "learning_rate": 7.868920093700728e-06,
      "loss": 0.0093,
      "step": 9106
    },
    {
      "epoch": 2.221219512195122,
      "grad_norm": 0.16097332537174225,
      "learning_rate": 7.86426973193007e-06,
      "loss": 0.0127,
      "step": 9107
    },
    {
      "epoch": 2.2214634146341465,
      "grad_norm": 0.142170250415802,
      "learning_rate": 7.859620488213365e-06,
      "loss": 0.0146,
      "step": 9108
    },
    {
      "epoch": 2.2217073170731707,
      "grad_norm": 0.10191221535205841,
      "learning_rate": 7.854972362853951e-06,
      "loss": 0.0164,
      "step": 9109
    },
    {
      "epoch": 2.2219512195121953,
      "grad_norm": 0.08698556572198868,
      "learning_rate": 7.850325356155098e-06,
      "loss": 0.0138,
      "step": 9110
    },
    {
      "epoch": 2.2221951219512195,
      "grad_norm": 0.11099119484424591,
      "learning_rate": 7.845679468420022e-06,
      "loss": 0.0152,
      "step": 9111
    },
    {
      "epoch": 2.222439024390244,
      "grad_norm": 0.05935295671224594,
      "learning_rate": 7.841034699951839e-06,
      "loss": 0.0051,
      "step": 9112
    },
    {
      "epoch": 2.2226829268292683,
      "grad_norm": 0.1128087118268013,
      "learning_rate": 7.83639105105362e-06,
      "loss": 0.0161,
      "step": 9113
    },
    {
      "epoch": 2.222926829268293,
      "grad_norm": 0.11369079351425171,
      "learning_rate": 7.831748522028343e-06,
      "loss": 0.0122,
      "step": 9114
    },
    {
      "epoch": 2.223170731707317,
      "grad_norm": 0.1373448520898819,
      "learning_rate": 7.82710711317891e-06,
      "loss": 0.0148,
      "step": 9115
    },
    {
      "epoch": 2.2234146341463417,
      "grad_norm": 0.10976912081241608,
      "learning_rate": 7.822466824808175e-06,
      "loss": 0.019,
      "step": 9116
    },
    {
      "epoch": 2.223658536585366,
      "grad_norm": 0.1426529884338379,
      "learning_rate": 7.817827657218893e-06,
      "loss": 0.0147,
      "step": 9117
    },
    {
      "epoch": 2.2239024390243904,
      "grad_norm": 0.07014971971511841,
      "learning_rate": 7.813189610713748e-06,
      "loss": 0.0122,
      "step": 9118
    },
    {
      "epoch": 2.2241463414634146,
      "grad_norm": 0.15499205887317657,
      "learning_rate": 7.808552685595378e-06,
      "loss": 0.0389,
      "step": 9119
    },
    {
      "epoch": 2.2243902439024392,
      "grad_norm": 0.1342805176973343,
      "learning_rate": 7.803916882166307e-06,
      "loss": 0.0119,
      "step": 9120
    },
    {
      "epoch": 2.2246341463414634,
      "grad_norm": 0.11271445453166962,
      "learning_rate": 7.799282200729025e-06,
      "loss": 0.011,
      "step": 9121
    },
    {
      "epoch": 2.224878048780488,
      "grad_norm": 0.15462256968021393,
      "learning_rate": 7.794648641585922e-06,
      "loss": 0.0264,
      "step": 9122
    },
    {
      "epoch": 2.225121951219512,
      "grad_norm": 0.1688927710056305,
      "learning_rate": 7.790016205039328e-06,
      "loss": 0.023,
      "step": 9123
    },
    {
      "epoch": 2.225365853658537,
      "grad_norm": 0.13870662450790405,
      "learning_rate": 7.78538489139149e-06,
      "loss": 0.0135,
      "step": 9124
    },
    {
      "epoch": 2.225609756097561,
      "grad_norm": 0.08853760361671448,
      "learning_rate": 7.780754700944578e-06,
      "loss": 0.0142,
      "step": 9125
    },
    {
      "epoch": 2.2258536585365856,
      "grad_norm": 0.11243286728858948,
      "learning_rate": 7.776125634000722e-06,
      "loss": 0.0235,
      "step": 9126
    },
    {
      "epoch": 2.2260975609756097,
      "grad_norm": 0.09187249094247818,
      "learning_rate": 7.771497690861937e-06,
      "loss": 0.0106,
      "step": 9127
    },
    {
      "epoch": 2.2263414634146343,
      "grad_norm": 0.1424763798713684,
      "learning_rate": 7.766870871830179e-06,
      "loss": 0.0215,
      "step": 9128
    },
    {
      "epoch": 2.2265853658536585,
      "grad_norm": 0.09421014785766602,
      "learning_rate": 7.76224517720735e-06,
      "loss": 0.0179,
      "step": 9129
    },
    {
      "epoch": 2.226829268292683,
      "grad_norm": 0.10076664388179779,
      "learning_rate": 7.757620607295252e-06,
      "loss": 0.0113,
      "step": 9130
    },
    {
      "epoch": 2.2270731707317073,
      "grad_norm": 0.11574218422174454,
      "learning_rate": 7.75299716239562e-06,
      "loss": 0.0185,
      "step": 9131
    },
    {
      "epoch": 2.227317073170732,
      "grad_norm": 0.11290226876735687,
      "learning_rate": 7.74837484281013e-06,
      "loss": 0.02,
      "step": 9132
    },
    {
      "epoch": 2.227560975609756,
      "grad_norm": 0.1458580046892166,
      "learning_rate": 7.743753648840363e-06,
      "loss": 0.0198,
      "step": 9133
    },
    {
      "epoch": 2.2278048780487807,
      "grad_norm": 0.19598594307899475,
      "learning_rate": 7.73913358078785e-06,
      "loss": 0.0287,
      "step": 9134
    },
    {
      "epoch": 2.228048780487805,
      "grad_norm": 0.08500778675079346,
      "learning_rate": 7.734514638954032e-06,
      "loss": 0.0112,
      "step": 9135
    },
    {
      "epoch": 2.2282926829268295,
      "grad_norm": 0.14451570808887482,
      "learning_rate": 7.72989682364027e-06,
      "loss": 0.0157,
      "step": 9136
    },
    {
      "epoch": 2.2285365853658536,
      "grad_norm": 0.1642223596572876,
      "learning_rate": 7.72528013514788e-06,
      "loss": 0.0147,
      "step": 9137
    },
    {
      "epoch": 2.2287804878048783,
      "grad_norm": 0.09349368512630463,
      "learning_rate": 7.72066457377808e-06,
      "loss": 0.0172,
      "step": 9138
    },
    {
      "epoch": 2.2290243902439024,
      "grad_norm": 0.05623159185051918,
      "learning_rate": 7.716050139832018e-06,
      "loss": 0.0055,
      "step": 9139
    },
    {
      "epoch": 2.229268292682927,
      "grad_norm": 0.09019173681735992,
      "learning_rate": 7.711436833610772e-06,
      "loss": 0.0153,
      "step": 9140
    },
    {
      "epoch": 2.229512195121951,
      "grad_norm": 0.1167636513710022,
      "learning_rate": 7.70682465541534e-06,
      "loss": 0.0147,
      "step": 9141
    },
    {
      "epoch": 2.229756097560976,
      "grad_norm": 0.11180073767900467,
      "learning_rate": 7.702213605546669e-06,
      "loss": 0.0126,
      "step": 9142
    },
    {
      "epoch": 2.23,
      "grad_norm": 0.1608647108078003,
      "learning_rate": 7.697603684305604e-06,
      "loss": 0.0263,
      "step": 9143
    },
    {
      "epoch": 2.2302439024390246,
      "grad_norm": 0.12363199889659882,
      "learning_rate": 7.692994891992925e-06,
      "loss": 0.0152,
      "step": 9144
    },
    {
      "epoch": 2.2304878048780488,
      "grad_norm": 0.15272966027259827,
      "learning_rate": 7.688387228909355e-06,
      "loss": 0.0144,
      "step": 9145
    },
    {
      "epoch": 2.2307317073170734,
      "grad_norm": 0.15584638714790344,
      "learning_rate": 7.683780695355514e-06,
      "loss": 0.0188,
      "step": 9146
    },
    {
      "epoch": 2.2309756097560975,
      "grad_norm": 0.11573193222284317,
      "learning_rate": 7.67917529163198e-06,
      "loss": 0.0208,
      "step": 9147
    },
    {
      "epoch": 2.231219512195122,
      "grad_norm": 0.09021185338497162,
      "learning_rate": 7.674571018039234e-06,
      "loss": 0.0136,
      "step": 9148
    },
    {
      "epoch": 2.2314634146341463,
      "grad_norm": 0.05690111219882965,
      "learning_rate": 7.669967874877684e-06,
      "loss": 0.0088,
      "step": 9149
    },
    {
      "epoch": 2.231707317073171,
      "grad_norm": 0.07912565767765045,
      "learning_rate": 7.665365862447685e-06,
      "loss": 0.0131,
      "step": 9150
    },
    {
      "epoch": 2.231951219512195,
      "grad_norm": 0.12709760665893555,
      "learning_rate": 7.660764981049498e-06,
      "loss": 0.0126,
      "step": 9151
    },
    {
      "epoch": 2.2321951219512197,
      "grad_norm": 0.20443709194660187,
      "learning_rate": 7.656165230983309e-06,
      "loss": 0.0189,
      "step": 9152
    },
    {
      "epoch": 2.232439024390244,
      "grad_norm": 0.17129063606262207,
      "learning_rate": 7.651566612549249e-06,
      "loss": 0.0141,
      "step": 9153
    },
    {
      "epoch": 2.2326829268292685,
      "grad_norm": 0.11466798186302185,
      "learning_rate": 7.646969126047359e-06,
      "loss": 0.0152,
      "step": 9154
    },
    {
      "epoch": 2.2329268292682927,
      "grad_norm": 0.28290697932243347,
      "learning_rate": 7.642372771777605e-06,
      "loss": 0.0365,
      "step": 9155
    },
    {
      "epoch": 2.2331707317073173,
      "grad_norm": 0.1311292052268982,
      "learning_rate": 7.637777550039906e-06,
      "loss": 0.0177,
      "step": 9156
    },
    {
      "epoch": 2.2334146341463414,
      "grad_norm": 0.08625508844852448,
      "learning_rate": 7.633183461134056e-06,
      "loss": 0.0109,
      "step": 9157
    },
    {
      "epoch": 2.233658536585366,
      "grad_norm": 0.07983187586069107,
      "learning_rate": 7.628590505359825e-06,
      "loss": 0.012,
      "step": 9158
    },
    {
      "epoch": 2.2339024390243902,
      "grad_norm": 0.08432146906852722,
      "learning_rate": 7.623998683016878e-06,
      "loss": 0.0167,
      "step": 9159
    },
    {
      "epoch": 2.234146341463415,
      "grad_norm": 0.08271290361881256,
      "learning_rate": 7.619407994404834e-06,
      "loss": 0.0168,
      "step": 9160
    },
    {
      "epoch": 2.234390243902439,
      "grad_norm": 0.09780348092317581,
      "learning_rate": 7.61481843982321e-06,
      "loss": 0.0151,
      "step": 9161
    },
    {
      "epoch": 2.234634146341463,
      "grad_norm": 0.13269802927970886,
      "learning_rate": 7.610230019571452e-06,
      "loss": 0.0313,
      "step": 9162
    },
    {
      "epoch": 2.234878048780488,
      "grad_norm": 0.17925408482551575,
      "learning_rate": 7.6056427339489585e-06,
      "loss": 0.0174,
      "step": 9163
    },
    {
      "epoch": 2.2351219512195124,
      "grad_norm": 0.187400221824646,
      "learning_rate": 7.601056583255029e-06,
      "loss": 0.034,
      "step": 9164
    },
    {
      "epoch": 2.2353658536585366,
      "grad_norm": 0.06820393353700638,
      "learning_rate": 7.596471567788885e-06,
      "loss": 0.0208,
      "step": 9165
    },
    {
      "epoch": 2.2356097560975607,
      "grad_norm": 0.085249163210392,
      "learning_rate": 7.591887687849703e-06,
      "loss": 0.0117,
      "step": 9166
    },
    {
      "epoch": 2.2358536585365854,
      "grad_norm": 0.12012477219104767,
      "learning_rate": 7.587304943736559e-06,
      "loss": 0.0213,
      "step": 9167
    },
    {
      "epoch": 2.23609756097561,
      "grad_norm": 0.15668006241321564,
      "learning_rate": 7.582723335748451e-06,
      "loss": 0.0199,
      "step": 9168
    },
    {
      "epoch": 2.236341463414634,
      "grad_norm": 0.0686793327331543,
      "learning_rate": 7.578142864184335e-06,
      "loss": 0.0075,
      "step": 9169
    },
    {
      "epoch": 2.2365853658536583,
      "grad_norm": 0.13543526828289032,
      "learning_rate": 7.573563529343056e-06,
      "loss": 0.0181,
      "step": 9170
    },
    {
      "epoch": 2.236829268292683,
      "grad_norm": 0.15482594072818756,
      "learning_rate": 7.568985331523417e-06,
      "loss": 0.0102,
      "step": 9171
    },
    {
      "epoch": 2.2370731707317075,
      "grad_norm": 0.09098118543624878,
      "learning_rate": 7.5644082710241235e-06,
      "loss": 0.0094,
      "step": 9172
    },
    {
      "epoch": 2.2373170731707317,
      "grad_norm": 0.16957201063632965,
      "learning_rate": 7.559832348143817e-06,
      "loss": 0.0131,
      "step": 9173
    },
    {
      "epoch": 2.237560975609756,
      "grad_norm": 0.15083937346935272,
      "learning_rate": 7.555257563181059e-06,
      "loss": 0.0226,
      "step": 9174
    },
    {
      "epoch": 2.2378048780487805,
      "grad_norm": 0.24496927857398987,
      "learning_rate": 7.5506839164343315e-06,
      "loss": 0.0193,
      "step": 9175
    },
    {
      "epoch": 2.238048780487805,
      "grad_norm": 0.17851901054382324,
      "learning_rate": 7.546111408202072e-06,
      "loss": 0.0142,
      "step": 9176
    },
    {
      "epoch": 2.2382926829268293,
      "grad_norm": 0.15627817809581757,
      "learning_rate": 7.5415400387826105e-06,
      "loss": 0.0193,
      "step": 9177
    },
    {
      "epoch": 2.2385365853658534,
      "grad_norm": 0.1398289054632187,
      "learning_rate": 7.536969808474206e-06,
      "loss": 0.015,
      "step": 9178
    },
    {
      "epoch": 2.238780487804878,
      "grad_norm": 0.09445188194513321,
      "learning_rate": 7.532400717575072e-06,
      "loss": 0.0139,
      "step": 9179
    },
    {
      "epoch": 2.2390243902439027,
      "grad_norm": 0.07063069194555283,
      "learning_rate": 7.527832766383316e-06,
      "loss": 0.0151,
      "step": 9180
    },
    {
      "epoch": 2.239268292682927,
      "grad_norm": 0.10852283239364624,
      "learning_rate": 7.523265955196978e-06,
      "loss": 0.014,
      "step": 9181
    },
    {
      "epoch": 2.239512195121951,
      "grad_norm": 0.22767607867717743,
      "learning_rate": 7.518700284314045e-06,
      "loss": 0.019,
      "step": 9182
    },
    {
      "epoch": 2.2397560975609756,
      "grad_norm": 0.09716622531414032,
      "learning_rate": 7.514135754032392e-06,
      "loss": 0.0118,
      "step": 9183
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.08172281086444855,
      "learning_rate": 7.509572364649861e-06,
      "loss": 0.0122,
      "step": 9184
    },
    {
      "epoch": 2.2402439024390244,
      "grad_norm": 0.098269023001194,
      "learning_rate": 7.505010116464189e-06,
      "loss": 0.0226,
      "step": 9185
    },
    {
      "epoch": 2.2404878048780485,
      "grad_norm": 0.2111886739730835,
      "learning_rate": 7.500449009773044e-06,
      "loss": 0.0236,
      "step": 9186
    },
    {
      "epoch": 2.240731707317073,
      "grad_norm": 0.19722802937030792,
      "learning_rate": 7.495889044874036e-06,
      "loss": 0.0179,
      "step": 9187
    },
    {
      "epoch": 2.2409756097560978,
      "grad_norm": 0.10223661363124847,
      "learning_rate": 7.491330222064682e-06,
      "loss": 0.0132,
      "step": 9188
    },
    {
      "epoch": 2.241219512195122,
      "grad_norm": 0.0698593258857727,
      "learning_rate": 7.486772541642434e-06,
      "loss": 0.0156,
      "step": 9189
    },
    {
      "epoch": 2.241463414634146,
      "grad_norm": 0.08211702108383179,
      "learning_rate": 7.482216003904663e-06,
      "loss": 0.0159,
      "step": 9190
    },
    {
      "epoch": 2.2417073170731707,
      "grad_norm": 0.15999248623847961,
      "learning_rate": 7.477660609148662e-06,
      "loss": 0.0129,
      "step": 9191
    },
    {
      "epoch": 2.2419512195121953,
      "grad_norm": 0.10316663235425949,
      "learning_rate": 7.473106357671672e-06,
      "loss": 0.0239,
      "step": 9192
    },
    {
      "epoch": 2.2421951219512195,
      "grad_norm": 0.10502512753009796,
      "learning_rate": 7.468553249770841e-06,
      "loss": 0.0282,
      "step": 9193
    },
    {
      "epoch": 2.2424390243902437,
      "grad_norm": 0.1698400378227234,
      "learning_rate": 7.46400128574323e-06,
      "loss": 0.0241,
      "step": 9194
    },
    {
      "epoch": 2.2426829268292683,
      "grad_norm": 0.09115981310606003,
      "learning_rate": 7.4594504658858615e-06,
      "loss": 0.0174,
      "step": 9195
    },
    {
      "epoch": 2.242926829268293,
      "grad_norm": 0.14217345416545868,
      "learning_rate": 7.454900790495647e-06,
      "loss": 0.0147,
      "step": 9196
    },
    {
      "epoch": 2.243170731707317,
      "grad_norm": 0.2556186318397522,
      "learning_rate": 7.4503522598694535e-06,
      "loss": 0.022,
      "step": 9197
    },
    {
      "epoch": 2.2434146341463412,
      "grad_norm": 0.17557242512702942,
      "learning_rate": 7.445804874304046e-06,
      "loss": 0.0223,
      "step": 9198
    },
    {
      "epoch": 2.243658536585366,
      "grad_norm": 0.12324613332748413,
      "learning_rate": 7.44125863409613e-06,
      "loss": 0.0081,
      "step": 9199
    },
    {
      "epoch": 2.2439024390243905,
      "grad_norm": 0.08733300864696503,
      "learning_rate": 7.4367135395423395e-06,
      "loss": 0.011,
      "step": 9200
    },
    {
      "epoch": 2.2441463414634146,
      "grad_norm": 0.1259046047925949,
      "learning_rate": 7.4321695909392255e-06,
      "loss": 0.035,
      "step": 9201
    },
    {
      "epoch": 2.244390243902439,
      "grad_norm": 0.08926764875650406,
      "learning_rate": 7.427626788583256e-06,
      "loss": 0.0106,
      "step": 9202
    },
    {
      "epoch": 2.2446341463414634,
      "grad_norm": 0.21001817286014557,
      "learning_rate": 7.423085132770854e-06,
      "loss": 0.0221,
      "step": 9203
    },
    {
      "epoch": 2.244878048780488,
      "grad_norm": 0.25961145758628845,
      "learning_rate": 7.418544623798329e-06,
      "loss": 0.0339,
      "step": 9204
    },
    {
      "epoch": 2.245121951219512,
      "grad_norm": 0.11325317621231079,
      "learning_rate": 7.414005261961962e-06,
      "loss": 0.018,
      "step": 9205
    },
    {
      "epoch": 2.2453658536585364,
      "grad_norm": 0.1965802162885666,
      "learning_rate": 7.4094670475579105e-06,
      "loss": 0.0172,
      "step": 9206
    },
    {
      "epoch": 2.245609756097561,
      "grad_norm": 0.11028830707073212,
      "learning_rate": 7.404929980882272e-06,
      "loss": 0.0246,
      "step": 9207
    },
    {
      "epoch": 2.2458536585365856,
      "grad_norm": 0.15791037678718567,
      "learning_rate": 7.4003940622311e-06,
      "loss": 0.0225,
      "step": 9208
    },
    {
      "epoch": 2.2460975609756098,
      "grad_norm": 0.10232614725828171,
      "learning_rate": 7.395859291900329e-06,
      "loss": 0.0155,
      "step": 9209
    },
    {
      "epoch": 2.246341463414634,
      "grad_norm": 0.1529746651649475,
      "learning_rate": 7.391325670185856e-06,
      "loss": 0.0146,
      "step": 9210
    },
    {
      "epoch": 2.2465853658536585,
      "grad_norm": 0.11931639164686203,
      "learning_rate": 7.386793197383476e-06,
      "loss": 0.0182,
      "step": 9211
    },
    {
      "epoch": 2.246829268292683,
      "grad_norm": 0.11561321467161179,
      "learning_rate": 7.382261873788915e-06,
      "loss": 0.018,
      "step": 9212
    },
    {
      "epoch": 2.2470731707317073,
      "grad_norm": 0.363811731338501,
      "learning_rate": 7.37773169969784e-06,
      "loss": 0.0117,
      "step": 9213
    },
    {
      "epoch": 2.2473170731707315,
      "grad_norm": 0.07565806061029434,
      "learning_rate": 7.373202675405824e-06,
      "loss": 0.013,
      "step": 9214
    },
    {
      "epoch": 2.247560975609756,
      "grad_norm": 0.09497026354074478,
      "learning_rate": 7.3686748012083665e-06,
      "loss": 0.0166,
      "step": 9215
    },
    {
      "epoch": 2.2478048780487807,
      "grad_norm": 0.12752152979373932,
      "learning_rate": 7.364148077400912e-06,
      "loss": 0.028,
      "step": 9216
    },
    {
      "epoch": 2.248048780487805,
      "grad_norm": 0.12331973016262054,
      "learning_rate": 7.359622504278796e-06,
      "loss": 0.0119,
      "step": 9217
    },
    {
      "epoch": 2.248292682926829,
      "grad_norm": 0.06306610256433487,
      "learning_rate": 7.355098082137321e-06,
      "loss": 0.0105,
      "step": 9218
    },
    {
      "epoch": 2.2485365853658537,
      "grad_norm": 0.10205540060997009,
      "learning_rate": 7.350574811271676e-06,
      "loss": 0.016,
      "step": 9219
    },
    {
      "epoch": 2.2487804878048783,
      "grad_norm": 0.08356275409460068,
      "learning_rate": 7.34605269197699e-06,
      "loss": 0.0141,
      "step": 9220
    },
    {
      "epoch": 2.2490243902439024,
      "grad_norm": 0.2404555082321167,
      "learning_rate": 7.341531724548329e-06,
      "loss": 0.0204,
      "step": 9221
    },
    {
      "epoch": 2.2492682926829266,
      "grad_norm": 0.26651111245155334,
      "learning_rate": 7.337011909280664e-06,
      "loss": 0.0272,
      "step": 9222
    },
    {
      "epoch": 2.249512195121951,
      "grad_norm": 0.05842183530330658,
      "learning_rate": 7.3324932464689e-06,
      "loss": 0.0049,
      "step": 9223
    },
    {
      "epoch": 2.249756097560976,
      "grad_norm": 0.16479630768299103,
      "learning_rate": 7.327975736407866e-06,
      "loss": 0.0172,
      "step": 9224
    },
    {
      "epoch": 2.25,
      "grad_norm": 0.06353393197059631,
      "learning_rate": 7.323459379392311e-06,
      "loss": 0.0062,
      "step": 9225
    },
    {
      "epoch": 2.250243902439024,
      "grad_norm": 0.11340897530317307,
      "learning_rate": 7.318944175716924e-06,
      "loss": 0.012,
      "step": 9226
    },
    {
      "epoch": 2.250487804878049,
      "grad_norm": 0.11649966239929199,
      "learning_rate": 7.314430125676303e-06,
      "loss": 0.0082,
      "step": 9227
    },
    {
      "epoch": 2.2507317073170734,
      "grad_norm": 0.09853161126375198,
      "learning_rate": 7.3099172295649665e-06,
      "loss": 0.0181,
      "step": 9228
    },
    {
      "epoch": 2.2509756097560976,
      "grad_norm": 0.06881562620401382,
      "learning_rate": 7.3054054876773855e-06,
      "loss": 0.0128,
      "step": 9229
    },
    {
      "epoch": 2.2512195121951217,
      "grad_norm": 0.1629607230424881,
      "learning_rate": 7.30089490030792e-06,
      "loss": 0.0206,
      "step": 9230
    },
    {
      "epoch": 2.2514634146341463,
      "grad_norm": 0.08006057888269424,
      "learning_rate": 7.296385467750888e-06,
      "loss": 0.0093,
      "step": 9231
    },
    {
      "epoch": 2.251707317073171,
      "grad_norm": 0.091053307056427,
      "learning_rate": 7.291877190300509e-06,
      "loss": 0.0171,
      "step": 9232
    },
    {
      "epoch": 2.251951219512195,
      "grad_norm": 0.10703577846288681,
      "learning_rate": 7.287370068250926e-06,
      "loss": 0.0159,
      "step": 9233
    },
    {
      "epoch": 2.2521951219512193,
      "grad_norm": 0.12682031095027924,
      "learning_rate": 7.282864101896228e-06,
      "loss": 0.0138,
      "step": 9234
    },
    {
      "epoch": 2.252439024390244,
      "grad_norm": 0.16284266114234924,
      "learning_rate": 7.278359291530412e-06,
      "loss": 0.027,
      "step": 9235
    },
    {
      "epoch": 2.2526829268292685,
      "grad_norm": 0.158376544713974,
      "learning_rate": 7.273855637447391e-06,
      "loss": 0.0247,
      "step": 9236
    },
    {
      "epoch": 2.2529268292682927,
      "grad_norm": 0.19575171172618866,
      "learning_rate": 7.269353139941035e-06,
      "loss": 0.0202,
      "step": 9237
    },
    {
      "epoch": 2.253170731707317,
      "grad_norm": 0.10008643567562103,
      "learning_rate": 7.264851799305109e-06,
      "loss": 0.0173,
      "step": 9238
    },
    {
      "epoch": 2.2534146341463415,
      "grad_norm": 0.14525654911994934,
      "learning_rate": 7.260351615833308e-06,
      "loss": 0.0348,
      "step": 9239
    },
    {
      "epoch": 2.253658536585366,
      "grad_norm": 0.1376826912164688,
      "learning_rate": 7.255852589819256e-06,
      "loss": 0.0219,
      "step": 9240
    },
    {
      "epoch": 2.2539024390243902,
      "grad_norm": 0.10443990677595139,
      "learning_rate": 7.251354721556497e-06,
      "loss": 0.0116,
      "step": 9241
    },
    {
      "epoch": 2.2541463414634144,
      "grad_norm": 0.1520317792892456,
      "learning_rate": 7.246858011338517e-06,
      "loss": 0.0125,
      "step": 9242
    },
    {
      "epoch": 2.254390243902439,
      "grad_norm": 0.12339629977941513,
      "learning_rate": 7.242362459458696e-06,
      "loss": 0.0211,
      "step": 9243
    },
    {
      "epoch": 2.2546341463414636,
      "grad_norm": 0.13171908259391785,
      "learning_rate": 7.2378680662103706e-06,
      "loss": 0.0266,
      "step": 9244
    },
    {
      "epoch": 2.254878048780488,
      "grad_norm": 0.14876459538936615,
      "learning_rate": 7.23337483188678e-06,
      "loss": 0.0106,
      "step": 9245
    },
    {
      "epoch": 2.255121951219512,
      "grad_norm": 0.17051857709884644,
      "learning_rate": 7.228882756781083e-06,
      "loss": 0.0214,
      "step": 9246
    },
    {
      "epoch": 2.2553658536585366,
      "grad_norm": 0.12241639196872711,
      "learning_rate": 7.224391841186392e-06,
      "loss": 0.0154,
      "step": 9247
    },
    {
      "epoch": 2.255609756097561,
      "grad_norm": 0.1353539675474167,
      "learning_rate": 7.2199020853957185e-06,
      "loss": 0.0148,
      "step": 9248
    },
    {
      "epoch": 2.2558536585365854,
      "grad_norm": 0.13922500610351562,
      "learning_rate": 7.215413489701994e-06,
      "loss": 0.0268,
      "step": 9249
    },
    {
      "epoch": 2.2560975609756095,
      "grad_norm": 0.4130847454071045,
      "learning_rate": 7.2109260543981035e-06,
      "loss": 0.0209,
      "step": 9250
    },
    {
      "epoch": 2.256341463414634,
      "grad_norm": 0.10733553022146225,
      "learning_rate": 7.206439779776825e-06,
      "loss": 0.0121,
      "step": 9251
    },
    {
      "epoch": 2.2565853658536588,
      "grad_norm": 0.1336221843957901,
      "learning_rate": 7.201954666130883e-06,
      "loss": 0.0109,
      "step": 9252
    },
    {
      "epoch": 2.256829268292683,
      "grad_norm": 0.06864382326602936,
      "learning_rate": 7.197470713752918e-06,
      "loss": 0.0104,
      "step": 9253
    },
    {
      "epoch": 2.257073170731707,
      "grad_norm": 0.0769418403506279,
      "learning_rate": 7.192987922935482e-06,
      "loss": 0.0114,
      "step": 9254
    },
    {
      "epoch": 2.2573170731707317,
      "grad_norm": 0.10115931183099747,
      "learning_rate": 7.188506293971084e-06,
      "loss": 0.0159,
      "step": 9255
    },
    {
      "epoch": 2.2575609756097563,
      "grad_norm": 0.1823274940252304,
      "learning_rate": 7.18402582715211e-06,
      "loss": 0.028,
      "step": 9256
    },
    {
      "epoch": 2.2578048780487805,
      "grad_norm": 0.17652486264705658,
      "learning_rate": 7.179546522770919e-06,
      "loss": 0.0246,
      "step": 9257
    },
    {
      "epoch": 2.2580487804878047,
      "grad_norm": 0.20993244647979736,
      "learning_rate": 7.175068381119765e-06,
      "loss": 0.0241,
      "step": 9258
    },
    {
      "epoch": 2.2582926829268293,
      "grad_norm": 0.13014134764671326,
      "learning_rate": 7.170591402490823e-06,
      "loss": 0.022,
      "step": 9259
    },
    {
      "epoch": 2.258536585365854,
      "grad_norm": 0.09972872585058212,
      "learning_rate": 7.16611558717622e-06,
      "loss": 0.0179,
      "step": 9260
    },
    {
      "epoch": 2.258780487804878,
      "grad_norm": 0.11658239364624023,
      "learning_rate": 7.161640935467978e-06,
      "loss": 0.0174,
      "step": 9261
    },
    {
      "epoch": 2.2590243902439022,
      "grad_norm": 0.10084567219018936,
      "learning_rate": 7.157167447658047e-06,
      "loss": 0.0107,
      "step": 9262
    },
    {
      "epoch": 2.259268292682927,
      "grad_norm": 0.13890600204467773,
      "learning_rate": 7.152695124038331e-06,
      "loss": 0.031,
      "step": 9263
    },
    {
      "epoch": 2.2595121951219515,
      "grad_norm": 0.25002193450927734,
      "learning_rate": 7.148223964900611e-06,
      "loss": 0.0301,
      "step": 9264
    },
    {
      "epoch": 2.2597560975609756,
      "grad_norm": 0.17841976881027222,
      "learning_rate": 7.143753970536635e-06,
      "loss": 0.0225,
      "step": 9265
    },
    {
      "epoch": 2.26,
      "grad_norm": 0.13886213302612305,
      "learning_rate": 7.13928514123805e-06,
      "loss": 0.013,
      "step": 9266
    },
    {
      "epoch": 2.2602439024390244,
      "grad_norm": 0.16183212399482727,
      "learning_rate": 7.1348174772964264e-06,
      "loss": 0.0268,
      "step": 9267
    },
    {
      "epoch": 2.260487804878049,
      "grad_norm": 0.12582282721996307,
      "learning_rate": 7.130350979003278e-06,
      "loss": 0.0129,
      "step": 9268
    },
    {
      "epoch": 2.260731707317073,
      "grad_norm": 0.22778905928134918,
      "learning_rate": 7.125885646650024e-06,
      "loss": 0.0259,
      "step": 9269
    },
    {
      "epoch": 2.2609756097560973,
      "grad_norm": 0.09741406887769699,
      "learning_rate": 7.1214214805280066e-06,
      "loss": 0.0139,
      "step": 9270
    },
    {
      "epoch": 2.261219512195122,
      "grad_norm": 0.06412949413061142,
      "learning_rate": 7.116958480928523e-06,
      "loss": 0.0082,
      "step": 9271
    },
    {
      "epoch": 2.2614634146341466,
      "grad_norm": 0.04938177391886711,
      "learning_rate": 7.112496648142736e-06,
      "loss": 0.0059,
      "step": 9272
    },
    {
      "epoch": 2.2617073170731707,
      "grad_norm": 0.1235387921333313,
      "learning_rate": 7.108035982461791e-06,
      "loss": 0.0193,
      "step": 9273
    },
    {
      "epoch": 2.261951219512195,
      "grad_norm": 0.15784212946891785,
      "learning_rate": 7.103576484176727e-06,
      "loss": 0.0196,
      "step": 9274
    },
    {
      "epoch": 2.2621951219512195,
      "grad_norm": 0.08751249313354492,
      "learning_rate": 7.099118153578502e-06,
      "loss": 0.0204,
      "step": 9275
    },
    {
      "epoch": 2.262439024390244,
      "grad_norm": 0.2003447711467743,
      "learning_rate": 7.094660990958024e-06,
      "loss": 0.0321,
      "step": 9276
    },
    {
      "epoch": 2.2626829268292683,
      "grad_norm": 0.16541577875614166,
      "learning_rate": 7.090204996606098e-06,
      "loss": 0.0235,
      "step": 9277
    },
    {
      "epoch": 2.2629268292682925,
      "grad_norm": 0.12396649271249771,
      "learning_rate": 7.085750170813476e-06,
      "loss": 0.0174,
      "step": 9278
    },
    {
      "epoch": 2.263170731707317,
      "grad_norm": 0.12527886033058167,
      "learning_rate": 7.081296513870814e-06,
      "loss": 0.0101,
      "step": 9279
    },
    {
      "epoch": 2.2634146341463417,
      "grad_norm": 0.16508275270462036,
      "learning_rate": 7.0768440260686935e-06,
      "loss": 0.0142,
      "step": 9280
    },
    {
      "epoch": 2.263658536585366,
      "grad_norm": 0.14981739223003387,
      "learning_rate": 7.072392707697639e-06,
      "loss": 0.0283,
      "step": 9281
    },
    {
      "epoch": 2.26390243902439,
      "grad_norm": 0.09943445026874542,
      "learning_rate": 7.067942559048077e-06,
      "loss": 0.0141,
      "step": 9282
    },
    {
      "epoch": 2.2641463414634146,
      "grad_norm": 0.15266574919223785,
      "learning_rate": 7.063493580410363e-06,
      "loss": 0.021,
      "step": 9283
    },
    {
      "epoch": 2.2643902439024393,
      "grad_norm": 0.10159140080213547,
      "learning_rate": 7.059045772074791e-06,
      "loss": 0.0162,
      "step": 9284
    },
    {
      "epoch": 2.2646341463414634,
      "grad_norm": 0.1488310694694519,
      "learning_rate": 7.054599134331557e-06,
      "loss": 0.0198,
      "step": 9285
    },
    {
      "epoch": 2.2648780487804876,
      "grad_norm": 0.15233919024467468,
      "learning_rate": 7.050153667470791e-06,
      "loss": 0.0219,
      "step": 9286
    },
    {
      "epoch": 2.265121951219512,
      "grad_norm": 0.1102941483259201,
      "learning_rate": 7.045709371782552e-06,
      "loss": 0.0088,
      "step": 9287
    },
    {
      "epoch": 2.265365853658537,
      "grad_norm": 0.20301885902881622,
      "learning_rate": 7.041266247556813e-06,
      "loss": 0.0269,
      "step": 9288
    },
    {
      "epoch": 2.265609756097561,
      "grad_norm": 0.15239137411117554,
      "learning_rate": 7.0368242950834775e-06,
      "loss": 0.021,
      "step": 9289
    },
    {
      "epoch": 2.265853658536585,
      "grad_norm": 0.29932478070259094,
      "learning_rate": 7.032383514652355e-06,
      "loss": 0.0269,
      "step": 9290
    },
    {
      "epoch": 2.2660975609756098,
      "grad_norm": 0.09417007863521576,
      "learning_rate": 7.027943906553214e-06,
      "loss": 0.0166,
      "step": 9291
    },
    {
      "epoch": 2.2663414634146344,
      "grad_norm": 0.10069341212511063,
      "learning_rate": 7.023505471075714e-06,
      "loss": 0.0175,
      "step": 9292
    },
    {
      "epoch": 2.2665853658536586,
      "grad_norm": 0.08573119342327118,
      "learning_rate": 7.019068208509444e-06,
      "loss": 0.0113,
      "step": 9293
    },
    {
      "epoch": 2.2668292682926827,
      "grad_norm": 0.39096131920814514,
      "learning_rate": 7.014632119143935e-06,
      "loss": 0.0295,
      "step": 9294
    },
    {
      "epoch": 2.2670731707317073,
      "grad_norm": 0.13157540559768677,
      "learning_rate": 7.010197203268623e-06,
      "loss": 0.0238,
      "step": 9295
    },
    {
      "epoch": 2.267317073170732,
      "grad_norm": 0.1671382635831833,
      "learning_rate": 7.005763461172863e-06,
      "loss": 0.0114,
      "step": 9296
    },
    {
      "epoch": 2.267560975609756,
      "grad_norm": 0.3368861675262451,
      "learning_rate": 7.00133089314596e-06,
      "loss": 0.0422,
      "step": 9297
    },
    {
      "epoch": 2.2678048780487803,
      "grad_norm": 0.19507798552513123,
      "learning_rate": 6.9968994994771166e-06,
      "loss": 0.0287,
      "step": 9298
    },
    {
      "epoch": 2.268048780487805,
      "grad_norm": 0.1120118573307991,
      "learning_rate": 6.9924692804554605e-06,
      "loss": 0.0082,
      "step": 9299
    },
    {
      "epoch": 2.2682926829268295,
      "grad_norm": 0.10773307830095291,
      "learning_rate": 6.988040236370063e-06,
      "loss": 0.0213,
      "step": 9300
    },
    {
      "epoch": 2.2685365853658537,
      "grad_norm": 0.07904897630214691,
      "learning_rate": 6.983612367509895e-06,
      "loss": 0.0129,
      "step": 9301
    },
    {
      "epoch": 2.268780487804878,
      "grad_norm": 0.15154872834682465,
      "learning_rate": 6.979185674163874e-06,
      "loss": 0.0252,
      "step": 9302
    },
    {
      "epoch": 2.2690243902439025,
      "grad_norm": 0.15021410584449768,
      "learning_rate": 6.974760156620821e-06,
      "loss": 0.0254,
      "step": 9303
    },
    {
      "epoch": 2.269268292682927,
      "grad_norm": 0.19585134088993073,
      "learning_rate": 6.9703358151694845e-06,
      "loss": 0.0188,
      "step": 9304
    },
    {
      "epoch": 2.2695121951219512,
      "grad_norm": 0.15306071937084198,
      "learning_rate": 6.965912650098547e-06,
      "loss": 0.0199,
      "step": 9305
    },
    {
      "epoch": 2.2697560975609754,
      "grad_norm": 0.10484637320041656,
      "learning_rate": 6.961490661696591e-06,
      "loss": 0.013,
      "step": 9306
    },
    {
      "epoch": 2.27,
      "grad_norm": 0.12777891755104065,
      "learning_rate": 6.9570698502521545e-06,
      "loss": 0.0135,
      "step": 9307
    },
    {
      "epoch": 2.2702439024390246,
      "grad_norm": 0.2015332281589508,
      "learning_rate": 6.9526502160536775e-06,
      "loss": 0.0203,
      "step": 9308
    },
    {
      "epoch": 2.270487804878049,
      "grad_norm": 0.09090662747621536,
      "learning_rate": 6.948231759389514e-06,
      "loss": 0.0158,
      "step": 9309
    },
    {
      "epoch": 2.270731707317073,
      "grad_norm": 0.10968596488237381,
      "learning_rate": 6.943814480547981e-06,
      "loss": 0.0099,
      "step": 9310
    },
    {
      "epoch": 2.2709756097560976,
      "grad_norm": 0.1572771519422531,
      "learning_rate": 6.939398379817272e-06,
      "loss": 0.0195,
      "step": 9311
    },
    {
      "epoch": 2.271219512195122,
      "grad_norm": 0.06716019660234451,
      "learning_rate": 6.934983457485525e-06,
      "loss": 0.0133,
      "step": 9312
    },
    {
      "epoch": 2.2714634146341464,
      "grad_norm": 0.1328669786453247,
      "learning_rate": 6.930569713840812e-06,
      "loss": 0.0273,
      "step": 9313
    },
    {
      "epoch": 2.2717073170731705,
      "grad_norm": 0.2085629552602768,
      "learning_rate": 6.926157149171103e-06,
      "loss": 0.0272,
      "step": 9314
    },
    {
      "epoch": 2.271951219512195,
      "grad_norm": 0.20513606071472168,
      "learning_rate": 6.921745763764317e-06,
      "loss": 0.0154,
      "step": 9315
    },
    {
      "epoch": 2.2721951219512198,
      "grad_norm": 0.10524047166109085,
      "learning_rate": 6.917335557908278e-06,
      "loss": 0.0121,
      "step": 9316
    },
    {
      "epoch": 2.272439024390244,
      "grad_norm": 0.10158047080039978,
      "learning_rate": 6.912926531890729e-06,
      "loss": 0.0193,
      "step": 9317
    },
    {
      "epoch": 2.272682926829268,
      "grad_norm": 0.09689292311668396,
      "learning_rate": 6.90851868599936e-06,
      "loss": 0.0169,
      "step": 9318
    },
    {
      "epoch": 2.2729268292682927,
      "grad_norm": 0.14112652838230133,
      "learning_rate": 6.904112020521766e-06,
      "loss": 0.0135,
      "step": 9319
    },
    {
      "epoch": 2.2731707317073173,
      "grad_norm": 0.06275589019060135,
      "learning_rate": 6.899706535745457e-06,
      "loss": 0.0091,
      "step": 9320
    },
    {
      "epoch": 2.2734146341463415,
      "grad_norm": 0.09653376042842865,
      "learning_rate": 6.895302231957901e-06,
      "loss": 0.006,
      "step": 9321
    },
    {
      "epoch": 2.2736585365853657,
      "grad_norm": 0.18422862887382507,
      "learning_rate": 6.890899109446436e-06,
      "loss": 0.0219,
      "step": 9322
    },
    {
      "epoch": 2.2739024390243903,
      "grad_norm": 0.25593942403793335,
      "learning_rate": 6.886497168498374e-06,
      "loss": 0.0134,
      "step": 9323
    },
    {
      "epoch": 2.274146341463415,
      "grad_norm": 0.13821130990982056,
      "learning_rate": 6.882096409400923e-06,
      "loss": 0.017,
      "step": 9324
    },
    {
      "epoch": 2.274390243902439,
      "grad_norm": 0.09858287125825882,
      "learning_rate": 6.877696832441207e-06,
      "loss": 0.0151,
      "step": 9325
    },
    {
      "epoch": 2.274634146341463,
      "grad_norm": 0.2017054408788681,
      "learning_rate": 6.873298437906303e-06,
      "loss": 0.0176,
      "step": 9326
    },
    {
      "epoch": 2.274878048780488,
      "grad_norm": 0.09523165971040726,
      "learning_rate": 6.86890122608318e-06,
      "loss": 0.0202,
      "step": 9327
    },
    {
      "epoch": 2.2751219512195124,
      "grad_norm": 0.18256089091300964,
      "learning_rate": 6.864505197258753e-06,
      "loss": 0.0211,
      "step": 9328
    },
    {
      "epoch": 2.2753658536585366,
      "grad_norm": 0.08307202905416489,
      "learning_rate": 6.860110351719842e-06,
      "loss": 0.0141,
      "step": 9329
    },
    {
      "epoch": 2.2756097560975608,
      "grad_norm": 0.15715405344963074,
      "learning_rate": 6.8557166897531945e-06,
      "loss": 0.0059,
      "step": 9330
    },
    {
      "epoch": 2.2758536585365854,
      "grad_norm": 0.10865353792905807,
      "learning_rate": 6.851324211645493e-06,
      "loss": 0.0129,
      "step": 9331
    },
    {
      "epoch": 2.27609756097561,
      "grad_norm": 0.09560468047857285,
      "learning_rate": 6.84693291768333e-06,
      "loss": 0.0124,
      "step": 9332
    },
    {
      "epoch": 2.276341463414634,
      "grad_norm": 0.08069182187318802,
      "learning_rate": 6.842542808153213e-06,
      "loss": 0.021,
      "step": 9333
    },
    {
      "epoch": 2.2765853658536583,
      "grad_norm": 0.1028706356883049,
      "learning_rate": 6.8381538833416e-06,
      "loss": 0.0127,
      "step": 9334
    },
    {
      "epoch": 2.276829268292683,
      "grad_norm": 0.1263205111026764,
      "learning_rate": 6.833766143534842e-06,
      "loss": 0.0125,
      "step": 9335
    },
    {
      "epoch": 2.2770731707317076,
      "grad_norm": 0.21747589111328125,
      "learning_rate": 6.829379589019238e-06,
      "loss": 0.0207,
      "step": 9336
    },
    {
      "epoch": 2.2773170731707317,
      "grad_norm": 0.14580269157886505,
      "learning_rate": 6.824994220080996e-06,
      "loss": 0.0136,
      "step": 9337
    },
    {
      "epoch": 2.277560975609756,
      "grad_norm": 0.0819421112537384,
      "learning_rate": 6.8206100370062285e-06,
      "loss": 0.0137,
      "step": 9338
    },
    {
      "epoch": 2.2778048780487805,
      "grad_norm": 0.09259071946144104,
      "learning_rate": 6.816227040081011e-06,
      "loss": 0.014,
      "step": 9339
    },
    {
      "epoch": 2.278048780487805,
      "grad_norm": 0.15271081030368805,
      "learning_rate": 6.811845229591305e-06,
      "loss": 0.0394,
      "step": 9340
    },
    {
      "epoch": 2.2782926829268293,
      "grad_norm": 0.11777462810277939,
      "learning_rate": 6.807464605823027e-06,
      "loss": 0.0204,
      "step": 9341
    },
    {
      "epoch": 2.2785365853658535,
      "grad_norm": 0.08287515491247177,
      "learning_rate": 6.80308516906199e-06,
      "loss": 0.0072,
      "step": 9342
    },
    {
      "epoch": 2.278780487804878,
      "grad_norm": 0.08388478308916092,
      "learning_rate": 6.798706919593934e-06,
      "loss": 0.0162,
      "step": 9343
    },
    {
      "epoch": 2.2790243902439027,
      "grad_norm": 0.10028266906738281,
      "learning_rate": 6.794329857704537e-06,
      "loss": 0.0065,
      "step": 9344
    },
    {
      "epoch": 2.279268292682927,
      "grad_norm": 0.14537005126476288,
      "learning_rate": 6.789953983679384e-06,
      "loss": 0.0259,
      "step": 9345
    },
    {
      "epoch": 2.279512195121951,
      "grad_norm": 0.04128653183579445,
      "learning_rate": 6.785579297803982e-06,
      "loss": 0.004,
      "step": 9346
    },
    {
      "epoch": 2.2797560975609756,
      "grad_norm": 0.12177225202322006,
      "learning_rate": 6.781205800363777e-06,
      "loss": 0.0133,
      "step": 9347
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 0.11262750625610352,
      "learning_rate": 6.776833491644113e-06,
      "loss": 0.0206,
      "step": 9348
    },
    {
      "epoch": 2.2802439024390244,
      "grad_norm": 0.1311708688735962,
      "learning_rate": 6.772462371930285e-06,
      "loss": 0.0127,
      "step": 9349
    },
    {
      "epoch": 2.2804878048780486,
      "grad_norm": 0.20971278846263885,
      "learning_rate": 6.768092441507487e-06,
      "loss": 0.0194,
      "step": 9350
    },
    {
      "epoch": 2.280731707317073,
      "grad_norm": 0.16082553565502167,
      "learning_rate": 6.7637237006608375e-06,
      "loss": 0.0149,
      "step": 9351
    },
    {
      "epoch": 2.280975609756098,
      "grad_norm": 0.10915753990411758,
      "learning_rate": 6.7593561496753945e-06,
      "loss": 0.0069,
      "step": 9352
    },
    {
      "epoch": 2.281219512195122,
      "grad_norm": 0.16620485484600067,
      "learning_rate": 6.754989788836124e-06,
      "loss": 0.0221,
      "step": 9353
    },
    {
      "epoch": 2.281463414634146,
      "grad_norm": 0.23398694396018982,
      "learning_rate": 6.75062461842792e-06,
      "loss": 0.0165,
      "step": 9354
    },
    {
      "epoch": 2.2817073170731708,
      "grad_norm": 0.23882897198200226,
      "learning_rate": 6.746260638735591e-06,
      "loss": 0.035,
      "step": 9355
    },
    {
      "epoch": 2.281951219512195,
      "grad_norm": 0.12075070291757584,
      "learning_rate": 6.7418978500438655e-06,
      "loss": 0.0203,
      "step": 9356
    },
    {
      "epoch": 2.2821951219512195,
      "grad_norm": 0.129987433552742,
      "learning_rate": 6.737536252637422e-06,
      "loss": 0.0185,
      "step": 9357
    },
    {
      "epoch": 2.2824390243902437,
      "grad_norm": 0.15056966245174408,
      "learning_rate": 6.733175846800829e-06,
      "loss": 0.0136,
      "step": 9358
    },
    {
      "epoch": 2.2826829268292683,
      "grad_norm": 0.15869344770908356,
      "learning_rate": 6.728816632818585e-06,
      "loss": 0.0151,
      "step": 9359
    },
    {
      "epoch": 2.2829268292682925,
      "grad_norm": 0.14633303880691528,
      "learning_rate": 6.724458610975132e-06,
      "loss": 0.0068,
      "step": 9360
    },
    {
      "epoch": 2.283170731707317,
      "grad_norm": 0.10764238238334656,
      "learning_rate": 6.720101781554797e-06,
      "loss": 0.0151,
      "step": 9361
    },
    {
      "epoch": 2.2834146341463413,
      "grad_norm": 0.12313447147607803,
      "learning_rate": 6.71574614484187e-06,
      "loss": 0.0143,
      "step": 9362
    },
    {
      "epoch": 2.283658536585366,
      "grad_norm": 0.14053894579410553,
      "learning_rate": 6.711391701120534e-06,
      "loss": 0.022,
      "step": 9363
    },
    {
      "epoch": 2.28390243902439,
      "grad_norm": 0.15595965087413788,
      "learning_rate": 6.707038450674894e-06,
      "loss": 0.0052,
      "step": 9364
    },
    {
      "epoch": 2.2841463414634147,
      "grad_norm": 0.08588225394487381,
      "learning_rate": 6.702686393789004e-06,
      "loss": 0.0069,
      "step": 9365
    },
    {
      "epoch": 2.284390243902439,
      "grad_norm": 0.18360812962055206,
      "learning_rate": 6.69833553074681e-06,
      "loss": 0.0221,
      "step": 9366
    },
    {
      "epoch": 2.2846341463414634,
      "grad_norm": 0.10385461151599884,
      "learning_rate": 6.693985861832191e-06,
      "loss": 0.0174,
      "step": 9367
    },
    {
      "epoch": 2.2848780487804876,
      "grad_norm": 0.13756488263607025,
      "learning_rate": 6.689637387328962e-06,
      "loss": 0.0127,
      "step": 9368
    },
    {
      "epoch": 2.2851219512195122,
      "grad_norm": 0.10777703672647476,
      "learning_rate": 6.685290107520834e-06,
      "loss": 0.0156,
      "step": 9369
    },
    {
      "epoch": 2.2853658536585364,
      "grad_norm": 0.10442344844341278,
      "learning_rate": 6.680944022691471e-06,
      "loss": 0.0104,
      "step": 9370
    },
    {
      "epoch": 2.285609756097561,
      "grad_norm": 0.2752869725227356,
      "learning_rate": 6.676599133124426e-06,
      "loss": 0.0211,
      "step": 9371
    },
    {
      "epoch": 2.285853658536585,
      "grad_norm": 0.17325498163700104,
      "learning_rate": 6.672255439103187e-06,
      "loss": 0.0247,
      "step": 9372
    },
    {
      "epoch": 2.28609756097561,
      "grad_norm": 0.12419436126947403,
      "learning_rate": 6.667912940911178e-06,
      "loss": 0.0121,
      "step": 9373
    },
    {
      "epoch": 2.286341463414634,
      "grad_norm": 0.09247225522994995,
      "learning_rate": 6.663571638831725e-06,
      "loss": 0.013,
      "step": 9374
    },
    {
      "epoch": 2.2865853658536586,
      "grad_norm": 0.11771100014448166,
      "learning_rate": 6.659231533148097e-06,
      "loss": 0.0117,
      "step": 9375
    },
    {
      "epoch": 2.2868292682926827,
      "grad_norm": 0.15716132521629333,
      "learning_rate": 6.654892624143464e-06,
      "loss": 0.0252,
      "step": 9376
    },
    {
      "epoch": 2.2870731707317074,
      "grad_norm": 0.13838408887386322,
      "learning_rate": 6.65055491210092e-06,
      "loss": 0.0283,
      "step": 9377
    },
    {
      "epoch": 2.2873170731707315,
      "grad_norm": 0.08454538136720657,
      "learning_rate": 6.646218397303503e-06,
      "loss": 0.0123,
      "step": 9378
    },
    {
      "epoch": 2.287560975609756,
      "grad_norm": 0.06849408894777298,
      "learning_rate": 6.64188308003415e-06,
      "loss": 0.0054,
      "step": 9379
    },
    {
      "epoch": 2.2878048780487803,
      "grad_norm": 0.17110183835029602,
      "learning_rate": 6.637548960575718e-06,
      "loss": 0.0241,
      "step": 9380
    },
    {
      "epoch": 2.288048780487805,
      "grad_norm": 0.10634007304906845,
      "learning_rate": 6.63321603921101e-06,
      "loss": 0.0217,
      "step": 9381
    },
    {
      "epoch": 2.288292682926829,
      "grad_norm": 0.09549900144338608,
      "learning_rate": 6.628884316222722e-06,
      "loss": 0.0223,
      "step": 9382
    },
    {
      "epoch": 2.2885365853658537,
      "grad_norm": 0.1012764498591423,
      "learning_rate": 6.6245537918935036e-06,
      "loss": 0.0098,
      "step": 9383
    },
    {
      "epoch": 2.288780487804878,
      "grad_norm": 0.09762249886989594,
      "learning_rate": 6.620224466505895e-06,
      "loss": 0.0193,
      "step": 9384
    },
    {
      "epoch": 2.2890243902439025,
      "grad_norm": 0.1448775827884674,
      "learning_rate": 6.615896340342364e-06,
      "loss": 0.0351,
      "step": 9385
    },
    {
      "epoch": 2.2892682926829266,
      "grad_norm": 0.09758290648460388,
      "learning_rate": 6.611569413685329e-06,
      "loss": 0.0072,
      "step": 9386
    },
    {
      "epoch": 2.2895121951219513,
      "grad_norm": 0.18861179053783417,
      "learning_rate": 6.607243686817097e-06,
      "loss": 0.0152,
      "step": 9387
    },
    {
      "epoch": 2.2897560975609754,
      "grad_norm": 0.09538627415895462,
      "learning_rate": 6.602919160019908e-06,
      "loss": 0.0147,
      "step": 9388
    },
    {
      "epoch": 2.29,
      "grad_norm": 0.11679814755916595,
      "learning_rate": 6.598595833575924e-06,
      "loss": 0.0139,
      "step": 9389
    },
    {
      "epoch": 2.290243902439024,
      "grad_norm": 0.09313028305768967,
      "learning_rate": 6.594273707767223e-06,
      "loss": 0.0118,
      "step": 9390
    },
    {
      "epoch": 2.290487804878049,
      "grad_norm": 0.11318664252758026,
      "learning_rate": 6.5899527828758274e-06,
      "loss": 0.0101,
      "step": 9391
    },
    {
      "epoch": 2.290731707317073,
      "grad_norm": 0.15216855704784393,
      "learning_rate": 6.585633059183652e-06,
      "loss": 0.0193,
      "step": 9392
    },
    {
      "epoch": 2.2909756097560976,
      "grad_norm": 0.11231034249067307,
      "learning_rate": 6.581314536972538e-06,
      "loss": 0.0142,
      "step": 9393
    },
    {
      "epoch": 2.2912195121951218,
      "grad_norm": 0.12065788358449936,
      "learning_rate": 6.576997216524278e-06,
      "loss": 0.0221,
      "step": 9394
    },
    {
      "epoch": 2.2914634146341464,
      "grad_norm": 0.11022935062646866,
      "learning_rate": 6.572681098120539e-06,
      "loss": 0.016,
      "step": 9395
    },
    {
      "epoch": 2.2917073170731705,
      "grad_norm": 0.12260036915540695,
      "learning_rate": 6.5683661820429564e-06,
      "loss": 0.0149,
      "step": 9396
    },
    {
      "epoch": 2.291951219512195,
      "grad_norm": 0.12008704245090485,
      "learning_rate": 6.564052468573056e-06,
      "loss": 0.018,
      "step": 9397
    },
    {
      "epoch": 2.2921951219512193,
      "grad_norm": 0.17360058426856995,
      "learning_rate": 6.5597399579922864e-06,
      "loss": 0.0167,
      "step": 9398
    },
    {
      "epoch": 2.292439024390244,
      "grad_norm": 0.13532793521881104,
      "learning_rate": 6.555428650582041e-06,
      "loss": 0.0138,
      "step": 9399
    },
    {
      "epoch": 2.292682926829268,
      "grad_norm": 0.08112137019634247,
      "learning_rate": 6.551118546623611e-06,
      "loss": 0.0157,
      "step": 9400
    },
    {
      "epoch": 2.2929268292682927,
      "grad_norm": 0.18519333004951477,
      "learning_rate": 6.546809646398214e-06,
      "loss": 0.0294,
      "step": 9401
    },
    {
      "epoch": 2.293170731707317,
      "grad_norm": 0.1059546172618866,
      "learning_rate": 6.542501950187e-06,
      "loss": 0.0061,
      "step": 9402
    },
    {
      "epoch": 2.2934146341463415,
      "grad_norm": 0.1291491836309433,
      "learning_rate": 6.538195458271032e-06,
      "loss": 0.0121,
      "step": 9403
    },
    {
      "epoch": 2.2936585365853657,
      "grad_norm": 0.12675362825393677,
      "learning_rate": 6.533890170931295e-06,
      "loss": 0.0147,
      "step": 9404
    },
    {
      "epoch": 2.2939024390243903,
      "grad_norm": 0.08558973670005798,
      "learning_rate": 6.529586088448692e-06,
      "loss": 0.0151,
      "step": 9405
    },
    {
      "epoch": 2.2941463414634145,
      "grad_norm": 0.24865564703941345,
      "learning_rate": 6.525283211104047e-06,
      "loss": 0.0319,
      "step": 9406
    },
    {
      "epoch": 2.294390243902439,
      "grad_norm": 0.13330471515655518,
      "learning_rate": 6.5209815391781225e-06,
      "loss": 0.028,
      "step": 9407
    },
    {
      "epoch": 2.2946341463414632,
      "grad_norm": 0.12792964279651642,
      "learning_rate": 6.516681072951575e-06,
      "loss": 0.0192,
      "step": 9408
    },
    {
      "epoch": 2.294878048780488,
      "grad_norm": 0.17019349336624146,
      "learning_rate": 6.512381812705012e-06,
      "loss": 0.0139,
      "step": 9409
    },
    {
      "epoch": 2.295121951219512,
      "grad_norm": 0.11839772760868073,
      "learning_rate": 6.508083758718941e-06,
      "loss": 0.0101,
      "step": 9410
    },
    {
      "epoch": 2.2953658536585366,
      "grad_norm": 0.13173502683639526,
      "learning_rate": 6.503786911273787e-06,
      "loss": 0.0136,
      "step": 9411
    },
    {
      "epoch": 2.295609756097561,
      "grad_norm": 0.10227710753679276,
      "learning_rate": 6.499491270649924e-06,
      "loss": 0.0151,
      "step": 9412
    },
    {
      "epoch": 2.2958536585365854,
      "grad_norm": 0.10133710503578186,
      "learning_rate": 6.49519683712762e-06,
      "loss": 0.0076,
      "step": 9413
    },
    {
      "epoch": 2.2960975609756096,
      "grad_norm": 0.12026418745517731,
      "learning_rate": 6.490903610987065e-06,
      "loss": 0.0124,
      "step": 9414
    },
    {
      "epoch": 2.296341463414634,
      "grad_norm": 0.14353452622890472,
      "learning_rate": 6.4866115925083955e-06,
      "loss": 0.0229,
      "step": 9415
    },
    {
      "epoch": 2.2965853658536584,
      "grad_norm": 0.26041844487190247,
      "learning_rate": 6.482320781971643e-06,
      "loss": 0.0393,
      "step": 9416
    },
    {
      "epoch": 2.296829268292683,
      "grad_norm": 0.10041329264640808,
      "learning_rate": 6.478031179656766e-06,
      "loss": 0.0165,
      "step": 9417
    },
    {
      "epoch": 2.297073170731707,
      "grad_norm": 0.08798278123140335,
      "learning_rate": 6.473742785843662e-06,
      "loss": 0.0182,
      "step": 9418
    },
    {
      "epoch": 2.2973170731707317,
      "grad_norm": 0.2718077600002289,
      "learning_rate": 6.469455600812116e-06,
      "loss": 0.0313,
      "step": 9419
    },
    {
      "epoch": 2.297560975609756,
      "grad_norm": 0.09421373903751373,
      "learning_rate": 6.4651696248418824e-06,
      "loss": 0.0149,
      "step": 9420
    },
    {
      "epoch": 2.2978048780487805,
      "grad_norm": 0.1290547400712967,
      "learning_rate": 6.460884858212573e-06,
      "loss": 0.032,
      "step": 9421
    },
    {
      "epoch": 2.2980487804878047,
      "grad_norm": 0.15844888985157013,
      "learning_rate": 6.456601301203782e-06,
      "loss": 0.0179,
      "step": 9422
    },
    {
      "epoch": 2.2982926829268293,
      "grad_norm": 0.12844634056091309,
      "learning_rate": 6.452318954094988e-06,
      "loss": 0.0186,
      "step": 9423
    },
    {
      "epoch": 2.2985365853658535,
      "grad_norm": 0.10225062072277069,
      "learning_rate": 6.448037817165595e-06,
      "loss": 0.0071,
      "step": 9424
    },
    {
      "epoch": 2.298780487804878,
      "grad_norm": 0.07913299649953842,
      "learning_rate": 6.443757890694951e-06,
      "loss": 0.0091,
      "step": 9425
    },
    {
      "epoch": 2.2990243902439023,
      "grad_norm": 0.09027405083179474,
      "learning_rate": 6.4394791749622955e-06,
      "loss": 0.0104,
      "step": 9426
    },
    {
      "epoch": 2.299268292682927,
      "grad_norm": 0.23859736323356628,
      "learning_rate": 6.4352016702467985e-06,
      "loss": 0.0209,
      "step": 9427
    },
    {
      "epoch": 2.299512195121951,
      "grad_norm": 0.15608477592468262,
      "learning_rate": 6.4309253768275676e-06,
      "loss": 0.0135,
      "step": 9428
    },
    {
      "epoch": 2.2997560975609757,
      "grad_norm": 0.07175888121128082,
      "learning_rate": 6.426650294983613e-06,
      "loss": 0.0053,
      "step": 9429
    },
    {
      "epoch": 2.3,
      "grad_norm": 0.10317384451627731,
      "learning_rate": 6.422376424993859e-06,
      "loss": 0.019,
      "step": 9430
    },
    {
      "epoch": 2.3002439024390244,
      "grad_norm": 0.12391222268342972,
      "learning_rate": 6.418103767137176e-06,
      "loss": 0.0203,
      "step": 9431
    },
    {
      "epoch": 2.3004878048780486,
      "grad_norm": 0.18764963746070862,
      "learning_rate": 6.413832321692334e-06,
      "loss": 0.0189,
      "step": 9432
    },
    {
      "epoch": 2.300731707317073,
      "grad_norm": 0.19720874726772308,
      "learning_rate": 6.4095620889380425e-06,
      "loss": 0.0166,
      "step": 9433
    },
    {
      "epoch": 2.3009756097560974,
      "grad_norm": 0.11201173067092896,
      "learning_rate": 6.405293069152912e-06,
      "loss": 0.0221,
      "step": 9434
    },
    {
      "epoch": 2.301219512195122,
      "grad_norm": 0.129493847489357,
      "learning_rate": 6.4010252626154784e-06,
      "loss": 0.023,
      "step": 9435
    },
    {
      "epoch": 2.301463414634146,
      "grad_norm": 0.21532820165157318,
      "learning_rate": 6.396758669604224e-06,
      "loss": 0.0227,
      "step": 9436
    },
    {
      "epoch": 2.3017073170731708,
      "grad_norm": 0.08844897896051407,
      "learning_rate": 6.392493290397503e-06,
      "loss": 0.0167,
      "step": 9437
    },
    {
      "epoch": 2.301951219512195,
      "grad_norm": 0.08196240663528442,
      "learning_rate": 6.388229125273637e-06,
      "loss": 0.0144,
      "step": 9438
    },
    {
      "epoch": 2.3021951219512196,
      "grad_norm": 0.09127679467201233,
      "learning_rate": 6.383966174510847e-06,
      "loss": 0.0143,
      "step": 9439
    },
    {
      "epoch": 2.3024390243902437,
      "grad_norm": 0.2689189314842224,
      "learning_rate": 6.3797044383872655e-06,
      "loss": 0.0261,
      "step": 9440
    },
    {
      "epoch": 2.3026829268292683,
      "grad_norm": 0.23439161479473114,
      "learning_rate": 6.375443917180974e-06,
      "loss": 0.0206,
      "step": 9441
    },
    {
      "epoch": 2.3029268292682925,
      "grad_norm": 0.08643742650747299,
      "learning_rate": 6.371184611169953e-06,
      "loss": 0.0124,
      "step": 9442
    },
    {
      "epoch": 2.303170731707317,
      "grad_norm": 0.12831071019172668,
      "learning_rate": 6.3669265206321e-06,
      "loss": 0.0154,
      "step": 9443
    },
    {
      "epoch": 2.3034146341463413,
      "grad_norm": 0.22183899581432343,
      "learning_rate": 6.362669645845259e-06,
      "loss": 0.0208,
      "step": 9444
    },
    {
      "epoch": 2.303658536585366,
      "grad_norm": 0.16189564764499664,
      "learning_rate": 6.358413987087161e-06,
      "loss": 0.0098,
      "step": 9445
    },
    {
      "epoch": 2.30390243902439,
      "grad_norm": 0.0938808023929596,
      "learning_rate": 6.3541595446354906e-06,
      "loss": 0.0122,
      "step": 9446
    },
    {
      "epoch": 2.3041463414634147,
      "grad_norm": 0.051867589354515076,
      "learning_rate": 6.349906318767829e-06,
      "loss": 0.0084,
      "step": 9447
    },
    {
      "epoch": 2.304390243902439,
      "grad_norm": 0.14734652638435364,
      "learning_rate": 6.3456543097616785e-06,
      "loss": 0.0226,
      "step": 9448
    },
    {
      "epoch": 2.3046341463414635,
      "grad_norm": 0.14114966988563538,
      "learning_rate": 6.341403517894487e-06,
      "loss": 0.0207,
      "step": 9449
    },
    {
      "epoch": 2.3048780487804876,
      "grad_norm": 0.09929527342319489,
      "learning_rate": 6.337153943443594e-06,
      "loss": 0.0127,
      "step": 9450
    },
    {
      "epoch": 2.3051219512195122,
      "grad_norm": 0.09657473862171173,
      "learning_rate": 6.332905586686269e-06,
      "loss": 0.0161,
      "step": 9451
    },
    {
      "epoch": 2.3053658536585364,
      "grad_norm": 0.0986667349934578,
      "learning_rate": 6.328658447899716e-06,
      "loss": 0.0255,
      "step": 9452
    },
    {
      "epoch": 2.305609756097561,
      "grad_norm": 0.13554054498672485,
      "learning_rate": 6.32441252736104e-06,
      "loss": 0.0165,
      "step": 9453
    },
    {
      "epoch": 2.305853658536585,
      "grad_norm": 0.09781944006681442,
      "learning_rate": 6.320167825347276e-06,
      "loss": 0.018,
      "step": 9454
    },
    {
      "epoch": 2.30609756097561,
      "grad_norm": 0.140181303024292,
      "learning_rate": 6.315924342135374e-06,
      "loss": 0.0192,
      "step": 9455
    },
    {
      "epoch": 2.306341463414634,
      "grad_norm": 0.07501337677240372,
      "learning_rate": 6.311682078002207e-06,
      "loss": 0.0221,
      "step": 9456
    },
    {
      "epoch": 2.3065853658536586,
      "grad_norm": 0.12014209479093552,
      "learning_rate": 6.3074410332245815e-06,
      "loss": 0.016,
      "step": 9457
    },
    {
      "epoch": 2.3068292682926828,
      "grad_norm": 0.12431804835796356,
      "learning_rate": 6.303201208079196e-06,
      "loss": 0.0109,
      "step": 9458
    },
    {
      "epoch": 2.3070731707317074,
      "grad_norm": 0.13843123614788055,
      "learning_rate": 6.298962602842704e-06,
      "loss": 0.0181,
      "step": 9459
    },
    {
      "epoch": 2.3073170731707315,
      "grad_norm": 0.27552729845046997,
      "learning_rate": 6.29472521779165e-06,
      "loss": 0.0271,
      "step": 9460
    },
    {
      "epoch": 2.307560975609756,
      "grad_norm": 0.0974227711558342,
      "learning_rate": 6.290489053202509e-06,
      "loss": 0.0178,
      "step": 9461
    },
    {
      "epoch": 2.3078048780487803,
      "grad_norm": 0.09511490166187286,
      "learning_rate": 6.286254109351689e-06,
      "loss": 0.0199,
      "step": 9462
    },
    {
      "epoch": 2.308048780487805,
      "grad_norm": 0.14954684674739838,
      "learning_rate": 6.282020386515499e-06,
      "loss": 0.0238,
      "step": 9463
    },
    {
      "epoch": 2.308292682926829,
      "grad_norm": 0.14441806077957153,
      "learning_rate": 6.277787884970171e-06,
      "loss": 0.031,
      "step": 9464
    },
    {
      "epoch": 2.3085365853658537,
      "grad_norm": 0.19545914232730865,
      "learning_rate": 6.273556604991876e-06,
      "loss": 0.0202,
      "step": 9465
    },
    {
      "epoch": 2.308780487804878,
      "grad_norm": 0.12333979457616806,
      "learning_rate": 6.269326546856677e-06,
      "loss": 0.0116,
      "step": 9466
    },
    {
      "epoch": 2.3090243902439025,
      "grad_norm": 0.11289039254188538,
      "learning_rate": 6.2650977108405895e-06,
      "loss": 0.0152,
      "step": 9467
    },
    {
      "epoch": 2.3092682926829267,
      "grad_norm": 0.23573783040046692,
      "learning_rate": 6.260870097219524e-06,
      "loss": 0.0193,
      "step": 9468
    },
    {
      "epoch": 2.3095121951219513,
      "grad_norm": 0.3006337285041809,
      "learning_rate": 6.256643706269319e-06,
      "loss": 0.0266,
      "step": 9469
    },
    {
      "epoch": 2.3097560975609754,
      "grad_norm": 0.14417928457260132,
      "learning_rate": 6.252418538265731e-06,
      "loss": 0.0312,
      "step": 9470
    },
    {
      "epoch": 2.31,
      "grad_norm": 0.11690756678581238,
      "learning_rate": 6.248194593484436e-06,
      "loss": 0.0153,
      "step": 9471
    },
    {
      "epoch": 2.310243902439024,
      "grad_norm": 0.16558200120925903,
      "learning_rate": 6.243971872201046e-06,
      "loss": 0.0192,
      "step": 9472
    },
    {
      "epoch": 2.310487804878049,
      "grad_norm": 0.10269901901483536,
      "learning_rate": 6.239750374691073e-06,
      "loss": 0.0128,
      "step": 9473
    },
    {
      "epoch": 2.310731707317073,
      "grad_norm": 0.4110296070575714,
      "learning_rate": 6.23553010122995e-06,
      "loss": 0.0389,
      "step": 9474
    },
    {
      "epoch": 2.3109756097560976,
      "grad_norm": 0.10584402084350586,
      "learning_rate": 6.231311052093053e-06,
      "loss": 0.015,
      "step": 9475
    },
    {
      "epoch": 2.311219512195122,
      "grad_norm": 0.105011485517025,
      "learning_rate": 6.227093227555653e-06,
      "loss": 0.0232,
      "step": 9476
    },
    {
      "epoch": 2.3114634146341464,
      "grad_norm": 0.10403309017419815,
      "learning_rate": 6.22287662789294e-06,
      "loss": 0.0183,
      "step": 9477
    },
    {
      "epoch": 2.3117073170731706,
      "grad_norm": 0.07854500412940979,
      "learning_rate": 6.218661253380056e-06,
      "loss": 0.0132,
      "step": 9478
    },
    {
      "epoch": 2.311951219512195,
      "grad_norm": 0.07873783260583878,
      "learning_rate": 6.2144471042920205e-06,
      "loss": 0.0111,
      "step": 9479
    },
    {
      "epoch": 2.3121951219512193,
      "grad_norm": 0.23100534081459045,
      "learning_rate": 6.210234180903812e-06,
      "loss": 0.0265,
      "step": 9480
    },
    {
      "epoch": 2.312439024390244,
      "grad_norm": 0.09776702523231506,
      "learning_rate": 6.206022483490303e-06,
      "loss": 0.0118,
      "step": 9481
    },
    {
      "epoch": 2.312682926829268,
      "grad_norm": 0.1609005481004715,
      "learning_rate": 6.201812012326283e-06,
      "loss": 0.0164,
      "step": 9482
    },
    {
      "epoch": 2.3129268292682927,
      "grad_norm": 0.15470929443836212,
      "learning_rate": 6.1976027676864916e-06,
      "loss": 0.0176,
      "step": 9483
    },
    {
      "epoch": 2.313170731707317,
      "grad_norm": 0.10140787810087204,
      "learning_rate": 6.19339474984556e-06,
      "loss": 0.0078,
      "step": 9484
    },
    {
      "epoch": 2.3134146341463415,
      "grad_norm": 0.0948578491806984,
      "learning_rate": 6.189187959078041e-06,
      "loss": 0.0074,
      "step": 9485
    },
    {
      "epoch": 2.3136585365853657,
      "grad_norm": 0.08472637087106705,
      "learning_rate": 6.184982395658437e-06,
      "loss": 0.0155,
      "step": 9486
    },
    {
      "epoch": 2.3139024390243903,
      "grad_norm": 0.1400182694196701,
      "learning_rate": 6.180778059861117e-06,
      "loss": 0.0207,
      "step": 9487
    },
    {
      "epoch": 2.3141463414634145,
      "grad_norm": 0.10674824565649033,
      "learning_rate": 6.176574951960426e-06,
      "loss": 0.02,
      "step": 9488
    },
    {
      "epoch": 2.314390243902439,
      "grad_norm": 0.12785637378692627,
      "learning_rate": 6.172373072230597e-06,
      "loss": 0.0177,
      "step": 9489
    },
    {
      "epoch": 2.3146341463414632,
      "grad_norm": 0.11144532263278961,
      "learning_rate": 6.168172420945778e-06,
      "loss": 0.0108,
      "step": 9490
    },
    {
      "epoch": 2.314878048780488,
      "grad_norm": 0.13446597754955292,
      "learning_rate": 6.163972998380069e-06,
      "loss": 0.0166,
      "step": 9491
    },
    {
      "epoch": 2.315121951219512,
      "grad_norm": 0.12173271924257278,
      "learning_rate": 6.15977480480745e-06,
      "loss": 0.0081,
      "step": 9492
    },
    {
      "epoch": 2.3153658536585366,
      "grad_norm": 0.11619308590888977,
      "learning_rate": 6.155577840501855e-06,
      "loss": 0.0186,
      "step": 9493
    },
    {
      "epoch": 2.315609756097561,
      "grad_norm": 0.2281261831521988,
      "learning_rate": 6.151382105737122e-06,
      "loss": 0.0286,
      "step": 9494
    },
    {
      "epoch": 2.3158536585365854,
      "grad_norm": 0.08117765188217163,
      "learning_rate": 6.147187600786994e-06,
      "loss": 0.0099,
      "step": 9495
    },
    {
      "epoch": 2.3160975609756096,
      "grad_norm": 0.14207427203655243,
      "learning_rate": 6.142994325925172e-06,
      "loss": 0.0133,
      "step": 9496
    },
    {
      "epoch": 2.316341463414634,
      "grad_norm": 0.15460951626300812,
      "learning_rate": 6.138802281425241e-06,
      "loss": 0.0189,
      "step": 9497
    },
    {
      "epoch": 2.3165853658536584,
      "grad_norm": 0.1520576924085617,
      "learning_rate": 6.134611467560714e-06,
      "loss": 0.0196,
      "step": 9498
    },
    {
      "epoch": 2.316829268292683,
      "grad_norm": 0.19248904287815094,
      "learning_rate": 6.130421884605045e-06,
      "loss": 0.0212,
      "step": 9499
    },
    {
      "epoch": 2.317073170731707,
      "grad_norm": 0.1282186061143875,
      "learning_rate": 6.126233532831574e-06,
      "loss": 0.0124,
      "step": 9500
    },
    {
      "epoch": 2.3173170731707318,
      "grad_norm": 0.10417287051677704,
      "learning_rate": 6.122046412513596e-06,
      "loss": 0.0115,
      "step": 9501
    },
    {
      "epoch": 2.317560975609756,
      "grad_norm": 0.09200387448072433,
      "learning_rate": 6.117860523924304e-06,
      "loss": 0.0088,
      "step": 9502
    },
    {
      "epoch": 2.3178048780487805,
      "grad_norm": 0.09335308521986008,
      "learning_rate": 6.113675867336796e-06,
      "loss": 0.0107,
      "step": 9503
    },
    {
      "epoch": 2.3180487804878047,
      "grad_norm": 0.31667619943618774,
      "learning_rate": 6.109492443024128e-06,
      "loss": 0.0253,
      "step": 9504
    },
    {
      "epoch": 2.3182926829268293,
      "grad_norm": 0.19016678631305695,
      "learning_rate": 6.105310251259241e-06,
      "loss": 0.0183,
      "step": 9505
    },
    {
      "epoch": 2.3185365853658535,
      "grad_norm": 0.2593429386615753,
      "learning_rate": 6.101129292315025e-06,
      "loss": 0.0107,
      "step": 9506
    },
    {
      "epoch": 2.318780487804878,
      "grad_norm": 0.15380147099494934,
      "learning_rate": 6.0969495664642685e-06,
      "loss": 0.0153,
      "step": 9507
    },
    {
      "epoch": 2.3190243902439023,
      "grad_norm": 0.12186669558286667,
      "learning_rate": 6.092771073979675e-06,
      "loss": 0.0109,
      "step": 9508
    },
    {
      "epoch": 2.319268292682927,
      "grad_norm": 0.20747213065624237,
      "learning_rate": 6.0885938151339e-06,
      "loss": 0.0186,
      "step": 9509
    },
    {
      "epoch": 2.319512195121951,
      "grad_norm": 0.10598738491535187,
      "learning_rate": 6.0844177901994814e-06,
      "loss": 0.016,
      "step": 9510
    },
    {
      "epoch": 2.3197560975609757,
      "grad_norm": 0.05111735314130783,
      "learning_rate": 6.08024299944889e-06,
      "loss": 0.0059,
      "step": 9511
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.13461917638778687,
      "learning_rate": 6.076069443154533e-06,
      "loss": 0.0134,
      "step": 9512
    },
    {
      "epoch": 2.3202439024390245,
      "grad_norm": 0.2079142928123474,
      "learning_rate": 6.071897121588704e-06,
      "loss": 0.026,
      "step": 9513
    },
    {
      "epoch": 2.3204878048780486,
      "grad_norm": 0.16908149421215057,
      "learning_rate": 6.067726035023649e-06,
      "loss": 0.0341,
      "step": 9514
    },
    {
      "epoch": 2.3207317073170732,
      "grad_norm": 0.10908609628677368,
      "learning_rate": 6.063556183731517e-06,
      "loss": 0.0238,
      "step": 9515
    },
    {
      "epoch": 2.3209756097560974,
      "grad_norm": 0.16383704543113708,
      "learning_rate": 6.059387567984362e-06,
      "loss": 0.0199,
      "step": 9516
    },
    {
      "epoch": 2.321219512195122,
      "grad_norm": 0.09725111722946167,
      "learning_rate": 6.0552201880541974e-06,
      "loss": 0.0123,
      "step": 9517
    },
    {
      "epoch": 2.321463414634146,
      "grad_norm": 0.14071084558963776,
      "learning_rate": 6.051054044212917e-06,
      "loss": 0.0155,
      "step": 9518
    },
    {
      "epoch": 2.321707317073171,
      "grad_norm": 0.312204509973526,
      "learning_rate": 6.046889136732353e-06,
      "loss": 0.0285,
      "step": 9519
    },
    {
      "epoch": 2.321951219512195,
      "grad_norm": 0.12693946063518524,
      "learning_rate": 6.04272546588425e-06,
      "loss": 0.0211,
      "step": 9520
    },
    {
      "epoch": 2.3221951219512196,
      "grad_norm": 0.19924144446849823,
      "learning_rate": 6.038563031940272e-06,
      "loss": 0.021,
      "step": 9521
    },
    {
      "epoch": 2.3224390243902437,
      "grad_norm": 0.3027177155017853,
      "learning_rate": 6.034401835172016e-06,
      "loss": 0.016,
      "step": 9522
    },
    {
      "epoch": 2.3226829268292684,
      "grad_norm": 0.18150416016578674,
      "learning_rate": 6.030241875850981e-06,
      "loss": 0.0237,
      "step": 9523
    },
    {
      "epoch": 2.3229268292682925,
      "grad_norm": 0.1089802235364914,
      "learning_rate": 6.0260831542485824e-06,
      "loss": 0.0283,
      "step": 9524
    },
    {
      "epoch": 2.323170731707317,
      "grad_norm": 0.14069044589996338,
      "learning_rate": 6.021925670636183e-06,
      "loss": 0.0338,
      "step": 9525
    },
    {
      "epoch": 2.3234146341463413,
      "grad_norm": 0.11216792464256287,
      "learning_rate": 6.017769425285027e-06,
      "loss": 0.0117,
      "step": 9526
    },
    {
      "epoch": 2.323658536585366,
      "grad_norm": 0.24359141290187836,
      "learning_rate": 6.013614418466315e-06,
      "loss": 0.0261,
      "step": 9527
    },
    {
      "epoch": 2.32390243902439,
      "grad_norm": 0.20181170105934143,
      "learning_rate": 6.009460650451137e-06,
      "loss": 0.0125,
      "step": 9528
    },
    {
      "epoch": 2.3241463414634147,
      "grad_norm": 0.1327909529209137,
      "learning_rate": 6.005308121510511e-06,
      "loss": 0.0203,
      "step": 9529
    },
    {
      "epoch": 2.324390243902439,
      "grad_norm": 0.07968273758888245,
      "learning_rate": 6.0011568319153885e-06,
      "loss": 0.0156,
      "step": 9530
    },
    {
      "epoch": 2.3246341463414635,
      "grad_norm": 0.22760629653930664,
      "learning_rate": 5.9970067819366205e-06,
      "loss": 0.0263,
      "step": 9531
    },
    {
      "epoch": 2.3248780487804876,
      "grad_norm": 0.1352137178182602,
      "learning_rate": 5.992857971844981e-06,
      "loss": 0.0131,
      "step": 9532
    },
    {
      "epoch": 2.3251219512195123,
      "grad_norm": 0.15312480926513672,
      "learning_rate": 5.988710401911177e-06,
      "loss": 0.0213,
      "step": 9533
    },
    {
      "epoch": 2.3253658536585364,
      "grad_norm": 0.1581994891166687,
      "learning_rate": 5.9845640724058215e-06,
      "loss": 0.0215,
      "step": 9534
    },
    {
      "epoch": 2.325609756097561,
      "grad_norm": 0.11539255827665329,
      "learning_rate": 5.980418983599448e-06,
      "loss": 0.0146,
      "step": 9535
    },
    {
      "epoch": 2.325853658536585,
      "grad_norm": 0.09909699857234955,
      "learning_rate": 5.976275135762513e-06,
      "loss": 0.0172,
      "step": 9536
    },
    {
      "epoch": 2.32609756097561,
      "grad_norm": 0.1812540739774704,
      "learning_rate": 5.972132529165378e-06,
      "loss": 0.0238,
      "step": 9537
    },
    {
      "epoch": 2.326341463414634,
      "grad_norm": 0.09938448667526245,
      "learning_rate": 5.967991164078357e-06,
      "loss": 0.0129,
      "step": 9538
    },
    {
      "epoch": 2.3265853658536586,
      "grad_norm": 0.0881473496556282,
      "learning_rate": 5.9638510407716394e-06,
      "loss": 0.0174,
      "step": 9539
    },
    {
      "epoch": 2.3268292682926828,
      "grad_norm": 0.06650067120790482,
      "learning_rate": 5.9597121595153734e-06,
      "loss": 0.009,
      "step": 9540
    },
    {
      "epoch": 2.3270731707317074,
      "grad_norm": 0.15028269588947296,
      "learning_rate": 5.955574520579602e-06,
      "loss": 0.0287,
      "step": 9541
    },
    {
      "epoch": 2.3273170731707316,
      "grad_norm": 0.17674298584461212,
      "learning_rate": 5.951438124234285e-06,
      "loss": 0.028,
      "step": 9542
    },
    {
      "epoch": 2.327560975609756,
      "grad_norm": 0.1487562358379364,
      "learning_rate": 5.9473029707493245e-06,
      "loss": 0.0166,
      "step": 9543
    },
    {
      "epoch": 2.3278048780487803,
      "grad_norm": 0.10611621290445328,
      "learning_rate": 5.943169060394518e-06,
      "loss": 0.0193,
      "step": 9544
    },
    {
      "epoch": 2.328048780487805,
      "grad_norm": 0.14979957044124603,
      "learning_rate": 5.939036393439587e-06,
      "loss": 0.0159,
      "step": 9545
    },
    {
      "epoch": 2.328292682926829,
      "grad_norm": 0.16053324937820435,
      "learning_rate": 5.934904970154187e-06,
      "loss": 0.0148,
      "step": 9546
    },
    {
      "epoch": 2.3285365853658537,
      "grad_norm": 0.2013317346572876,
      "learning_rate": 5.930774790807872e-06,
      "loss": 0.0228,
      "step": 9547
    },
    {
      "epoch": 2.328780487804878,
      "grad_norm": 0.09864361584186554,
      "learning_rate": 5.926645855670121e-06,
      "loss": 0.0098,
      "step": 9548
    },
    {
      "epoch": 2.3290243902439025,
      "grad_norm": 0.10520573705434799,
      "learning_rate": 5.922518165010343e-06,
      "loss": 0.0187,
      "step": 9549
    },
    {
      "epoch": 2.3292682926829267,
      "grad_norm": 0.1734938770532608,
      "learning_rate": 5.918391719097846e-06,
      "loss": 0.0205,
      "step": 9550
    },
    {
      "epoch": 2.3295121951219513,
      "grad_norm": 0.08245029300451279,
      "learning_rate": 5.914266518201883e-06,
      "loss": 0.0144,
      "step": 9551
    },
    {
      "epoch": 2.3297560975609755,
      "grad_norm": 0.09911572188138962,
      "learning_rate": 5.910142562591603e-06,
      "loss": 0.0185,
      "step": 9552
    },
    {
      "epoch": 2.33,
      "grad_norm": 0.15127085149288177,
      "learning_rate": 5.906019852536082e-06,
      "loss": 0.0159,
      "step": 9553
    },
    {
      "epoch": 2.3302439024390242,
      "grad_norm": 0.13606342673301697,
      "learning_rate": 5.901898388304311e-06,
      "loss": 0.0222,
      "step": 9554
    },
    {
      "epoch": 2.330487804878049,
      "grad_norm": 0.08843108266592026,
      "learning_rate": 5.897778170165199e-06,
      "loss": 0.0095,
      "step": 9555
    },
    {
      "epoch": 2.330731707317073,
      "grad_norm": 0.20646145939826965,
      "learning_rate": 5.893659198387594e-06,
      "loss": 0.0266,
      "step": 9556
    },
    {
      "epoch": 2.3309756097560976,
      "grad_norm": 0.0845901295542717,
      "learning_rate": 5.889541473240231e-06,
      "loss": 0.0099,
      "step": 9557
    },
    {
      "epoch": 2.331219512195122,
      "grad_norm": 0.19011376798152924,
      "learning_rate": 5.885424994991781e-06,
      "loss": 0.0264,
      "step": 9558
    },
    {
      "epoch": 2.3314634146341464,
      "grad_norm": 0.2551010549068451,
      "learning_rate": 5.881309763910839e-06,
      "loss": 0.0324,
      "step": 9559
    },
    {
      "epoch": 2.3317073170731706,
      "grad_norm": 0.16762800514698029,
      "learning_rate": 5.87719578026591e-06,
      "loss": 0.0238,
      "step": 9560
    },
    {
      "epoch": 2.331951219512195,
      "grad_norm": 0.22964753210544586,
      "learning_rate": 5.873083044325406e-06,
      "loss": 0.0141,
      "step": 9561
    },
    {
      "epoch": 2.3321951219512194,
      "grad_norm": 0.15828238427639008,
      "learning_rate": 5.868971556357688e-06,
      "loss": 0.021,
      "step": 9562
    },
    {
      "epoch": 2.332439024390244,
      "grad_norm": 0.1517878919839859,
      "learning_rate": 5.864861316631004e-06,
      "loss": 0.0164,
      "step": 9563
    },
    {
      "epoch": 2.332682926829268,
      "grad_norm": 0.13059359788894653,
      "learning_rate": 5.860752325413549e-06,
      "loss": 0.0177,
      "step": 9564
    },
    {
      "epoch": 2.3329268292682928,
      "grad_norm": 0.12560877203941345,
      "learning_rate": 5.856644582973411e-06,
      "loss": 0.0158,
      "step": 9565
    },
    {
      "epoch": 2.333170731707317,
      "grad_norm": 0.13937997817993164,
      "learning_rate": 5.852538089578605e-06,
      "loss": 0.0222,
      "step": 9566
    },
    {
      "epoch": 2.3334146341463415,
      "grad_norm": 0.15216878056526184,
      "learning_rate": 5.848432845497079e-06,
      "loss": 0.0178,
      "step": 9567
    },
    {
      "epoch": 2.3336585365853657,
      "grad_norm": 0.14811722934246063,
      "learning_rate": 5.844328850996683e-06,
      "loss": 0.0144,
      "step": 9568
    },
    {
      "epoch": 2.3339024390243903,
      "grad_norm": 0.1167294830083847,
      "learning_rate": 5.840226106345187e-06,
      "loss": 0.0106,
      "step": 9569
    },
    {
      "epoch": 2.3341463414634145,
      "grad_norm": 0.1040983721613884,
      "learning_rate": 5.836124611810284e-06,
      "loss": 0.019,
      "step": 9570
    },
    {
      "epoch": 2.334390243902439,
      "grad_norm": 0.12664686143398285,
      "learning_rate": 5.832024367659578e-06,
      "loss": 0.0219,
      "step": 9571
    },
    {
      "epoch": 2.3346341463414633,
      "grad_norm": 0.12428876757621765,
      "learning_rate": 5.827925374160609e-06,
      "loss": 0.0159,
      "step": 9572
    },
    {
      "epoch": 2.334878048780488,
      "grad_norm": 0.1556338369846344,
      "learning_rate": 5.8238276315808164e-06,
      "loss": 0.0289,
      "step": 9573
    },
    {
      "epoch": 2.335121951219512,
      "grad_norm": 0.1251724362373352,
      "learning_rate": 5.819731140187562e-06,
      "loss": 0.0087,
      "step": 9574
    },
    {
      "epoch": 2.3353658536585367,
      "grad_norm": 0.09752244502305984,
      "learning_rate": 5.81563590024814e-06,
      "loss": 0.0254,
      "step": 9575
    },
    {
      "epoch": 2.335609756097561,
      "grad_norm": 0.10222840309143066,
      "learning_rate": 5.811541912029742e-06,
      "loss": 0.0123,
      "step": 9576
    },
    {
      "epoch": 2.3358536585365854,
      "grad_norm": 0.13438373804092407,
      "learning_rate": 5.807449175799498e-06,
      "loss": 0.0285,
      "step": 9577
    },
    {
      "epoch": 2.3360975609756096,
      "grad_norm": 0.19855648279190063,
      "learning_rate": 5.80335769182444e-06,
      "loss": 0.0308,
      "step": 9578
    },
    {
      "epoch": 2.3363414634146342,
      "grad_norm": 0.16350634396076202,
      "learning_rate": 5.799267460371521e-06,
      "loss": 0.0221,
      "step": 9579
    },
    {
      "epoch": 2.3365853658536584,
      "grad_norm": 0.1417710781097412,
      "learning_rate": 5.795178481707628e-06,
      "loss": 0.0186,
      "step": 9580
    },
    {
      "epoch": 2.336829268292683,
      "grad_norm": 0.15278436243534088,
      "learning_rate": 5.791090756099546e-06,
      "loss": 0.0374,
      "step": 9581
    },
    {
      "epoch": 2.337073170731707,
      "grad_norm": 0.15243464708328247,
      "learning_rate": 5.787004283813982e-06,
      "loss": 0.0125,
      "step": 9582
    },
    {
      "epoch": 2.337317073170732,
      "grad_norm": 0.26849883794784546,
      "learning_rate": 5.7829190651175785e-06,
      "loss": 0.0223,
      "step": 9583
    },
    {
      "epoch": 2.337560975609756,
      "grad_norm": 0.18467240035533905,
      "learning_rate": 5.778835100276872e-06,
      "loss": 0.0191,
      "step": 9584
    },
    {
      "epoch": 2.3378048780487806,
      "grad_norm": 0.12952688336372375,
      "learning_rate": 5.774752389558344e-06,
      "loss": 0.0282,
      "step": 9585
    },
    {
      "epoch": 2.3380487804878047,
      "grad_norm": 0.2022242248058319,
      "learning_rate": 5.770670933228364e-06,
      "loss": 0.0281,
      "step": 9586
    },
    {
      "epoch": 2.3382926829268293,
      "grad_norm": 0.1507120281457901,
      "learning_rate": 5.7665907315532314e-06,
      "loss": 0.0211,
      "step": 9587
    },
    {
      "epoch": 2.3385365853658535,
      "grad_norm": 0.1358967125415802,
      "learning_rate": 5.762511784799182e-06,
      "loss": 0.0167,
      "step": 9588
    },
    {
      "epoch": 2.338780487804878,
      "grad_norm": 0.05848660320043564,
      "learning_rate": 5.75843409323234e-06,
      "loss": 0.0087,
      "step": 9589
    },
    {
      "epoch": 2.3390243902439023,
      "grad_norm": 0.16468073427677155,
      "learning_rate": 5.7543576571187756e-06,
      "loss": 0.0203,
      "step": 9590
    },
    {
      "epoch": 2.339268292682927,
      "grad_norm": 0.1160716861486435,
      "learning_rate": 5.750282476724456e-06,
      "loss": 0.0154,
      "step": 9591
    },
    {
      "epoch": 2.339512195121951,
      "grad_norm": 0.10071128606796265,
      "learning_rate": 5.746208552315271e-06,
      "loss": 0.0132,
      "step": 9592
    },
    {
      "epoch": 2.3397560975609757,
      "grad_norm": 0.10666757822036743,
      "learning_rate": 5.742135884157043e-06,
      "loss": 0.0093,
      "step": 9593
    },
    {
      "epoch": 2.34,
      "grad_norm": 0.0790262445807457,
      "learning_rate": 5.738064472515495e-06,
      "loss": 0.0113,
      "step": 9594
    },
    {
      "epoch": 2.3402439024390245,
      "grad_norm": 0.13482989370822906,
      "learning_rate": 5.733994317656266e-06,
      "loss": 0.026,
      "step": 9595
    },
    {
      "epoch": 2.3404878048780486,
      "grad_norm": 0.24361205101013184,
      "learning_rate": 5.729925419844934e-06,
      "loss": 0.016,
      "step": 9596
    },
    {
      "epoch": 2.3407317073170733,
      "grad_norm": 0.08446498215198517,
      "learning_rate": 5.725857779346974e-06,
      "loss": 0.0091,
      "step": 9597
    },
    {
      "epoch": 2.3409756097560974,
      "grad_norm": 0.15293914079666138,
      "learning_rate": 5.7217913964277945e-06,
      "loss": 0.0061,
      "step": 9598
    },
    {
      "epoch": 2.341219512195122,
      "grad_norm": 0.09902611374855042,
      "learning_rate": 5.717726271352711e-06,
      "loss": 0.0189,
      "step": 9599
    },
    {
      "epoch": 2.341463414634146,
      "grad_norm": 0.23449081182479858,
      "learning_rate": 5.713662404386952e-06,
      "loss": 0.0169,
      "step": 9600
    },
    {
      "epoch": 2.341707317073171,
      "grad_norm": 0.1167168989777565,
      "learning_rate": 5.709599795795695e-06,
      "loss": 0.0197,
      "step": 9601
    },
    {
      "epoch": 2.341951219512195,
      "grad_norm": 0.13230328261852264,
      "learning_rate": 5.705538445843983e-06,
      "loss": 0.0202,
      "step": 9602
    },
    {
      "epoch": 2.3421951219512196,
      "grad_norm": 0.15499967336654663,
      "learning_rate": 5.701478354796827e-06,
      "loss": 0.0237,
      "step": 9603
    },
    {
      "epoch": 2.3424390243902438,
      "grad_norm": 0.15268413722515106,
      "learning_rate": 5.697419522919131e-06,
      "loss": 0.021,
      "step": 9604
    },
    {
      "epoch": 2.3426829268292684,
      "grad_norm": 0.20796296000480652,
      "learning_rate": 5.693361950475715e-06,
      "loss": 0.0262,
      "step": 9605
    },
    {
      "epoch": 2.3429268292682925,
      "grad_norm": 0.2985880374908447,
      "learning_rate": 5.6893056377313315e-06,
      "loss": 0.0192,
      "step": 9606
    },
    {
      "epoch": 2.343170731707317,
      "grad_norm": 0.1149863451719284,
      "learning_rate": 5.685250584950644e-06,
      "loss": 0.0111,
      "step": 9607
    },
    {
      "epoch": 2.3434146341463413,
      "grad_norm": 0.0981365442276001,
      "learning_rate": 5.681196792398219e-06,
      "loss": 0.0214,
      "step": 9608
    },
    {
      "epoch": 2.343658536585366,
      "grad_norm": 0.12758760154247284,
      "learning_rate": 5.677144260338569e-06,
      "loss": 0.016,
      "step": 9609
    },
    {
      "epoch": 2.34390243902439,
      "grad_norm": 0.18960413336753845,
      "learning_rate": 5.673092989036097e-06,
      "loss": 0.0255,
      "step": 9610
    },
    {
      "epoch": 2.3441463414634147,
      "grad_norm": 0.19142776727676392,
      "learning_rate": 5.669042978755149e-06,
      "loss": 0.0182,
      "step": 9611
    },
    {
      "epoch": 2.344390243902439,
      "grad_norm": 0.11597539484500885,
      "learning_rate": 5.664994229759968e-06,
      "loss": 0.0184,
      "step": 9612
    },
    {
      "epoch": 2.3446341463414635,
      "grad_norm": 0.10260064899921417,
      "learning_rate": 5.660946742314716e-06,
      "loss": 0.0111,
      "step": 9613
    },
    {
      "epoch": 2.3448780487804877,
      "grad_norm": 0.1677837371826172,
      "learning_rate": 5.656900516683494e-06,
      "loss": 0.0196,
      "step": 9614
    },
    {
      "epoch": 2.3451219512195123,
      "grad_norm": 0.14082001149654388,
      "learning_rate": 5.652855553130299e-06,
      "loss": 0.0197,
      "step": 9615
    },
    {
      "epoch": 2.3453658536585364,
      "grad_norm": 0.1688576489686966,
      "learning_rate": 5.648811851919045e-06,
      "loss": 0.0273,
      "step": 9616
    },
    {
      "epoch": 2.345609756097561,
      "grad_norm": 0.09099035710096359,
      "learning_rate": 5.644769413313586e-06,
      "loss": 0.0116,
      "step": 9617
    },
    {
      "epoch": 2.3458536585365852,
      "grad_norm": 0.18609090149402618,
      "learning_rate": 5.64072823757767e-06,
      "loss": 0.0222,
      "step": 9618
    },
    {
      "epoch": 2.34609756097561,
      "grad_norm": 0.14969512820243835,
      "learning_rate": 5.63668832497497e-06,
      "loss": 0.0163,
      "step": 9619
    },
    {
      "epoch": 2.346341463414634,
      "grad_norm": 0.11583245545625687,
      "learning_rate": 5.6326496757690805e-06,
      "loss": 0.0101,
      "step": 9620
    },
    {
      "epoch": 2.3465853658536586,
      "grad_norm": 0.05524192005395889,
      "learning_rate": 5.628612290223506e-06,
      "loss": 0.0073,
      "step": 9621
    },
    {
      "epoch": 2.346829268292683,
      "grad_norm": 0.1531648188829422,
      "learning_rate": 5.624576168601683e-06,
      "loss": 0.017,
      "step": 9622
    },
    {
      "epoch": 2.3470731707317074,
      "grad_norm": 0.11372634768486023,
      "learning_rate": 5.620541311166944e-06,
      "loss": 0.0281,
      "step": 9623
    },
    {
      "epoch": 2.3473170731707316,
      "grad_norm": 0.158589169383049,
      "learning_rate": 5.616507718182565e-06,
      "loss": 0.0144,
      "step": 9624
    },
    {
      "epoch": 2.347560975609756,
      "grad_norm": 0.19776470959186554,
      "learning_rate": 5.61247538991172e-06,
      "loss": 0.0205,
      "step": 9625
    },
    {
      "epoch": 2.3478048780487804,
      "grad_norm": 0.11079641431570053,
      "learning_rate": 5.6084443266174985e-06,
      "loss": 0.0248,
      "step": 9626
    },
    {
      "epoch": 2.348048780487805,
      "grad_norm": 0.10267632454633713,
      "learning_rate": 5.6044145285629265e-06,
      "loss": 0.0218,
      "step": 9627
    },
    {
      "epoch": 2.348292682926829,
      "grad_norm": 0.13415849208831787,
      "learning_rate": 5.600385996010932e-06,
      "loss": 0.0159,
      "step": 9628
    },
    {
      "epoch": 2.3485365853658537,
      "grad_norm": 0.1131608858704567,
      "learning_rate": 5.596358729224358e-06,
      "loss": 0.0238,
      "step": 9629
    },
    {
      "epoch": 2.348780487804878,
      "grad_norm": 0.14042258262634277,
      "learning_rate": 5.592332728465982e-06,
      "loss": 0.0178,
      "step": 9630
    },
    {
      "epoch": 2.3490243902439025,
      "grad_norm": 0.10309063643217087,
      "learning_rate": 5.5883079939984765e-06,
      "loss": 0.0188,
      "step": 9631
    },
    {
      "epoch": 2.3492682926829267,
      "grad_norm": 0.1498083770275116,
      "learning_rate": 5.584284526084454e-06,
      "loss": 0.0173,
      "step": 9632
    },
    {
      "epoch": 2.3495121951219513,
      "grad_norm": 0.11657337099313736,
      "learning_rate": 5.580262324986432e-06,
      "loss": 0.0122,
      "step": 9633
    },
    {
      "epoch": 2.3497560975609755,
      "grad_norm": 0.1892196536064148,
      "learning_rate": 5.576241390966844e-06,
      "loss": 0.0171,
      "step": 9634
    },
    {
      "epoch": 2.35,
      "grad_norm": 0.23746566474437714,
      "learning_rate": 5.5722217242880406e-06,
      "loss": 0.0237,
      "step": 9635
    },
    {
      "epoch": 2.3502439024390243,
      "grad_norm": 0.08894159644842148,
      "learning_rate": 5.56820332521229e-06,
      "loss": 0.0128,
      "step": 9636
    },
    {
      "epoch": 2.350487804878049,
      "grad_norm": 0.16233880817890167,
      "learning_rate": 5.5641861940017946e-06,
      "loss": 0.0168,
      "step": 9637
    },
    {
      "epoch": 2.350731707317073,
      "grad_norm": 0.10082776099443436,
      "learning_rate": 5.560170330918649e-06,
      "loss": 0.0221,
      "step": 9638
    },
    {
      "epoch": 2.3509756097560977,
      "grad_norm": 0.13201430439949036,
      "learning_rate": 5.556155736224872e-06,
      "loss": 0.0202,
      "step": 9639
    },
    {
      "epoch": 2.351219512195122,
      "grad_norm": 0.10041200369596481,
      "learning_rate": 5.5521424101824145e-06,
      "loss": 0.0166,
      "step": 9640
    },
    {
      "epoch": 2.3514634146341464,
      "grad_norm": 0.212278351187706,
      "learning_rate": 5.5481303530531324e-06,
      "loss": 0.0143,
      "step": 9641
    },
    {
      "epoch": 2.3517073170731706,
      "grad_norm": 0.08569564670324326,
      "learning_rate": 5.54411956509879e-06,
      "loss": 0.0191,
      "step": 9642
    },
    {
      "epoch": 2.351951219512195,
      "grad_norm": 0.08685871213674545,
      "learning_rate": 5.54011004658109e-06,
      "loss": 0.0183,
      "step": 9643
    },
    {
      "epoch": 2.3521951219512194,
      "grad_norm": 0.13888727128505707,
      "learning_rate": 5.536101797761633e-06,
      "loss": 0.016,
      "step": 9644
    },
    {
      "epoch": 2.352439024390244,
      "grad_norm": 0.31690284609794617,
      "learning_rate": 5.532094818901956e-06,
      "loss": 0.0191,
      "step": 9645
    },
    {
      "epoch": 2.352682926829268,
      "grad_norm": 0.09631223231554031,
      "learning_rate": 5.528089110263493e-06,
      "loss": 0.0106,
      "step": 9646
    },
    {
      "epoch": 2.3529268292682928,
      "grad_norm": 0.0916607454419136,
      "learning_rate": 5.524084672107599e-06,
      "loss": 0.0115,
      "step": 9647
    },
    {
      "epoch": 2.353170731707317,
      "grad_norm": 0.13268612325191498,
      "learning_rate": 5.520081504695568e-06,
      "loss": 0.0263,
      "step": 9648
    },
    {
      "epoch": 2.3534146341463416,
      "grad_norm": 0.08361170440912247,
      "learning_rate": 5.516079608288585e-06,
      "loss": 0.0174,
      "step": 9649
    },
    {
      "epoch": 2.3536585365853657,
      "grad_norm": 0.08183084428310394,
      "learning_rate": 5.512078983147753e-06,
      "loss": 0.0116,
      "step": 9650
    },
    {
      "epoch": 2.3539024390243903,
      "grad_norm": 0.10351380705833435,
      "learning_rate": 5.508079629534124e-06,
      "loss": 0.0119,
      "step": 9651
    },
    {
      "epoch": 2.3541463414634145,
      "grad_norm": 0.19343224167823792,
      "learning_rate": 5.504081547708615e-06,
      "loss": 0.0225,
      "step": 9652
    },
    {
      "epoch": 2.354390243902439,
      "grad_norm": 0.08402808010578156,
      "learning_rate": 5.5000847379321065e-06,
      "loss": 0.0073,
      "step": 9653
    },
    {
      "epoch": 2.3546341463414633,
      "grad_norm": 0.2334153652191162,
      "learning_rate": 5.496089200465376e-06,
      "loss": 0.0179,
      "step": 9654
    },
    {
      "epoch": 2.354878048780488,
      "grad_norm": 0.05765969678759575,
      "learning_rate": 5.4920949355691115e-06,
      "loss": 0.0121,
      "step": 9655
    },
    {
      "epoch": 2.355121951219512,
      "grad_norm": 0.205901101231575,
      "learning_rate": 5.48810194350394e-06,
      "loss": 0.0327,
      "step": 9656
    },
    {
      "epoch": 2.3553658536585367,
      "grad_norm": 0.19157612323760986,
      "learning_rate": 5.484110224530375e-06,
      "loss": 0.0224,
      "step": 9657
    },
    {
      "epoch": 2.355609756097561,
      "grad_norm": 0.07429617643356323,
      "learning_rate": 5.480119778908885e-06,
      "loss": 0.012,
      "step": 9658
    },
    {
      "epoch": 2.3558536585365855,
      "grad_norm": 0.1941448301076889,
      "learning_rate": 5.476130606899821e-06,
      "loss": 0.0085,
      "step": 9659
    },
    {
      "epoch": 2.3560975609756096,
      "grad_norm": 0.12793338298797607,
      "learning_rate": 5.472142708763461e-06,
      "loss": 0.0248,
      "step": 9660
    },
    {
      "epoch": 2.3563414634146342,
      "grad_norm": 0.17858901619911194,
      "learning_rate": 5.468156084760015e-06,
      "loss": 0.0191,
      "step": 9661
    },
    {
      "epoch": 2.3565853658536584,
      "grad_norm": 0.18930621445178986,
      "learning_rate": 5.464170735149593e-06,
      "loss": 0.016,
      "step": 9662
    },
    {
      "epoch": 2.356829268292683,
      "grad_norm": 0.16197408735752106,
      "learning_rate": 5.460186660192218e-06,
      "loss": 0.0261,
      "step": 9663
    },
    {
      "epoch": 2.357073170731707,
      "grad_norm": 0.1684676706790924,
      "learning_rate": 5.456203860147852e-06,
      "loss": 0.0132,
      "step": 9664
    },
    {
      "epoch": 2.357317073170732,
      "grad_norm": 0.09776199609041214,
      "learning_rate": 5.452222335276355e-06,
      "loss": 0.0043,
      "step": 9665
    },
    {
      "epoch": 2.357560975609756,
      "grad_norm": 0.08536068350076675,
      "learning_rate": 5.4482420858375045e-06,
      "loss": 0.0182,
      "step": 9666
    },
    {
      "epoch": 2.3578048780487806,
      "grad_norm": 0.11037436872720718,
      "learning_rate": 5.444263112091013e-06,
      "loss": 0.0142,
      "step": 9667
    },
    {
      "epoch": 2.3580487804878048,
      "grad_norm": 0.16187696158885956,
      "learning_rate": 5.4402854142964785e-06,
      "loss": 0.0245,
      "step": 9668
    },
    {
      "epoch": 2.3582926829268294,
      "grad_norm": 0.10204704105854034,
      "learning_rate": 5.436308992713449e-06,
      "loss": 0.0101,
      "step": 9669
    },
    {
      "epoch": 2.3585365853658535,
      "grad_norm": 0.1112208142876625,
      "learning_rate": 5.432333847601359e-06,
      "loss": 0.0213,
      "step": 9670
    },
    {
      "epoch": 2.358780487804878,
      "grad_norm": 0.06528295576572418,
      "learning_rate": 5.428359979219591e-06,
      "loss": 0.014,
      "step": 9671
    },
    {
      "epoch": 2.3590243902439023,
      "grad_norm": 0.1302305907011032,
      "learning_rate": 5.42438738782742e-06,
      "loss": 0.0121,
      "step": 9672
    },
    {
      "epoch": 2.359268292682927,
      "grad_norm": 0.08851920813322067,
      "learning_rate": 5.420416073684037e-06,
      "loss": 0.0207,
      "step": 9673
    },
    {
      "epoch": 2.359512195121951,
      "grad_norm": 0.11286012828350067,
      "learning_rate": 5.416446037048575e-06,
      "loss": 0.0108,
      "step": 9674
    },
    {
      "epoch": 2.3597560975609757,
      "grad_norm": 0.10541222989559174,
      "learning_rate": 5.412477278180059e-06,
      "loss": 0.0189,
      "step": 9675
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.19536979496479034,
      "learning_rate": 5.408509797337427e-06,
      "loss": 0.0192,
      "step": 9676
    },
    {
      "epoch": 2.3602439024390245,
      "grad_norm": 0.1032588928937912,
      "learning_rate": 5.404543594779565e-06,
      "loss": 0.0142,
      "step": 9677
    },
    {
      "epoch": 2.3604878048780487,
      "grad_norm": 0.1749153882265091,
      "learning_rate": 5.400578670765244e-06,
      "loss": 0.0363,
      "step": 9678
    },
    {
      "epoch": 2.3607317073170733,
      "grad_norm": 0.1328630894422531,
      "learning_rate": 5.3966150255531615e-06,
      "loss": 0.0128,
      "step": 9679
    },
    {
      "epoch": 2.3609756097560974,
      "grad_norm": 0.11846338957548141,
      "learning_rate": 5.3926526594019395e-06,
      "loss": 0.0195,
      "step": 9680
    },
    {
      "epoch": 2.361219512195122,
      "grad_norm": 0.08130628615617752,
      "learning_rate": 5.388691572570104e-06,
      "loss": 0.0152,
      "step": 9681
    },
    {
      "epoch": 2.361463414634146,
      "grad_norm": 0.15624162554740906,
      "learning_rate": 5.384731765316115e-06,
      "loss": 0.0128,
      "step": 9682
    },
    {
      "epoch": 2.361707317073171,
      "grad_norm": 0.09637785702943802,
      "learning_rate": 5.380773237898326e-06,
      "loss": 0.013,
      "step": 9683
    },
    {
      "epoch": 2.361951219512195,
      "grad_norm": 0.1335827112197876,
      "learning_rate": 5.376815990575027e-06,
      "loss": 0.0242,
      "step": 9684
    },
    {
      "epoch": 2.3621951219512196,
      "grad_norm": 0.14922286570072174,
      "learning_rate": 5.372860023604409e-06,
      "loss": 0.0158,
      "step": 9685
    },
    {
      "epoch": 2.362439024390244,
      "grad_norm": 0.08612454682588577,
      "learning_rate": 5.368905337244587e-06,
      "loss": 0.0159,
      "step": 9686
    },
    {
      "epoch": 2.3626829268292684,
      "grad_norm": 0.2513783574104309,
      "learning_rate": 5.364951931753598e-06,
      "loss": 0.0324,
      "step": 9687
    },
    {
      "epoch": 2.3629268292682926,
      "grad_norm": 0.281675785779953,
      "learning_rate": 5.3609998073893885e-06,
      "loss": 0.0397,
      "step": 9688
    },
    {
      "epoch": 2.363170731707317,
      "grad_norm": 0.13110031187534332,
      "learning_rate": 5.357048964409814e-06,
      "loss": 0.023,
      "step": 9689
    },
    {
      "epoch": 2.3634146341463413,
      "grad_norm": 0.13435639441013336,
      "learning_rate": 5.35309940307267e-06,
      "loss": 0.0115,
      "step": 9690
    },
    {
      "epoch": 2.363658536585366,
      "grad_norm": 0.09355790913105011,
      "learning_rate": 5.3491511236356425e-06,
      "loss": 0.0118,
      "step": 9691
    },
    {
      "epoch": 2.36390243902439,
      "grad_norm": 0.17700204253196716,
      "learning_rate": 5.3452041263563425e-06,
      "loss": 0.0144,
      "step": 9692
    },
    {
      "epoch": 2.3641463414634147,
      "grad_norm": 0.12309610843658447,
      "learning_rate": 5.341258411492312e-06,
      "loss": 0.0181,
      "step": 9693
    },
    {
      "epoch": 2.364390243902439,
      "grad_norm": 0.17934265732765198,
      "learning_rate": 5.3373139793009785e-06,
      "loss": 0.0243,
      "step": 9694
    },
    {
      "epoch": 2.3646341463414635,
      "grad_norm": 0.18400859832763672,
      "learning_rate": 5.3333708300397215e-06,
      "loss": 0.0135,
      "step": 9695
    },
    {
      "epoch": 2.3648780487804877,
      "grad_norm": 0.11514610797166824,
      "learning_rate": 5.329428963965816e-06,
      "loss": 0.0137,
      "step": 9696
    },
    {
      "epoch": 2.3651219512195123,
      "grad_norm": 0.1301223337650299,
      "learning_rate": 5.325488381336443e-06,
      "loss": 0.0174,
      "step": 9697
    },
    {
      "epoch": 2.3653658536585365,
      "grad_norm": 0.1051308885216713,
      "learning_rate": 5.321549082408733e-06,
      "loss": 0.0187,
      "step": 9698
    },
    {
      "epoch": 2.365609756097561,
      "grad_norm": 0.12644872069358826,
      "learning_rate": 5.317611067439704e-06,
      "loss": 0.0199,
      "step": 9699
    },
    {
      "epoch": 2.3658536585365852,
      "grad_norm": 0.16837869584560394,
      "learning_rate": 5.313674336686297e-06,
      "loss": 0.0271,
      "step": 9700
    },
    {
      "epoch": 2.36609756097561,
      "grad_norm": 0.13309751451015472,
      "learning_rate": 5.309738890405374e-06,
      "loss": 0.0114,
      "step": 9701
    },
    {
      "epoch": 2.366341463414634,
      "grad_norm": 0.08943518996238708,
      "learning_rate": 5.305804728853705e-06,
      "loss": 0.0168,
      "step": 9702
    },
    {
      "epoch": 2.3665853658536586,
      "grad_norm": 0.1678764820098877,
      "learning_rate": 5.301871852287995e-06,
      "loss": 0.0188,
      "step": 9703
    },
    {
      "epoch": 2.366829268292683,
      "grad_norm": 0.11528196185827255,
      "learning_rate": 5.2979402609648456e-06,
      "loss": 0.0109,
      "step": 9704
    },
    {
      "epoch": 2.3670731707317074,
      "grad_norm": 0.11236556619405746,
      "learning_rate": 5.294009955140772e-06,
      "loss": 0.0209,
      "step": 9705
    },
    {
      "epoch": 2.3673170731707316,
      "grad_norm": 0.16206379234790802,
      "learning_rate": 5.290080935072231e-06,
      "loss": 0.0179,
      "step": 9706
    },
    {
      "epoch": 2.367560975609756,
      "grad_norm": 0.0901055559515953,
      "learning_rate": 5.286153201015562e-06,
      "loss": 0.0077,
      "step": 9707
    },
    {
      "epoch": 2.3678048780487804,
      "grad_norm": 0.12871913611888885,
      "learning_rate": 5.282226753227057e-06,
      "loss": 0.0176,
      "step": 9708
    },
    {
      "epoch": 2.368048780487805,
      "grad_norm": 0.07587867975234985,
      "learning_rate": 5.278301591962892e-06,
      "loss": 0.0106,
      "step": 9709
    },
    {
      "epoch": 2.368292682926829,
      "grad_norm": 0.1360843926668167,
      "learning_rate": 5.274377717479168e-06,
      "loss": 0.0135,
      "step": 9710
    },
    {
      "epoch": 2.3685365853658538,
      "grad_norm": 0.2272133082151413,
      "learning_rate": 5.270455130031917e-06,
      "loss": 0.018,
      "step": 9711
    },
    {
      "epoch": 2.368780487804878,
      "grad_norm": 0.0964704155921936,
      "learning_rate": 5.266533829877074e-06,
      "loss": 0.0193,
      "step": 9712
    },
    {
      "epoch": 2.3690243902439025,
      "grad_norm": 0.1352677047252655,
      "learning_rate": 5.262613817270479e-06,
      "loss": 0.0226,
      "step": 9713
    },
    {
      "epoch": 2.3692682926829267,
      "grad_norm": 0.19684556126594543,
      "learning_rate": 5.258695092467919e-06,
      "loss": 0.0148,
      "step": 9714
    },
    {
      "epoch": 2.3695121951219513,
      "grad_norm": 0.1276116669178009,
      "learning_rate": 5.254777655725063e-06,
      "loss": 0.0192,
      "step": 9715
    },
    {
      "epoch": 2.3697560975609755,
      "grad_norm": 0.12928582727909088,
      "learning_rate": 5.250861507297527e-06,
      "loss": 0.0106,
      "step": 9716
    },
    {
      "epoch": 2.37,
      "grad_norm": 0.12986472249031067,
      "learning_rate": 5.246946647440826e-06,
      "loss": 0.0143,
      "step": 9717
    },
    {
      "epoch": 2.3702439024390243,
      "grad_norm": 0.11872126162052155,
      "learning_rate": 5.243033076410373e-06,
      "loss": 0.0129,
      "step": 9718
    },
    {
      "epoch": 2.370487804878049,
      "grad_norm": 0.262065589427948,
      "learning_rate": 5.239120794461536e-06,
      "loss": 0.0163,
      "step": 9719
    },
    {
      "epoch": 2.370731707317073,
      "grad_norm": 0.111066535115242,
      "learning_rate": 5.235209801849566e-06,
      "loss": 0.019,
      "step": 9720
    },
    {
      "epoch": 2.3709756097560977,
      "grad_norm": 0.11764885485172272,
      "learning_rate": 5.231300098829658e-06,
      "loss": 0.0078,
      "step": 9721
    },
    {
      "epoch": 2.371219512195122,
      "grad_norm": 0.14304561913013458,
      "learning_rate": 5.227391685656899e-06,
      "loss": 0.0294,
      "step": 9722
    },
    {
      "epoch": 2.3714634146341464,
      "grad_norm": 0.06927371770143509,
      "learning_rate": 5.223484562586297e-06,
      "loss": 0.0091,
      "step": 9723
    },
    {
      "epoch": 2.3717073170731706,
      "grad_norm": 0.15552948415279388,
      "learning_rate": 5.219578729872796e-06,
      "loss": 0.0219,
      "step": 9724
    },
    {
      "epoch": 2.3719512195121952,
      "grad_norm": 0.34016504883766174,
      "learning_rate": 5.215674187771225e-06,
      "loss": 0.0201,
      "step": 9725
    },
    {
      "epoch": 2.3721951219512194,
      "grad_norm": 0.173378124833107,
      "learning_rate": 5.211770936536342e-06,
      "loss": 0.011,
      "step": 9726
    },
    {
      "epoch": 2.372439024390244,
      "grad_norm": 0.15314292907714844,
      "learning_rate": 5.207868976422834e-06,
      "loss": 0.0268,
      "step": 9727
    },
    {
      "epoch": 2.372682926829268,
      "grad_norm": 0.13213443756103516,
      "learning_rate": 5.203968307685281e-06,
      "loss": 0.0162,
      "step": 9728
    },
    {
      "epoch": 2.372926829268293,
      "grad_norm": 0.12424318492412567,
      "learning_rate": 5.2000689305782e-06,
      "loss": 0.014,
      "step": 9729
    },
    {
      "epoch": 2.373170731707317,
      "grad_norm": 0.17441637814044952,
      "learning_rate": 5.1961708453560074e-06,
      "loss": 0.0193,
      "step": 9730
    },
    {
      "epoch": 2.3734146341463416,
      "grad_norm": 0.15985609591007233,
      "learning_rate": 5.192274052273036e-06,
      "loss": 0.0166,
      "step": 9731
    },
    {
      "epoch": 2.3736585365853657,
      "grad_norm": 0.0760701522231102,
      "learning_rate": 5.188378551583551e-06,
      "loss": 0.0093,
      "step": 9732
    },
    {
      "epoch": 2.3739024390243904,
      "grad_norm": 0.13867706060409546,
      "learning_rate": 5.184484343541715e-06,
      "loss": 0.0244,
      "step": 9733
    },
    {
      "epoch": 2.3741463414634145,
      "grad_norm": 0.12964950501918793,
      "learning_rate": 5.180591428401618e-06,
      "loss": 0.02,
      "step": 9734
    },
    {
      "epoch": 2.374390243902439,
      "grad_norm": 0.0872042253613472,
      "learning_rate": 5.176699806417254e-06,
      "loss": 0.0126,
      "step": 9735
    },
    {
      "epoch": 2.3746341463414633,
      "grad_norm": 0.21880528330802917,
      "learning_rate": 5.172809477842539e-06,
      "loss": 0.0269,
      "step": 9736
    },
    {
      "epoch": 2.374878048780488,
      "grad_norm": 0.08489677309989929,
      "learning_rate": 5.168920442931314e-06,
      "loss": 0.01,
      "step": 9737
    },
    {
      "epoch": 2.375121951219512,
      "grad_norm": 0.11031515151262283,
      "learning_rate": 5.1650327019373195e-06,
      "loss": 0.0099,
      "step": 9738
    },
    {
      "epoch": 2.3753658536585367,
      "grad_norm": 0.1031763106584549,
      "learning_rate": 5.161146255114216e-06,
      "loss": 0.0213,
      "step": 9739
    },
    {
      "epoch": 2.375609756097561,
      "grad_norm": 0.05365028232336044,
      "learning_rate": 5.157261102715594e-06,
      "loss": 0.0063,
      "step": 9740
    },
    {
      "epoch": 2.3758536585365855,
      "grad_norm": 0.09898336976766586,
      "learning_rate": 5.153377244994934e-06,
      "loss": 0.011,
      "step": 9741
    },
    {
      "epoch": 2.3760975609756096,
      "grad_norm": 0.12639549374580383,
      "learning_rate": 5.1494946822056575e-06,
      "loss": 0.0204,
      "step": 9742
    },
    {
      "epoch": 2.3763414634146343,
      "grad_norm": 0.09304125607013702,
      "learning_rate": 5.145613414601086e-06,
      "loss": 0.0153,
      "step": 9743
    },
    {
      "epoch": 2.3765853658536584,
      "grad_norm": 0.21989111602306366,
      "learning_rate": 5.141733442434452e-06,
      "loss": 0.0162,
      "step": 9744
    },
    {
      "epoch": 2.376829268292683,
      "grad_norm": 0.11149542033672333,
      "learning_rate": 5.137854765958924e-06,
      "loss": 0.0052,
      "step": 9745
    },
    {
      "epoch": 2.377073170731707,
      "grad_norm": 0.1261674463748932,
      "learning_rate": 5.133977385427571e-06,
      "loss": 0.0355,
      "step": 9746
    },
    {
      "epoch": 2.377317073170732,
      "grad_norm": 0.10630842298269272,
      "learning_rate": 5.130101301093371e-06,
      "loss": 0.0086,
      "step": 9747
    },
    {
      "epoch": 2.377560975609756,
      "grad_norm": 0.13000847399234772,
      "learning_rate": 5.126226513209237e-06,
      "loss": 0.0251,
      "step": 9748
    },
    {
      "epoch": 2.3778048780487806,
      "grad_norm": 0.10355781763792038,
      "learning_rate": 5.122353022027987e-06,
      "loss": 0.0241,
      "step": 9749
    },
    {
      "epoch": 2.3780487804878048,
      "grad_norm": 0.12426172941923141,
      "learning_rate": 5.11848082780235e-06,
      "loss": 0.0263,
      "step": 9750
    },
    {
      "epoch": 2.3782926829268294,
      "grad_norm": 0.0644930899143219,
      "learning_rate": 5.114609930784975e-06,
      "loss": 0.0061,
      "step": 9751
    },
    {
      "epoch": 2.3785365853658536,
      "grad_norm": 0.1611747443675995,
      "learning_rate": 5.110740331228423e-06,
      "loss": 0.0259,
      "step": 9752
    },
    {
      "epoch": 2.378780487804878,
      "grad_norm": 0.11001968383789062,
      "learning_rate": 5.106872029385182e-06,
      "loss": 0.0235,
      "step": 9753
    },
    {
      "epoch": 2.3790243902439023,
      "grad_norm": 0.06717102974653244,
      "learning_rate": 5.1030050255076345e-06,
      "loss": 0.0077,
      "step": 9754
    },
    {
      "epoch": 2.379268292682927,
      "grad_norm": 0.15382321178913116,
      "learning_rate": 5.099139319848106e-06,
      "loss": 0.0165,
      "step": 9755
    },
    {
      "epoch": 2.379512195121951,
      "grad_norm": 0.17233256995677948,
      "learning_rate": 5.095274912658818e-06,
      "loss": 0.0195,
      "step": 9756
    },
    {
      "epoch": 2.3797560975609757,
      "grad_norm": 0.14967592060565948,
      "learning_rate": 5.091411804191898e-06,
      "loss": 0.0257,
      "step": 9757
    },
    {
      "epoch": 2.38,
      "grad_norm": 0.11305851489305496,
      "learning_rate": 5.087549994699417e-06,
      "loss": 0.0223,
      "step": 9758
    },
    {
      "epoch": 2.3802439024390245,
      "grad_norm": 0.12340359389781952,
      "learning_rate": 5.0836894844333434e-06,
      "loss": 0.0207,
      "step": 9759
    },
    {
      "epoch": 2.3804878048780487,
      "grad_norm": 0.15424342453479767,
      "learning_rate": 5.079830273645553e-06,
      "loss": 0.0203,
      "step": 9760
    },
    {
      "epoch": 2.3807317073170733,
      "grad_norm": 0.1442481279373169,
      "learning_rate": 5.075972362587863e-06,
      "loss": 0.0149,
      "step": 9761
    },
    {
      "epoch": 2.3809756097560975,
      "grad_norm": 0.1658102422952652,
      "learning_rate": 5.072115751511977e-06,
      "loss": 0.0071,
      "step": 9762
    },
    {
      "epoch": 2.381219512195122,
      "grad_norm": 0.08653343468904495,
      "learning_rate": 5.06826044066954e-06,
      "loss": 0.0114,
      "step": 9763
    },
    {
      "epoch": 2.3814634146341462,
      "grad_norm": 0.1341215968132019,
      "learning_rate": 5.064406430312091e-06,
      "loss": 0.0252,
      "step": 9764
    },
    {
      "epoch": 2.381707317073171,
      "grad_norm": 0.1973433792591095,
      "learning_rate": 5.060553720691088e-06,
      "loss": 0.0218,
      "step": 9765
    },
    {
      "epoch": 2.381951219512195,
      "grad_norm": 0.18375997245311737,
      "learning_rate": 5.056702312057929e-06,
      "loss": 0.0126,
      "step": 9766
    },
    {
      "epoch": 2.3821951219512196,
      "grad_norm": 0.09312738478183746,
      "learning_rate": 5.0528522046638775e-06,
      "loss": 0.0165,
      "step": 9767
    },
    {
      "epoch": 2.382439024390244,
      "grad_norm": 0.10567781329154968,
      "learning_rate": 5.0490033987601605e-06,
      "loss": 0.0068,
      "step": 9768
    },
    {
      "epoch": 2.3826829268292684,
      "grad_norm": 0.2302677184343338,
      "learning_rate": 5.0451558945978976e-06,
      "loss": 0.0275,
      "step": 9769
    },
    {
      "epoch": 2.3829268292682926,
      "grad_norm": 0.1542401760816574,
      "learning_rate": 5.041309692428117e-06,
      "loss": 0.0112,
      "step": 9770
    },
    {
      "epoch": 2.383170731707317,
      "grad_norm": 0.1595696359872818,
      "learning_rate": 5.037464792501786e-06,
      "loss": 0.0224,
      "step": 9771
    },
    {
      "epoch": 2.3834146341463414,
      "grad_norm": 0.09947953373193741,
      "learning_rate": 5.033621195069768e-06,
      "loss": 0.0184,
      "step": 9772
    },
    {
      "epoch": 2.383658536585366,
      "grad_norm": 0.11991696059703827,
      "learning_rate": 5.029778900382837e-06,
      "loss": 0.0133,
      "step": 9773
    },
    {
      "epoch": 2.38390243902439,
      "grad_norm": 0.19460642337799072,
      "learning_rate": 5.025937908691703e-06,
      "loss": 0.0185,
      "step": 9774
    },
    {
      "epoch": 2.3841463414634148,
      "grad_norm": 0.12016163021326065,
      "learning_rate": 5.022098220246968e-06,
      "loss": 0.0164,
      "step": 9775
    },
    {
      "epoch": 2.384390243902439,
      "grad_norm": 0.10911694169044495,
      "learning_rate": 5.0182598352991746e-06,
      "loss": 0.0107,
      "step": 9776
    },
    {
      "epoch": 2.3846341463414635,
      "grad_norm": 0.14072392880916595,
      "learning_rate": 5.014422754098755e-06,
      "loss": 0.0078,
      "step": 9777
    },
    {
      "epoch": 2.3848780487804877,
      "grad_norm": 0.16106455028057098,
      "learning_rate": 5.010586976896062e-06,
      "loss": 0.0118,
      "step": 9778
    },
    {
      "epoch": 2.3851219512195123,
      "grad_norm": 0.1355668157339096,
      "learning_rate": 5.006752503941383e-06,
      "loss": 0.0285,
      "step": 9779
    },
    {
      "epoch": 2.3853658536585365,
      "grad_norm": 0.16311059892177582,
      "learning_rate": 5.002919335484899e-06,
      "loss": 0.0142,
      "step": 9780
    },
    {
      "epoch": 2.385609756097561,
      "grad_norm": 0.13945402204990387,
      "learning_rate": 4.9990874717767005e-06,
      "loss": 0.0084,
      "step": 9781
    },
    {
      "epoch": 2.3858536585365853,
      "grad_norm": 0.12324889004230499,
      "learning_rate": 4.995256913066826e-06,
      "loss": 0.0132,
      "step": 9782
    },
    {
      "epoch": 2.38609756097561,
      "grad_norm": 0.09146379679441452,
      "learning_rate": 4.991427659605196e-06,
      "loss": 0.0113,
      "step": 9783
    },
    {
      "epoch": 2.386341463414634,
      "grad_norm": 0.06251687556505203,
      "learning_rate": 4.98759971164166e-06,
      "loss": 0.0082,
      "step": 9784
    },
    {
      "epoch": 2.3865853658536587,
      "grad_norm": 0.10802177339792252,
      "learning_rate": 4.983773069425979e-06,
      "loss": 0.0136,
      "step": 9785
    },
    {
      "epoch": 2.386829268292683,
      "grad_norm": 0.07585258781909943,
      "learning_rate": 4.9799477332078236e-06,
      "loss": 0.0088,
      "step": 9786
    },
    {
      "epoch": 2.3870731707317074,
      "grad_norm": 0.20205062627792358,
      "learning_rate": 4.976123703236799e-06,
      "loss": 0.0238,
      "step": 9787
    },
    {
      "epoch": 2.3873170731707316,
      "grad_norm": 0.1011677086353302,
      "learning_rate": 4.972300979762392e-06,
      "loss": 0.0133,
      "step": 9788
    },
    {
      "epoch": 2.387560975609756,
      "grad_norm": 0.15774385631084442,
      "learning_rate": 4.968479563034048e-06,
      "loss": 0.0252,
      "step": 9789
    },
    {
      "epoch": 2.3878048780487804,
      "grad_norm": 0.13182365894317627,
      "learning_rate": 4.9646594533010875e-06,
      "loss": 0.0175,
      "step": 9790
    },
    {
      "epoch": 2.388048780487805,
      "grad_norm": 0.0879625678062439,
      "learning_rate": 4.960840650812757e-06,
      "loss": 0.0158,
      "step": 9791
    },
    {
      "epoch": 2.388292682926829,
      "grad_norm": 0.22409430146217346,
      "learning_rate": 4.957023155818236e-06,
      "loss": 0.0115,
      "step": 9792
    },
    {
      "epoch": 2.388536585365854,
      "grad_norm": 0.19522498548030853,
      "learning_rate": 4.953206968566596e-06,
      "loss": 0.0146,
      "step": 9793
    },
    {
      "epoch": 2.388780487804878,
      "grad_norm": 0.16310296952724457,
      "learning_rate": 4.949392089306823e-06,
      "loss": 0.0162,
      "step": 9794
    },
    {
      "epoch": 2.3890243902439026,
      "grad_norm": 0.24607114493846893,
      "learning_rate": 4.945578518287844e-06,
      "loss": 0.033,
      "step": 9795
    },
    {
      "epoch": 2.3892682926829267,
      "grad_norm": 0.11426559090614319,
      "learning_rate": 4.941766255758476e-06,
      "loss": 0.0145,
      "step": 9796
    },
    {
      "epoch": 2.3895121951219513,
      "grad_norm": 0.06605630367994308,
      "learning_rate": 4.9379553019674475e-06,
      "loss": 0.0066,
      "step": 9797
    },
    {
      "epoch": 2.3897560975609755,
      "grad_norm": 0.06433909386396408,
      "learning_rate": 4.934145657163425e-06,
      "loss": 0.0087,
      "step": 9798
    },
    {
      "epoch": 2.39,
      "grad_norm": 0.17367306351661682,
      "learning_rate": 4.930337321594971e-06,
      "loss": 0.0198,
      "step": 9799
    },
    {
      "epoch": 2.3902439024390243,
      "grad_norm": 0.1405784785747528,
      "learning_rate": 4.92653029551057e-06,
      "loss": 0.0164,
      "step": 9800
    },
    {
      "epoch": 2.390487804878049,
      "grad_norm": 0.15151666104793549,
      "learning_rate": 4.9227245791586076e-06,
      "loss": 0.0152,
      "step": 9801
    },
    {
      "epoch": 2.390731707317073,
      "grad_norm": 0.32615575194358826,
      "learning_rate": 4.918920172787414e-06,
      "loss": 0.009,
      "step": 9802
    },
    {
      "epoch": 2.3909756097560977,
      "grad_norm": 0.17858631908893585,
      "learning_rate": 4.9151170766452005e-06,
      "loss": 0.0281,
      "step": 9803
    },
    {
      "epoch": 2.391219512195122,
      "grad_norm": 0.08186487853527069,
      "learning_rate": 4.9113152909801095e-06,
      "loss": 0.0133,
      "step": 9804
    },
    {
      "epoch": 2.3914634146341465,
      "grad_norm": 0.10465200245380402,
      "learning_rate": 4.907514816040204e-06,
      "loss": 0.0214,
      "step": 9805
    },
    {
      "epoch": 2.3917073170731706,
      "grad_norm": 0.11491375416517258,
      "learning_rate": 4.903715652073446e-06,
      "loss": 0.0177,
      "step": 9806
    },
    {
      "epoch": 2.3919512195121952,
      "grad_norm": 0.07794637978076935,
      "learning_rate": 4.899917799327716e-06,
      "loss": 0.0133,
      "step": 9807
    },
    {
      "epoch": 2.3921951219512194,
      "grad_norm": 0.11507449299097061,
      "learning_rate": 4.896121258050823e-06,
      "loss": 0.0241,
      "step": 9808
    },
    {
      "epoch": 2.392439024390244,
      "grad_norm": 0.08745788037776947,
      "learning_rate": 4.892326028490474e-06,
      "loss": 0.0096,
      "step": 9809
    },
    {
      "epoch": 2.392682926829268,
      "grad_norm": 0.0682595819234848,
      "learning_rate": 4.8885321108942894e-06,
      "loss": 0.0061,
      "step": 9810
    },
    {
      "epoch": 2.392926829268293,
      "grad_norm": 0.05348602309823036,
      "learning_rate": 4.884739505509825e-06,
      "loss": 0.0041,
      "step": 9811
    },
    {
      "epoch": 2.393170731707317,
      "grad_norm": 0.15844111144542694,
      "learning_rate": 4.8809482125845215e-06,
      "loss": 0.0159,
      "step": 9812
    },
    {
      "epoch": 2.3934146341463416,
      "grad_norm": 0.2406347393989563,
      "learning_rate": 4.877158232365764e-06,
      "loss": 0.0288,
      "step": 9813
    },
    {
      "epoch": 2.3936585365853658,
      "grad_norm": 0.20207931101322174,
      "learning_rate": 4.87336956510083e-06,
      "loss": 0.0209,
      "step": 9814
    },
    {
      "epoch": 2.3939024390243904,
      "grad_norm": 0.10337622463703156,
      "learning_rate": 4.86958221103691e-06,
      "loss": 0.0136,
      "step": 9815
    },
    {
      "epoch": 2.3941463414634145,
      "grad_norm": 0.1429453194141388,
      "learning_rate": 4.865796170421141e-06,
      "loss": 0.0168,
      "step": 9816
    },
    {
      "epoch": 2.394390243902439,
      "grad_norm": 0.10087835788726807,
      "learning_rate": 4.862011443500522e-06,
      "loss": 0.0113,
      "step": 9817
    },
    {
      "epoch": 2.3946341463414633,
      "grad_norm": 0.13566254079341888,
      "learning_rate": 4.8582280305220134e-06,
      "loss": 0.0158,
      "step": 9818
    },
    {
      "epoch": 2.394878048780488,
      "grad_norm": 0.10899633914232254,
      "learning_rate": 4.854445931732468e-06,
      "loss": 0.0214,
      "step": 9819
    },
    {
      "epoch": 2.395121951219512,
      "grad_norm": 0.14119617640972137,
      "learning_rate": 4.850665147378644e-06,
      "loss": 0.0146,
      "step": 9820
    },
    {
      "epoch": 2.3953658536585367,
      "grad_norm": 0.14751216769218445,
      "learning_rate": 4.846885677707247e-06,
      "loss": 0.0186,
      "step": 9821
    },
    {
      "epoch": 2.395609756097561,
      "grad_norm": 0.20239073038101196,
      "learning_rate": 4.843107522964862e-06,
      "loss": 0.0225,
      "step": 9822
    },
    {
      "epoch": 2.3958536585365855,
      "grad_norm": 0.1066102385520935,
      "learning_rate": 4.839330683398e-06,
      "loss": 0.0144,
      "step": 9823
    },
    {
      "epoch": 2.3960975609756097,
      "grad_norm": 0.09398431330919266,
      "learning_rate": 4.835555159253096e-06,
      "loss": 0.0131,
      "step": 9824
    },
    {
      "epoch": 2.3963414634146343,
      "grad_norm": 0.05813555046916008,
      "learning_rate": 4.831780950776485e-06,
      "loss": 0.0171,
      "step": 9825
    },
    {
      "epoch": 2.3965853658536584,
      "grad_norm": 0.09971447288990021,
      "learning_rate": 4.828008058214434e-06,
      "loss": 0.0179,
      "step": 9826
    },
    {
      "epoch": 2.396829268292683,
      "grad_norm": 0.07770755887031555,
      "learning_rate": 4.824236481813102e-06,
      "loss": 0.0141,
      "step": 9827
    },
    {
      "epoch": 2.3970731707317072,
      "grad_norm": 0.1532374918460846,
      "learning_rate": 4.82046622181857e-06,
      "loss": 0.0143,
      "step": 9828
    },
    {
      "epoch": 2.397317073170732,
      "grad_norm": 0.17097285389900208,
      "learning_rate": 4.81669727847685e-06,
      "loss": 0.0179,
      "step": 9829
    },
    {
      "epoch": 2.397560975609756,
      "grad_norm": 0.14179177582263947,
      "learning_rate": 4.812929652033843e-06,
      "loss": 0.0189,
      "step": 9830
    },
    {
      "epoch": 2.3978048780487806,
      "grad_norm": 0.1687275916337967,
      "learning_rate": 4.8091633427353735e-06,
      "loss": 0.0144,
      "step": 9831
    },
    {
      "epoch": 2.398048780487805,
      "grad_norm": 0.1681928187608719,
      "learning_rate": 4.805398350827198e-06,
      "loss": 0.0268,
      "step": 9832
    },
    {
      "epoch": 2.3982926829268294,
      "grad_norm": 0.18735724687576294,
      "learning_rate": 4.8016346765549446e-06,
      "loss": 0.017,
      "step": 9833
    },
    {
      "epoch": 2.3985365853658536,
      "grad_norm": 0.18086646497249603,
      "learning_rate": 4.797872320164201e-06,
      "loss": 0.0241,
      "step": 9834
    },
    {
      "epoch": 2.398780487804878,
      "grad_norm": 0.1818065643310547,
      "learning_rate": 4.794111281900446e-06,
      "loss": 0.027,
      "step": 9835
    },
    {
      "epoch": 2.3990243902439023,
      "grad_norm": 0.11186486482620239,
      "learning_rate": 4.790351562009065e-06,
      "loss": 0.0227,
      "step": 9836
    },
    {
      "epoch": 2.399268292682927,
      "grad_norm": 0.1700614094734192,
      "learning_rate": 4.786593160735386e-06,
      "loss": 0.0305,
      "step": 9837
    },
    {
      "epoch": 2.399512195121951,
      "grad_norm": 0.06600987911224365,
      "learning_rate": 4.782836078324615e-06,
      "loss": 0.0096,
      "step": 9838
    },
    {
      "epoch": 2.3997560975609757,
      "grad_norm": 0.10664961487054825,
      "learning_rate": 4.779080315021908e-06,
      "loss": 0.013,
      "step": 9839
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.15113621950149536,
      "learning_rate": 4.775325871072309e-06,
      "loss": 0.012,
      "step": 9840
    },
    {
      "epoch": 2.4002439024390245,
      "grad_norm": 0.19277553260326385,
      "learning_rate": 4.771572746720774e-06,
      "loss": 0.0334,
      "step": 9841
    },
    {
      "epoch": 2.4004878048780487,
      "grad_norm": 0.11638548225164413,
      "learning_rate": 4.767820942212198e-06,
      "loss": 0.0121,
      "step": 9842
    },
    {
      "epoch": 2.4007317073170733,
      "grad_norm": 0.10562769323587418,
      "learning_rate": 4.7640704577913724e-06,
      "loss": 0.0133,
      "step": 9843
    },
    {
      "epoch": 2.4009756097560975,
      "grad_norm": 0.24279038608074188,
      "learning_rate": 4.760321293702993e-06,
      "loss": 0.0253,
      "step": 9844
    },
    {
      "epoch": 2.401219512195122,
      "grad_norm": 0.18221209943294525,
      "learning_rate": 4.756573450191695e-06,
      "loss": 0.0183,
      "step": 9845
    },
    {
      "epoch": 2.4014634146341463,
      "grad_norm": 0.15474650263786316,
      "learning_rate": 4.752826927502002e-06,
      "loss": 0.0232,
      "step": 9846
    },
    {
      "epoch": 2.401707317073171,
      "grad_norm": 0.1414080709218979,
      "learning_rate": 4.749081725878376e-06,
      "loss": 0.015,
      "step": 9847
    },
    {
      "epoch": 2.401951219512195,
      "grad_norm": 0.15761792659759521,
      "learning_rate": 4.745337845565176e-06,
      "loss": 0.0303,
      "step": 9848
    },
    {
      "epoch": 2.4021951219512196,
      "grad_norm": 0.13525082170963287,
      "learning_rate": 4.741595286806674e-06,
      "loss": 0.0277,
      "step": 9849
    },
    {
      "epoch": 2.402439024390244,
      "grad_norm": 0.13974091410636902,
      "learning_rate": 4.737854049847062e-06,
      "loss": 0.0182,
      "step": 9850
    },
    {
      "epoch": 2.4026829268292684,
      "grad_norm": 0.09351172298192978,
      "learning_rate": 4.734114134930439e-06,
      "loss": 0.009,
      "step": 9851
    },
    {
      "epoch": 2.4029268292682926,
      "grad_norm": 0.11427057534456253,
      "learning_rate": 4.730375542300838e-06,
      "loss": 0.0262,
      "step": 9852
    },
    {
      "epoch": 2.403170731707317,
      "grad_norm": 0.20481319725513458,
      "learning_rate": 4.726638272202177e-06,
      "loss": 0.0256,
      "step": 9853
    },
    {
      "epoch": 2.4034146341463414,
      "grad_norm": 0.09510537981987,
      "learning_rate": 4.722902324878303e-06,
      "loss": 0.0149,
      "step": 9854
    },
    {
      "epoch": 2.403658536585366,
      "grad_norm": 0.14717471599578857,
      "learning_rate": 4.719167700572982e-06,
      "loss": 0.0126,
      "step": 9855
    },
    {
      "epoch": 2.40390243902439,
      "grad_norm": 0.08884534239768982,
      "learning_rate": 4.715434399529886e-06,
      "loss": 0.0159,
      "step": 9856
    },
    {
      "epoch": 2.4041463414634148,
      "grad_norm": 0.0942843034863472,
      "learning_rate": 4.711702421992589e-06,
      "loss": 0.0201,
      "step": 9857
    },
    {
      "epoch": 2.404390243902439,
      "grad_norm": 0.08757676184177399,
      "learning_rate": 4.707971768204608e-06,
      "loss": 0.0088,
      "step": 9858
    },
    {
      "epoch": 2.4046341463414636,
      "grad_norm": 0.1330779641866684,
      "learning_rate": 4.704242438409345e-06,
      "loss": 0.0207,
      "step": 9859
    },
    {
      "epoch": 2.4048780487804877,
      "grad_norm": 0.2490328699350357,
      "learning_rate": 4.700514432850134e-06,
      "loss": 0.0115,
      "step": 9860
    },
    {
      "epoch": 2.4051219512195123,
      "grad_norm": 0.09140004962682724,
      "learning_rate": 4.696787751770218e-06,
      "loss": 0.021,
      "step": 9861
    },
    {
      "epoch": 2.4053658536585365,
      "grad_norm": 0.10842477530241013,
      "learning_rate": 4.693062395412736e-06,
      "loss": 0.0135,
      "step": 9862
    },
    {
      "epoch": 2.405609756097561,
      "grad_norm": 0.12717561423778534,
      "learning_rate": 4.689338364020776e-06,
      "loss": 0.0144,
      "step": 9863
    },
    {
      "epoch": 2.4058536585365853,
      "grad_norm": 0.13834743201732635,
      "learning_rate": 4.6856156578373116e-06,
      "loss": 0.0194,
      "step": 9864
    },
    {
      "epoch": 2.40609756097561,
      "grad_norm": 0.24813975393772125,
      "learning_rate": 4.6818942771052365e-06,
      "loss": 0.024,
      "step": 9865
    },
    {
      "epoch": 2.406341463414634,
      "grad_norm": 0.12289655208587646,
      "learning_rate": 4.6781742220673605e-06,
      "loss": 0.019,
      "step": 9866
    },
    {
      "epoch": 2.4065853658536587,
      "grad_norm": 0.14109890162944794,
      "learning_rate": 4.674455492966398e-06,
      "loss": 0.0143,
      "step": 9867
    },
    {
      "epoch": 2.406829268292683,
      "grad_norm": 0.26035168766975403,
      "learning_rate": 4.6707380900449985e-06,
      "loss": 0.021,
      "step": 9868
    },
    {
      "epoch": 2.4070731707317075,
      "grad_norm": 0.10113390535116196,
      "learning_rate": 4.667022013545707e-06,
      "loss": 0.0122,
      "step": 9869
    },
    {
      "epoch": 2.4073170731707316,
      "grad_norm": 0.14833088219165802,
      "learning_rate": 4.663307263710975e-06,
      "loss": 0.0258,
      "step": 9870
    },
    {
      "epoch": 2.4075609756097562,
      "grad_norm": 0.13235880434513092,
      "learning_rate": 4.659593840783194e-06,
      "loss": 0.0134,
      "step": 9871
    },
    {
      "epoch": 2.4078048780487804,
      "grad_norm": 0.14499492943286896,
      "learning_rate": 4.6558817450046425e-06,
      "loss": 0.0159,
      "step": 9872
    },
    {
      "epoch": 2.408048780487805,
      "grad_norm": 0.08733978867530823,
      "learning_rate": 4.652170976617534e-06,
      "loss": 0.0089,
      "step": 9873
    },
    {
      "epoch": 2.408292682926829,
      "grad_norm": 0.15677385032176971,
      "learning_rate": 4.6484615358639765e-06,
      "loss": 0.0206,
      "step": 9874
    },
    {
      "epoch": 2.408536585365854,
      "grad_norm": 0.10855134576559067,
      "learning_rate": 4.644753422985998e-06,
      "loss": 0.0149,
      "step": 9875
    },
    {
      "epoch": 2.408780487804878,
      "grad_norm": 0.14512482285499573,
      "learning_rate": 4.641046638225549e-06,
      "loss": 0.0154,
      "step": 9876
    },
    {
      "epoch": 2.4090243902439026,
      "grad_norm": 0.08889035880565643,
      "learning_rate": 4.637341181824484e-06,
      "loss": 0.0105,
      "step": 9877
    },
    {
      "epoch": 2.4092682926829267,
      "grad_norm": 0.09747485816478729,
      "learning_rate": 4.633637054024561e-06,
      "loss": 0.0162,
      "step": 9878
    },
    {
      "epoch": 2.4095121951219514,
      "grad_norm": 0.09225554764270782,
      "learning_rate": 4.629934255067484e-06,
      "loss": 0.0159,
      "step": 9879
    },
    {
      "epoch": 2.4097560975609755,
      "grad_norm": 0.10818639397621155,
      "learning_rate": 4.626232785194837e-06,
      "loss": 0.0044,
      "step": 9880
    },
    {
      "epoch": 2.41,
      "grad_norm": 0.09621136635541916,
      "learning_rate": 4.622532644648123e-06,
      "loss": 0.0179,
      "step": 9881
    },
    {
      "epoch": 2.4102439024390243,
      "grad_norm": 0.24752849340438843,
      "learning_rate": 4.618833833668784e-06,
      "loss": 0.0199,
      "step": 9882
    },
    {
      "epoch": 2.410487804878049,
      "grad_norm": 0.11943967640399933,
      "learning_rate": 4.615136352498134e-06,
      "loss": 0.0156,
      "step": 9883
    },
    {
      "epoch": 2.410731707317073,
      "grad_norm": 0.20970936119556427,
      "learning_rate": 4.611440201377437e-06,
      "loss": 0.0186,
      "step": 9884
    },
    {
      "epoch": 2.4109756097560977,
      "grad_norm": 0.09777773171663284,
      "learning_rate": 4.607745380547848e-06,
      "loss": 0.015,
      "step": 9885
    },
    {
      "epoch": 2.411219512195122,
      "grad_norm": 0.09106390178203583,
      "learning_rate": 4.604051890250452e-06,
      "loss": 0.0133,
      "step": 9886
    },
    {
      "epoch": 2.4114634146341465,
      "grad_norm": 0.08645854145288467,
      "learning_rate": 4.600359730726231e-06,
      "loss": 0.0194,
      "step": 9887
    },
    {
      "epoch": 2.4117073170731707,
      "grad_norm": 0.21418839693069458,
      "learning_rate": 4.596668902216084e-06,
      "loss": 0.023,
      "step": 9888
    },
    {
      "epoch": 2.4119512195121953,
      "grad_norm": 0.16250433027744293,
      "learning_rate": 4.5929794049608345e-06,
      "loss": 0.0194,
      "step": 9889
    },
    {
      "epoch": 2.4121951219512194,
      "grad_norm": 0.15962977707386017,
      "learning_rate": 4.589291239201207e-06,
      "loss": 0.0168,
      "step": 9890
    },
    {
      "epoch": 2.412439024390244,
      "grad_norm": 0.13519324362277985,
      "learning_rate": 4.5856044051778375e-06,
      "loss": 0.016,
      "step": 9891
    },
    {
      "epoch": 2.412682926829268,
      "grad_norm": 0.1199013739824295,
      "learning_rate": 4.581918903131293e-06,
      "loss": 0.0109,
      "step": 9892
    },
    {
      "epoch": 2.412926829268293,
      "grad_norm": 0.08238726854324341,
      "learning_rate": 4.578234733302028e-06,
      "loss": 0.0179,
      "step": 9893
    },
    {
      "epoch": 2.413170731707317,
      "grad_norm": 0.09595157206058502,
      "learning_rate": 4.574551895930434e-06,
      "loss": 0.018,
      "step": 9894
    },
    {
      "epoch": 2.4134146341463416,
      "grad_norm": 0.14231336116790771,
      "learning_rate": 4.570870391256804e-06,
      "loss": 0.0142,
      "step": 9895
    },
    {
      "epoch": 2.4136585365853658,
      "grad_norm": 0.10978881269693375,
      "learning_rate": 4.5671902195213304e-06,
      "loss": 0.013,
      "step": 9896
    },
    {
      "epoch": 2.4139024390243904,
      "grad_norm": 0.1844412237405777,
      "learning_rate": 4.563511380964155e-06,
      "loss": 0.0272,
      "step": 9897
    },
    {
      "epoch": 2.4141463414634146,
      "grad_norm": 0.08266814798116684,
      "learning_rate": 4.559833875825298e-06,
      "loss": 0.012,
      "step": 9898
    },
    {
      "epoch": 2.414390243902439,
      "grad_norm": 0.11540669202804565,
      "learning_rate": 4.556157704344707e-06,
      "loss": 0.0296,
      "step": 9899
    },
    {
      "epoch": 2.4146341463414633,
      "grad_norm": 0.16560529172420502,
      "learning_rate": 4.552482866762242e-06,
      "loss": 0.0273,
      "step": 9900
    },
    {
      "epoch": 2.414878048780488,
      "grad_norm": 0.11535682529211044,
      "learning_rate": 4.548809363317666e-06,
      "loss": 0.0141,
      "step": 9901
    },
    {
      "epoch": 2.415121951219512,
      "grad_norm": 0.1394118368625641,
      "learning_rate": 4.5451371942506785e-06,
      "loss": 0.0252,
      "step": 9902
    },
    {
      "epoch": 2.4153658536585367,
      "grad_norm": 0.1751347780227661,
      "learning_rate": 4.541466359800872e-06,
      "loss": 0.0186,
      "step": 9903
    },
    {
      "epoch": 2.415609756097561,
      "grad_norm": 0.10668488591909409,
      "learning_rate": 4.537796860207749e-06,
      "loss": 0.0172,
      "step": 9904
    },
    {
      "epoch": 2.4158536585365855,
      "grad_norm": 0.09265577048063278,
      "learning_rate": 4.534128695710746e-06,
      "loss": 0.0108,
      "step": 9905
    },
    {
      "epoch": 2.4160975609756097,
      "grad_norm": 0.24464553594589233,
      "learning_rate": 4.53046186654919e-06,
      "loss": 0.0216,
      "step": 9906
    },
    {
      "epoch": 2.4163414634146343,
      "grad_norm": 0.10036283731460571,
      "learning_rate": 4.526796372962338e-06,
      "loss": 0.017,
      "step": 9907
    },
    {
      "epoch": 2.4165853658536585,
      "grad_norm": 0.1515708714723587,
      "learning_rate": 4.523132215189346e-06,
      "loss": 0.0229,
      "step": 9908
    },
    {
      "epoch": 2.416829268292683,
      "grad_norm": 0.06903239339590073,
      "learning_rate": 4.519469393469289e-06,
      "loss": 0.0118,
      "step": 9909
    },
    {
      "epoch": 2.4170731707317072,
      "grad_norm": 0.12268144637346268,
      "learning_rate": 4.515807908041161e-06,
      "loss": 0.0096,
      "step": 9910
    },
    {
      "epoch": 2.417317073170732,
      "grad_norm": 0.1391601264476776,
      "learning_rate": 4.512147759143859e-06,
      "loss": 0.0207,
      "step": 9911
    },
    {
      "epoch": 2.417560975609756,
      "grad_norm": 0.11761786043643951,
      "learning_rate": 4.508488947016193e-06,
      "loss": 0.0167,
      "step": 9912
    },
    {
      "epoch": 2.4178048780487806,
      "grad_norm": 0.09471026062965393,
      "learning_rate": 4.504831471896895e-06,
      "loss": 0.0261,
      "step": 9913
    },
    {
      "epoch": 2.418048780487805,
      "grad_norm": 0.115768201649189,
      "learning_rate": 4.5011753340246045e-06,
      "loss": 0.0128,
      "step": 9914
    },
    {
      "epoch": 2.4182926829268294,
      "grad_norm": 0.08384186029434204,
      "learning_rate": 4.4975205336378696e-06,
      "loss": 0.0103,
      "step": 9915
    },
    {
      "epoch": 2.4185365853658536,
      "grad_norm": 0.04796571657061577,
      "learning_rate": 4.493867070975155e-06,
      "loss": 0.0051,
      "step": 9916
    },
    {
      "epoch": 2.418780487804878,
      "grad_norm": 0.1015595868229866,
      "learning_rate": 4.490214946274831e-06,
      "loss": 0.0161,
      "step": 9917
    },
    {
      "epoch": 2.4190243902439024,
      "grad_norm": 0.11050956696271896,
      "learning_rate": 4.486564159775206e-06,
      "loss": 0.0193,
      "step": 9918
    },
    {
      "epoch": 2.419268292682927,
      "grad_norm": 0.10268598049879074,
      "learning_rate": 4.482914711714462e-06,
      "loss": 0.0206,
      "step": 9919
    },
    {
      "epoch": 2.419512195121951,
      "grad_norm": 0.11933252215385437,
      "learning_rate": 4.479266602330731e-06,
      "loss": 0.0113,
      "step": 9920
    },
    {
      "epoch": 2.4197560975609758,
      "grad_norm": 0.08756793290376663,
      "learning_rate": 4.475619831862035e-06,
      "loss": 0.0116,
      "step": 9921
    },
    {
      "epoch": 2.42,
      "grad_norm": 0.09702819585800171,
      "learning_rate": 4.471974400546308e-06,
      "loss": 0.0099,
      "step": 9922
    },
    {
      "epoch": 2.4202439024390245,
      "grad_norm": 0.18603700399398804,
      "learning_rate": 4.468330308621416e-06,
      "loss": 0.0192,
      "step": 9923
    },
    {
      "epoch": 2.4204878048780487,
      "grad_norm": 0.1665995717048645,
      "learning_rate": 4.464687556325114e-06,
      "loss": 0.0206,
      "step": 9924
    },
    {
      "epoch": 2.4207317073170733,
      "grad_norm": 0.2664159834384918,
      "learning_rate": 4.461046143895081e-06,
      "loss": 0.0367,
      "step": 9925
    },
    {
      "epoch": 2.4209756097560975,
      "grad_norm": 0.13955923914909363,
      "learning_rate": 4.457406071568918e-06,
      "loss": 0.0106,
      "step": 9926
    },
    {
      "epoch": 2.421219512195122,
      "grad_norm": 0.05792770907282829,
      "learning_rate": 4.453767339584122e-06,
      "loss": 0.0097,
      "step": 9927
    },
    {
      "epoch": 2.4214634146341463,
      "grad_norm": 0.11177971214056015,
      "learning_rate": 4.450129948178103e-06,
      "loss": 0.0143,
      "step": 9928
    },
    {
      "epoch": 2.421707317073171,
      "grad_norm": 0.11466128379106522,
      "learning_rate": 4.4464938975882e-06,
      "loss": 0.0258,
      "step": 9929
    },
    {
      "epoch": 2.421951219512195,
      "grad_norm": 0.1281675100326538,
      "learning_rate": 4.442859188051645e-06,
      "loss": 0.0109,
      "step": 9930
    },
    {
      "epoch": 2.4221951219512197,
      "grad_norm": 0.21272073686122894,
      "learning_rate": 4.439225819805609e-06,
      "loss": 0.0303,
      "step": 9931
    },
    {
      "epoch": 2.422439024390244,
      "grad_norm": 0.0998939797282219,
      "learning_rate": 4.435593793087131e-06,
      "loss": 0.0095,
      "step": 9932
    },
    {
      "epoch": 2.4226829268292684,
      "grad_norm": 0.1397734433412552,
      "learning_rate": 4.431963108133213e-06,
      "loss": 0.0101,
      "step": 9933
    },
    {
      "epoch": 2.4229268292682926,
      "grad_norm": 0.11467394977807999,
      "learning_rate": 4.4283337651807365e-06,
      "loss": 0.0193,
      "step": 9934
    },
    {
      "epoch": 2.4231707317073172,
      "grad_norm": 0.13872870802879333,
      "learning_rate": 4.4247057644664995e-06,
      "loss": 0.0244,
      "step": 9935
    },
    {
      "epoch": 2.4234146341463414,
      "grad_norm": 0.16798020899295807,
      "learning_rate": 4.421079106227233e-06,
      "loss": 0.0195,
      "step": 9936
    },
    {
      "epoch": 2.423658536585366,
      "grad_norm": 0.1519196629524231,
      "learning_rate": 4.417453790699555e-06,
      "loss": 0.0246,
      "step": 9937
    },
    {
      "epoch": 2.42390243902439,
      "grad_norm": 0.12469425052404404,
      "learning_rate": 4.4138298181199995e-06,
      "loss": 0.0226,
      "step": 9938
    },
    {
      "epoch": 2.424146341463415,
      "grad_norm": 0.12303336709737778,
      "learning_rate": 4.410207188725038e-06,
      "loss": 0.018,
      "step": 9939
    },
    {
      "epoch": 2.424390243902439,
      "grad_norm": 0.0877569168806076,
      "learning_rate": 4.406585902751023e-06,
      "loss": 0.0222,
      "step": 9940
    },
    {
      "epoch": 2.4246341463414636,
      "grad_norm": 0.1278889775276184,
      "learning_rate": 4.40296596043423e-06,
      "loss": 0.0272,
      "step": 9941
    },
    {
      "epoch": 2.4248780487804877,
      "grad_norm": 0.14174716174602509,
      "learning_rate": 4.399347362010861e-06,
      "loss": 0.0217,
      "step": 9942
    },
    {
      "epoch": 2.4251219512195124,
      "grad_norm": 0.15985019505023956,
      "learning_rate": 4.395730107717005e-06,
      "loss": 0.0261,
      "step": 9943
    },
    {
      "epoch": 2.4253658536585365,
      "grad_norm": 0.11919011920690536,
      "learning_rate": 4.392114197788691e-06,
      "loss": 0.0324,
      "step": 9944
    },
    {
      "epoch": 2.425609756097561,
      "grad_norm": 0.06399331241846085,
      "learning_rate": 4.3884996324618396e-06,
      "loss": 0.0081,
      "step": 9945
    },
    {
      "epoch": 2.4258536585365853,
      "grad_norm": 0.16007089614868164,
      "learning_rate": 4.384886411972283e-06,
      "loss": 0.0163,
      "step": 9946
    },
    {
      "epoch": 2.42609756097561,
      "grad_norm": 0.14381848275661469,
      "learning_rate": 4.381274536555782e-06,
      "loss": 0.0129,
      "step": 9947
    },
    {
      "epoch": 2.426341463414634,
      "grad_norm": 0.24769607186317444,
      "learning_rate": 4.377664006448001e-06,
      "loss": 0.0241,
      "step": 9948
    },
    {
      "epoch": 2.4265853658536587,
      "grad_norm": 0.1313423067331314,
      "learning_rate": 4.374054821884513e-06,
      "loss": 0.0112,
      "step": 9949
    },
    {
      "epoch": 2.426829268292683,
      "grad_norm": 0.12500204145908356,
      "learning_rate": 4.370446983100804e-06,
      "loss": 0.0142,
      "step": 9950
    },
    {
      "epoch": 2.4270731707317075,
      "grad_norm": 0.18632808327674866,
      "learning_rate": 4.366840490332269e-06,
      "loss": 0.017,
      "step": 9951
    },
    {
      "epoch": 2.4273170731707316,
      "grad_norm": 0.14463675022125244,
      "learning_rate": 4.363235343814237e-06,
      "loss": 0.0203,
      "step": 9952
    },
    {
      "epoch": 2.4275609756097563,
      "grad_norm": 0.11347000300884247,
      "learning_rate": 4.359631543781923e-06,
      "loss": 0.0139,
      "step": 9953
    },
    {
      "epoch": 2.4278048780487804,
      "grad_norm": 0.12222620844841003,
      "learning_rate": 4.356029090470456e-06,
      "loss": 0.0185,
      "step": 9954
    },
    {
      "epoch": 2.428048780487805,
      "grad_norm": 0.2262917011976242,
      "learning_rate": 4.352427984114902e-06,
      "loss": 0.0377,
      "step": 9955
    },
    {
      "epoch": 2.428292682926829,
      "grad_norm": 0.21339964866638184,
      "learning_rate": 4.348828224950205e-06,
      "loss": 0.0208,
      "step": 9956
    },
    {
      "epoch": 2.428536585365854,
      "grad_norm": 0.11397497355937958,
      "learning_rate": 4.345229813211254e-06,
      "loss": 0.0172,
      "step": 9957
    },
    {
      "epoch": 2.428780487804878,
      "grad_norm": 0.11677148193120956,
      "learning_rate": 4.341632749132829e-06,
      "loss": 0.0316,
      "step": 9958
    },
    {
      "epoch": 2.4290243902439026,
      "grad_norm": 0.10461977869272232,
      "learning_rate": 4.338037032949618e-06,
      "loss": 0.016,
      "step": 9959
    },
    {
      "epoch": 2.4292682926829268,
      "grad_norm": 0.24017393589019775,
      "learning_rate": 4.334442664896246e-06,
      "loss": 0.0227,
      "step": 9960
    },
    {
      "epoch": 2.4295121951219514,
      "grad_norm": 0.2963792383670807,
      "learning_rate": 4.330849645207227e-06,
      "loss": 0.0289,
      "step": 9961
    },
    {
      "epoch": 2.4297560975609755,
      "grad_norm": 0.30612409114837646,
      "learning_rate": 4.327257974116986e-06,
      "loss": 0.0159,
      "step": 9962
    },
    {
      "epoch": 2.43,
      "grad_norm": 0.1370091587305069,
      "learning_rate": 4.323667651859886e-06,
      "loss": 0.015,
      "step": 9963
    },
    {
      "epoch": 2.4302439024390243,
      "grad_norm": 0.11666537821292877,
      "learning_rate": 4.320078678670167e-06,
      "loss": 0.0093,
      "step": 9964
    },
    {
      "epoch": 2.430487804878049,
      "grad_norm": 0.061230145394802094,
      "learning_rate": 4.316491054782018e-06,
      "loss": 0.0145,
      "step": 9965
    },
    {
      "epoch": 2.430731707317073,
      "grad_norm": 0.13305990397930145,
      "learning_rate": 4.312904780429506e-06,
      "loss": 0.0157,
      "step": 9966
    },
    {
      "epoch": 2.4309756097560977,
      "grad_norm": 0.07389456778764725,
      "learning_rate": 4.309319855846622e-06,
      "loss": 0.0065,
      "step": 9967
    },
    {
      "epoch": 2.431219512195122,
      "grad_norm": 0.1400735229253769,
      "learning_rate": 4.305736281267281e-06,
      "loss": 0.0167,
      "step": 9968
    },
    {
      "epoch": 2.4314634146341465,
      "grad_norm": 0.09063361585140228,
      "learning_rate": 4.30215405692529e-06,
      "loss": 0.0099,
      "step": 9969
    },
    {
      "epoch": 2.4317073170731707,
      "grad_norm": 0.24924015998840332,
      "learning_rate": 4.298573183054391e-06,
      "loss": 0.0095,
      "step": 9970
    },
    {
      "epoch": 2.4319512195121953,
      "grad_norm": 0.08738531172275543,
      "learning_rate": 4.294993659888222e-06,
      "loss": 0.022,
      "step": 9971
    },
    {
      "epoch": 2.4321951219512195,
      "grad_norm": 0.07510369271039963,
      "learning_rate": 4.2914154876603215e-06,
      "loss": 0.0122,
      "step": 9972
    },
    {
      "epoch": 2.432439024390244,
      "grad_norm": 0.10791907459497452,
      "learning_rate": 4.287838666604177e-06,
      "loss": 0.0163,
      "step": 9973
    },
    {
      "epoch": 2.4326829268292682,
      "grad_norm": 0.1025899276137352,
      "learning_rate": 4.284263196953151e-06,
      "loss": 0.0063,
      "step": 9974
    },
    {
      "epoch": 2.432926829268293,
      "grad_norm": 0.11968734860420227,
      "learning_rate": 4.280689078940525e-06,
      "loss": 0.0186,
      "step": 9975
    },
    {
      "epoch": 2.433170731707317,
      "grad_norm": 0.17226740717887878,
      "learning_rate": 4.277116312799517e-06,
      "loss": 0.017,
      "step": 9976
    },
    {
      "epoch": 2.4334146341463416,
      "grad_norm": 0.17225292325019836,
      "learning_rate": 4.2735448987632265e-06,
      "loss": 0.0157,
      "step": 9977
    },
    {
      "epoch": 2.433658536585366,
      "grad_norm": 0.08101259917020798,
      "learning_rate": 4.269974837064686e-06,
      "loss": 0.0125,
      "step": 9978
    },
    {
      "epoch": 2.4339024390243904,
      "grad_norm": 0.16375979781150818,
      "learning_rate": 4.266406127936828e-06,
      "loss": 0.0179,
      "step": 9979
    },
    {
      "epoch": 2.4341463414634146,
      "grad_norm": 0.0986136719584465,
      "learning_rate": 4.262838771612493e-06,
      "loss": 0.0091,
      "step": 9980
    },
    {
      "epoch": 2.434390243902439,
      "grad_norm": 0.24687223136425018,
      "learning_rate": 4.259272768324455e-06,
      "loss": 0.0447,
      "step": 9981
    },
    {
      "epoch": 2.4346341463414634,
      "grad_norm": 0.15363390743732452,
      "learning_rate": 4.255708118305365e-06,
      "loss": 0.0175,
      "step": 9982
    },
    {
      "epoch": 2.434878048780488,
      "grad_norm": 0.14465315639972687,
      "learning_rate": 4.2521448217878204e-06,
      "loss": 0.0232,
      "step": 9983
    },
    {
      "epoch": 2.435121951219512,
      "grad_norm": 0.07050482928752899,
      "learning_rate": 4.248582879004312e-06,
      "loss": 0.014,
      "step": 9984
    },
    {
      "epoch": 2.4353658536585368,
      "grad_norm": 0.11760614812374115,
      "learning_rate": 4.245022290187239e-06,
      "loss": 0.0124,
      "step": 9985
    },
    {
      "epoch": 2.435609756097561,
      "grad_norm": 0.11220286786556244,
      "learning_rate": 4.241463055568931e-06,
      "loss": 0.015,
      "step": 9986
    },
    {
      "epoch": 2.4358536585365855,
      "grad_norm": 0.07543175667524338,
      "learning_rate": 4.23790517538161e-06,
      "loss": 0.0093,
      "step": 9987
    },
    {
      "epoch": 2.4360975609756097,
      "grad_norm": 0.1350889950990677,
      "learning_rate": 4.234348649857412e-06,
      "loss": 0.025,
      "step": 9988
    },
    {
      "epoch": 2.4363414634146343,
      "grad_norm": 0.18197648227214813,
      "learning_rate": 4.230793479228398e-06,
      "loss": 0.0174,
      "step": 9989
    },
    {
      "epoch": 2.4365853658536585,
      "grad_norm": 0.17779836058616638,
      "learning_rate": 4.227239663726526e-06,
      "loss": 0.0234,
      "step": 9990
    },
    {
      "epoch": 2.436829268292683,
      "grad_norm": 0.1317143738269806,
      "learning_rate": 4.223687203583679e-06,
      "loss": 0.013,
      "step": 9991
    },
    {
      "epoch": 2.4370731707317073,
      "grad_norm": 0.14310292899608612,
      "learning_rate": 4.220136099031641e-06,
      "loss": 0.0169,
      "step": 9992
    },
    {
      "epoch": 2.437317073170732,
      "grad_norm": 0.06694171577692032,
      "learning_rate": 4.216586350302101e-06,
      "loss": 0.0122,
      "step": 9993
    },
    {
      "epoch": 2.437560975609756,
      "grad_norm": 0.08260894566774368,
      "learning_rate": 4.213037957626684e-06,
      "loss": 0.0159,
      "step": 9994
    },
    {
      "epoch": 2.4378048780487807,
      "grad_norm": 0.16150912642478943,
      "learning_rate": 4.2094909212369055e-06,
      "loss": 0.0196,
      "step": 9995
    },
    {
      "epoch": 2.438048780487805,
      "grad_norm": 0.1514892578125,
      "learning_rate": 4.205945241364193e-06,
      "loss": 0.0155,
      "step": 9996
    },
    {
      "epoch": 2.4382926829268294,
      "grad_norm": 0.15487638115882874,
      "learning_rate": 4.202400918239904e-06,
      "loss": 0.0199,
      "step": 9997
    },
    {
      "epoch": 2.4385365853658536,
      "grad_norm": 0.11136208474636078,
      "learning_rate": 4.198857952095287e-06,
      "loss": 0.0181,
      "step": 9998
    },
    {
      "epoch": 2.438780487804878,
      "grad_norm": 0.09232497960329056,
      "learning_rate": 4.19531634316151e-06,
      "loss": 0.0134,
      "step": 9999
    },
    {
      "epoch": 2.4390243902439024,
      "grad_norm": 0.2181316614151001,
      "learning_rate": 4.19177609166965e-06,
      "loss": 0.0143,
      "step": 10000
    },
    {
      "epoch": 2.439268292682927,
      "grad_norm": 0.19607751071453094,
      "learning_rate": 4.188237197850697e-06,
      "loss": 0.0158,
      "step": 10001
    },
    {
      "epoch": 2.439512195121951,
      "grad_norm": 0.1417457014322281,
      "learning_rate": 4.184699661935559e-06,
      "loss": 0.0202,
      "step": 10002
    },
    {
      "epoch": 2.439756097560976,
      "grad_norm": 0.15298420190811157,
      "learning_rate": 4.181163484155043e-06,
      "loss": 0.0153,
      "step": 10003
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.2755621075630188,
      "learning_rate": 4.177628664739883e-06,
      "loss": 0.0267,
      "step": 10004
    },
    {
      "epoch": 2.4402439024390246,
      "grad_norm": 0.1457517445087433,
      "learning_rate": 4.174095203920708e-06,
      "loss": 0.0232,
      "step": 10005
    },
    {
      "epoch": 2.4404878048780487,
      "grad_norm": 0.10853137075901031,
      "learning_rate": 4.17056310192806e-06,
      "loss": 0.0245,
      "step": 10006
    },
    {
      "epoch": 2.4407317073170733,
      "grad_norm": 0.15630623698234558,
      "learning_rate": 4.167032358992412e-06,
      "loss": 0.0283,
      "step": 10007
    },
    {
      "epoch": 2.4409756097560975,
      "grad_norm": 0.17028965055942535,
      "learning_rate": 4.163502975344127e-06,
      "loss": 0.0209,
      "step": 10008
    },
    {
      "epoch": 2.441219512195122,
      "grad_norm": 0.09112592041492462,
      "learning_rate": 4.15997495121348e-06,
      "loss": 0.0068,
      "step": 10009
    },
    {
      "epoch": 2.4414634146341463,
      "grad_norm": 0.0878346785902977,
      "learning_rate": 4.156448286830675e-06,
      "loss": 0.0123,
      "step": 10010
    },
    {
      "epoch": 2.441707317073171,
      "grad_norm": 0.09570681303739548,
      "learning_rate": 4.152922982425811e-06,
      "loss": 0.0128,
      "step": 10011
    },
    {
      "epoch": 2.441951219512195,
      "grad_norm": 0.13868433237075806,
      "learning_rate": 4.1493990382288985e-06,
      "loss": 0.0154,
      "step": 10012
    },
    {
      "epoch": 2.4421951219512197,
      "grad_norm": 0.14899951219558716,
      "learning_rate": 4.1458764544698745e-06,
      "loss": 0.0204,
      "step": 10013
    },
    {
      "epoch": 2.442439024390244,
      "grad_norm": 0.07852602750062943,
      "learning_rate": 4.142355231378572e-06,
      "loss": 0.0078,
      "step": 10014
    },
    {
      "epoch": 2.4426829268292685,
      "grad_norm": 0.17831143736839294,
      "learning_rate": 4.138835369184738e-06,
      "loss": 0.0303,
      "step": 10015
    },
    {
      "epoch": 2.4429268292682926,
      "grad_norm": 0.12796331942081451,
      "learning_rate": 4.135316868118028e-06,
      "loss": 0.0125,
      "step": 10016
    },
    {
      "epoch": 2.4431707317073172,
      "grad_norm": 0.10220044106245041,
      "learning_rate": 4.131799728408026e-06,
      "loss": 0.0198,
      "step": 10017
    },
    {
      "epoch": 2.4434146341463414,
      "grad_norm": 0.09044720977544785,
      "learning_rate": 4.128283950284209e-06,
      "loss": 0.0152,
      "step": 10018
    },
    {
      "epoch": 2.443658536585366,
      "grad_norm": 0.3081206977367401,
      "learning_rate": 4.124769533975961e-06,
      "loss": 0.0165,
      "step": 10019
    },
    {
      "epoch": 2.44390243902439,
      "grad_norm": 0.12774911522865295,
      "learning_rate": 4.121256479712604e-06,
      "loss": 0.018,
      "step": 10020
    },
    {
      "epoch": 2.444146341463415,
      "grad_norm": 0.09228368103504181,
      "learning_rate": 4.117744787723346e-06,
      "loss": 0.0145,
      "step": 10021
    },
    {
      "epoch": 2.444390243902439,
      "grad_norm": 0.18070577085018158,
      "learning_rate": 4.114234458237306e-06,
      "loss": 0.0343,
      "step": 10022
    },
    {
      "epoch": 2.4446341463414636,
      "grad_norm": 0.10719244182109833,
      "learning_rate": 4.110725491483539e-06,
      "loss": 0.0167,
      "step": 10023
    },
    {
      "epoch": 2.4448780487804878,
      "grad_norm": 0.13417786359786987,
      "learning_rate": 4.107217887690975e-06,
      "loss": 0.0231,
      "step": 10024
    },
    {
      "epoch": 2.4451219512195124,
      "grad_norm": 0.07701259851455688,
      "learning_rate": 4.103711647088495e-06,
      "loss": 0.0111,
      "step": 10025
    },
    {
      "epoch": 2.4453658536585365,
      "grad_norm": 0.2538968324661255,
      "learning_rate": 4.100206769904858e-06,
      "loss": 0.0186,
      "step": 10026
    },
    {
      "epoch": 2.445609756097561,
      "grad_norm": 0.24754536151885986,
      "learning_rate": 4.096703256368745e-06,
      "loss": 0.0278,
      "step": 10027
    },
    {
      "epoch": 2.4458536585365853,
      "grad_norm": 0.10868845134973526,
      "learning_rate": 4.093201106708758e-06,
      "loss": 0.0218,
      "step": 10028
    },
    {
      "epoch": 2.44609756097561,
      "grad_norm": 0.11476368457078934,
      "learning_rate": 4.0897003211533974e-06,
      "loss": 0.0234,
      "step": 10029
    },
    {
      "epoch": 2.446341463414634,
      "grad_norm": 0.07703227549791336,
      "learning_rate": 4.086200899931072e-06,
      "loss": 0.0081,
      "step": 10030
    },
    {
      "epoch": 2.4465853658536587,
      "grad_norm": 0.1589343100786209,
      "learning_rate": 4.082702843270126e-06,
      "loss": 0.0159,
      "step": 10031
    },
    {
      "epoch": 2.446829268292683,
      "grad_norm": 0.05942613258957863,
      "learning_rate": 4.079206151398774e-06,
      "loss": 0.0077,
      "step": 10032
    },
    {
      "epoch": 2.4470731707317075,
      "grad_norm": 0.1722736358642578,
      "learning_rate": 4.075710824545181e-06,
      "loss": 0.015,
      "step": 10033
    },
    {
      "epoch": 2.4473170731707317,
      "grad_norm": 0.12882988154888153,
      "learning_rate": 4.072216862937403e-06,
      "loss": 0.0118,
      "step": 10034
    },
    {
      "epoch": 2.4475609756097563,
      "grad_norm": 0.11440041661262512,
      "learning_rate": 4.068724266803403e-06,
      "loss": 0.019,
      "step": 10035
    },
    {
      "epoch": 2.4478048780487804,
      "grad_norm": 0.10424689203500748,
      "learning_rate": 4.065233036371072e-06,
      "loss": 0.0141,
      "step": 10036
    },
    {
      "epoch": 2.448048780487805,
      "grad_norm": 0.12051552534103394,
      "learning_rate": 4.061743171868193e-06,
      "loss": 0.0174,
      "step": 10037
    },
    {
      "epoch": 2.4482926829268292,
      "grad_norm": 0.14140988886356354,
      "learning_rate": 4.0582546735224795e-06,
      "loss": 0.0203,
      "step": 10038
    },
    {
      "epoch": 2.448536585365854,
      "grad_norm": 0.1204286515712738,
      "learning_rate": 4.054767541561538e-06,
      "loss": 0.0218,
      "step": 10039
    },
    {
      "epoch": 2.448780487804878,
      "grad_norm": 0.13898402452468872,
      "learning_rate": 4.051281776212892e-06,
      "loss": 0.0172,
      "step": 10040
    },
    {
      "epoch": 2.4490243902439026,
      "grad_norm": 0.12345540523529053,
      "learning_rate": 4.047797377703985e-06,
      "loss": 0.02,
      "step": 10041
    },
    {
      "epoch": 2.449268292682927,
      "grad_norm": 0.13892272114753723,
      "learning_rate": 4.044314346262157e-06,
      "loss": 0.0103,
      "step": 10042
    },
    {
      "epoch": 2.4495121951219514,
      "grad_norm": 0.18381758034229279,
      "learning_rate": 4.040832682114662e-06,
      "loss": 0.0248,
      "step": 10043
    },
    {
      "epoch": 2.4497560975609756,
      "grad_norm": 0.09280095249414444,
      "learning_rate": 4.037352385488677e-06,
      "loss": 0.014,
      "step": 10044
    },
    {
      "epoch": 2.45,
      "grad_norm": 0.12238011509180069,
      "learning_rate": 4.033873456611276e-06,
      "loss": 0.0118,
      "step": 10045
    },
    {
      "epoch": 2.4502439024390243,
      "grad_norm": 0.06223477050662041,
      "learning_rate": 4.030395895709443e-06,
      "loss": 0.0076,
      "step": 10046
    },
    {
      "epoch": 2.450487804878049,
      "grad_norm": 0.08800162374973297,
      "learning_rate": 4.026919703010096e-06,
      "loss": 0.0103,
      "step": 10047
    },
    {
      "epoch": 2.450731707317073,
      "grad_norm": 0.16623440384864807,
      "learning_rate": 4.023444878740021e-06,
      "loss": 0.0082,
      "step": 10048
    },
    {
      "epoch": 2.4509756097560977,
      "grad_norm": 0.20019982755184174,
      "learning_rate": 4.0199714231259596e-06,
      "loss": 0.0237,
      "step": 10049
    },
    {
      "epoch": 2.451219512195122,
      "grad_norm": 0.1117124930024147,
      "learning_rate": 4.01649933639453e-06,
      "loss": 0.0183,
      "step": 10050
    },
    {
      "epoch": 2.4514634146341465,
      "grad_norm": 0.2340196669101715,
      "learning_rate": 4.013028618772288e-06,
      "loss": 0.0247,
      "step": 10051
    },
    {
      "epoch": 2.4517073170731707,
      "grad_norm": 0.10556851327419281,
      "learning_rate": 4.00955927048568e-06,
      "loss": 0.0195,
      "step": 10052
    },
    {
      "epoch": 2.4519512195121953,
      "grad_norm": 0.19761019945144653,
      "learning_rate": 4.006091291761066e-06,
      "loss": 0.0236,
      "step": 10053
    },
    {
      "epoch": 2.4521951219512195,
      "grad_norm": 0.08801862597465515,
      "learning_rate": 4.002624682824735e-06,
      "loss": 0.0105,
      "step": 10054
    },
    {
      "epoch": 2.452439024390244,
      "grad_norm": 0.1026746854186058,
      "learning_rate": 3.999159443902861e-06,
      "loss": 0.0096,
      "step": 10055
    },
    {
      "epoch": 2.4526829268292683,
      "grad_norm": 0.08820179104804993,
      "learning_rate": 3.995695575221542e-06,
      "loss": 0.0188,
      "step": 10056
    },
    {
      "epoch": 2.452926829268293,
      "grad_norm": 0.18946519494056702,
      "learning_rate": 3.992233077006788e-06,
      "loss": 0.0211,
      "step": 10057
    },
    {
      "epoch": 2.453170731707317,
      "grad_norm": 0.14291004836559296,
      "learning_rate": 3.9887719494845185e-06,
      "loss": 0.0165,
      "step": 10058
    },
    {
      "epoch": 2.4534146341463416,
      "grad_norm": 0.08007433265447617,
      "learning_rate": 3.98531219288055e-06,
      "loss": 0.01,
      "step": 10059
    },
    {
      "epoch": 2.453658536585366,
      "grad_norm": 0.22855715453624725,
      "learning_rate": 3.981853807420632e-06,
      "loss": 0.0385,
      "step": 10060
    },
    {
      "epoch": 2.4539024390243904,
      "grad_norm": 0.10844047367572784,
      "learning_rate": 3.978396793330408e-06,
      "loss": 0.0201,
      "step": 10061
    },
    {
      "epoch": 2.4541463414634146,
      "grad_norm": 0.1507299840450287,
      "learning_rate": 3.974941150835446e-06,
      "loss": 0.0181,
      "step": 10062
    },
    {
      "epoch": 2.454390243902439,
      "grad_norm": 0.09091316908597946,
      "learning_rate": 3.971486880161207e-06,
      "loss": 0.0172,
      "step": 10063
    },
    {
      "epoch": 2.4546341463414634,
      "grad_norm": 0.11600904166698456,
      "learning_rate": 3.968033981533079e-06,
      "loss": 0.0087,
      "step": 10064
    },
    {
      "epoch": 2.454878048780488,
      "grad_norm": 0.15839116275310516,
      "learning_rate": 3.9645824551763435e-06,
      "loss": 0.023,
      "step": 10065
    },
    {
      "epoch": 2.455121951219512,
      "grad_norm": 0.18285252153873444,
      "learning_rate": 3.961132301316206e-06,
      "loss": 0.0089,
      "step": 10066
    },
    {
      "epoch": 2.4553658536585368,
      "grad_norm": 0.1684780865907669,
      "learning_rate": 3.957683520177783e-06,
      "loss": 0.0184,
      "step": 10067
    },
    {
      "epoch": 2.455609756097561,
      "grad_norm": 0.07996722310781479,
      "learning_rate": 3.9542361119860925e-06,
      "loss": 0.0101,
      "step": 10068
    },
    {
      "epoch": 2.4558536585365855,
      "grad_norm": 0.20345142483711243,
      "learning_rate": 3.950790076966065e-06,
      "loss": 0.0272,
      "step": 10069
    },
    {
      "epoch": 2.4560975609756097,
      "grad_norm": 0.19692297279834747,
      "learning_rate": 3.9473454153425515e-06,
      "loss": 0.0254,
      "step": 10070
    },
    {
      "epoch": 2.4563414634146343,
      "grad_norm": 0.08326629549264908,
      "learning_rate": 3.943902127340304e-06,
      "loss": 0.02,
      "step": 10071
    },
    {
      "epoch": 2.4565853658536585,
      "grad_norm": 0.376309335231781,
      "learning_rate": 3.940460213183975e-06,
      "loss": 0.016,
      "step": 10072
    },
    {
      "epoch": 2.456829268292683,
      "grad_norm": 0.13938650488853455,
      "learning_rate": 3.937019673098155e-06,
      "loss": 0.0194,
      "step": 10073
    },
    {
      "epoch": 2.4570731707317073,
      "grad_norm": 0.08783095329999924,
      "learning_rate": 3.933580507307313e-06,
      "loss": 0.0142,
      "step": 10074
    },
    {
      "epoch": 2.457317073170732,
      "grad_norm": 0.1907922923564911,
      "learning_rate": 3.930142716035862e-06,
      "loss": 0.0134,
      "step": 10075
    },
    {
      "epoch": 2.457560975609756,
      "grad_norm": 0.2184002548456192,
      "learning_rate": 3.926706299508095e-06,
      "loss": 0.0287,
      "step": 10076
    },
    {
      "epoch": 2.4578048780487807,
      "grad_norm": 0.12472480535507202,
      "learning_rate": 3.9232712579482276e-06,
      "loss": 0.0176,
      "step": 10077
    },
    {
      "epoch": 2.458048780487805,
      "grad_norm": 0.1368609219789505,
      "learning_rate": 3.919837591580391e-06,
      "loss": 0.0158,
      "step": 10078
    },
    {
      "epoch": 2.4582926829268295,
      "grad_norm": 0.23223908245563507,
      "learning_rate": 3.91640530062862e-06,
      "loss": 0.0159,
      "step": 10079
    },
    {
      "epoch": 2.4585365853658536,
      "grad_norm": 0.11563356220722198,
      "learning_rate": 3.912974385316862e-06,
      "loss": 0.0292,
      "step": 10080
    },
    {
      "epoch": 2.4587804878048782,
      "grad_norm": 0.14628471434116364,
      "learning_rate": 3.909544845868971e-06,
      "loss": 0.0225,
      "step": 10081
    },
    {
      "epoch": 2.4590243902439024,
      "grad_norm": 0.19711902737617493,
      "learning_rate": 3.906116682508707e-06,
      "loss": 0.0329,
      "step": 10082
    },
    {
      "epoch": 2.459268292682927,
      "grad_norm": 0.19416598975658417,
      "learning_rate": 3.90268989545976e-06,
      "loss": 0.012,
      "step": 10083
    },
    {
      "epoch": 2.459512195121951,
      "grad_norm": 0.19500340521335602,
      "learning_rate": 3.899264484945717e-06,
      "loss": 0.0231,
      "step": 10084
    },
    {
      "epoch": 2.459756097560976,
      "grad_norm": 0.12095926702022552,
      "learning_rate": 3.895840451190061e-06,
      "loss": 0.0079,
      "step": 10085
    },
    {
      "epoch": 2.46,
      "grad_norm": 0.1305941939353943,
      "learning_rate": 3.892417794416217e-06,
      "loss": 0.015,
      "step": 10086
    },
    {
      "epoch": 2.4602439024390246,
      "grad_norm": 0.11334514617919922,
      "learning_rate": 3.888996514847487e-06,
      "loss": 0.0251,
      "step": 10087
    },
    {
      "epoch": 2.4604878048780487,
      "grad_norm": 0.17088410258293152,
      "learning_rate": 3.885576612707115e-06,
      "loss": 0.0118,
      "step": 10088
    },
    {
      "epoch": 2.4607317073170734,
      "grad_norm": 0.19517141580581665,
      "learning_rate": 3.882158088218232e-06,
      "loss": 0.0146,
      "step": 10089
    },
    {
      "epoch": 2.4609756097560975,
      "grad_norm": 0.13624638319015503,
      "learning_rate": 3.878740941603881e-06,
      "loss": 0.0125,
      "step": 10090
    },
    {
      "epoch": 2.461219512195122,
      "grad_norm": 0.11539983004331589,
      "learning_rate": 3.87532517308703e-06,
      "loss": 0.0157,
      "step": 10091
    },
    {
      "epoch": 2.4614634146341463,
      "grad_norm": 0.10387145727872849,
      "learning_rate": 3.871910782890542e-06,
      "loss": 0.0117,
      "step": 10092
    },
    {
      "epoch": 2.461707317073171,
      "grad_norm": 0.08170554041862488,
      "learning_rate": 3.8684977712371895e-06,
      "loss": 0.0143,
      "step": 10093
    },
    {
      "epoch": 2.461951219512195,
      "grad_norm": 0.23156408965587616,
      "learning_rate": 3.865086138349675e-06,
      "loss": 0.0195,
      "step": 10094
    },
    {
      "epoch": 2.4621951219512197,
      "grad_norm": 0.23335304856300354,
      "learning_rate": 3.861675884450586e-06,
      "loss": 0.021,
      "step": 10095
    },
    {
      "epoch": 2.462439024390244,
      "grad_norm": 0.1625477522611618,
      "learning_rate": 3.8582670097624364e-06,
      "loss": 0.0198,
      "step": 10096
    },
    {
      "epoch": 2.4626829268292685,
      "grad_norm": 0.12630946934223175,
      "learning_rate": 3.854859514507653e-06,
      "loss": 0.0168,
      "step": 10097
    },
    {
      "epoch": 2.4629268292682926,
      "grad_norm": 0.21546177566051483,
      "learning_rate": 3.851453398908539e-06,
      "loss": 0.0199,
      "step": 10098
    },
    {
      "epoch": 2.4631707317073173,
      "grad_norm": 0.1083466112613678,
      "learning_rate": 3.848048663187359e-06,
      "loss": 0.0116,
      "step": 10099
    },
    {
      "epoch": 2.4634146341463414,
      "grad_norm": 0.13176606595516205,
      "learning_rate": 3.844645307566241e-06,
      "loss": 0.0087,
      "step": 10100
    },
    {
      "epoch": 2.463658536585366,
      "grad_norm": 0.0692315474152565,
      "learning_rate": 3.841243332267263e-06,
      "loss": 0.0112,
      "step": 10101
    },
    {
      "epoch": 2.46390243902439,
      "grad_norm": 0.10564175993204117,
      "learning_rate": 3.837842737512384e-06,
      "loss": 0.019,
      "step": 10102
    },
    {
      "epoch": 2.464146341463415,
      "grad_norm": 0.10470244288444519,
      "learning_rate": 3.834443523523479e-06,
      "loss": 0.0323,
      "step": 10103
    },
    {
      "epoch": 2.464390243902439,
      "grad_norm": 0.1444357931613922,
      "learning_rate": 3.831045690522342e-06,
      "loss": 0.0172,
      "step": 10104
    },
    {
      "epoch": 2.4646341463414636,
      "grad_norm": 0.20961415767669678,
      "learning_rate": 3.8276492387306715e-06,
      "loss": 0.0117,
      "step": 10105
    },
    {
      "epoch": 2.4648780487804878,
      "grad_norm": 0.13743628561496735,
      "learning_rate": 3.8242541683700676e-06,
      "loss": 0.0213,
      "step": 10106
    },
    {
      "epoch": 2.4651219512195124,
      "grad_norm": 0.1703098714351654,
      "learning_rate": 3.820860479662058e-06,
      "loss": 0.0165,
      "step": 10107
    },
    {
      "epoch": 2.4653658536585366,
      "grad_norm": 0.2767958641052246,
      "learning_rate": 3.817468172828062e-06,
      "loss": 0.0101,
      "step": 10108
    },
    {
      "epoch": 2.465609756097561,
      "grad_norm": 0.09048303216695786,
      "learning_rate": 3.814077248089429e-06,
      "loss": 0.0154,
      "step": 10109
    },
    {
      "epoch": 2.4658536585365853,
      "grad_norm": 0.10902179032564163,
      "learning_rate": 3.8106877056673954e-06,
      "loss": 0.0225,
      "step": 10110
    },
    {
      "epoch": 2.46609756097561,
      "grad_norm": 0.08972853422164917,
      "learning_rate": 3.807299545783119e-06,
      "loss": 0.0208,
      "step": 10111
    },
    {
      "epoch": 2.466341463414634,
      "grad_norm": 0.19512896239757538,
      "learning_rate": 3.8039127686576777e-06,
      "loss": 0.0241,
      "step": 10112
    },
    {
      "epoch": 2.4665853658536587,
      "grad_norm": 0.08001074194908142,
      "learning_rate": 3.800527374512039e-06,
      "loss": 0.0159,
      "step": 10113
    },
    {
      "epoch": 2.466829268292683,
      "grad_norm": 0.1342872679233551,
      "learning_rate": 3.7971433635670906e-06,
      "loss": 0.0177,
      "step": 10114
    },
    {
      "epoch": 2.4670731707317075,
      "grad_norm": 0.11025547981262207,
      "learning_rate": 3.7937607360436305e-06,
      "loss": 0.0179,
      "step": 10115
    },
    {
      "epoch": 2.4673170731707317,
      "grad_norm": 0.05878074839711189,
      "learning_rate": 3.7903794921623575e-06,
      "loss": 0.0068,
      "step": 10116
    },
    {
      "epoch": 2.4675609756097563,
      "grad_norm": 0.09291153401136398,
      "learning_rate": 3.7869996321439006e-06,
      "loss": 0.0067,
      "step": 10117
    },
    {
      "epoch": 2.4678048780487805,
      "grad_norm": 0.1614554524421692,
      "learning_rate": 3.7836211562087777e-06,
      "loss": 0.0149,
      "step": 10118
    },
    {
      "epoch": 2.468048780487805,
      "grad_norm": 0.09451229125261307,
      "learning_rate": 3.7802440645774177e-06,
      "loss": 0.0218,
      "step": 10119
    },
    {
      "epoch": 2.4682926829268292,
      "grad_norm": 0.15370072424411774,
      "learning_rate": 3.7768683574701813e-06,
      "loss": 0.0229,
      "step": 10120
    },
    {
      "epoch": 2.468536585365854,
      "grad_norm": 0.1640002429485321,
      "learning_rate": 3.7734940351073055e-06,
      "loss": 0.0193,
      "step": 10121
    },
    {
      "epoch": 2.468780487804878,
      "grad_norm": 0.1462242752313614,
      "learning_rate": 3.7701210977089703e-06,
      "loss": 0.0145,
      "step": 10122
    },
    {
      "epoch": 2.469024390243902,
      "grad_norm": 0.11230634152889252,
      "learning_rate": 3.7667495454952436e-06,
      "loss": 0.0165,
      "step": 10123
    },
    {
      "epoch": 2.469268292682927,
      "grad_norm": 0.08239389955997467,
      "learning_rate": 3.763379378686102e-06,
      "loss": 0.0096,
      "step": 10124
    },
    {
      "epoch": 2.4695121951219514,
      "grad_norm": 0.07184062898159027,
      "learning_rate": 3.7600105975014484e-06,
      "loss": 0.0123,
      "step": 10125
    },
    {
      "epoch": 2.4697560975609756,
      "grad_norm": 0.20685037970542908,
      "learning_rate": 3.7566432021610836e-06,
      "loss": 0.0227,
      "step": 10126
    },
    {
      "epoch": 2.4699999999999998,
      "grad_norm": 0.14072947204113007,
      "learning_rate": 3.7532771928847106e-06,
      "loss": 0.0123,
      "step": 10127
    },
    {
      "epoch": 2.4702439024390244,
      "grad_norm": 0.11226748675107956,
      "learning_rate": 3.749912569891964e-06,
      "loss": 0.0176,
      "step": 10128
    },
    {
      "epoch": 2.470487804878049,
      "grad_norm": 0.18077178299427032,
      "learning_rate": 3.7465493334023688e-06,
      "loss": 0.0285,
      "step": 10129
    },
    {
      "epoch": 2.470731707317073,
      "grad_norm": 0.1619371771812439,
      "learning_rate": 3.7431874836353688e-06,
      "loss": 0.0224,
      "step": 10130
    },
    {
      "epoch": 2.4709756097560973,
      "grad_norm": 0.07365725934505463,
      "learning_rate": 3.73982702081031e-06,
      "loss": 0.0078,
      "step": 10131
    },
    {
      "epoch": 2.471219512195122,
      "grad_norm": 0.10716259479522705,
      "learning_rate": 3.7364679451464493e-06,
      "loss": 0.0104,
      "step": 10132
    },
    {
      "epoch": 2.4714634146341465,
      "grad_norm": 0.0821797251701355,
      "learning_rate": 3.7331102568629676e-06,
      "loss": 0.011,
      "step": 10133
    },
    {
      "epoch": 2.4717073170731707,
      "grad_norm": 0.10485716909170151,
      "learning_rate": 3.7297539561789303e-06,
      "loss": 0.0145,
      "step": 10134
    },
    {
      "epoch": 2.471951219512195,
      "grad_norm": 0.1045738160610199,
      "learning_rate": 3.7263990433133417e-06,
      "loss": 0.0148,
      "step": 10135
    },
    {
      "epoch": 2.4721951219512195,
      "grad_norm": 0.1109834685921669,
      "learning_rate": 3.723045518485088e-06,
      "loss": 0.0164,
      "step": 10136
    },
    {
      "epoch": 2.472439024390244,
      "grad_norm": 0.14038695394992828,
      "learning_rate": 3.7196933819129725e-06,
      "loss": 0.0275,
      "step": 10137
    },
    {
      "epoch": 2.4726829268292683,
      "grad_norm": 0.1669730544090271,
      "learning_rate": 3.716342633815728e-06,
      "loss": 0.0182,
      "step": 10138
    },
    {
      "epoch": 2.4729268292682924,
      "grad_norm": 0.09416718035936356,
      "learning_rate": 3.7129932744119687e-06,
      "loss": 0.0127,
      "step": 10139
    },
    {
      "epoch": 2.473170731707317,
      "grad_norm": 0.09971954673528671,
      "learning_rate": 3.709645303920228e-06,
      "loss": 0.0145,
      "step": 10140
    },
    {
      "epoch": 2.4734146341463417,
      "grad_norm": 0.07518988102674484,
      "learning_rate": 3.70629872255896e-06,
      "loss": 0.0076,
      "step": 10141
    },
    {
      "epoch": 2.473658536585366,
      "grad_norm": 0.1182224303483963,
      "learning_rate": 3.702953530546513e-06,
      "loss": 0.0249,
      "step": 10142
    },
    {
      "epoch": 2.47390243902439,
      "grad_norm": 0.1502625197172165,
      "learning_rate": 3.699609728101147e-06,
      "loss": 0.0113,
      "step": 10143
    },
    {
      "epoch": 2.4741463414634146,
      "grad_norm": 0.1848883181810379,
      "learning_rate": 3.696267315441046e-06,
      "loss": 0.0114,
      "step": 10144
    },
    {
      "epoch": 2.4743902439024392,
      "grad_norm": 0.1321888118982315,
      "learning_rate": 3.6929262927842796e-06,
      "loss": 0.0231,
      "step": 10145
    },
    {
      "epoch": 2.4746341463414634,
      "grad_norm": 0.08415055274963379,
      "learning_rate": 3.6895866603488566e-06,
      "loss": 0.0133,
      "step": 10146
    },
    {
      "epoch": 2.4748780487804876,
      "grad_norm": 0.1507902294397354,
      "learning_rate": 3.686248418352656e-06,
      "loss": 0.0138,
      "step": 10147
    },
    {
      "epoch": 2.475121951219512,
      "grad_norm": 0.12077578902244568,
      "learning_rate": 3.6829115670135026e-06,
      "loss": 0.0112,
      "step": 10148
    },
    {
      "epoch": 2.475365853658537,
      "grad_norm": 0.09762966632843018,
      "learning_rate": 3.6795761065491126e-06,
      "loss": 0.016,
      "step": 10149
    },
    {
      "epoch": 2.475609756097561,
      "grad_norm": 0.20229430496692657,
      "learning_rate": 3.676242037177105e-06,
      "loss": 0.0212,
      "step": 10150
    },
    {
      "epoch": 2.475853658536585,
      "grad_norm": 0.11416950076818466,
      "learning_rate": 3.6729093591150345e-06,
      "loss": 0.0115,
      "step": 10151
    },
    {
      "epoch": 2.4760975609756097,
      "grad_norm": 0.17861667275428772,
      "learning_rate": 3.6695780725803404e-06,
      "loss": 0.0123,
      "step": 10152
    },
    {
      "epoch": 2.4763414634146343,
      "grad_norm": 0.0910957008600235,
      "learning_rate": 3.6662481777903714e-06,
      "loss": 0.0099,
      "step": 10153
    },
    {
      "epoch": 2.4765853658536585,
      "grad_norm": 0.10521741956472397,
      "learning_rate": 3.662919674962406e-06,
      "loss": 0.0115,
      "step": 10154
    },
    {
      "epoch": 2.4768292682926827,
      "grad_norm": 0.13338255882263184,
      "learning_rate": 3.6595925643136074e-06,
      "loss": 0.0246,
      "step": 10155
    },
    {
      "epoch": 2.4770731707317073,
      "grad_norm": 0.06871969252824783,
      "learning_rate": 3.656266846061071e-06,
      "loss": 0.012,
      "step": 10156
    },
    {
      "epoch": 2.477317073170732,
      "grad_norm": 0.10813840478658676,
      "learning_rate": 3.6529425204217816e-06,
      "loss": 0.012,
      "step": 10157
    },
    {
      "epoch": 2.477560975609756,
      "grad_norm": 0.20300297439098358,
      "learning_rate": 3.6496195876126405e-06,
      "loss": 0.0334,
      "step": 10158
    },
    {
      "epoch": 2.4778048780487802,
      "grad_norm": 0.1520591527223587,
      "learning_rate": 3.6462980478504634e-06,
      "loss": 0.0193,
      "step": 10159
    },
    {
      "epoch": 2.478048780487805,
      "grad_norm": 0.11544211953878403,
      "learning_rate": 3.642977901351971e-06,
      "loss": 0.0175,
      "step": 10160
    },
    {
      "epoch": 2.4782926829268295,
      "grad_norm": 0.12966755032539368,
      "learning_rate": 3.639659148333785e-06,
      "loss": 0.0188,
      "step": 10161
    },
    {
      "epoch": 2.4785365853658536,
      "grad_norm": 0.1379113495349884,
      "learning_rate": 3.636341789012454e-06,
      "loss": 0.0196,
      "step": 10162
    },
    {
      "epoch": 2.478780487804878,
      "grad_norm": 0.11717867851257324,
      "learning_rate": 3.633025823604422e-06,
      "loss": 0.0122,
      "step": 10163
    },
    {
      "epoch": 2.4790243902439024,
      "grad_norm": 0.09543884545564651,
      "learning_rate": 3.6297112523260428e-06,
      "loss": 0.01,
      "step": 10164
    },
    {
      "epoch": 2.479268292682927,
      "grad_norm": 0.1963527947664261,
      "learning_rate": 3.626398075393583e-06,
      "loss": 0.0306,
      "step": 10165
    },
    {
      "epoch": 2.479512195121951,
      "grad_norm": 0.15019530057907104,
      "learning_rate": 3.6230862930232144e-06,
      "loss": 0.0117,
      "step": 10166
    },
    {
      "epoch": 2.4797560975609754,
      "grad_norm": 0.1484493613243103,
      "learning_rate": 3.6197759054310303e-06,
      "loss": 0.009,
      "step": 10167
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.14604489505290985,
      "learning_rate": 3.616466912833008e-06,
      "loss": 0.0194,
      "step": 10168
    },
    {
      "epoch": 2.4802439024390246,
      "grad_norm": 0.152896910905838,
      "learning_rate": 3.6131593154450666e-06,
      "loss": 0.0251,
      "step": 10169
    },
    {
      "epoch": 2.4804878048780488,
      "grad_norm": 0.16395138204097748,
      "learning_rate": 3.6098531134830083e-06,
      "loss": 0.0215,
      "step": 10170
    },
    {
      "epoch": 2.480731707317073,
      "grad_norm": 0.15064388513565063,
      "learning_rate": 3.6065483071625496e-06,
      "loss": 0.0103,
      "step": 10171
    },
    {
      "epoch": 2.4809756097560975,
      "grad_norm": 0.18676000833511353,
      "learning_rate": 3.603244896699323e-06,
      "loss": 0.0193,
      "step": 10172
    },
    {
      "epoch": 2.481219512195122,
      "grad_norm": 0.06852533668279648,
      "learning_rate": 3.5999428823088684e-06,
      "loss": 0.0082,
      "step": 10173
    },
    {
      "epoch": 2.4814634146341463,
      "grad_norm": 0.2089938372373581,
      "learning_rate": 3.596642264206623e-06,
      "loss": 0.0194,
      "step": 10174
    },
    {
      "epoch": 2.4817073170731705,
      "grad_norm": 0.19766263663768768,
      "learning_rate": 3.5933430426079524e-06,
      "loss": 0.013,
      "step": 10175
    },
    {
      "epoch": 2.481951219512195,
      "grad_norm": 0.12591154873371124,
      "learning_rate": 3.5900452177281162e-06,
      "loss": 0.0103,
      "step": 10176
    },
    {
      "epoch": 2.4821951219512197,
      "grad_norm": 0.2026737481355667,
      "learning_rate": 3.586748789782282e-06,
      "loss": 0.0186,
      "step": 10177
    },
    {
      "epoch": 2.482439024390244,
      "grad_norm": 0.07073180377483368,
      "learning_rate": 3.5834537589855443e-06,
      "loss": 0.0083,
      "step": 10178
    },
    {
      "epoch": 2.482682926829268,
      "grad_norm": 0.08802114427089691,
      "learning_rate": 3.580160125552884e-06,
      "loss": 0.0179,
      "step": 10179
    },
    {
      "epoch": 2.4829268292682927,
      "grad_norm": 0.15134546160697937,
      "learning_rate": 3.5768678896992036e-06,
      "loss": 0.0202,
      "step": 10180
    },
    {
      "epoch": 2.4831707317073173,
      "grad_norm": 0.1512424349784851,
      "learning_rate": 3.573577051639307e-06,
      "loss": 0.0195,
      "step": 10181
    },
    {
      "epoch": 2.4834146341463414,
      "grad_norm": 0.08294831961393356,
      "learning_rate": 3.570287611587919e-06,
      "loss": 0.0087,
      "step": 10182
    },
    {
      "epoch": 2.4836585365853656,
      "grad_norm": 0.22955738008022308,
      "learning_rate": 3.566999569759663e-06,
      "loss": 0.0307,
      "step": 10183
    },
    {
      "epoch": 2.4839024390243902,
      "grad_norm": 0.25474220514297485,
      "learning_rate": 3.5637129263690645e-06,
      "loss": 0.0243,
      "step": 10184
    },
    {
      "epoch": 2.484146341463415,
      "grad_norm": 0.12200109660625458,
      "learning_rate": 3.560427681630582e-06,
      "loss": 0.0155,
      "step": 10185
    },
    {
      "epoch": 2.484390243902439,
      "grad_norm": 0.07972302287817001,
      "learning_rate": 3.5571438357585613e-06,
      "loss": 0.0194,
      "step": 10186
    },
    {
      "epoch": 2.484634146341463,
      "grad_norm": 0.09111662954092026,
      "learning_rate": 3.553861388967256e-06,
      "loss": 0.0084,
      "step": 10187
    },
    {
      "epoch": 2.484878048780488,
      "grad_norm": 0.16446083784103394,
      "learning_rate": 3.5505803414708467e-06,
      "loss": 0.0118,
      "step": 10188
    },
    {
      "epoch": 2.4851219512195124,
      "grad_norm": 0.13245072960853577,
      "learning_rate": 3.5473006934834074e-06,
      "loss": 0.0204,
      "step": 10189
    },
    {
      "epoch": 2.4853658536585366,
      "grad_norm": 0.14264826476573944,
      "learning_rate": 3.5440224452189223e-06,
      "loss": 0.0243,
      "step": 10190
    },
    {
      "epoch": 2.4856097560975607,
      "grad_norm": 0.09982552379369736,
      "learning_rate": 3.540745596891293e-06,
      "loss": 0.018,
      "step": 10191
    },
    {
      "epoch": 2.4858536585365854,
      "grad_norm": 0.15168870985507965,
      "learning_rate": 3.537470148714314e-06,
      "loss": 0.0316,
      "step": 10192
    },
    {
      "epoch": 2.48609756097561,
      "grad_norm": 0.1578153669834137,
      "learning_rate": 3.53419610090171e-06,
      "loss": 0.0184,
      "step": 10193
    },
    {
      "epoch": 2.486341463414634,
      "grad_norm": 0.1379736363887787,
      "learning_rate": 3.5309234536670984e-06,
      "loss": 0.01,
      "step": 10194
    },
    {
      "epoch": 2.4865853658536583,
      "grad_norm": 0.20308394730091095,
      "learning_rate": 3.5276522072240026e-06,
      "loss": 0.0172,
      "step": 10195
    },
    {
      "epoch": 2.486829268292683,
      "grad_norm": 0.24300794303417206,
      "learning_rate": 3.5243823617858775e-06,
      "loss": 0.021,
      "step": 10196
    },
    {
      "epoch": 2.4870731707317075,
      "grad_norm": 0.16203323006629944,
      "learning_rate": 3.521113917566052e-06,
      "loss": 0.0129,
      "step": 10197
    },
    {
      "epoch": 2.4873170731707317,
      "grad_norm": 0.13016627728939056,
      "learning_rate": 3.5178468747777914e-06,
      "loss": 0.0194,
      "step": 10198
    },
    {
      "epoch": 2.487560975609756,
      "grad_norm": 0.13543573021888733,
      "learning_rate": 3.514581233634262e-06,
      "loss": 0.0251,
      "step": 10199
    },
    {
      "epoch": 2.4878048780487805,
      "grad_norm": 0.13927049934864044,
      "learning_rate": 3.5113169943485263e-06,
      "loss": 0.0163,
      "step": 10200
    },
    {
      "epoch": 2.488048780487805,
      "grad_norm": 0.16475602984428406,
      "learning_rate": 3.5080541571335804e-06,
      "loss": 0.0171,
      "step": 10201
    },
    {
      "epoch": 2.4882926829268293,
      "grad_norm": 0.12299033999443054,
      "learning_rate": 3.5047927222023074e-06,
      "loss": 0.0199,
      "step": 10202
    },
    {
      "epoch": 2.4885365853658534,
      "grad_norm": 0.13538970053195953,
      "learning_rate": 3.5015326897675005e-06,
      "loss": 0.0218,
      "step": 10203
    },
    {
      "epoch": 2.488780487804878,
      "grad_norm": 0.16091375052928925,
      "learning_rate": 3.4982740600418785e-06,
      "loss": 0.017,
      "step": 10204
    },
    {
      "epoch": 2.4890243902439027,
      "grad_norm": 0.1069410890340805,
      "learning_rate": 3.4950168332380463e-06,
      "loss": 0.0172,
      "step": 10205
    },
    {
      "epoch": 2.489268292682927,
      "grad_norm": 0.17154164612293243,
      "learning_rate": 3.4917610095685366e-06,
      "loss": 0.0209,
      "step": 10206
    },
    {
      "epoch": 2.489512195121951,
      "grad_norm": 0.17017675936222076,
      "learning_rate": 3.4885065892457795e-06,
      "loss": 0.0269,
      "step": 10207
    },
    {
      "epoch": 2.4897560975609756,
      "grad_norm": 0.07251153886318207,
      "learning_rate": 3.4852535724821058e-06,
      "loss": 0.0105,
      "step": 10208
    },
    {
      "epoch": 2.49,
      "grad_norm": 0.14889921247959137,
      "learning_rate": 3.4820019594897806e-06,
      "loss": 0.0203,
      "step": 10209
    },
    {
      "epoch": 2.4902439024390244,
      "grad_norm": 0.10928196460008621,
      "learning_rate": 3.4787517504809573e-06,
      "loss": 0.0073,
      "step": 10210
    },
    {
      "epoch": 2.4904878048780485,
      "grad_norm": 0.21847835183143616,
      "learning_rate": 3.4755029456676915e-06,
      "loss": 0.0221,
      "step": 10211
    },
    {
      "epoch": 2.490731707317073,
      "grad_norm": 0.07331695407629013,
      "learning_rate": 3.4722555452619765e-06,
      "loss": 0.0089,
      "step": 10212
    },
    {
      "epoch": 2.4909756097560978,
      "grad_norm": 0.0907425582408905,
      "learning_rate": 3.469009549475674e-06,
      "loss": 0.0151,
      "step": 10213
    },
    {
      "epoch": 2.491219512195122,
      "grad_norm": 0.14121413230895996,
      "learning_rate": 3.46576495852059e-06,
      "loss": 0.0141,
      "step": 10214
    },
    {
      "epoch": 2.491463414634146,
      "grad_norm": 0.07814306020736694,
      "learning_rate": 3.462521772608421e-06,
      "loss": 0.0132,
      "step": 10215
    },
    {
      "epoch": 2.4917073170731707,
      "grad_norm": 0.12321574240922928,
      "learning_rate": 3.4592799919507674e-06,
      "loss": 0.0174,
      "step": 10216
    },
    {
      "epoch": 2.4919512195121953,
      "grad_norm": 0.2729295790195465,
      "learning_rate": 3.4560396167591602e-06,
      "loss": 0.0191,
      "step": 10217
    },
    {
      "epoch": 2.4921951219512195,
      "grad_norm": 0.14902092516422272,
      "learning_rate": 3.4528006472450082e-06,
      "loss": 0.0214,
      "step": 10218
    },
    {
      "epoch": 2.4924390243902437,
      "grad_norm": 0.1566072255373001,
      "learning_rate": 3.449563083619656e-06,
      "loss": 0.0234,
      "step": 10219
    },
    {
      "epoch": 2.4926829268292683,
      "grad_norm": 0.11727485805749893,
      "learning_rate": 3.4463269260943425e-06,
      "loss": 0.0084,
      "step": 10220
    },
    {
      "epoch": 2.492926829268293,
      "grad_norm": 0.19919690489768982,
      "learning_rate": 3.443092174880208e-06,
      "loss": 0.0099,
      "step": 10221
    },
    {
      "epoch": 2.493170731707317,
      "grad_norm": 0.09585991501808167,
      "learning_rate": 3.4398588301883224e-06,
      "loss": 0.0188,
      "step": 10222
    },
    {
      "epoch": 2.4934146341463412,
      "grad_norm": 0.0957581102848053,
      "learning_rate": 3.4366268922296467e-06,
      "loss": 0.0141,
      "step": 10223
    },
    {
      "epoch": 2.493658536585366,
      "grad_norm": 0.16824397444725037,
      "learning_rate": 3.4333963612150492e-06,
      "loss": 0.0282,
      "step": 10224
    },
    {
      "epoch": 2.4939024390243905,
      "grad_norm": 0.16267570853233337,
      "learning_rate": 3.430167237355322e-06,
      "loss": 0.0109,
      "step": 10225
    },
    {
      "epoch": 2.4941463414634146,
      "grad_norm": 0.24140477180480957,
      "learning_rate": 3.426939520861147e-06,
      "loss": 0.0268,
      "step": 10226
    },
    {
      "epoch": 2.494390243902439,
      "grad_norm": 0.13504354655742645,
      "learning_rate": 3.42371321194313e-06,
      "loss": 0.0099,
      "step": 10227
    },
    {
      "epoch": 2.4946341463414634,
      "grad_norm": 0.14277204871177673,
      "learning_rate": 3.420488310811776e-06,
      "loss": 0.0191,
      "step": 10228
    },
    {
      "epoch": 2.494878048780488,
      "grad_norm": 0.31324678659439087,
      "learning_rate": 3.4172648176774985e-06,
      "loss": 0.02,
      "step": 10229
    },
    {
      "epoch": 2.495121951219512,
      "grad_norm": 0.11154308915138245,
      "learning_rate": 3.414042732750622e-06,
      "loss": 0.0145,
      "step": 10230
    },
    {
      "epoch": 2.4953658536585364,
      "grad_norm": 0.15048082172870636,
      "learning_rate": 3.4108220562413696e-06,
      "loss": 0.0137,
      "step": 10231
    },
    {
      "epoch": 2.495609756097561,
      "grad_norm": 0.10153036564588547,
      "learning_rate": 3.4076027883598926e-06,
      "loss": 0.0144,
      "step": 10232
    },
    {
      "epoch": 2.4958536585365856,
      "grad_norm": 0.058828771114349365,
      "learning_rate": 3.4043849293162315e-06,
      "loss": 0.0026,
      "step": 10233
    },
    {
      "epoch": 2.4960975609756098,
      "grad_norm": 0.12024980783462524,
      "learning_rate": 3.401168479320338e-06,
      "loss": 0.0145,
      "step": 10234
    },
    {
      "epoch": 2.496341463414634,
      "grad_norm": 0.09146818518638611,
      "learning_rate": 3.3979534385820884e-06,
      "loss": 0.0122,
      "step": 10235
    },
    {
      "epoch": 2.4965853658536585,
      "grad_norm": 0.1968749761581421,
      "learning_rate": 3.394739807311245e-06,
      "loss": 0.0315,
      "step": 10236
    },
    {
      "epoch": 2.496829268292683,
      "grad_norm": 0.09433631598949432,
      "learning_rate": 3.3915275857174857e-06,
      "loss": 0.0204,
      "step": 10237
    },
    {
      "epoch": 2.4970731707317073,
      "grad_norm": 0.13270989060401917,
      "learning_rate": 3.3883167740104057e-06,
      "loss": 0.017,
      "step": 10238
    },
    {
      "epoch": 2.4973170731707315,
      "grad_norm": 0.19307705760002136,
      "learning_rate": 3.3851073723994904e-06,
      "loss": 0.0215,
      "step": 10239
    },
    {
      "epoch": 2.497560975609756,
      "grad_norm": 0.12195048481225967,
      "learning_rate": 3.3818993810941557e-06,
      "loss": 0.0122,
      "step": 10240
    },
    {
      "epoch": 2.4978048780487807,
      "grad_norm": 0.17705199122428894,
      "learning_rate": 3.3786928003037067e-06,
      "loss": 0.0208,
      "step": 10241
    },
    {
      "epoch": 2.498048780487805,
      "grad_norm": 0.10564982891082764,
      "learning_rate": 3.375487630237359e-06,
      "loss": 0.0084,
      "step": 10242
    },
    {
      "epoch": 2.498292682926829,
      "grad_norm": 0.1558304876089096,
      "learning_rate": 3.372283871104248e-06,
      "loss": 0.0244,
      "step": 10243
    },
    {
      "epoch": 2.4985365853658537,
      "grad_norm": 0.15574952960014343,
      "learning_rate": 3.369081523113407e-06,
      "loss": 0.0182,
      "step": 10244
    },
    {
      "epoch": 2.4987804878048783,
      "grad_norm": 0.11032979190349579,
      "learning_rate": 3.3658805864737768e-06,
      "loss": 0.0244,
      "step": 10245
    },
    {
      "epoch": 2.4990243902439024,
      "grad_norm": 0.164432093501091,
      "learning_rate": 3.3626810613942102e-06,
      "loss": 0.016,
      "step": 10246
    },
    {
      "epoch": 2.4992682926829266,
      "grad_norm": 0.1816033571958542,
      "learning_rate": 3.3594829480834617e-06,
      "loss": 0.0207,
      "step": 10247
    },
    {
      "epoch": 2.499512195121951,
      "grad_norm": 0.16259300708770752,
      "learning_rate": 3.3562862467502093e-06,
      "loss": 0.0223,
      "step": 10248
    },
    {
      "epoch": 2.499756097560976,
      "grad_norm": 0.11084098368883133,
      "learning_rate": 3.3530909576030195e-06,
      "loss": 0.0123,
      "step": 10249
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.12287413328886032,
      "learning_rate": 3.3498970808503727e-06,
      "loss": 0.0257,
      "step": 10250
    },
    {
      "epoch": 2.500243902439024,
      "grad_norm": 0.21631048619747162,
      "learning_rate": 3.346704616700669e-06,
      "loss": 0.0158,
      "step": 10251
    },
    {
      "epoch": 2.500487804878049,
      "grad_norm": 0.15278390049934387,
      "learning_rate": 3.343513565362197e-06,
      "loss": 0.0228,
      "step": 10252
    },
    {
      "epoch": 2.5007317073170734,
      "grad_norm": 0.09695722907781601,
      "learning_rate": 3.340323927043171e-06,
      "loss": 0.0174,
      "step": 10253
    },
    {
      "epoch": 2.5009756097560976,
      "grad_norm": 0.052013806998729706,
      "learning_rate": 3.3371357019517046e-06,
      "loss": 0.0082,
      "step": 10254
    },
    {
      "epoch": 2.5012195121951217,
      "grad_norm": 0.12551309168338776,
      "learning_rate": 3.3339488902958128e-06,
      "loss": 0.0177,
      "step": 10255
    },
    {
      "epoch": 2.5014634146341463,
      "grad_norm": 0.10844358056783676,
      "learning_rate": 3.3307634922834342e-06,
      "loss": 0.014,
      "step": 10256
    },
    {
      "epoch": 2.501707317073171,
      "grad_norm": 0.11853166669607162,
      "learning_rate": 3.3275795081224e-06,
      "loss": 0.0153,
      "step": 10257
    },
    {
      "epoch": 2.501951219512195,
      "grad_norm": 0.129853755235672,
      "learning_rate": 3.324396938020452e-06,
      "loss": 0.0187,
      "step": 10258
    },
    {
      "epoch": 2.5021951219512193,
      "grad_norm": 0.09287609159946442,
      "learning_rate": 3.3212157821852554e-06,
      "loss": 0.0119,
      "step": 10259
    },
    {
      "epoch": 2.502439024390244,
      "grad_norm": 0.12360671907663345,
      "learning_rate": 3.3180360408243656e-06,
      "loss": 0.0093,
      "step": 10260
    },
    {
      "epoch": 2.5026829268292685,
      "grad_norm": 0.1705251783132553,
      "learning_rate": 3.314857714145242e-06,
      "loss": 0.0149,
      "step": 10261
    },
    {
      "epoch": 2.5029268292682927,
      "grad_norm": 0.06259289383888245,
      "learning_rate": 3.311680802355277e-06,
      "loss": 0.0038,
      "step": 10262
    },
    {
      "epoch": 2.503170731707317,
      "grad_norm": 0.2296586036682129,
      "learning_rate": 3.3085053056617354e-06,
      "loss": 0.0173,
      "step": 10263
    },
    {
      "epoch": 2.5034146341463415,
      "grad_norm": 0.1401311159133911,
      "learning_rate": 3.3053312242718265e-06,
      "loss": 0.0215,
      "step": 10264
    },
    {
      "epoch": 2.503658536585366,
      "grad_norm": 0.07499796152114868,
      "learning_rate": 3.3021585583926346e-06,
      "loss": 0.0058,
      "step": 10265
    },
    {
      "epoch": 2.5039024390243902,
      "grad_norm": 0.10208689421415329,
      "learning_rate": 3.298987308231177e-06,
      "loss": 0.0195,
      "step": 10266
    },
    {
      "epoch": 2.5041463414634144,
      "grad_norm": 0.13609234988689423,
      "learning_rate": 3.295817473994367e-06,
      "loss": 0.0163,
      "step": 10267
    },
    {
      "epoch": 2.504390243902439,
      "grad_norm": 0.13931319117546082,
      "learning_rate": 3.292649055889016e-06,
      "loss": 0.0202,
      "step": 10268
    },
    {
      "epoch": 2.5046341463414636,
      "grad_norm": 0.08005010336637497,
      "learning_rate": 3.2894820541218672e-06,
      "loss": 0.0179,
      "step": 10269
    },
    {
      "epoch": 2.504878048780488,
      "grad_norm": 0.3773420751094818,
      "learning_rate": 3.2863164688995556e-06,
      "loss": 0.0303,
      "step": 10270
    },
    {
      "epoch": 2.505121951219512,
      "grad_norm": 0.048047035932540894,
      "learning_rate": 3.283152300428613e-06,
      "loss": 0.0075,
      "step": 10271
    },
    {
      "epoch": 2.5053658536585366,
      "grad_norm": 0.1478041708469391,
      "learning_rate": 3.2799895489155077e-06,
      "loss": 0.0175,
      "step": 10272
    },
    {
      "epoch": 2.505609756097561,
      "grad_norm": 0.2649945020675659,
      "learning_rate": 3.2768282145665937e-06,
      "loss": 0.0284,
      "step": 10273
    },
    {
      "epoch": 2.5058536585365854,
      "grad_norm": 0.1808529645204544,
      "learning_rate": 3.273668297588131e-06,
      "loss": 0.0182,
      "step": 10274
    },
    {
      "epoch": 2.5060975609756095,
      "grad_norm": 0.21425879001617432,
      "learning_rate": 3.270509798186308e-06,
      "loss": 0.0247,
      "step": 10275
    },
    {
      "epoch": 2.506341463414634,
      "grad_norm": 0.24795597791671753,
      "learning_rate": 3.267352716567196e-06,
      "loss": 0.0252,
      "step": 10276
    },
    {
      "epoch": 2.5065853658536588,
      "grad_norm": 0.1612626612186432,
      "learning_rate": 3.2641970529367934e-06,
      "loss": 0.0286,
      "step": 10277
    },
    {
      "epoch": 2.506829268292683,
      "grad_norm": 0.15163728594779968,
      "learning_rate": 3.2610428075009913e-06,
      "loss": 0.023,
      "step": 10278
    },
    {
      "epoch": 2.507073170731707,
      "grad_norm": 0.1972469836473465,
      "learning_rate": 3.2578899804656003e-06,
      "loss": 0.0162,
      "step": 10279
    },
    {
      "epoch": 2.5073170731707317,
      "grad_norm": 0.09498593956232071,
      "learning_rate": 3.2547385720363255e-06,
      "loss": 0.0149,
      "step": 10280
    },
    {
      "epoch": 2.5075609756097563,
      "grad_norm": 0.22636207938194275,
      "learning_rate": 3.251588582418788e-06,
      "loss": 0.0401,
      "step": 10281
    },
    {
      "epoch": 2.5078048780487805,
      "grad_norm": 0.13752102851867676,
      "learning_rate": 3.248440011818521e-06,
      "loss": 0.02,
      "step": 10282
    },
    {
      "epoch": 2.5080487804878047,
      "grad_norm": 0.16785192489624023,
      "learning_rate": 3.2452928604409544e-06,
      "loss": 0.0232,
      "step": 10283
    },
    {
      "epoch": 2.5082926829268293,
      "grad_norm": 0.07931802421808243,
      "learning_rate": 3.2421471284914266e-06,
      "loss": 0.0102,
      "step": 10284
    },
    {
      "epoch": 2.508536585365854,
      "grad_norm": 0.05191538855433464,
      "learning_rate": 3.239002816175199e-06,
      "loss": 0.0046,
      "step": 10285
    },
    {
      "epoch": 2.508780487804878,
      "grad_norm": 0.135962575674057,
      "learning_rate": 3.2358599236974124e-06,
      "loss": 0.0142,
      "step": 10286
    },
    {
      "epoch": 2.5090243902439022,
      "grad_norm": 0.25683653354644775,
      "learning_rate": 3.2327184512631474e-06,
      "loss": 0.0106,
      "step": 10287
    },
    {
      "epoch": 2.509268292682927,
      "grad_norm": 0.10651180148124695,
      "learning_rate": 3.2295783990773655e-06,
      "loss": 0.028,
      "step": 10288
    },
    {
      "epoch": 2.5095121951219515,
      "grad_norm": 0.11615236103534698,
      "learning_rate": 3.2264397673449407e-06,
      "loss": 0.0192,
      "step": 10289
    },
    {
      "epoch": 2.5097560975609756,
      "grad_norm": 0.10780786722898483,
      "learning_rate": 3.223302556270674e-06,
      "loss": 0.0203,
      "step": 10290
    },
    {
      "epoch": 2.51,
      "grad_norm": 0.08121860772371292,
      "learning_rate": 3.2201667660592517e-06,
      "loss": 0.0055,
      "step": 10291
    },
    {
      "epoch": 2.5102439024390244,
      "grad_norm": 0.08443505316972733,
      "learning_rate": 3.217032396915265e-06,
      "loss": 0.0159,
      "step": 10292
    },
    {
      "epoch": 2.510487804878049,
      "grad_norm": 0.10922771692276001,
      "learning_rate": 3.2138994490432367e-06,
      "loss": 0.0097,
      "step": 10293
    },
    {
      "epoch": 2.510731707317073,
      "grad_norm": 0.06482509523630142,
      "learning_rate": 3.2107679226475755e-06,
      "loss": 0.0057,
      "step": 10294
    },
    {
      "epoch": 2.5109756097560973,
      "grad_norm": 0.14061091840267181,
      "learning_rate": 3.2076378179326043e-06,
      "loss": 0.0265,
      "step": 10295
    },
    {
      "epoch": 2.511219512195122,
      "grad_norm": 0.21358166635036469,
      "learning_rate": 3.2045091351025513e-06,
      "loss": 0.011,
      "step": 10296
    },
    {
      "epoch": 2.5114634146341466,
      "grad_norm": 0.16503103077411652,
      "learning_rate": 3.201381874361553e-06,
      "loss": 0.0322,
      "step": 10297
    },
    {
      "epoch": 2.5117073170731707,
      "grad_norm": 0.12825988233089447,
      "learning_rate": 3.198256035913658e-06,
      "loss": 0.0115,
      "step": 10298
    },
    {
      "epoch": 2.511951219512195,
      "grad_norm": 0.09934809803962708,
      "learning_rate": 3.1951316199628102e-06,
      "loss": 0.0177,
      "step": 10299
    },
    {
      "epoch": 2.5121951219512195,
      "grad_norm": 0.1383051872253418,
      "learning_rate": 3.192008626712878e-06,
      "loss": 0.0119,
      "step": 10300
    },
    {
      "epoch": 2.512439024390244,
      "grad_norm": 0.08481772989034653,
      "learning_rate": 3.188887056367623e-06,
      "loss": 0.0162,
      "step": 10301
    },
    {
      "epoch": 2.5126829268292683,
      "grad_norm": 0.07995320856571198,
      "learning_rate": 3.1857669091307135e-06,
      "loss": 0.0145,
      "step": 10302
    },
    {
      "epoch": 2.5129268292682925,
      "grad_norm": 0.15988542139530182,
      "learning_rate": 3.182648185205736e-06,
      "loss": 0.0275,
      "step": 10303
    },
    {
      "epoch": 2.513170731707317,
      "grad_norm": 0.18769559264183044,
      "learning_rate": 3.1795308847961754e-06,
      "loss": 0.0206,
      "step": 10304
    },
    {
      "epoch": 2.5134146341463417,
      "grad_norm": 0.057396769523620605,
      "learning_rate": 3.1764150081054213e-06,
      "loss": 0.0169,
      "step": 10305
    },
    {
      "epoch": 2.513658536585366,
      "grad_norm": 0.028514621779322624,
      "learning_rate": 3.173300555336786e-06,
      "loss": 0.0044,
      "step": 10306
    },
    {
      "epoch": 2.51390243902439,
      "grad_norm": 0.07823823392391205,
      "learning_rate": 3.170187526693469e-06,
      "loss": 0.0113,
      "step": 10307
    },
    {
      "epoch": 2.5141463414634146,
      "grad_norm": 0.08394713699817657,
      "learning_rate": 3.1670759223785846e-06,
      "loss": 0.0163,
      "step": 10308
    },
    {
      "epoch": 2.5143902439024393,
      "grad_norm": 0.1108119934797287,
      "learning_rate": 3.1639657425951653e-06,
      "loss": 0.0187,
      "step": 10309
    },
    {
      "epoch": 2.5146341463414634,
      "grad_norm": 0.15356680750846863,
      "learning_rate": 3.160856987546126e-06,
      "loss": 0.0142,
      "step": 10310
    },
    {
      "epoch": 2.5148780487804876,
      "grad_norm": 0.06947991251945496,
      "learning_rate": 3.157749657434328e-06,
      "loss": 0.0066,
      "step": 10311
    },
    {
      "epoch": 2.515121951219512,
      "grad_norm": 0.10247388482093811,
      "learning_rate": 3.1546437524624856e-06,
      "loss": 0.0131,
      "step": 10312
    },
    {
      "epoch": 2.515365853658537,
      "grad_norm": 0.08735178411006927,
      "learning_rate": 3.1515392728332678e-06,
      "loss": 0.0164,
      "step": 10313
    },
    {
      "epoch": 2.515609756097561,
      "grad_norm": 0.15483900904655457,
      "learning_rate": 3.1484362187492287e-06,
      "loss": 0.0216,
      "step": 10314
    },
    {
      "epoch": 2.515853658536585,
      "grad_norm": 0.0730922669172287,
      "learning_rate": 3.1453345904128296e-06,
      "loss": 0.0061,
      "step": 10315
    },
    {
      "epoch": 2.5160975609756098,
      "grad_norm": 0.10624600946903229,
      "learning_rate": 3.1422343880264465e-06,
      "loss": 0.0233,
      "step": 10316
    },
    {
      "epoch": 2.5163414634146344,
      "grad_norm": 0.18913379311561584,
      "learning_rate": 3.1391356117923598e-06,
      "loss": 0.0353,
      "step": 10317
    },
    {
      "epoch": 2.5165853658536586,
      "grad_norm": 0.15291942656040192,
      "learning_rate": 3.1360382619127435e-06,
      "loss": 0.0181,
      "step": 10318
    },
    {
      "epoch": 2.5168292682926827,
      "grad_norm": 0.18671999871730804,
      "learning_rate": 3.132942338589706e-06,
      "loss": 0.0112,
      "step": 10319
    },
    {
      "epoch": 2.5170731707317073,
      "grad_norm": 0.0818074494600296,
      "learning_rate": 3.129847842025238e-06,
      "loss": 0.0138,
      "step": 10320
    },
    {
      "epoch": 2.517317073170732,
      "grad_norm": 0.12479253113269806,
      "learning_rate": 3.126754772421242e-06,
      "loss": 0.0149,
      "step": 10321
    },
    {
      "epoch": 2.517560975609756,
      "grad_norm": 0.10593384504318237,
      "learning_rate": 3.1236631299795426e-06,
      "loss": 0.0258,
      "step": 10322
    },
    {
      "epoch": 2.5178048780487803,
      "grad_norm": 0.0961214080452919,
      "learning_rate": 3.120572914901851e-06,
      "loss": 0.0157,
      "step": 10323
    },
    {
      "epoch": 2.518048780487805,
      "grad_norm": 0.09975341707468033,
      "learning_rate": 3.1174841273898e-06,
      "loss": 0.0173,
      "step": 10324
    },
    {
      "epoch": 2.5182926829268295,
      "grad_norm": 0.23369595408439636,
      "learning_rate": 3.11439676764492e-06,
      "loss": 0.0179,
      "step": 10325
    },
    {
      "epoch": 2.5185365853658537,
      "grad_norm": 0.13804082572460175,
      "learning_rate": 3.11131083586865e-06,
      "loss": 0.0176,
      "step": 10326
    },
    {
      "epoch": 2.518780487804878,
      "grad_norm": 0.07606983929872513,
      "learning_rate": 3.108226332262343e-06,
      "loss": 0.0126,
      "step": 10327
    },
    {
      "epoch": 2.5190243902439025,
      "grad_norm": 0.10472489893436432,
      "learning_rate": 3.1051432570272544e-06,
      "loss": 0.0124,
      "step": 10328
    },
    {
      "epoch": 2.519268292682927,
      "grad_norm": 0.14233431220054626,
      "learning_rate": 3.1020616103645373e-06,
      "loss": 0.0122,
      "step": 10329
    },
    {
      "epoch": 2.5195121951219512,
      "grad_norm": 0.06433185935020447,
      "learning_rate": 3.098981392475267e-06,
      "loss": 0.0079,
      "step": 10330
    },
    {
      "epoch": 2.5197560975609754,
      "grad_norm": 0.14674724638462067,
      "learning_rate": 3.0959026035604077e-06,
      "loss": 0.0213,
      "step": 10331
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.18439315259456635,
      "learning_rate": 3.092825243820857e-06,
      "loss": 0.0266,
      "step": 10332
    },
    {
      "epoch": 2.5202439024390246,
      "grad_norm": 0.14399221539497375,
      "learning_rate": 3.0897493134573934e-06,
      "loss": 0.0077,
      "step": 10333
    },
    {
      "epoch": 2.520487804878049,
      "grad_norm": 0.11872132867574692,
      "learning_rate": 3.0866748126707062e-06,
      "loss": 0.0121,
      "step": 10334
    },
    {
      "epoch": 2.520731707317073,
      "grad_norm": 0.18000099062919617,
      "learning_rate": 3.0836017416614127e-06,
      "loss": 0.0234,
      "step": 10335
    },
    {
      "epoch": 2.5209756097560976,
      "grad_norm": 0.1363433450460434,
      "learning_rate": 3.0805301006300052e-06,
      "loss": 0.0112,
      "step": 10336
    },
    {
      "epoch": 2.521219512195122,
      "grad_norm": 0.2303905338048935,
      "learning_rate": 3.0774598897769154e-06,
      "loss": 0.0174,
      "step": 10337
    },
    {
      "epoch": 2.5214634146341464,
      "grad_norm": 0.15039637684822083,
      "learning_rate": 3.0743911093024547e-06,
      "loss": 0.0376,
      "step": 10338
    },
    {
      "epoch": 2.5217073170731705,
      "grad_norm": 0.21811442077159882,
      "learning_rate": 3.071323759406847e-06,
      "loss": 0.0196,
      "step": 10339
    },
    {
      "epoch": 2.521951219512195,
      "grad_norm": 0.10564033687114716,
      "learning_rate": 3.0682578402902395e-06,
      "loss": 0.0202,
      "step": 10340
    },
    {
      "epoch": 2.5221951219512198,
      "grad_norm": 0.08395904302597046,
      "learning_rate": 3.065193352152668e-06,
      "loss": 0.0093,
      "step": 10341
    },
    {
      "epoch": 2.522439024390244,
      "grad_norm": 0.059459950774908066,
      "learning_rate": 3.0621302951940766e-06,
      "loss": 0.0093,
      "step": 10342
    },
    {
      "epoch": 2.522682926829268,
      "grad_norm": 0.138885036110878,
      "learning_rate": 3.0590686696143316e-06,
      "loss": 0.0166,
      "step": 10343
    },
    {
      "epoch": 2.5229268292682927,
      "grad_norm": 0.21765074133872986,
      "learning_rate": 3.0560084756131866e-06,
      "loss": 0.0218,
      "step": 10344
    },
    {
      "epoch": 2.5231707317073173,
      "grad_norm": 0.16160723567008972,
      "learning_rate": 3.052949713390313e-06,
      "loss": 0.0158,
      "step": 10345
    },
    {
      "epoch": 2.5234146341463415,
      "grad_norm": 0.1218077689409256,
      "learning_rate": 3.0498923831452837e-06,
      "loss": 0.0114,
      "step": 10346
    },
    {
      "epoch": 2.5236585365853657,
      "grad_norm": 0.11606530845165253,
      "learning_rate": 3.0468364850775754e-06,
      "loss": 0.0212,
      "step": 10347
    },
    {
      "epoch": 2.5239024390243903,
      "grad_norm": 0.11731304973363876,
      "learning_rate": 3.043782019386587e-06,
      "loss": 0.0089,
      "step": 10348
    },
    {
      "epoch": 2.524146341463415,
      "grad_norm": 0.07362207770347595,
      "learning_rate": 3.0407289862716014e-06,
      "loss": 0.0216,
      "step": 10349
    },
    {
      "epoch": 2.524390243902439,
      "grad_norm": 0.11711039394140244,
      "learning_rate": 3.037677385931831e-06,
      "loss": 0.0156,
      "step": 10350
    },
    {
      "epoch": 2.524634146341463,
      "grad_norm": 0.08926346153020859,
      "learning_rate": 3.034627218566377e-06,
      "loss": 0.0195,
      "step": 10351
    },
    {
      "epoch": 2.524878048780488,
      "grad_norm": 0.11477036774158478,
      "learning_rate": 3.0315784843742507e-06,
      "loss": 0.0161,
      "step": 10352
    },
    {
      "epoch": 2.5251219512195124,
      "grad_norm": 0.11502981930971146,
      "learning_rate": 3.028531183554381e-06,
      "loss": 0.0159,
      "step": 10353
    },
    {
      "epoch": 2.5253658536585366,
      "grad_norm": 0.15092350542545319,
      "learning_rate": 3.025485316305593e-06,
      "loss": 0.036,
      "step": 10354
    },
    {
      "epoch": 2.5256097560975608,
      "grad_norm": 0.2860429286956787,
      "learning_rate": 3.0224408828266105e-06,
      "loss": 0.0327,
      "step": 10355
    },
    {
      "epoch": 2.5258536585365854,
      "grad_norm": 0.11667805165052414,
      "learning_rate": 3.019397883316086e-06,
      "loss": 0.0201,
      "step": 10356
    },
    {
      "epoch": 2.52609756097561,
      "grad_norm": 0.13599686324596405,
      "learning_rate": 3.0163563179725546e-06,
      "loss": 0.016,
      "step": 10357
    },
    {
      "epoch": 2.526341463414634,
      "grad_norm": 0.09337174147367477,
      "learning_rate": 3.0133161869944803e-06,
      "loss": 0.0114,
      "step": 10358
    },
    {
      "epoch": 2.5265853658536583,
      "grad_norm": 0.14066238701343536,
      "learning_rate": 3.0102774905802185e-06,
      "loss": 0.0135,
      "step": 10359
    },
    {
      "epoch": 2.526829268292683,
      "grad_norm": 0.15024864673614502,
      "learning_rate": 3.0072402289280267e-06,
      "loss": 0.031,
      "step": 10360
    },
    {
      "epoch": 2.5270731707317076,
      "grad_norm": 0.13476577401161194,
      "learning_rate": 3.004204402236094e-06,
      "loss": 0.0209,
      "step": 10361
    },
    {
      "epoch": 2.5273170731707317,
      "grad_norm": 0.16485998034477234,
      "learning_rate": 3.0011700107024783e-06,
      "loss": 0.0163,
      "step": 10362
    },
    {
      "epoch": 2.527560975609756,
      "grad_norm": 0.12732338905334473,
      "learning_rate": 2.9981370545251825e-06,
      "loss": 0.0182,
      "step": 10363
    },
    {
      "epoch": 2.5278048780487805,
      "grad_norm": 0.0988272950053215,
      "learning_rate": 2.995105533902087e-06,
      "loss": 0.0183,
      "step": 10364
    },
    {
      "epoch": 2.528048780487805,
      "grad_norm": 0.06866473704576492,
      "learning_rate": 2.9920754490309873e-06,
      "loss": 0.0152,
      "step": 10365
    },
    {
      "epoch": 2.5282926829268293,
      "grad_norm": 0.10083571821451187,
      "learning_rate": 2.9890468001095964e-06,
      "loss": 0.0163,
      "step": 10366
    },
    {
      "epoch": 2.5285365853658535,
      "grad_norm": 0.13260503113269806,
      "learning_rate": 2.9860195873355202e-06,
      "loss": 0.0135,
      "step": 10367
    },
    {
      "epoch": 2.528780487804878,
      "grad_norm": 0.10557864606380463,
      "learning_rate": 2.9829938109062705e-06,
      "loss": 0.0096,
      "step": 10368
    },
    {
      "epoch": 2.5290243902439027,
      "grad_norm": 0.18193547427654266,
      "learning_rate": 2.9799694710192782e-06,
      "loss": 0.0169,
      "step": 10369
    },
    {
      "epoch": 2.529268292682927,
      "grad_norm": 0.084202341735363,
      "learning_rate": 2.976946567871863e-06,
      "loss": 0.0127,
      "step": 10370
    },
    {
      "epoch": 2.529512195121951,
      "grad_norm": 0.22439570724964142,
      "learning_rate": 2.9739251016612705e-06,
      "loss": 0.0208,
      "step": 10371
    },
    {
      "epoch": 2.5297560975609756,
      "grad_norm": 0.18387269973754883,
      "learning_rate": 2.970905072584637e-06,
      "loss": 0.0199,
      "step": 10372
    },
    {
      "epoch": 2.5300000000000002,
      "grad_norm": 0.12612445652484894,
      "learning_rate": 2.967886480839005e-06,
      "loss": 0.0156,
      "step": 10373
    },
    {
      "epoch": 2.5302439024390244,
      "grad_norm": 0.20596574246883392,
      "learning_rate": 2.964869326621336e-06,
      "loss": 0.02,
      "step": 10374
    },
    {
      "epoch": 2.5304878048780486,
      "grad_norm": 0.10354969650506973,
      "learning_rate": 2.96185361012849e-06,
      "loss": 0.0164,
      "step": 10375
    },
    {
      "epoch": 2.530731707317073,
      "grad_norm": 0.11433680355548859,
      "learning_rate": 2.9588393315572253e-06,
      "loss": 0.0189,
      "step": 10376
    },
    {
      "epoch": 2.530975609756098,
      "grad_norm": 0.14031925797462463,
      "learning_rate": 2.9558264911042297e-06,
      "loss": 0.0287,
      "step": 10377
    },
    {
      "epoch": 2.531219512195122,
      "grad_norm": 0.16191059350967407,
      "learning_rate": 2.9528150889660627e-06,
      "loss": 0.0365,
      "step": 10378
    },
    {
      "epoch": 2.531463414634146,
      "grad_norm": 0.20674729347229004,
      "learning_rate": 2.9498051253392223e-06,
      "loss": 0.0369,
      "step": 10379
    },
    {
      "epoch": 2.5317073170731708,
      "grad_norm": 0.120185486972332,
      "learning_rate": 2.946796600420096e-06,
      "loss": 0.0134,
      "step": 10380
    },
    {
      "epoch": 2.5319512195121954,
      "grad_norm": 0.35073673725128174,
      "learning_rate": 2.9437895144049736e-06,
      "loss": 0.0275,
      "step": 10381
    },
    {
      "epoch": 2.5321951219512195,
      "grad_norm": 0.104063980281353,
      "learning_rate": 2.940783867490071e-06,
      "loss": 0.0148,
      "step": 10382
    },
    {
      "epoch": 2.5324390243902437,
      "grad_norm": 0.2831583321094513,
      "learning_rate": 2.937779659871487e-06,
      "loss": 0.0126,
      "step": 10383
    },
    {
      "epoch": 2.5326829268292683,
      "grad_norm": 0.26059016585350037,
      "learning_rate": 2.9347768917452445e-06,
      "loss": 0.0171,
      "step": 10384
    },
    {
      "epoch": 2.532926829268293,
      "grad_norm": 0.08108828961849213,
      "learning_rate": 2.9317755633072625e-06,
      "loss": 0.0103,
      "step": 10385
    },
    {
      "epoch": 2.533170731707317,
      "grad_norm": 0.15813253819942474,
      "learning_rate": 2.9287756747533645e-06,
      "loss": 0.0216,
      "step": 10386
    },
    {
      "epoch": 2.5334146341463413,
      "grad_norm": 0.1665821075439453,
      "learning_rate": 2.9257772262792886e-06,
      "loss": 0.0112,
      "step": 10387
    },
    {
      "epoch": 2.533658536585366,
      "grad_norm": 0.1188959851861,
      "learning_rate": 2.922780218080676e-06,
      "loss": 0.018,
      "step": 10388
    },
    {
      "epoch": 2.5339024390243905,
      "grad_norm": 0.08078915625810623,
      "learning_rate": 2.9197846503530637e-06,
      "loss": 0.0131,
      "step": 10389
    },
    {
      "epoch": 2.5341463414634147,
      "grad_norm": 0.15074557065963745,
      "learning_rate": 2.9167905232919123e-06,
      "loss": 0.0324,
      "step": 10390
    },
    {
      "epoch": 2.534390243902439,
      "grad_norm": 0.1387212723493576,
      "learning_rate": 2.9137978370925774e-06,
      "loss": 0.0316,
      "step": 10391
    },
    {
      "epoch": 2.5346341463414634,
      "grad_norm": 0.1592807024717331,
      "learning_rate": 2.910806591950316e-06,
      "loss": 0.0253,
      "step": 10392
    },
    {
      "epoch": 2.534878048780488,
      "grad_norm": 0.19332483410835266,
      "learning_rate": 2.907816788060308e-06,
      "loss": 0.0252,
      "step": 10393
    },
    {
      "epoch": 2.5351219512195122,
      "grad_norm": 0.11396186798810959,
      "learning_rate": 2.9048284256176257e-06,
      "loss": 0.0139,
      "step": 10394
    },
    {
      "epoch": 2.5353658536585364,
      "grad_norm": 0.106586754322052,
      "learning_rate": 2.901841504817246e-06,
      "loss": 0.0142,
      "step": 10395
    },
    {
      "epoch": 2.535609756097561,
      "grad_norm": 0.3113737404346466,
      "learning_rate": 2.8988560258540548e-06,
      "loss": 0.0424,
      "step": 10396
    },
    {
      "epoch": 2.5358536585365856,
      "grad_norm": 0.09727353602647781,
      "learning_rate": 2.8958719889228573e-06,
      "loss": 0.0169,
      "step": 10397
    },
    {
      "epoch": 2.53609756097561,
      "grad_norm": 0.18714265525341034,
      "learning_rate": 2.892889394218343e-06,
      "loss": 0.0092,
      "step": 10398
    },
    {
      "epoch": 2.536341463414634,
      "grad_norm": 0.1899406760931015,
      "learning_rate": 2.889908241935113e-06,
      "loss": 0.0157,
      "step": 10399
    },
    {
      "epoch": 2.5365853658536586,
      "grad_norm": 0.12866757810115814,
      "learning_rate": 2.8869285322676938e-06,
      "loss": 0.0251,
      "step": 10400
    },
    {
      "epoch": 2.536829268292683,
      "grad_norm": 0.16297659277915955,
      "learning_rate": 2.88395026541049e-06,
      "loss": 0.0216,
      "step": 10401
    },
    {
      "epoch": 2.5370731707317074,
      "grad_norm": 0.0915951207280159,
      "learning_rate": 2.8809734415578243e-06,
      "loss": 0.0043,
      "step": 10402
    },
    {
      "epoch": 2.5373170731707315,
      "grad_norm": 0.11491668224334717,
      "learning_rate": 2.877998060903933e-06,
      "loss": 0.0369,
      "step": 10403
    },
    {
      "epoch": 2.537560975609756,
      "grad_norm": 0.09714169800281525,
      "learning_rate": 2.875024123642944e-06,
      "loss": 0.0115,
      "step": 10404
    },
    {
      "epoch": 2.5378048780487807,
      "grad_norm": 0.07682869583368301,
      "learning_rate": 2.872051629968897e-06,
      "loss": 0.0069,
      "step": 10405
    },
    {
      "epoch": 2.538048780487805,
      "grad_norm": 0.3191080093383789,
      "learning_rate": 2.8690805800757477e-06,
      "loss": 0.0364,
      "step": 10406
    },
    {
      "epoch": 2.538292682926829,
      "grad_norm": 0.23505839705467224,
      "learning_rate": 2.866110974157335e-06,
      "loss": 0.0226,
      "step": 10407
    },
    {
      "epoch": 2.5385365853658537,
      "grad_norm": 0.08318280428647995,
      "learning_rate": 2.8631428124074267e-06,
      "loss": 0.0092,
      "step": 10408
    },
    {
      "epoch": 2.5387804878048783,
      "grad_norm": 0.14924760162830353,
      "learning_rate": 2.860176095019684e-06,
      "loss": 0.0132,
      "step": 10409
    },
    {
      "epoch": 2.5390243902439025,
      "grad_norm": 0.11506108939647675,
      "learning_rate": 2.8572108221876724e-06,
      "loss": 0.0115,
      "step": 10410
    },
    {
      "epoch": 2.5392682926829266,
      "grad_norm": 0.15982690453529358,
      "learning_rate": 2.8542469941048726e-06,
      "loss": 0.0242,
      "step": 10411
    },
    {
      "epoch": 2.5395121951219513,
      "grad_norm": 0.12603621184825897,
      "learning_rate": 2.8512846109646553e-06,
      "loss": 0.0273,
      "step": 10412
    },
    {
      "epoch": 2.539756097560976,
      "grad_norm": 0.12124107778072357,
      "learning_rate": 2.848323672960318e-06,
      "loss": 0.0117,
      "step": 10413
    },
    {
      "epoch": 2.54,
      "grad_norm": 0.24400955438613892,
      "learning_rate": 2.8453641802850456e-06,
      "loss": 0.0172,
      "step": 10414
    },
    {
      "epoch": 2.540243902439024,
      "grad_norm": 0.17018866539001465,
      "learning_rate": 2.842406133131936e-06,
      "loss": 0.0178,
      "step": 10415
    },
    {
      "epoch": 2.540487804878049,
      "grad_norm": 0.15892952680587769,
      "learning_rate": 2.839449531694002e-06,
      "loss": 0.0128,
      "step": 10416
    },
    {
      "epoch": 2.5407317073170734,
      "grad_norm": 0.08401172608137131,
      "learning_rate": 2.8364943761641395e-06,
      "loss": 0.0218,
      "step": 10417
    },
    {
      "epoch": 2.5409756097560976,
      "grad_norm": 0.12905588746070862,
      "learning_rate": 2.8335406667351738e-06,
      "loss": 0.0241,
      "step": 10418
    },
    {
      "epoch": 2.5412195121951218,
      "grad_norm": 0.15969718992710114,
      "learning_rate": 2.830588403599824e-06,
      "loss": 0.0222,
      "step": 10419
    },
    {
      "epoch": 2.5414634146341464,
      "grad_norm": 0.08996903151273727,
      "learning_rate": 2.8276375869507073e-06,
      "loss": 0.0162,
      "step": 10420
    },
    {
      "epoch": 2.541707317073171,
      "grad_norm": 0.34088143706321716,
      "learning_rate": 2.8246882169803672e-06,
      "loss": 0.0293,
      "step": 10421
    },
    {
      "epoch": 2.541951219512195,
      "grad_norm": 0.13838231563568115,
      "learning_rate": 2.8217402938812365e-06,
      "loss": 0.0304,
      "step": 10422
    },
    {
      "epoch": 2.5421951219512193,
      "grad_norm": 0.15610580146312714,
      "learning_rate": 2.8187938178456526e-06,
      "loss": 0.0119,
      "step": 10423
    },
    {
      "epoch": 2.542439024390244,
      "grad_norm": 0.12212002277374268,
      "learning_rate": 2.8158487890658725e-06,
      "loss": 0.0191,
      "step": 10424
    },
    {
      "epoch": 2.5426829268292686,
      "grad_norm": 0.2266262024641037,
      "learning_rate": 2.8129052077340457e-06,
      "loss": 0.034,
      "step": 10425
    },
    {
      "epoch": 2.5429268292682927,
      "grad_norm": 0.1739640235900879,
      "learning_rate": 2.8099630740422295e-06,
      "loss": 0.0174,
      "step": 10426
    },
    {
      "epoch": 2.543170731707317,
      "grad_norm": 0.11802756786346436,
      "learning_rate": 2.8070223881824038e-06,
      "loss": 0.0229,
      "step": 10427
    },
    {
      "epoch": 2.5434146341463415,
      "grad_norm": 0.13416263461112976,
      "learning_rate": 2.8040831503464175e-06,
      "loss": 0.0159,
      "step": 10428
    },
    {
      "epoch": 2.543658536585366,
      "grad_norm": 0.07737738639116287,
      "learning_rate": 2.8011453607260597e-06,
      "loss": 0.0122,
      "step": 10429
    },
    {
      "epoch": 2.5439024390243903,
      "grad_norm": 0.09102972596883774,
      "learning_rate": 2.7982090195130067e-06,
      "loss": 0.0106,
      "step": 10430
    },
    {
      "epoch": 2.5441463414634145,
      "grad_norm": 0.10495486110448837,
      "learning_rate": 2.795274126898856e-06,
      "loss": 0.0143,
      "step": 10431
    },
    {
      "epoch": 2.544390243902439,
      "grad_norm": 0.2370884269475937,
      "learning_rate": 2.7923406830750897e-06,
      "loss": 0.0227,
      "step": 10432
    },
    {
      "epoch": 2.5446341463414637,
      "grad_norm": 0.13789807260036469,
      "learning_rate": 2.7894086882331082e-06,
      "loss": 0.0106,
      "step": 10433
    },
    {
      "epoch": 2.544878048780488,
      "grad_norm": 0.15955890715122223,
      "learning_rate": 2.7864781425642193e-06,
      "loss": 0.0351,
      "step": 10434
    },
    {
      "epoch": 2.545121951219512,
      "grad_norm": 0.13263343274593353,
      "learning_rate": 2.7835490462596287e-06,
      "loss": 0.016,
      "step": 10435
    },
    {
      "epoch": 2.5453658536585366,
      "grad_norm": 0.1142156571149826,
      "learning_rate": 2.7806213995104495e-06,
      "loss": 0.0155,
      "step": 10436
    },
    {
      "epoch": 2.5456097560975612,
      "grad_norm": 0.09448184818029404,
      "learning_rate": 2.7776952025077076e-06,
      "loss": 0.0131,
      "step": 10437
    },
    {
      "epoch": 2.5458536585365854,
      "grad_norm": 0.14506402611732483,
      "learning_rate": 2.774770455442324e-06,
      "loss": 0.0162,
      "step": 10438
    },
    {
      "epoch": 2.5460975609756096,
      "grad_norm": 0.12955930829048157,
      "learning_rate": 2.771847158505125e-06,
      "loss": 0.0166,
      "step": 10439
    },
    {
      "epoch": 2.546341463414634,
      "grad_norm": 0.10916002094745636,
      "learning_rate": 2.768925311886858e-06,
      "loss": 0.0128,
      "step": 10440
    },
    {
      "epoch": 2.546585365853659,
      "grad_norm": 0.09177117794752121,
      "learning_rate": 2.766004915778153e-06,
      "loss": 0.0124,
      "step": 10441
    },
    {
      "epoch": 2.546829268292683,
      "grad_norm": 0.11830755323171616,
      "learning_rate": 2.7630859703695694e-06,
      "loss": 0.0178,
      "step": 10442
    },
    {
      "epoch": 2.547073170731707,
      "grad_norm": 0.09182824194431305,
      "learning_rate": 2.760168475851552e-06,
      "loss": 0.0078,
      "step": 10443
    },
    {
      "epoch": 2.5473170731707317,
      "grad_norm": 0.0938636064529419,
      "learning_rate": 2.7572524324144563e-06,
      "loss": 0.0114,
      "step": 10444
    },
    {
      "epoch": 2.5475609756097564,
      "grad_norm": 0.10830103605985641,
      "learning_rate": 2.7543378402485496e-06,
      "loss": 0.0159,
      "step": 10445
    },
    {
      "epoch": 2.5478048780487805,
      "grad_norm": 0.2558155357837677,
      "learning_rate": 2.751424699543992e-06,
      "loss": 0.0179,
      "step": 10446
    },
    {
      "epoch": 2.5480487804878047,
      "grad_norm": 0.14127245545387268,
      "learning_rate": 2.748513010490869e-06,
      "loss": 0.0286,
      "step": 10447
    },
    {
      "epoch": 2.5482926829268293,
      "grad_norm": 0.11208833754062653,
      "learning_rate": 2.7456027732791543e-06,
      "loss": 0.0239,
      "step": 10448
    },
    {
      "epoch": 2.548536585365854,
      "grad_norm": 0.1127142384648323,
      "learning_rate": 2.742693988098727e-06,
      "loss": 0.0244,
      "step": 10449
    },
    {
      "epoch": 2.548780487804878,
      "grad_norm": 0.21697565913200378,
      "learning_rate": 2.739786655139387e-06,
      "loss": 0.0134,
      "step": 10450
    },
    {
      "epoch": 2.5490243902439023,
      "grad_norm": 0.20605425536632538,
      "learning_rate": 2.7368807745908194e-06,
      "loss": 0.0246,
      "step": 10451
    },
    {
      "epoch": 2.549268292682927,
      "grad_norm": 0.18384189903736115,
      "learning_rate": 2.733976346642625e-06,
      "loss": 0.0297,
      "step": 10452
    },
    {
      "epoch": 2.5495121951219515,
      "grad_norm": 0.20154087245464325,
      "learning_rate": 2.7310733714843136e-06,
      "loss": 0.0166,
      "step": 10453
    },
    {
      "epoch": 2.5497560975609757,
      "grad_norm": 0.07832223176956177,
      "learning_rate": 2.7281718493052887e-06,
      "loss": 0.0091,
      "step": 10454
    },
    {
      "epoch": 2.55,
      "grad_norm": 0.16513589024543762,
      "learning_rate": 2.7252717802948747e-06,
      "loss": 0.0235,
      "step": 10455
    },
    {
      "epoch": 2.5502439024390244,
      "grad_norm": 0.1688009798526764,
      "learning_rate": 2.722373164642289e-06,
      "loss": 0.0228,
      "step": 10456
    },
    {
      "epoch": 2.550487804878049,
      "grad_norm": 0.17938856780529022,
      "learning_rate": 2.71947600253665e-06,
      "loss": 0.0195,
      "step": 10457
    },
    {
      "epoch": 2.550731707317073,
      "grad_norm": 0.16326001286506653,
      "learning_rate": 2.716580294167001e-06,
      "loss": 0.0203,
      "step": 10458
    },
    {
      "epoch": 2.5509756097560974,
      "grad_norm": 0.16640475392341614,
      "learning_rate": 2.713686039722271e-06,
      "loss": 0.0233,
      "step": 10459
    },
    {
      "epoch": 2.551219512195122,
      "grad_norm": 0.15382522344589233,
      "learning_rate": 2.7107932393913018e-06,
      "loss": 0.0143,
      "step": 10460
    },
    {
      "epoch": 2.5514634146341466,
      "grad_norm": 0.1672552227973938,
      "learning_rate": 2.707901893362838e-06,
      "loss": 0.0263,
      "step": 10461
    },
    {
      "epoch": 2.5517073170731708,
      "grad_norm": 0.14541900157928467,
      "learning_rate": 2.70501200182553e-06,
      "loss": 0.014,
      "step": 10462
    },
    {
      "epoch": 2.551951219512195,
      "grad_norm": 0.15502889454364777,
      "learning_rate": 2.7021235649679425e-06,
      "loss": 0.0156,
      "step": 10463
    },
    {
      "epoch": 2.5521951219512196,
      "grad_norm": 0.06995672732591629,
      "learning_rate": 2.6992365829785317e-06,
      "loss": 0.0239,
      "step": 10464
    },
    {
      "epoch": 2.552439024390244,
      "grad_norm": 0.0975191593170166,
      "learning_rate": 2.6963510560456594e-06,
      "loss": 0.0224,
      "step": 10465
    },
    {
      "epoch": 2.5526829268292683,
      "grad_norm": 0.17975813150405884,
      "learning_rate": 2.6934669843576095e-06,
      "loss": 0.0248,
      "step": 10466
    },
    {
      "epoch": 2.5529268292682925,
      "grad_norm": 0.17229695618152618,
      "learning_rate": 2.6905843681025445e-06,
      "loss": 0.0126,
      "step": 10467
    },
    {
      "epoch": 2.553170731707317,
      "grad_norm": 0.07887686789035797,
      "learning_rate": 2.6877032074685587e-06,
      "loss": 0.0142,
      "step": 10468
    },
    {
      "epoch": 2.5534146341463417,
      "grad_norm": 0.14589838683605194,
      "learning_rate": 2.6848235026436354e-06,
      "loss": 0.0134,
      "step": 10469
    },
    {
      "epoch": 2.553658536585366,
      "grad_norm": 0.3508952856063843,
      "learning_rate": 2.68194525381566e-06,
      "loss": 0.028,
      "step": 10470
    },
    {
      "epoch": 2.55390243902439,
      "grad_norm": 0.12426798790693283,
      "learning_rate": 2.6790684611724404e-06,
      "loss": 0.0141,
      "step": 10471
    },
    {
      "epoch": 2.5541463414634147,
      "grad_norm": 0.14267399907112122,
      "learning_rate": 2.6761931249016716e-06,
      "loss": 0.014,
      "step": 10472
    },
    {
      "epoch": 2.5543902439024393,
      "grad_norm": 0.14383898675441742,
      "learning_rate": 2.673319245190958e-06,
      "loss": 0.0168,
      "step": 10473
    },
    {
      "epoch": 2.5546341463414635,
      "grad_norm": 0.11124058812856674,
      "learning_rate": 2.6704468222278177e-06,
      "loss": 0.0271,
      "step": 10474
    },
    {
      "epoch": 2.5548780487804876,
      "grad_norm": 0.13349729776382446,
      "learning_rate": 2.667575856199661e-06,
      "loss": 0.0131,
      "step": 10475
    },
    {
      "epoch": 2.5551219512195122,
      "grad_norm": 0.13690537214279175,
      "learning_rate": 2.6647063472938242e-06,
      "loss": 0.0127,
      "step": 10476
    },
    {
      "epoch": 2.555365853658537,
      "grad_norm": 0.19039985537528992,
      "learning_rate": 2.661838295697519e-06,
      "loss": 0.0195,
      "step": 10477
    },
    {
      "epoch": 2.555609756097561,
      "grad_norm": 0.12031804025173187,
      "learning_rate": 2.658971701597876e-06,
      "loss": 0.0275,
      "step": 10478
    },
    {
      "epoch": 2.555853658536585,
      "grad_norm": 0.09294018149375916,
      "learning_rate": 2.656106565181943e-06,
      "loss": 0.0107,
      "step": 10479
    },
    {
      "epoch": 2.55609756097561,
      "grad_norm": 0.17641720175743103,
      "learning_rate": 2.6532428866366475e-06,
      "loss": 0.0167,
      "step": 10480
    },
    {
      "epoch": 2.5563414634146344,
      "grad_norm": 0.09968503564596176,
      "learning_rate": 2.6503806661488496e-06,
      "loss": 0.0185,
      "step": 10481
    },
    {
      "epoch": 2.5565853658536586,
      "grad_norm": 0.0516374334692955,
      "learning_rate": 2.647519903905296e-06,
      "loss": 0.0083,
      "step": 10482
    },
    {
      "epoch": 2.5568292682926828,
      "grad_norm": 0.12940813601016998,
      "learning_rate": 2.6446606000926354e-06,
      "loss": 0.0164,
      "step": 10483
    },
    {
      "epoch": 2.5570731707317074,
      "grad_norm": 0.06405122578144073,
      "learning_rate": 2.6418027548974376e-06,
      "loss": 0.0079,
      "step": 10484
    },
    {
      "epoch": 2.557317073170732,
      "grad_norm": 0.16840092837810516,
      "learning_rate": 2.6389463685061677e-06,
      "loss": 0.0269,
      "step": 10485
    },
    {
      "epoch": 2.557560975609756,
      "grad_norm": 0.11512330919504166,
      "learning_rate": 2.6360914411051873e-06,
      "loss": 0.0168,
      "step": 10486
    },
    {
      "epoch": 2.5578048780487803,
      "grad_norm": 0.10103752464056015,
      "learning_rate": 2.633237972880781e-06,
      "loss": 0.012,
      "step": 10487
    },
    {
      "epoch": 2.558048780487805,
      "grad_norm": 0.1447771191596985,
      "learning_rate": 2.630385964019125e-06,
      "loss": 0.0178,
      "step": 10488
    },
    {
      "epoch": 2.5582926829268295,
      "grad_norm": 0.06499601155519485,
      "learning_rate": 2.627535414706306e-06,
      "loss": 0.0074,
      "step": 10489
    },
    {
      "epoch": 2.5585365853658537,
      "grad_norm": 0.13793329894542694,
      "learning_rate": 2.6246863251283144e-06,
      "loss": 0.0105,
      "step": 10490
    },
    {
      "epoch": 2.558780487804878,
      "grad_norm": 0.08643406629562378,
      "learning_rate": 2.621838695471038e-06,
      "loss": 0.025,
      "step": 10491
    },
    {
      "epoch": 2.5590243902439025,
      "grad_norm": 0.19113828241825104,
      "learning_rate": 2.6189925259202858e-06,
      "loss": 0.0122,
      "step": 10492
    },
    {
      "epoch": 2.559268292682927,
      "grad_norm": 0.07252553850412369,
      "learning_rate": 2.6161478166617543e-06,
      "loss": 0.0153,
      "step": 10493
    },
    {
      "epoch": 2.5595121951219513,
      "grad_norm": 0.08205100893974304,
      "learning_rate": 2.613304567881056e-06,
      "loss": 0.0081,
      "step": 10494
    },
    {
      "epoch": 2.5597560975609754,
      "grad_norm": 0.11726367473602295,
      "learning_rate": 2.6104627797637006e-06,
      "loss": 0.0187,
      "step": 10495
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.1072065457701683,
      "learning_rate": 2.607622452495104e-06,
      "loss": 0.0091,
      "step": 10496
    },
    {
      "epoch": 2.5602439024390247,
      "grad_norm": 0.1480395495891571,
      "learning_rate": 2.6047835862605977e-06,
      "loss": 0.0365,
      "step": 10497
    },
    {
      "epoch": 2.560487804878049,
      "grad_norm": 0.11656150221824646,
      "learning_rate": 2.6019461812454042e-06,
      "loss": 0.0245,
      "step": 10498
    },
    {
      "epoch": 2.560731707317073,
      "grad_norm": 0.23720678687095642,
      "learning_rate": 2.5991102376346526e-06,
      "loss": 0.0255,
      "step": 10499
    },
    {
      "epoch": 2.5609756097560976,
      "grad_norm": 0.12402624636888504,
      "learning_rate": 2.596275755613384e-06,
      "loss": 0.0086,
      "step": 10500
    },
    {
      "epoch": 2.5612195121951222,
      "grad_norm": 0.18879543244838715,
      "learning_rate": 2.593442735366533e-06,
      "loss": 0.0178,
      "step": 10501
    },
    {
      "epoch": 2.5614634146341464,
      "grad_norm": 0.13048593699932098,
      "learning_rate": 2.5906111770789583e-06,
      "loss": 0.0203,
      "step": 10502
    },
    {
      "epoch": 2.5617073170731706,
      "grad_norm": 0.2829071283340454,
      "learning_rate": 2.5877810809354e-06,
      "loss": 0.0248,
      "step": 10503
    },
    {
      "epoch": 2.561951219512195,
      "grad_norm": 0.10564325749874115,
      "learning_rate": 2.584952447120512e-06,
      "loss": 0.0131,
      "step": 10504
    },
    {
      "epoch": 2.56219512195122,
      "grad_norm": 0.1431513875722885,
      "learning_rate": 2.582125275818861e-06,
      "loss": 0.0287,
      "step": 10505
    },
    {
      "epoch": 2.562439024390244,
      "grad_norm": 0.10647779703140259,
      "learning_rate": 2.57929956721491e-06,
      "loss": 0.0183,
      "step": 10506
    },
    {
      "epoch": 2.562682926829268,
      "grad_norm": 0.08216048032045364,
      "learning_rate": 2.5764753214930214e-06,
      "loss": 0.0169,
      "step": 10507
    },
    {
      "epoch": 2.5629268292682927,
      "grad_norm": 0.13714636862277985,
      "learning_rate": 2.573652538837476e-06,
      "loss": 0.0276,
      "step": 10508
    },
    {
      "epoch": 2.563170731707317,
      "grad_norm": 0.06908730417490005,
      "learning_rate": 2.5708312194324513e-06,
      "loss": 0.0113,
      "step": 10509
    },
    {
      "epoch": 2.5634146341463415,
      "grad_norm": 0.20145508646965027,
      "learning_rate": 2.568011363462025e-06,
      "loss": 0.0232,
      "step": 10510
    },
    {
      "epoch": 2.5636585365853657,
      "grad_norm": 0.25680872797966003,
      "learning_rate": 2.5651929711101863e-06,
      "loss": 0.0274,
      "step": 10511
    },
    {
      "epoch": 2.5639024390243903,
      "grad_norm": 0.1345607489347458,
      "learning_rate": 2.562376042560824e-06,
      "loss": 0.0135,
      "step": 10512
    },
    {
      "epoch": 2.5641463414634145,
      "grad_norm": 0.12175319343805313,
      "learning_rate": 2.5595605779977384e-06,
      "loss": 0.0122,
      "step": 10513
    },
    {
      "epoch": 2.564390243902439,
      "grad_norm": 0.15958140790462494,
      "learning_rate": 2.556746577604627e-06,
      "loss": 0.0225,
      "step": 10514
    },
    {
      "epoch": 2.5646341463414632,
      "grad_norm": 0.18409642577171326,
      "learning_rate": 2.553934041565098e-06,
      "loss": 0.0074,
      "step": 10515
    },
    {
      "epoch": 2.564878048780488,
      "grad_norm": 0.15634183585643768,
      "learning_rate": 2.551122970062661e-06,
      "loss": 0.0111,
      "step": 10516
    },
    {
      "epoch": 2.565121951219512,
      "grad_norm": 0.16443881392478943,
      "learning_rate": 2.5483133632807217e-06,
      "loss": 0.017,
      "step": 10517
    },
    {
      "epoch": 2.5653658536585366,
      "grad_norm": 0.14139609038829803,
      "learning_rate": 2.545505221402608e-06,
      "loss": 0.0194,
      "step": 10518
    },
    {
      "epoch": 2.565609756097561,
      "grad_norm": 0.10156366229057312,
      "learning_rate": 2.542698544611541e-06,
      "loss": 0.016,
      "step": 10519
    },
    {
      "epoch": 2.5658536585365854,
      "grad_norm": 0.09934528917074203,
      "learning_rate": 2.539893333090637e-06,
      "loss": 0.0229,
      "step": 10520
    },
    {
      "epoch": 2.5660975609756096,
      "grad_norm": 0.12100426852703094,
      "learning_rate": 2.537089587022945e-06,
      "loss": 0.0185,
      "step": 10521
    },
    {
      "epoch": 2.566341463414634,
      "grad_norm": 0.17600873112678528,
      "learning_rate": 2.5342873065913903e-06,
      "loss": 0.0229,
      "step": 10522
    },
    {
      "epoch": 2.5665853658536584,
      "grad_norm": 0.15814071893692017,
      "learning_rate": 2.53148649197881e-06,
      "loss": 0.019,
      "step": 10523
    },
    {
      "epoch": 2.566829268292683,
      "grad_norm": 0.13710466027259827,
      "learning_rate": 2.5286871433679584e-06,
      "loss": 0.0182,
      "step": 10524
    },
    {
      "epoch": 2.567073170731707,
      "grad_norm": 0.24360115826129913,
      "learning_rate": 2.525889260941475e-06,
      "loss": 0.0336,
      "step": 10525
    },
    {
      "epoch": 2.5673170731707318,
      "grad_norm": 0.09693101793527603,
      "learning_rate": 2.5230928448819247e-06,
      "loss": 0.0124,
      "step": 10526
    },
    {
      "epoch": 2.567560975609756,
      "grad_norm": 0.1529303640127182,
      "learning_rate": 2.520297895371751e-06,
      "loss": 0.0214,
      "step": 10527
    },
    {
      "epoch": 2.5678048780487805,
      "grad_norm": 0.14658723771572113,
      "learning_rate": 2.5175044125933245e-06,
      "loss": 0.0124,
      "step": 10528
    },
    {
      "epoch": 2.5680487804878047,
      "grad_norm": 0.11398344486951828,
      "learning_rate": 2.5147123967289133e-06,
      "loss": 0.0152,
      "step": 10529
    },
    {
      "epoch": 2.5682926829268293,
      "grad_norm": 0.07153785228729248,
      "learning_rate": 2.511921847960677e-06,
      "loss": 0.0138,
      "step": 10530
    },
    {
      "epoch": 2.5685365853658535,
      "grad_norm": 0.1336940973997116,
      "learning_rate": 2.5091327664707008e-06,
      "loss": 0.0214,
      "step": 10531
    },
    {
      "epoch": 2.568780487804878,
      "grad_norm": 0.12949852645397186,
      "learning_rate": 2.5063451524409644e-06,
      "loss": 0.0114,
      "step": 10532
    },
    {
      "epoch": 2.5690243902439023,
      "grad_norm": 0.14323833584785461,
      "learning_rate": 2.503559006053341e-06,
      "loss": 0.026,
      "step": 10533
    },
    {
      "epoch": 2.569268292682927,
      "grad_norm": 0.15990005433559418,
      "learning_rate": 2.5007743274896274e-06,
      "loss": 0.0227,
      "step": 10534
    },
    {
      "epoch": 2.569512195121951,
      "grad_norm": 0.08659741282463074,
      "learning_rate": 2.4979911169315084e-06,
      "loss": 0.0211,
      "step": 10535
    },
    {
      "epoch": 2.5697560975609757,
      "grad_norm": 0.04815453290939331,
      "learning_rate": 2.495209374560589e-06,
      "loss": 0.0094,
      "step": 10536
    },
    {
      "epoch": 2.57,
      "grad_norm": 0.09881383180618286,
      "learning_rate": 2.4924291005583634e-06,
      "loss": 0.0095,
      "step": 10537
    },
    {
      "epoch": 2.5702439024390245,
      "grad_norm": 0.13835786283016205,
      "learning_rate": 2.4896502951062326e-06,
      "loss": 0.0153,
      "step": 10538
    },
    {
      "epoch": 2.5704878048780486,
      "grad_norm": 0.13261057436466217,
      "learning_rate": 2.4868729583855137e-06,
      "loss": 0.0114,
      "step": 10539
    },
    {
      "epoch": 2.5707317073170732,
      "grad_norm": 0.11131870001554489,
      "learning_rate": 2.484097090577414e-06,
      "loss": 0.0219,
      "step": 10540
    },
    {
      "epoch": 2.5709756097560974,
      "grad_norm": 0.21458415687084198,
      "learning_rate": 2.481322691863047e-06,
      "loss": 0.0309,
      "step": 10541
    },
    {
      "epoch": 2.571219512195122,
      "grad_norm": 0.08657542616128922,
      "learning_rate": 2.4785497624234487e-06,
      "loss": 0.0146,
      "step": 10542
    },
    {
      "epoch": 2.571463414634146,
      "grad_norm": 0.19948896765708923,
      "learning_rate": 2.475778302439524e-06,
      "loss": 0.0146,
      "step": 10543
    },
    {
      "epoch": 2.571707317073171,
      "grad_norm": 0.13369734585285187,
      "learning_rate": 2.4730083120921148e-06,
      "loss": 0.0208,
      "step": 10544
    },
    {
      "epoch": 2.571951219512195,
      "grad_norm": 0.11333071440458298,
      "learning_rate": 2.470239791561954e-06,
      "loss": 0.0129,
      "step": 10545
    },
    {
      "epoch": 2.5721951219512196,
      "grad_norm": 0.21349485218524933,
      "learning_rate": 2.4674727410296692e-06,
      "loss": 0.024,
      "step": 10546
    },
    {
      "epoch": 2.5724390243902437,
      "grad_norm": 0.15821890532970428,
      "learning_rate": 2.4647071606758167e-06,
      "loss": 0.0182,
      "step": 10547
    },
    {
      "epoch": 2.5726829268292684,
      "grad_norm": 0.08695987612009048,
      "learning_rate": 2.4619430506808288e-06,
      "loss": 0.0201,
      "step": 10548
    },
    {
      "epoch": 2.5729268292682925,
      "grad_norm": 0.15731799602508545,
      "learning_rate": 2.459180411225065e-06,
      "loss": 0.0127,
      "step": 10549
    },
    {
      "epoch": 2.573170731707317,
      "grad_norm": 0.13596859574317932,
      "learning_rate": 2.456419242488778e-06,
      "loss": 0.0092,
      "step": 10550
    },
    {
      "epoch": 2.5734146341463413,
      "grad_norm": 0.12534549832344055,
      "learning_rate": 2.453659544652115e-06,
      "loss": 0.0154,
      "step": 10551
    },
    {
      "epoch": 2.573658536585366,
      "grad_norm": 0.0949973464012146,
      "learning_rate": 2.4509013178951517e-06,
      "loss": 0.0185,
      "step": 10552
    },
    {
      "epoch": 2.57390243902439,
      "grad_norm": 0.1609029918909073,
      "learning_rate": 2.448144562397847e-06,
      "loss": 0.0221,
      "step": 10553
    },
    {
      "epoch": 2.5741463414634147,
      "grad_norm": 0.1756347268819809,
      "learning_rate": 2.4453892783400683e-06,
      "loss": 0.0215,
      "step": 10554
    },
    {
      "epoch": 2.574390243902439,
      "grad_norm": 0.08299827575683594,
      "learning_rate": 2.442635465901599e-06,
      "loss": 0.0141,
      "step": 10555
    },
    {
      "epoch": 2.5746341463414635,
      "grad_norm": 0.0982506275177002,
      "learning_rate": 2.4398831252621103e-06,
      "loss": 0.0124,
      "step": 10556
    },
    {
      "epoch": 2.5748780487804876,
      "grad_norm": 0.1572151780128479,
      "learning_rate": 2.4371322566011766e-06,
      "loss": 0.0189,
      "step": 10557
    },
    {
      "epoch": 2.5751219512195123,
      "grad_norm": 0.09648198634386063,
      "learning_rate": 2.4343828600983e-06,
      "loss": 0.0174,
      "step": 10558
    },
    {
      "epoch": 2.5753658536585364,
      "grad_norm": 0.1044643297791481,
      "learning_rate": 2.431634935932861e-06,
      "loss": 0.0186,
      "step": 10559
    },
    {
      "epoch": 2.575609756097561,
      "grad_norm": 0.12372437864542007,
      "learning_rate": 2.428888484284153e-06,
      "loss": 0.0182,
      "step": 10560
    },
    {
      "epoch": 2.575853658536585,
      "grad_norm": 0.16868863999843597,
      "learning_rate": 2.4261435053313713e-06,
      "loss": 0.0166,
      "step": 10561
    },
    {
      "epoch": 2.57609756097561,
      "grad_norm": 0.13089849054813385,
      "learning_rate": 2.423399999253623e-06,
      "loss": 0.0287,
      "step": 10562
    },
    {
      "epoch": 2.576341463414634,
      "grad_norm": 0.1051100417971611,
      "learning_rate": 2.420657966229914e-06,
      "loss": 0.0092,
      "step": 10563
    },
    {
      "epoch": 2.5765853658536586,
      "grad_norm": 0.11409898102283478,
      "learning_rate": 2.4179174064391434e-06,
      "loss": 0.0107,
      "step": 10564
    },
    {
      "epoch": 2.5768292682926828,
      "grad_norm": 0.15587086975574493,
      "learning_rate": 2.415178320060138e-06,
      "loss": 0.0151,
      "step": 10565
    },
    {
      "epoch": 2.5770731707317074,
      "grad_norm": 0.07473409175872803,
      "learning_rate": 2.412440707271607e-06,
      "loss": 0.0127,
      "step": 10566
    },
    {
      "epoch": 2.5773170731707316,
      "grad_norm": 0.08173877000808716,
      "learning_rate": 2.409704568252169e-06,
      "loss": 0.013,
      "step": 10567
    },
    {
      "epoch": 2.577560975609756,
      "grad_norm": 0.10978374630212784,
      "learning_rate": 2.406969903180359e-06,
      "loss": 0.0073,
      "step": 10568
    },
    {
      "epoch": 2.5778048780487803,
      "grad_norm": 0.19196590781211853,
      "learning_rate": 2.404236712234595e-06,
      "loss": 0.0247,
      "step": 10569
    },
    {
      "epoch": 2.578048780487805,
      "grad_norm": 0.15371073782444,
      "learning_rate": 2.401504995593212e-06,
      "loss": 0.0275,
      "step": 10570
    },
    {
      "epoch": 2.578292682926829,
      "grad_norm": 0.0780065655708313,
      "learning_rate": 2.398774753434449e-06,
      "loss": 0.0185,
      "step": 10571
    },
    {
      "epoch": 2.5785365853658537,
      "grad_norm": 0.13408149778842926,
      "learning_rate": 2.396045985936443e-06,
      "loss": 0.011,
      "step": 10572
    },
    {
      "epoch": 2.578780487804878,
      "grad_norm": 0.1340521275997162,
      "learning_rate": 2.393318693277241e-06,
      "loss": 0.0259,
      "step": 10573
    },
    {
      "epoch": 2.5790243902439025,
      "grad_norm": 0.07745259255170822,
      "learning_rate": 2.3905928756347895e-06,
      "loss": 0.019,
      "step": 10574
    },
    {
      "epoch": 2.5792682926829267,
      "grad_norm": 0.15258856117725372,
      "learning_rate": 2.3878685331869403e-06,
      "loss": 0.0153,
      "step": 10575
    },
    {
      "epoch": 2.5795121951219513,
      "grad_norm": 0.07844483107328415,
      "learning_rate": 2.3851456661114434e-06,
      "loss": 0.0095,
      "step": 10576
    },
    {
      "epoch": 2.5797560975609755,
      "grad_norm": 0.08958084136247635,
      "learning_rate": 2.382424274585959e-06,
      "loss": 0.017,
      "step": 10577
    },
    {
      "epoch": 2.58,
      "grad_norm": 0.10877691209316254,
      "learning_rate": 2.3797043587880568e-06,
      "loss": 0.014,
      "step": 10578
    },
    {
      "epoch": 2.5802439024390242,
      "grad_norm": 0.09629378467798233,
      "learning_rate": 2.376985918895197e-06,
      "loss": 0.0058,
      "step": 10579
    },
    {
      "epoch": 2.580487804878049,
      "grad_norm": 0.12756523489952087,
      "learning_rate": 2.3742689550847465e-06,
      "loss": 0.0097,
      "step": 10580
    },
    {
      "epoch": 2.580731707317073,
      "grad_norm": 0.19837328791618347,
      "learning_rate": 2.371553467533985e-06,
      "loss": 0.0271,
      "step": 10581
    },
    {
      "epoch": 2.5809756097560976,
      "grad_norm": 0.08796826750040054,
      "learning_rate": 2.368839456420088e-06,
      "loss": 0.0184,
      "step": 10582
    },
    {
      "epoch": 2.581219512195122,
      "grad_norm": 0.14025543630123138,
      "learning_rate": 2.366126921920134e-06,
      "loss": 0.0199,
      "step": 10583
    },
    {
      "epoch": 2.5814634146341464,
      "grad_norm": 0.1146923080086708,
      "learning_rate": 2.3634158642111103e-06,
      "loss": 0.0195,
      "step": 10584
    },
    {
      "epoch": 2.5817073170731706,
      "grad_norm": 0.08919259160757065,
      "learning_rate": 2.360706283469902e-06,
      "loss": 0.0147,
      "step": 10585
    },
    {
      "epoch": 2.581951219512195,
      "grad_norm": 0.07883521914482117,
      "learning_rate": 2.357998179873308e-06,
      "loss": 0.0114,
      "step": 10586
    },
    {
      "epoch": 2.5821951219512194,
      "grad_norm": 0.12242698669433594,
      "learning_rate": 2.3552915535980187e-06,
      "loss": 0.023,
      "step": 10587
    },
    {
      "epoch": 2.582439024390244,
      "grad_norm": 0.12647059559822083,
      "learning_rate": 2.352586404820628e-06,
      "loss": 0.0211,
      "step": 10588
    },
    {
      "epoch": 2.582682926829268,
      "grad_norm": 0.08299674093723297,
      "learning_rate": 2.3498827337176483e-06,
      "loss": 0.0187,
      "step": 10589
    },
    {
      "epoch": 2.5829268292682928,
      "grad_norm": 0.1457817703485489,
      "learning_rate": 2.347180540465482e-06,
      "loss": 0.0132,
      "step": 10590
    },
    {
      "epoch": 2.583170731707317,
      "grad_norm": 0.11506251990795135,
      "learning_rate": 2.344479825240434e-06,
      "loss": 0.0146,
      "step": 10591
    },
    {
      "epoch": 2.5834146341463415,
      "grad_norm": 0.06203833594918251,
      "learning_rate": 2.341780588218731e-06,
      "loss": 0.0155,
      "step": 10592
    },
    {
      "epoch": 2.5836585365853657,
      "grad_norm": 0.32926201820373535,
      "learning_rate": 2.339082829576472e-06,
      "loss": 0.0226,
      "step": 10593
    },
    {
      "epoch": 2.5839024390243903,
      "grad_norm": 0.16060799360275269,
      "learning_rate": 2.3363865494896913e-06,
      "loss": 0.0144,
      "step": 10594
    },
    {
      "epoch": 2.5841463414634145,
      "grad_norm": 0.12077594548463821,
      "learning_rate": 2.333691748134309e-06,
      "loss": 0.0134,
      "step": 10595
    },
    {
      "epoch": 2.584390243902439,
      "grad_norm": 0.14977803826332092,
      "learning_rate": 2.3309984256861483e-06,
      "loss": 0.0275,
      "step": 10596
    },
    {
      "epoch": 2.5846341463414633,
      "grad_norm": 0.099138043820858,
      "learning_rate": 2.3283065823209465e-06,
      "loss": 0.0195,
      "step": 10597
    },
    {
      "epoch": 2.584878048780488,
      "grad_norm": 0.1037915050983429,
      "learning_rate": 2.32561621821433e-06,
      "loss": 0.018,
      "step": 10598
    },
    {
      "epoch": 2.585121951219512,
      "grad_norm": 0.09907477349042892,
      "learning_rate": 2.3229273335418493e-06,
      "loss": 0.0093,
      "step": 10599
    },
    {
      "epoch": 2.5853658536585367,
      "grad_norm": 0.0920952707529068,
      "learning_rate": 2.3202399284789372e-06,
      "loss": 0.0078,
      "step": 10600
    },
    {
      "epoch": 2.585609756097561,
      "grad_norm": 0.13485202193260193,
      "learning_rate": 2.3175540032009363e-06,
      "loss": 0.0142,
      "step": 10601
    },
    {
      "epoch": 2.5858536585365854,
      "grad_norm": 0.10038988292217255,
      "learning_rate": 2.3148695578831035e-06,
      "loss": 0.0122,
      "step": 10602
    },
    {
      "epoch": 2.5860975609756096,
      "grad_norm": 0.09085404872894287,
      "learning_rate": 2.3121865927005883e-06,
      "loss": 0.0129,
      "step": 10603
    },
    {
      "epoch": 2.5863414634146342,
      "grad_norm": 0.12267252057790756,
      "learning_rate": 2.3095051078284356e-06,
      "loss": 0.0101,
      "step": 10604
    },
    {
      "epoch": 2.5865853658536584,
      "grad_norm": 0.08991549909114838,
      "learning_rate": 2.3068251034416203e-06,
      "loss": 0.0184,
      "step": 10605
    },
    {
      "epoch": 2.586829268292683,
      "grad_norm": 0.09186502546072006,
      "learning_rate": 2.3041465797149913e-06,
      "loss": 0.0126,
      "step": 10606
    },
    {
      "epoch": 2.587073170731707,
      "grad_norm": 0.17027193307876587,
      "learning_rate": 2.3014695368233223e-06,
      "loss": 0.0184,
      "step": 10607
    },
    {
      "epoch": 2.587317073170732,
      "grad_norm": 0.08716075867414474,
      "learning_rate": 2.298793974941285e-06,
      "loss": 0.0059,
      "step": 10608
    },
    {
      "epoch": 2.587560975609756,
      "grad_norm": 0.1645391881465912,
      "learning_rate": 2.296119894243434e-06,
      "loss": 0.0266,
      "step": 10609
    },
    {
      "epoch": 2.5878048780487806,
      "grad_norm": 0.12733757495880127,
      "learning_rate": 2.2934472949042633e-06,
      "loss": 0.0101,
      "step": 10610
    },
    {
      "epoch": 2.5880487804878047,
      "grad_norm": 0.11508718878030777,
      "learning_rate": 2.290776177098142e-06,
      "loss": 0.0176,
      "step": 10611
    },
    {
      "epoch": 2.5882926829268293,
      "grad_norm": 0.20821091532707214,
      "learning_rate": 2.28810654099936e-06,
      "loss": 0.0101,
      "step": 10612
    },
    {
      "epoch": 2.5885365853658535,
      "grad_norm": 0.17441056668758392,
      "learning_rate": 2.2854383867820963e-06,
      "loss": 0.0209,
      "step": 10613
    },
    {
      "epoch": 2.588780487804878,
      "grad_norm": 0.09313301742076874,
      "learning_rate": 2.282771714620438e-06,
      "loss": 0.012,
      "step": 10614
    },
    {
      "epoch": 2.5890243902439023,
      "grad_norm": 0.28569573163986206,
      "learning_rate": 2.2801065246883857e-06,
      "loss": 0.027,
      "step": 10615
    },
    {
      "epoch": 2.589268292682927,
      "grad_norm": 0.07062225043773651,
      "learning_rate": 2.2774428171598307e-06,
      "loss": 0.013,
      "step": 10616
    },
    {
      "epoch": 2.589512195121951,
      "grad_norm": 0.06694924086332321,
      "learning_rate": 2.2747805922085667e-06,
      "loss": 0.0106,
      "step": 10617
    },
    {
      "epoch": 2.5897560975609757,
      "grad_norm": 0.09021902829408646,
      "learning_rate": 2.2721198500083056e-06,
      "loss": 0.0208,
      "step": 10618
    },
    {
      "epoch": 2.59,
      "grad_norm": 0.18577396869659424,
      "learning_rate": 2.2694605907326416e-06,
      "loss": 0.012,
      "step": 10619
    },
    {
      "epoch": 2.5902439024390245,
      "grad_norm": 0.11505372822284698,
      "learning_rate": 2.266802814555097e-06,
      "loss": 0.0094,
      "step": 10620
    },
    {
      "epoch": 2.5904878048780486,
      "grad_norm": 0.12385239452123642,
      "learning_rate": 2.2641465216490713e-06,
      "loss": 0.0132,
      "step": 10621
    },
    {
      "epoch": 2.5907317073170733,
      "grad_norm": 0.1557760238647461,
      "learning_rate": 2.2614917121878828e-06,
      "loss": 0.0128,
      "step": 10622
    },
    {
      "epoch": 2.5909756097560974,
      "grad_norm": 0.1356729418039322,
      "learning_rate": 2.2588383863447555e-06,
      "loss": 0.0138,
      "step": 10623
    },
    {
      "epoch": 2.591219512195122,
      "grad_norm": 0.0927448496222496,
      "learning_rate": 2.2561865442928045e-06,
      "loss": 0.0122,
      "step": 10624
    },
    {
      "epoch": 2.591463414634146,
      "grad_norm": 0.1185084879398346,
      "learning_rate": 2.2535361862050547e-06,
      "loss": 0.0162,
      "step": 10625
    },
    {
      "epoch": 2.591707317073171,
      "grad_norm": 0.1624838411808014,
      "learning_rate": 2.2508873122544377e-06,
      "loss": 0.0166,
      "step": 10626
    },
    {
      "epoch": 2.591951219512195,
      "grad_norm": 0.11681888997554779,
      "learning_rate": 2.2482399226137763e-06,
      "loss": 0.02,
      "step": 10627
    },
    {
      "epoch": 2.5921951219512196,
      "grad_norm": 0.14871536195278168,
      "learning_rate": 2.2455940174558155e-06,
      "loss": 0.017,
      "step": 10628
    },
    {
      "epoch": 2.5924390243902438,
      "grad_norm": 0.1720406860113144,
      "learning_rate": 2.2429495969531866e-06,
      "loss": 0.0131,
      "step": 10629
    },
    {
      "epoch": 2.5926829268292684,
      "grad_norm": 0.13793647289276123,
      "learning_rate": 2.240306661278424e-06,
      "loss": 0.0262,
      "step": 10630
    },
    {
      "epoch": 2.5929268292682925,
      "grad_norm": 0.19571347534656525,
      "learning_rate": 2.237665210603984e-06,
      "loss": 0.0198,
      "step": 10631
    },
    {
      "epoch": 2.593170731707317,
      "grad_norm": 0.1875857710838318,
      "learning_rate": 2.235025245102204e-06,
      "loss": 0.0218,
      "step": 10632
    },
    {
      "epoch": 2.5934146341463413,
      "grad_norm": 0.2328190952539444,
      "learning_rate": 2.2323867649453377e-06,
      "loss": 0.0146,
      "step": 10633
    },
    {
      "epoch": 2.593658536585366,
      "grad_norm": 0.11586958914995193,
      "learning_rate": 2.229749770305539e-06,
      "loss": 0.0266,
      "step": 10634
    },
    {
      "epoch": 2.59390243902439,
      "grad_norm": 0.08105146139860153,
      "learning_rate": 2.2271142613548566e-06,
      "loss": 0.0142,
      "step": 10635
    },
    {
      "epoch": 2.5941463414634147,
      "grad_norm": 0.1471659392118454,
      "learning_rate": 2.2244802382652587e-06,
      "loss": 0.0131,
      "step": 10636
    },
    {
      "epoch": 2.594390243902439,
      "grad_norm": 0.07997729629278183,
      "learning_rate": 2.2218477012086015e-06,
      "loss": 0.0103,
      "step": 10637
    },
    {
      "epoch": 2.5946341463414635,
      "grad_norm": 0.13313627243041992,
      "learning_rate": 2.2192166503566452e-06,
      "loss": 0.0215,
      "step": 10638
    },
    {
      "epoch": 2.5948780487804877,
      "grad_norm": 0.15777866542339325,
      "learning_rate": 2.2165870858810724e-06,
      "loss": 0.0115,
      "step": 10639
    },
    {
      "epoch": 2.5951219512195123,
      "grad_norm": 0.10399972647428513,
      "learning_rate": 2.2139590079534423e-06,
      "loss": 0.0175,
      "step": 10640
    },
    {
      "epoch": 2.5953658536585364,
      "grad_norm": 0.15905305743217468,
      "learning_rate": 2.211332416745235e-06,
      "loss": 0.0326,
      "step": 10641
    },
    {
      "epoch": 2.595609756097561,
      "grad_norm": 0.1117074117064476,
      "learning_rate": 2.208707312427821e-06,
      "loss": 0.0123,
      "step": 10642
    },
    {
      "epoch": 2.5958536585365852,
      "grad_norm": 0.10355285555124283,
      "learning_rate": 2.206083695172481e-06,
      "loss": 0.0257,
      "step": 10643
    },
    {
      "epoch": 2.59609756097561,
      "grad_norm": 0.1587468385696411,
      "learning_rate": 2.203461565150408e-06,
      "loss": 0.0159,
      "step": 10644
    },
    {
      "epoch": 2.596341463414634,
      "grad_norm": 0.0922294557094574,
      "learning_rate": 2.2008409225326738e-06,
      "loss": 0.0116,
      "step": 10645
    },
    {
      "epoch": 2.5965853658536586,
      "grad_norm": 0.2539907395839691,
      "learning_rate": 2.19822176749028e-06,
      "loss": 0.0273,
      "step": 10646
    },
    {
      "epoch": 2.596829268292683,
      "grad_norm": 0.11943141371011734,
      "learning_rate": 2.1956041001941123e-06,
      "loss": 0.0156,
      "step": 10647
    },
    {
      "epoch": 2.5970731707317074,
      "grad_norm": 0.14237390458583832,
      "learning_rate": 2.1929879208149624e-06,
      "loss": 0.0173,
      "step": 10648
    },
    {
      "epoch": 2.5973170731707316,
      "grad_norm": 0.12579862773418427,
      "learning_rate": 2.1903732295235346e-06,
      "loss": 0.0224,
      "step": 10649
    },
    {
      "epoch": 2.597560975609756,
      "grad_norm": 0.10303088277578354,
      "learning_rate": 2.187760026490429e-06,
      "loss": 0.0177,
      "step": 10650
    },
    {
      "epoch": 2.5978048780487804,
      "grad_norm": 0.05402710288763046,
      "learning_rate": 2.1851483118861397e-06,
      "loss": 0.0096,
      "step": 10651
    },
    {
      "epoch": 2.598048780487805,
      "grad_norm": 0.0702747106552124,
      "learning_rate": 2.182538085881086e-06,
      "loss": 0.0129,
      "step": 10652
    },
    {
      "epoch": 2.598292682926829,
      "grad_norm": 0.118993379175663,
      "learning_rate": 2.1799293486455736e-06,
      "loss": 0.0157,
      "step": 10653
    },
    {
      "epoch": 2.5985365853658537,
      "grad_norm": 0.08445867151021957,
      "learning_rate": 2.1773221003498047e-06,
      "loss": 0.009,
      "step": 10654
    },
    {
      "epoch": 2.598780487804878,
      "grad_norm": 0.09348715096712112,
      "learning_rate": 2.1747163411639072e-06,
      "loss": 0.0253,
      "step": 10655
    },
    {
      "epoch": 2.5990243902439025,
      "grad_norm": 0.08320529013872147,
      "learning_rate": 2.172112071257887e-06,
      "loss": 0.0102,
      "step": 10656
    },
    {
      "epoch": 2.5992682926829267,
      "grad_norm": 0.2348298579454422,
      "learning_rate": 2.1695092908016775e-06,
      "loss": 0.0217,
      "step": 10657
    },
    {
      "epoch": 2.5995121951219513,
      "grad_norm": 0.2434486448764801,
      "learning_rate": 2.166907999965098e-06,
      "loss": 0.0149,
      "step": 10658
    },
    {
      "epoch": 2.5997560975609755,
      "grad_norm": 0.15230754017829895,
      "learning_rate": 2.164308198917869e-06,
      "loss": 0.0281,
      "step": 10659
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.09668122231960297,
      "learning_rate": 2.1617098878296243e-06,
      "loss": 0.0121,
      "step": 10660
    },
    {
      "epoch": 2.6002439024390243,
      "grad_norm": 0.09526274353265762,
      "learning_rate": 2.159113066869892e-06,
      "loss": 0.0139,
      "step": 10661
    },
    {
      "epoch": 2.600487804878049,
      "grad_norm": 0.09811675548553467,
      "learning_rate": 2.1565177362081134e-06,
      "loss": 0.0132,
      "step": 10662
    },
    {
      "epoch": 2.600731707317073,
      "grad_norm": 0.13122087717056274,
      "learning_rate": 2.153923896013621e-06,
      "loss": 0.0178,
      "step": 10663
    },
    {
      "epoch": 2.6009756097560977,
      "grad_norm": 0.1484600454568863,
      "learning_rate": 2.151331546455651e-06,
      "loss": 0.0126,
      "step": 10664
    },
    {
      "epoch": 2.601219512195122,
      "grad_norm": 0.12576274573802948,
      "learning_rate": 2.1487406877033573e-06,
      "loss": 0.0201,
      "step": 10665
    },
    {
      "epoch": 2.6014634146341464,
      "grad_norm": 0.10950198769569397,
      "learning_rate": 2.146151319925774e-06,
      "loss": 0.0269,
      "step": 10666
    },
    {
      "epoch": 2.6017073170731706,
      "grad_norm": 0.1830582171678543,
      "learning_rate": 2.1435634432918572e-06,
      "loss": 0.0234,
      "step": 10667
    },
    {
      "epoch": 2.601951219512195,
      "grad_norm": 0.07704637944698334,
      "learning_rate": 2.1409770579704584e-06,
      "loss": 0.011,
      "step": 10668
    },
    {
      "epoch": 2.6021951219512194,
      "grad_norm": 0.12763795256614685,
      "learning_rate": 2.1383921641303233e-06,
      "loss": 0.0111,
      "step": 10669
    },
    {
      "epoch": 2.602439024390244,
      "grad_norm": 0.11335442960262299,
      "learning_rate": 2.1358087619401166e-06,
      "loss": 0.0202,
      "step": 10670
    },
    {
      "epoch": 2.602682926829268,
      "grad_norm": 0.17077285051345825,
      "learning_rate": 2.133226851568396e-06,
      "loss": 0.0155,
      "step": 10671
    },
    {
      "epoch": 2.6029268292682928,
      "grad_norm": 0.14836479723453522,
      "learning_rate": 2.130646433183614e-06,
      "loss": 0.0156,
      "step": 10672
    },
    {
      "epoch": 2.603170731707317,
      "grad_norm": 0.13111038506031036,
      "learning_rate": 2.1280675069541484e-06,
      "loss": 0.0115,
      "step": 10673
    },
    {
      "epoch": 2.6034146341463416,
      "grad_norm": 0.1428825706243515,
      "learning_rate": 2.1254900730482615e-06,
      "loss": 0.0113,
      "step": 10674
    },
    {
      "epoch": 2.6036585365853657,
      "grad_norm": 0.08045044541358948,
      "learning_rate": 2.122914131634121e-06,
      "loss": 0.0108,
      "step": 10675
    },
    {
      "epoch": 2.6039024390243903,
      "grad_norm": 0.08575807511806488,
      "learning_rate": 2.1203396828797987e-06,
      "loss": 0.0074,
      "step": 10676
    },
    {
      "epoch": 2.6041463414634145,
      "grad_norm": 0.11748465895652771,
      "learning_rate": 2.1177667269532687e-06,
      "loss": 0.016,
      "step": 10677
    },
    {
      "epoch": 2.604390243902439,
      "grad_norm": 0.10328718274831772,
      "learning_rate": 2.1151952640224128e-06,
      "loss": 0.013,
      "step": 10678
    },
    {
      "epoch": 2.6046341463414633,
      "grad_norm": 0.10156048834323883,
      "learning_rate": 2.112625294255005e-06,
      "loss": 0.013,
      "step": 10679
    },
    {
      "epoch": 2.604878048780488,
      "grad_norm": 0.10553496330976486,
      "learning_rate": 2.110056817818737e-06,
      "loss": 0.0198,
      "step": 10680
    },
    {
      "epoch": 2.605121951219512,
      "grad_norm": 0.1004510149359703,
      "learning_rate": 2.1074898348811878e-06,
      "loss": 0.0211,
      "step": 10681
    },
    {
      "epoch": 2.6053658536585367,
      "grad_norm": 0.12198353558778763,
      "learning_rate": 2.1049243456098435e-06,
      "loss": 0.0134,
      "step": 10682
    },
    {
      "epoch": 2.605609756097561,
      "grad_norm": 0.16898208856582642,
      "learning_rate": 2.1023603501721e-06,
      "loss": 0.0237,
      "step": 10683
    },
    {
      "epoch": 2.6058536585365855,
      "grad_norm": 0.07655414938926697,
      "learning_rate": 2.0997978487352466e-06,
      "loss": 0.0124,
      "step": 10684
    },
    {
      "epoch": 2.6060975609756096,
      "grad_norm": 0.18738311529159546,
      "learning_rate": 2.097236841466474e-06,
      "loss": 0.0307,
      "step": 10685
    },
    {
      "epoch": 2.6063414634146342,
      "grad_norm": 0.18773771822452545,
      "learning_rate": 2.094677328532893e-06,
      "loss": 0.0185,
      "step": 10686
    },
    {
      "epoch": 2.6065853658536584,
      "grad_norm": 0.1552930623292923,
      "learning_rate": 2.092119310101495e-06,
      "loss": 0.0225,
      "step": 10687
    },
    {
      "epoch": 2.606829268292683,
      "grad_norm": 0.15132497251033783,
      "learning_rate": 2.0895627863391806e-06,
      "loss": 0.0225,
      "step": 10688
    },
    {
      "epoch": 2.607073170731707,
      "grad_norm": 0.06435412913560867,
      "learning_rate": 2.0870077574127623e-06,
      "loss": 0.0118,
      "step": 10689
    },
    {
      "epoch": 2.607317073170732,
      "grad_norm": 0.15568606555461884,
      "learning_rate": 2.084454223488938e-06,
      "loss": 0.0256,
      "step": 10690
    },
    {
      "epoch": 2.607560975609756,
      "grad_norm": 0.0933917686343193,
      "learning_rate": 2.0819021847343354e-06,
      "loss": 0.0222,
      "step": 10691
    },
    {
      "epoch": 2.6078048780487806,
      "grad_norm": 0.07657158374786377,
      "learning_rate": 2.0793516413154496e-06,
      "loss": 0.0107,
      "step": 10692
    },
    {
      "epoch": 2.6080487804878048,
      "grad_norm": 0.20609629154205322,
      "learning_rate": 2.0768025933987024e-06,
      "loss": 0.025,
      "step": 10693
    },
    {
      "epoch": 2.6082926829268294,
      "grad_norm": 0.17495420575141907,
      "learning_rate": 2.0742550411504136e-06,
      "loss": 0.0148,
      "step": 10694
    },
    {
      "epoch": 2.6085365853658535,
      "grad_norm": 0.25540226697921753,
      "learning_rate": 2.0717089847367975e-06,
      "loss": 0.0119,
      "step": 10695
    },
    {
      "epoch": 2.608780487804878,
      "grad_norm": 0.10511510819196701,
      "learning_rate": 2.0691644243239826e-06,
      "loss": 0.0196,
      "step": 10696
    },
    {
      "epoch": 2.6090243902439023,
      "grad_norm": 0.157319113612175,
      "learning_rate": 2.0666213600779916e-06,
      "loss": 0.0278,
      "step": 10697
    },
    {
      "epoch": 2.609268292682927,
      "grad_norm": 0.20645718276500702,
      "learning_rate": 2.064079792164747e-06,
      "loss": 0.0224,
      "step": 10698
    },
    {
      "epoch": 2.609512195121951,
      "grad_norm": 0.15491914749145508,
      "learning_rate": 2.061539720750086e-06,
      "loss": 0.0069,
      "step": 10699
    },
    {
      "epoch": 2.6097560975609757,
      "grad_norm": 0.14865310490131378,
      "learning_rate": 2.0590011459997364e-06,
      "loss": 0.0184,
      "step": 10700
    },
    {
      "epoch": 2.61,
      "grad_norm": 0.18787546455860138,
      "learning_rate": 2.056464068079331e-06,
      "loss": 0.014,
      "step": 10701
    },
    {
      "epoch": 2.6102439024390245,
      "grad_norm": 0.13891296088695526,
      "learning_rate": 2.0539284871544108e-06,
      "loss": 0.0142,
      "step": 10702
    },
    {
      "epoch": 2.6104878048780487,
      "grad_norm": 0.13960057497024536,
      "learning_rate": 2.051394403390408e-06,
      "loss": 0.0165,
      "step": 10703
    },
    {
      "epoch": 2.6107317073170733,
      "grad_norm": 0.15763139724731445,
      "learning_rate": 2.0488618169526715e-06,
      "loss": 0.0161,
      "step": 10704
    },
    {
      "epoch": 2.6109756097560974,
      "grad_norm": 0.08107137680053711,
      "learning_rate": 2.0463307280064432e-06,
      "loss": 0.0151,
      "step": 10705
    },
    {
      "epoch": 2.611219512195122,
      "grad_norm": 0.24443060159683228,
      "learning_rate": 2.0438011367168606e-06,
      "loss": 0.0163,
      "step": 10706
    },
    {
      "epoch": 2.611463414634146,
      "grad_norm": 0.12925124168395996,
      "learning_rate": 2.041273043248987e-06,
      "loss": 0.0246,
      "step": 10707
    },
    {
      "epoch": 2.611707317073171,
      "grad_norm": 0.11205282062292099,
      "learning_rate": 2.038746447767756e-06,
      "loss": 0.0154,
      "step": 10708
    },
    {
      "epoch": 2.611951219512195,
      "grad_norm": 0.12036901712417603,
      "learning_rate": 2.0362213504380336e-06,
      "loss": 0.0169,
      "step": 10709
    },
    {
      "epoch": 2.6121951219512196,
      "grad_norm": 0.07358571887016296,
      "learning_rate": 2.033697751424568e-06,
      "loss": 0.008,
      "step": 10710
    },
    {
      "epoch": 2.612439024390244,
      "grad_norm": 0.06901243329048157,
      "learning_rate": 2.0311756508920143e-06,
      "loss": 0.02,
      "step": 10711
    },
    {
      "epoch": 2.6126829268292684,
      "grad_norm": 0.15464235842227936,
      "learning_rate": 2.028655049004938e-06,
      "loss": 0.0332,
      "step": 10712
    },
    {
      "epoch": 2.6129268292682926,
      "grad_norm": 0.16683319211006165,
      "learning_rate": 2.0261359459277986e-06,
      "loss": 0.0172,
      "step": 10713
    },
    {
      "epoch": 2.613170731707317,
      "grad_norm": 0.05784231424331665,
      "learning_rate": 2.0236183418249537e-06,
      "loss": 0.0115,
      "step": 10714
    },
    {
      "epoch": 2.6134146341463413,
      "grad_norm": 0.15257549285888672,
      "learning_rate": 2.021102236860678e-06,
      "loss": 0.0179,
      "step": 10715
    },
    {
      "epoch": 2.613658536585366,
      "grad_norm": 0.11200034618377686,
      "learning_rate": 2.0185876311991342e-06,
      "loss": 0.0218,
      "step": 10716
    },
    {
      "epoch": 2.61390243902439,
      "grad_norm": 0.08560895919799805,
      "learning_rate": 2.016074525004397e-06,
      "loss": 0.0122,
      "step": 10717
    },
    {
      "epoch": 2.6141463414634147,
      "grad_norm": 0.21115173399448395,
      "learning_rate": 2.013562918440437e-06,
      "loss": 0.024,
      "step": 10718
    },
    {
      "epoch": 2.614390243902439,
      "grad_norm": 0.07861930876970291,
      "learning_rate": 2.0110528116711242e-06,
      "loss": 0.0101,
      "step": 10719
    },
    {
      "epoch": 2.6146341463414635,
      "grad_norm": 0.11366493999958038,
      "learning_rate": 2.008544204860244e-06,
      "loss": 0.0137,
      "step": 10720
    },
    {
      "epoch": 2.6148780487804877,
      "grad_norm": 0.15523208677768707,
      "learning_rate": 2.0060370981714704e-06,
      "loss": 0.0156,
      "step": 10721
    },
    {
      "epoch": 2.6151219512195123,
      "grad_norm": 0.11559868603944778,
      "learning_rate": 2.0035314917683817e-06,
      "loss": 0.0163,
      "step": 10722
    },
    {
      "epoch": 2.6153658536585365,
      "grad_norm": 0.11434982717037201,
      "learning_rate": 2.001027385814469e-06,
      "loss": 0.0221,
      "step": 10723
    },
    {
      "epoch": 2.615609756097561,
      "grad_norm": 0.12333200871944427,
      "learning_rate": 1.99852478047311e-06,
      "loss": 0.0121,
      "step": 10724
    },
    {
      "epoch": 2.6158536585365852,
      "grad_norm": 0.11423312872648239,
      "learning_rate": 1.996023675907599e-06,
      "loss": 0.0147,
      "step": 10725
    },
    {
      "epoch": 2.61609756097561,
      "grad_norm": 0.11482886970043182,
      "learning_rate": 1.9935240722811164e-06,
      "loss": 0.0194,
      "step": 10726
    },
    {
      "epoch": 2.616341463414634,
      "grad_norm": 0.10859470069408417,
      "learning_rate": 1.991025969756757e-06,
      "loss": 0.0123,
      "step": 10727
    },
    {
      "epoch": 2.6165853658536586,
      "grad_norm": 0.0908111035823822,
      "learning_rate": 1.9885293684975187e-06,
      "loss": 0.0111,
      "step": 10728
    },
    {
      "epoch": 2.616829268292683,
      "grad_norm": 0.13507196307182312,
      "learning_rate": 1.9860342686662926e-06,
      "loss": 0.0122,
      "step": 10729
    },
    {
      "epoch": 2.6170731707317074,
      "grad_norm": 0.12906546890735626,
      "learning_rate": 1.983540670425879e-06,
      "loss": 0.0164,
      "step": 10730
    },
    {
      "epoch": 2.6173170731707316,
      "grad_norm": 0.12139981985092163,
      "learning_rate": 1.981048573938979e-06,
      "loss": 0.0129,
      "step": 10731
    },
    {
      "epoch": 2.617560975609756,
      "grad_norm": 0.15224449336528778,
      "learning_rate": 1.9785579793681867e-06,
      "loss": 0.0219,
      "step": 10732
    },
    {
      "epoch": 2.6178048780487804,
      "grad_norm": 0.10254126787185669,
      "learning_rate": 1.9760688868760146e-06,
      "loss": 0.0178,
      "step": 10733
    },
    {
      "epoch": 2.618048780487805,
      "grad_norm": 0.18409036099910736,
      "learning_rate": 1.9735812966248655e-06,
      "loss": 0.0197,
      "step": 10734
    },
    {
      "epoch": 2.618292682926829,
      "grad_norm": 0.13357219099998474,
      "learning_rate": 1.9710952087770405e-06,
      "loss": 0.0209,
      "step": 10735
    },
    {
      "epoch": 2.6185365853658538,
      "grad_norm": 0.22770412266254425,
      "learning_rate": 1.9686106234947598e-06,
      "loss": 0.0275,
      "step": 10736
    },
    {
      "epoch": 2.618780487804878,
      "grad_norm": 0.18481481075286865,
      "learning_rate": 1.966127540940127e-06,
      "loss": 0.0314,
      "step": 10737
    },
    {
      "epoch": 2.6190243902439025,
      "grad_norm": 0.0928715243935585,
      "learning_rate": 1.9636459612751624e-06,
      "loss": 0.0143,
      "step": 10738
    },
    {
      "epoch": 2.6192682926829267,
      "grad_norm": 0.07872879505157471,
      "learning_rate": 1.9611658846617753e-06,
      "loss": 0.0059,
      "step": 10739
    },
    {
      "epoch": 2.6195121951219513,
      "grad_norm": 0.1509084403514862,
      "learning_rate": 1.9586873112617893e-06,
      "loss": 0.0297,
      "step": 10740
    },
    {
      "epoch": 2.6197560975609755,
      "grad_norm": 0.1313510686159134,
      "learning_rate": 1.9562102412369166e-06,
      "loss": 0.0108,
      "step": 10741
    },
    {
      "epoch": 2.62,
      "grad_norm": 0.15860901772975922,
      "learning_rate": 1.9537346747487808e-06,
      "loss": 0.0174,
      "step": 10742
    },
    {
      "epoch": 2.6202439024390243,
      "grad_norm": 0.39778855443000793,
      "learning_rate": 1.9512606119589076e-06,
      "loss": 0.0354,
      "step": 10743
    },
    {
      "epoch": 2.620487804878049,
      "grad_norm": 0.09616971760988235,
      "learning_rate": 1.9487880530287217e-06,
      "loss": 0.0156,
      "step": 10744
    },
    {
      "epoch": 2.620731707317073,
      "grad_norm": 0.10853945463895798,
      "learning_rate": 1.9463169981195464e-06,
      "loss": 0.0208,
      "step": 10745
    },
    {
      "epoch": 2.6209756097560977,
      "grad_norm": 0.13555647432804108,
      "learning_rate": 1.943847447392613e-06,
      "loss": 0.025,
      "step": 10746
    },
    {
      "epoch": 2.621219512195122,
      "grad_norm": 0.17208518087863922,
      "learning_rate": 1.941379401009055e-06,
      "loss": 0.0266,
      "step": 10747
    },
    {
      "epoch": 2.6214634146341464,
      "grad_norm": 0.1453368216753006,
      "learning_rate": 1.9389128591298988e-06,
      "loss": 0.0221,
      "step": 10748
    },
    {
      "epoch": 2.6217073170731706,
      "grad_norm": 0.12254881113767624,
      "learning_rate": 1.936447821916082e-06,
      "loss": 0.0144,
      "step": 10749
    },
    {
      "epoch": 2.6219512195121952,
      "grad_norm": 0.14913034439086914,
      "learning_rate": 1.93398428952844e-06,
      "loss": 0.0296,
      "step": 10750
    },
    {
      "epoch": 2.6221951219512194,
      "grad_norm": 0.11837045848369598,
      "learning_rate": 1.9315222621277136e-06,
      "loss": 0.0123,
      "step": 10751
    },
    {
      "epoch": 2.622439024390244,
      "grad_norm": 0.08172005414962769,
      "learning_rate": 1.929061739874541e-06,
      "loss": 0.0212,
      "step": 10752
    },
    {
      "epoch": 2.622682926829268,
      "grad_norm": 0.21249496936798096,
      "learning_rate": 1.9266027229294575e-06,
      "loss": 0.0163,
      "step": 10753
    },
    {
      "epoch": 2.622926829268293,
      "grad_norm": 0.10048691183328629,
      "learning_rate": 1.9241452114529184e-06,
      "loss": 0.0108,
      "step": 10754
    },
    {
      "epoch": 2.623170731707317,
      "grad_norm": 0.08689859509468079,
      "learning_rate": 1.9216892056052595e-06,
      "loss": 0.0136,
      "step": 10755
    },
    {
      "epoch": 2.6234146341463416,
      "grad_norm": 0.07000140845775604,
      "learning_rate": 1.919234705546727e-06,
      "loss": 0.0087,
      "step": 10756
    },
    {
      "epoch": 2.6236585365853657,
      "grad_norm": 0.16016250848770142,
      "learning_rate": 1.9167817114374818e-06,
      "loss": 0.0205,
      "step": 10757
    },
    {
      "epoch": 2.6239024390243904,
      "grad_norm": 0.13523045182228088,
      "learning_rate": 1.914330223437558e-06,
      "loss": 0.0193,
      "step": 10758
    },
    {
      "epoch": 2.6241463414634145,
      "grad_norm": 0.1439393162727356,
      "learning_rate": 1.9118802417069177e-06,
      "loss": 0.0152,
      "step": 10759
    },
    {
      "epoch": 2.624390243902439,
      "grad_norm": 0.0952279344201088,
      "learning_rate": 1.9094317664054147e-06,
      "loss": 0.0236,
      "step": 10760
    },
    {
      "epoch": 2.6246341463414633,
      "grad_norm": 0.14070458710193634,
      "learning_rate": 1.9069847976927958e-06,
      "loss": 0.0218,
      "step": 10761
    },
    {
      "epoch": 2.624878048780488,
      "grad_norm": 0.08441224694252014,
      "learning_rate": 1.9045393357287279e-06,
      "loss": 0.0126,
      "step": 10762
    },
    {
      "epoch": 2.625121951219512,
      "grad_norm": 0.08160621672868729,
      "learning_rate": 1.9020953806727637e-06,
      "loss": 0.0127,
      "step": 10763
    },
    {
      "epoch": 2.6253658536585367,
      "grad_norm": 0.0857962816953659,
      "learning_rate": 1.8996529326843676e-06,
      "loss": 0.0108,
      "step": 10764
    },
    {
      "epoch": 2.625609756097561,
      "grad_norm": 0.13756601512432098,
      "learning_rate": 1.8972119919229036e-06,
      "loss": 0.0143,
      "step": 10765
    },
    {
      "epoch": 2.6258536585365855,
      "grad_norm": 0.20968745648860931,
      "learning_rate": 1.8947725585476278e-06,
      "loss": 0.0257,
      "step": 10766
    },
    {
      "epoch": 2.6260975609756096,
      "grad_norm": 0.2072782963514328,
      "learning_rate": 1.8923346327177155e-06,
      "loss": 0.0319,
      "step": 10767
    },
    {
      "epoch": 2.6263414634146343,
      "grad_norm": 0.18425381183624268,
      "learning_rate": 1.8898982145922285e-06,
      "loss": 0.0245,
      "step": 10768
    },
    {
      "epoch": 2.6265853658536584,
      "grad_norm": 0.37998050451278687,
      "learning_rate": 1.8874633043301309e-06,
      "loss": 0.0237,
      "step": 10769
    },
    {
      "epoch": 2.626829268292683,
      "grad_norm": 0.12851792573928833,
      "learning_rate": 1.8850299020903045e-06,
      "loss": 0.0224,
      "step": 10770
    },
    {
      "epoch": 2.627073170731707,
      "grad_norm": 0.09436443448066711,
      "learning_rate": 1.8825980080315136e-06,
      "loss": 0.0151,
      "step": 10771
    },
    {
      "epoch": 2.627317073170732,
      "grad_norm": 0.16331620514392853,
      "learning_rate": 1.8801676223124316e-06,
      "loss": 0.0106,
      "step": 10772
    },
    {
      "epoch": 2.627560975609756,
      "grad_norm": 0.13832995295524597,
      "learning_rate": 1.8777387450916423e-06,
      "loss": 0.0106,
      "step": 10773
    },
    {
      "epoch": 2.6278048780487806,
      "grad_norm": 0.13526062667369843,
      "learning_rate": 1.8753113765276082e-06,
      "loss": 0.0234,
      "step": 10774
    },
    {
      "epoch": 2.6280487804878048,
      "grad_norm": 0.1091802641749382,
      "learning_rate": 1.872885516778719e-06,
      "loss": 0.0267,
      "step": 10775
    },
    {
      "epoch": 2.6282926829268294,
      "grad_norm": 0.12099792808294296,
      "learning_rate": 1.8704611660032457e-06,
      "loss": 0.0239,
      "step": 10776
    },
    {
      "epoch": 2.6285365853658536,
      "grad_norm": 0.1322358250617981,
      "learning_rate": 1.8680383243593807e-06,
      "loss": 0.0106,
      "step": 10777
    },
    {
      "epoch": 2.628780487804878,
      "grad_norm": 0.08615989238023758,
      "learning_rate": 1.8656169920052008e-06,
      "loss": 0.0153,
      "step": 10778
    },
    {
      "epoch": 2.6290243902439023,
      "grad_norm": 0.22277714312076569,
      "learning_rate": 1.8631971690986876e-06,
      "loss": 0.022,
      "step": 10779
    },
    {
      "epoch": 2.629268292682927,
      "grad_norm": 0.07256188243627548,
      "learning_rate": 1.8607788557977347e-06,
      "loss": 0.0043,
      "step": 10780
    },
    {
      "epoch": 2.629512195121951,
      "grad_norm": 0.19099090993404388,
      "learning_rate": 1.8583620522601237e-06,
      "loss": 0.031,
      "step": 10781
    },
    {
      "epoch": 2.6297560975609757,
      "grad_norm": 0.13132359087467194,
      "learning_rate": 1.8559467586435425e-06,
      "loss": 0.015,
      "step": 10782
    },
    {
      "epoch": 2.63,
      "grad_norm": 0.1279204934835434,
      "learning_rate": 1.8535329751055903e-06,
      "loss": 0.0103,
      "step": 10783
    },
    {
      "epoch": 2.6302439024390245,
      "grad_norm": 0.1520019769668579,
      "learning_rate": 1.8511207018037491e-06,
      "loss": 0.018,
      "step": 10784
    },
    {
      "epoch": 2.6304878048780487,
      "grad_norm": 0.11700225621461868,
      "learning_rate": 1.8487099388954155e-06,
      "loss": 0.0106,
      "step": 10785
    },
    {
      "epoch": 2.6307317073170733,
      "grad_norm": 0.39761024713516235,
      "learning_rate": 1.8463006865378913e-06,
      "loss": 0.0406,
      "step": 10786
    },
    {
      "epoch": 2.6309756097560975,
      "grad_norm": 0.1458546221256256,
      "learning_rate": 1.8438929448883591e-06,
      "loss": 0.0239,
      "step": 10787
    },
    {
      "epoch": 2.631219512195122,
      "grad_norm": 0.09204301238059998,
      "learning_rate": 1.8414867141039322e-06,
      "loss": 0.0199,
      "step": 10788
    },
    {
      "epoch": 2.6314634146341462,
      "grad_norm": 0.1681690663099289,
      "learning_rate": 1.8390819943415989e-06,
      "loss": 0.0146,
      "step": 10789
    },
    {
      "epoch": 2.631707317073171,
      "grad_norm": 0.09956546872854233,
      "learning_rate": 1.836678785758264e-06,
      "loss": 0.011,
      "step": 10790
    },
    {
      "epoch": 2.631951219512195,
      "grad_norm": 0.10707227885723114,
      "learning_rate": 1.8342770885107302e-06,
      "loss": 0.0349,
      "step": 10791
    },
    {
      "epoch": 2.6321951219512196,
      "grad_norm": 0.3284033238887787,
      "learning_rate": 1.8318769027556943e-06,
      "loss": 0.0338,
      "step": 10792
    },
    {
      "epoch": 2.632439024390244,
      "grad_norm": 0.15053319931030273,
      "learning_rate": 1.8294782286497698e-06,
      "loss": 0.025,
      "step": 10793
    },
    {
      "epoch": 2.6326829268292684,
      "grad_norm": 0.10021530836820602,
      "learning_rate": 1.827081066349459e-06,
      "loss": 0.0155,
      "step": 10794
    },
    {
      "epoch": 2.6329268292682926,
      "grad_norm": 0.17863237857818604,
      "learning_rate": 1.8246854160111653e-06,
      "loss": 0.017,
      "step": 10795
    },
    {
      "epoch": 2.633170731707317,
      "grad_norm": 0.07383248209953308,
      "learning_rate": 1.8222912777912077e-06,
      "loss": 0.0058,
      "step": 10796
    },
    {
      "epoch": 2.6334146341463414,
      "grad_norm": 0.12370080500841141,
      "learning_rate": 1.8198986518457834e-06,
      "loss": 0.0061,
      "step": 10797
    },
    {
      "epoch": 2.633658536585366,
      "grad_norm": 0.08362514525651932,
      "learning_rate": 1.8175075383310175e-06,
      "loss": 0.01,
      "step": 10798
    },
    {
      "epoch": 2.63390243902439,
      "grad_norm": 0.1047171801328659,
      "learning_rate": 1.815117937402916e-06,
      "loss": 0.0172,
      "step": 10799
    },
    {
      "epoch": 2.6341463414634148,
      "grad_norm": 0.06850653141736984,
      "learning_rate": 1.8127298492173872e-06,
      "loss": 0.0097,
      "step": 10800
    },
    {
      "epoch": 2.634390243902439,
      "grad_norm": 0.23831135034561157,
      "learning_rate": 1.8103432739302595e-06,
      "loss": 0.0185,
      "step": 10801
    },
    {
      "epoch": 2.6346341463414635,
      "grad_norm": 0.07749295234680176,
      "learning_rate": 1.8079582116972415e-06,
      "loss": 0.0104,
      "step": 10802
    },
    {
      "epoch": 2.6348780487804877,
      "grad_norm": 0.17472150921821594,
      "learning_rate": 1.8055746626739478e-06,
      "loss": 0.0172,
      "step": 10803
    },
    {
      "epoch": 2.6351219512195123,
      "grad_norm": 0.08559415489435196,
      "learning_rate": 1.8031926270159094e-06,
      "loss": 0.0071,
      "step": 10804
    },
    {
      "epoch": 2.6353658536585365,
      "grad_norm": 0.10233616828918457,
      "learning_rate": 1.8008121048785354e-06,
      "loss": 0.013,
      "step": 10805
    },
    {
      "epoch": 2.635609756097561,
      "grad_norm": 0.1357327252626419,
      "learning_rate": 1.798433096417157e-06,
      "loss": 0.0257,
      "step": 10806
    },
    {
      "epoch": 2.6358536585365853,
      "grad_norm": 0.09491351246833801,
      "learning_rate": 1.796055601786989e-06,
      "loss": 0.0097,
      "step": 10807
    },
    {
      "epoch": 2.63609756097561,
      "grad_norm": 0.08981490880250931,
      "learning_rate": 1.7936796211431545e-06,
      "loss": 0.0073,
      "step": 10808
    },
    {
      "epoch": 2.636341463414634,
      "grad_norm": 0.10881214588880539,
      "learning_rate": 1.7913051546406878e-06,
      "loss": 0.0264,
      "step": 10809
    },
    {
      "epoch": 2.6365853658536587,
      "grad_norm": 0.1297549605369568,
      "learning_rate": 1.7889322024345095e-06,
      "loss": 0.0141,
      "step": 10810
    },
    {
      "epoch": 2.636829268292683,
      "grad_norm": 0.16347095370292664,
      "learning_rate": 1.7865607646794512e-06,
      "loss": 0.0178,
      "step": 10811
    },
    {
      "epoch": 2.6370731707317074,
      "grad_norm": 0.12647615373134613,
      "learning_rate": 1.7841908415302394e-06,
      "loss": 0.0176,
      "step": 10812
    },
    {
      "epoch": 2.6373170731707316,
      "grad_norm": 0.15072207152843475,
      "learning_rate": 1.7818224331415e-06,
      "loss": 0.0174,
      "step": 10813
    },
    {
      "epoch": 2.637560975609756,
      "grad_norm": 0.23261871933937073,
      "learning_rate": 1.7794555396677765e-06,
      "loss": 0.0151,
      "step": 10814
    },
    {
      "epoch": 2.6378048780487804,
      "grad_norm": 0.2212541699409485,
      "learning_rate": 1.7770901612634894e-06,
      "loss": 0.0186,
      "step": 10815
    },
    {
      "epoch": 2.638048780487805,
      "grad_norm": 0.17081508040428162,
      "learning_rate": 1.7747262980829764e-06,
      "loss": 0.0146,
      "step": 10816
    },
    {
      "epoch": 2.638292682926829,
      "grad_norm": 0.09587621688842773,
      "learning_rate": 1.7723639502804785e-06,
      "loss": 0.0154,
      "step": 10817
    },
    {
      "epoch": 2.638536585365854,
      "grad_norm": 0.12365200370550156,
      "learning_rate": 1.7700031180101217e-06,
      "loss": 0.0151,
      "step": 10818
    },
    {
      "epoch": 2.638780487804878,
      "grad_norm": 0.06290854513645172,
      "learning_rate": 1.7676438014259472e-06,
      "loss": 0.0131,
      "step": 10819
    },
    {
      "epoch": 2.6390243902439026,
      "grad_norm": 0.18675284087657928,
      "learning_rate": 1.7652860006818955e-06,
      "loss": 0.0185,
      "step": 10820
    },
    {
      "epoch": 2.6392682926829267,
      "grad_norm": 0.1604841947555542,
      "learning_rate": 1.762929715931802e-06,
      "loss": 0.0094,
      "step": 10821
    },
    {
      "epoch": 2.6395121951219513,
      "grad_norm": 0.12177857011556625,
      "learning_rate": 1.760574947329413e-06,
      "loss": 0.014,
      "step": 10822
    },
    {
      "epoch": 2.6397560975609755,
      "grad_norm": 0.160456582903862,
      "learning_rate": 1.7582216950283642e-06,
      "loss": 0.0126,
      "step": 10823
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.10296106338500977,
      "learning_rate": 1.7558699591821993e-06,
      "loss": 0.0189,
      "step": 10824
    },
    {
      "epoch": 2.6402439024390243,
      "grad_norm": 0.14375445246696472,
      "learning_rate": 1.7535197399443648e-06,
      "loss": 0.0219,
      "step": 10825
    },
    {
      "epoch": 2.640487804878049,
      "grad_norm": 0.28834354877471924,
      "learning_rate": 1.7511710374681995e-06,
      "loss": 0.0219,
      "step": 10826
    },
    {
      "epoch": 2.640731707317073,
      "grad_norm": 0.11055058985948563,
      "learning_rate": 1.7488238519069528e-06,
      "loss": 0.0144,
      "step": 10827
    },
    {
      "epoch": 2.6409756097560977,
      "grad_norm": 0.15369921922683716,
      "learning_rate": 1.7464781834137744e-06,
      "loss": 0.0175,
      "step": 10828
    },
    {
      "epoch": 2.641219512195122,
      "grad_norm": 0.11454247683286667,
      "learning_rate": 1.7441340321417033e-06,
      "loss": 0.0138,
      "step": 10829
    },
    {
      "epoch": 2.6414634146341465,
      "grad_norm": 0.21998947858810425,
      "learning_rate": 1.7417913982436972e-06,
      "loss": 0.0151,
      "step": 10830
    },
    {
      "epoch": 2.6417073170731706,
      "grad_norm": 0.17311593890190125,
      "learning_rate": 1.7394502818726038e-06,
      "loss": 0.0223,
      "step": 10831
    },
    {
      "epoch": 2.6419512195121952,
      "grad_norm": 0.16231460869312286,
      "learning_rate": 1.7371106831811674e-06,
      "loss": 0.0462,
      "step": 10832
    },
    {
      "epoch": 2.6421951219512194,
      "grad_norm": 0.10353201627731323,
      "learning_rate": 1.734772602322049e-06,
      "loss": 0.0119,
      "step": 10833
    },
    {
      "epoch": 2.642439024390244,
      "grad_norm": 0.254545122385025,
      "learning_rate": 1.7324360394477906e-06,
      "loss": 0.0197,
      "step": 10834
    },
    {
      "epoch": 2.642682926829268,
      "grad_norm": 0.24229931831359863,
      "learning_rate": 1.7301009947108565e-06,
      "loss": 0.0224,
      "step": 10835
    },
    {
      "epoch": 2.642926829268293,
      "grad_norm": 0.05467740446329117,
      "learning_rate": 1.727767468263597e-06,
      "loss": 0.0106,
      "step": 10836
    },
    {
      "epoch": 2.643170731707317,
      "grad_norm": 0.20894969999790192,
      "learning_rate": 1.7254354602582624e-06,
      "loss": 0.0271,
      "step": 10837
    },
    {
      "epoch": 2.6434146341463416,
      "grad_norm": 0.17824748158454895,
      "learning_rate": 1.72310497084702e-06,
      "loss": 0.0144,
      "step": 10838
    },
    {
      "epoch": 2.6436585365853658,
      "grad_norm": 0.08734031766653061,
      "learning_rate": 1.7207760001819173e-06,
      "loss": 0.0125,
      "step": 10839
    },
    {
      "epoch": 2.6439024390243904,
      "grad_norm": 0.09281126409769058,
      "learning_rate": 1.7184485484149194e-06,
      "loss": 0.0137,
      "step": 10840
    },
    {
      "epoch": 2.6441463414634145,
      "grad_norm": 0.23567047715187073,
      "learning_rate": 1.7161226156978793e-06,
      "loss": 0.0154,
      "step": 10841
    },
    {
      "epoch": 2.644390243902439,
      "grad_norm": 0.11168511211872101,
      "learning_rate": 1.7137982021825566e-06,
      "loss": 0.0096,
      "step": 10842
    },
    {
      "epoch": 2.6446341463414633,
      "grad_norm": 0.06411148607730865,
      "learning_rate": 1.7114753080206215e-06,
      "loss": 0.02,
      "step": 10843
    },
    {
      "epoch": 2.644878048780488,
      "grad_norm": 0.120402492582798,
      "learning_rate": 1.7091539333636276e-06,
      "loss": 0.0135,
      "step": 10844
    },
    {
      "epoch": 2.645121951219512,
      "grad_norm": 0.11126870661973953,
      "learning_rate": 1.7068340783630344e-06,
      "loss": 0.0191,
      "step": 10845
    },
    {
      "epoch": 2.6453658536585367,
      "grad_norm": 0.06516410410404205,
      "learning_rate": 1.7045157431702153e-06,
      "loss": 0.0065,
      "step": 10846
    },
    {
      "epoch": 2.645609756097561,
      "grad_norm": 0.08289248496294022,
      "learning_rate": 1.702198927936427e-06,
      "loss": 0.0147,
      "step": 10847
    },
    {
      "epoch": 2.6458536585365855,
      "grad_norm": 0.1787353903055191,
      "learning_rate": 1.6998836328128403e-06,
      "loss": 0.0228,
      "step": 10848
    },
    {
      "epoch": 2.6460975609756097,
      "grad_norm": 0.1393243670463562,
      "learning_rate": 1.6975698579505178e-06,
      "loss": 0.0168,
      "step": 10849
    },
    {
      "epoch": 2.6463414634146343,
      "grad_norm": 0.07180096954107285,
      "learning_rate": 1.695257603500422e-06,
      "loss": 0.0051,
      "step": 10850
    },
    {
      "epoch": 2.6465853658536584,
      "grad_norm": 0.08801008015871048,
      "learning_rate": 1.6929468696134293e-06,
      "loss": 0.0129,
      "step": 10851
    },
    {
      "epoch": 2.646829268292683,
      "grad_norm": 0.30031049251556396,
      "learning_rate": 1.6906376564403025e-06,
      "loss": 0.0095,
      "step": 10852
    },
    {
      "epoch": 2.6470731707317072,
      "grad_norm": 0.19599615037441254,
      "learning_rate": 1.68832996413171e-06,
      "loss": 0.0183,
      "step": 10853
    },
    {
      "epoch": 2.647317073170732,
      "grad_norm": 0.07751081138849258,
      "learning_rate": 1.686023792838226e-06,
      "loss": 0.0069,
      "step": 10854
    },
    {
      "epoch": 2.647560975609756,
      "grad_norm": 0.16879916191101074,
      "learning_rate": 1.683719142710316e-06,
      "loss": 0.0265,
      "step": 10855
    },
    {
      "epoch": 2.6478048780487806,
      "grad_norm": 0.10751079767942429,
      "learning_rate": 1.6814160138983598e-06,
      "loss": 0.0124,
      "step": 10856
    },
    {
      "epoch": 2.648048780487805,
      "grad_norm": 0.08163923770189285,
      "learning_rate": 1.6791144065526205e-06,
      "loss": 0.0099,
      "step": 10857
    },
    {
      "epoch": 2.6482926829268294,
      "grad_norm": 0.10276849567890167,
      "learning_rate": 1.6768143208232722e-06,
      "loss": 0.0165,
      "step": 10858
    },
    {
      "epoch": 2.6485365853658536,
      "grad_norm": 0.1361665427684784,
      "learning_rate": 1.6745157568603926e-06,
      "loss": 0.0093,
      "step": 10859
    },
    {
      "epoch": 2.648780487804878,
      "grad_norm": 0.17454400658607483,
      "learning_rate": 1.6722187148139501e-06,
      "loss": 0.0154,
      "step": 10860
    },
    {
      "epoch": 2.6490243902439023,
      "grad_norm": 0.11701472848653793,
      "learning_rate": 1.6699231948338251e-06,
      "loss": 0.0179,
      "step": 10861
    },
    {
      "epoch": 2.649268292682927,
      "grad_norm": 0.1132192313671112,
      "learning_rate": 1.667629197069795e-06,
      "loss": 0.0244,
      "step": 10862
    },
    {
      "epoch": 2.649512195121951,
      "grad_norm": 0.14086899161338806,
      "learning_rate": 1.6653367216715259e-06,
      "loss": 0.0177,
      "step": 10863
    },
    {
      "epoch": 2.6497560975609757,
      "grad_norm": 0.08670130372047424,
      "learning_rate": 1.663045768788607e-06,
      "loss": 0.0168,
      "step": 10864
    },
    {
      "epoch": 2.65,
      "grad_norm": 0.0813073217868805,
      "learning_rate": 1.66075633857051e-06,
      "loss": 0.0085,
      "step": 10865
    },
    {
      "epoch": 2.6502439024390245,
      "grad_norm": 0.13926509022712708,
      "learning_rate": 1.65846843116661e-06,
      "loss": 0.0236,
      "step": 10866
    },
    {
      "epoch": 2.6504878048780487,
      "grad_norm": 0.14588336646556854,
      "learning_rate": 1.656182046726193e-06,
      "loss": 0.0113,
      "step": 10867
    },
    {
      "epoch": 2.6507317073170733,
      "grad_norm": 0.09589838236570358,
      "learning_rate": 1.6538971853984286e-06,
      "loss": 0.0203,
      "step": 10868
    },
    {
      "epoch": 2.6509756097560975,
      "grad_norm": 0.08731930702924728,
      "learning_rate": 1.6516138473324117e-06,
      "loss": 0.0155,
      "step": 10869
    },
    {
      "epoch": 2.651219512195122,
      "grad_norm": 0.2750001847743988,
      "learning_rate": 1.6493320326771118e-06,
      "loss": 0.0297,
      "step": 10870
    },
    {
      "epoch": 2.6514634146341463,
      "grad_norm": 0.11063438653945923,
      "learning_rate": 1.6470517415814123e-06,
      "loss": 0.0107,
      "step": 10871
    },
    {
      "epoch": 2.651707317073171,
      "grad_norm": 0.10287512093782425,
      "learning_rate": 1.6447729741941032e-06,
      "loss": 0.0172,
      "step": 10872
    },
    {
      "epoch": 2.651951219512195,
      "grad_norm": 0.16151182353496552,
      "learning_rate": 1.642495730663854e-06,
      "loss": 0.0255,
      "step": 10873
    },
    {
      "epoch": 2.6521951219512196,
      "grad_norm": 0.08648859709501266,
      "learning_rate": 1.640220011139254e-06,
      "loss": 0.0078,
      "step": 10874
    },
    {
      "epoch": 2.652439024390244,
      "grad_norm": 0.07429773360490799,
      "learning_rate": 1.6379458157687905e-06,
      "loss": 0.0081,
      "step": 10875
    },
    {
      "epoch": 2.6526829268292684,
      "grad_norm": 0.09486852586269379,
      "learning_rate": 1.635673144700839e-06,
      "loss": 0.0127,
      "step": 10876
    },
    {
      "epoch": 2.6529268292682926,
      "grad_norm": 0.10950976610183716,
      "learning_rate": 1.6334019980836946e-06,
      "loss": 0.012,
      "step": 10877
    },
    {
      "epoch": 2.653170731707317,
      "grad_norm": 0.2298174798488617,
      "learning_rate": 1.631132376065539e-06,
      "loss": 0.0238,
      "step": 10878
    },
    {
      "epoch": 2.6534146341463414,
      "grad_norm": 0.07884273678064346,
      "learning_rate": 1.6288642787944508e-06,
      "loss": 0.018,
      "step": 10879
    },
    {
      "epoch": 2.653658536585366,
      "grad_norm": 0.07567358016967773,
      "learning_rate": 1.6265977064184258e-06,
      "loss": 0.0163,
      "step": 10880
    },
    {
      "epoch": 2.65390243902439,
      "grad_norm": 0.08454173058271408,
      "learning_rate": 1.6243326590853452e-06,
      "loss": 0.024,
      "step": 10881
    },
    {
      "epoch": 2.6541463414634148,
      "grad_norm": 0.0626944974064827,
      "learning_rate": 1.6220691369430024e-06,
      "loss": 0.01,
      "step": 10882
    },
    {
      "epoch": 2.654390243902439,
      "grad_norm": 0.11466018855571747,
      "learning_rate": 1.619807140139082e-06,
      "loss": 0.0103,
      "step": 10883
    },
    {
      "epoch": 2.6546341463414636,
      "grad_norm": 0.1505616307258606,
      "learning_rate": 1.6175466688211682e-06,
      "loss": 0.0166,
      "step": 10884
    },
    {
      "epoch": 2.6548780487804877,
      "grad_norm": 0.09060440957546234,
      "learning_rate": 1.6152877231367576e-06,
      "loss": 0.014,
      "step": 10885
    },
    {
      "epoch": 2.6551219512195123,
      "grad_norm": 0.145745649933815,
      "learning_rate": 1.6130303032332373e-06,
      "loss": 0.0105,
      "step": 10886
    },
    {
      "epoch": 2.6553658536585365,
      "grad_norm": 0.12152478098869324,
      "learning_rate": 1.610774409257887e-06,
      "loss": 0.0103,
      "step": 10887
    },
    {
      "epoch": 2.655609756097561,
      "grad_norm": 0.10026554018259048,
      "learning_rate": 1.6085200413579139e-06,
      "loss": 0.0166,
      "step": 10888
    },
    {
      "epoch": 2.6558536585365853,
      "grad_norm": 0.10227207839488983,
      "learning_rate": 1.6062671996803973e-06,
      "loss": 0.0157,
      "step": 10889
    },
    {
      "epoch": 2.65609756097561,
      "grad_norm": 0.14274081587791443,
      "learning_rate": 1.6040158843723314e-06,
      "loss": 0.016,
      "step": 10890
    },
    {
      "epoch": 2.656341463414634,
      "grad_norm": 0.1337505728006363,
      "learning_rate": 1.6017660955806064e-06,
      "loss": 0.0113,
      "step": 10891
    },
    {
      "epoch": 2.6565853658536587,
      "grad_norm": 0.11685428768396378,
      "learning_rate": 1.5995178334520105e-06,
      "loss": 0.0197,
      "step": 10892
    },
    {
      "epoch": 2.656829268292683,
      "grad_norm": 0.16984495520591736,
      "learning_rate": 1.5972710981332462e-06,
      "loss": 0.0223,
      "step": 10893
    },
    {
      "epoch": 2.6570731707317075,
      "grad_norm": 0.11725838482379913,
      "learning_rate": 1.5950258897708931e-06,
      "loss": 0.0226,
      "step": 10894
    },
    {
      "epoch": 2.6573170731707316,
      "grad_norm": 0.12133339047431946,
      "learning_rate": 1.592782208511459e-06,
      "loss": 0.013,
      "step": 10895
    },
    {
      "epoch": 2.6575609756097562,
      "grad_norm": 0.13421443104743958,
      "learning_rate": 1.5905400545013271e-06,
      "loss": 0.0154,
      "step": 10896
    },
    {
      "epoch": 2.6578048780487804,
      "grad_norm": 0.08903393149375916,
      "learning_rate": 1.5882994278867885e-06,
      "loss": 0.0131,
      "step": 10897
    },
    {
      "epoch": 2.658048780487805,
      "grad_norm": 0.16131620109081268,
      "learning_rate": 1.5860603288140485e-06,
      "loss": 0.0138,
      "step": 10898
    },
    {
      "epoch": 2.658292682926829,
      "grad_norm": 0.17356131970882416,
      "learning_rate": 1.5838227574291931e-06,
      "loss": 0.0215,
      "step": 10899
    },
    {
      "epoch": 2.658536585365854,
      "grad_norm": 0.13325950503349304,
      "learning_rate": 1.5815867138782165e-06,
      "loss": 0.009,
      "step": 10900
    },
    {
      "epoch": 2.658780487804878,
      "grad_norm": 0.11461130529642105,
      "learning_rate": 1.5793521983070186e-06,
      "loss": 0.0113,
      "step": 10901
    },
    {
      "epoch": 2.6590243902439026,
      "grad_norm": 0.08107489347457886,
      "learning_rate": 1.577119210861394e-06,
      "loss": 0.0174,
      "step": 10902
    },
    {
      "epoch": 2.6592682926829267,
      "grad_norm": 0.14876767992973328,
      "learning_rate": 1.574887751687032e-06,
      "loss": 0.0221,
      "step": 10903
    },
    {
      "epoch": 2.6595121951219514,
      "grad_norm": 0.11749747395515442,
      "learning_rate": 1.572657820929535e-06,
      "loss": 0.0167,
      "step": 10904
    },
    {
      "epoch": 2.6597560975609755,
      "grad_norm": 0.12121370434761047,
      "learning_rate": 1.5704294187343954e-06,
      "loss": 0.0162,
      "step": 10905
    },
    {
      "epoch": 2.66,
      "grad_norm": 0.06734269112348557,
      "learning_rate": 1.5682025452470188e-06,
      "loss": 0.017,
      "step": 10906
    },
    {
      "epoch": 2.6602439024390243,
      "grad_norm": 0.12834453582763672,
      "learning_rate": 1.5659772006126894e-06,
      "loss": 0.0113,
      "step": 10907
    },
    {
      "epoch": 2.660487804878049,
      "grad_norm": 0.19222073256969452,
      "learning_rate": 1.56375338497661e-06,
      "loss": 0.0174,
      "step": 10908
    },
    {
      "epoch": 2.660731707317073,
      "grad_norm": 0.17869697511196136,
      "learning_rate": 1.561531098483876e-06,
      "loss": 0.0199,
      "step": 10909
    },
    {
      "epoch": 2.6609756097560977,
      "grad_norm": 0.1913762092590332,
      "learning_rate": 1.5593103412794853e-06,
      "loss": 0.0107,
      "step": 10910
    },
    {
      "epoch": 2.661219512195122,
      "grad_norm": 0.22416599094867706,
      "learning_rate": 1.5570911135083383e-06,
      "loss": 0.0359,
      "step": 10911
    },
    {
      "epoch": 2.6614634146341465,
      "grad_norm": 0.08320656418800354,
      "learning_rate": 1.5548734153152277e-06,
      "loss": 0.0105,
      "step": 10912
    },
    {
      "epoch": 2.6617073170731707,
      "grad_norm": 0.11976500600576401,
      "learning_rate": 1.5526572468448547e-06,
      "loss": 0.0138,
      "step": 10913
    },
    {
      "epoch": 2.6619512195121953,
      "grad_norm": 0.10138783603906631,
      "learning_rate": 1.55044260824182e-06,
      "loss": 0.0104,
      "step": 10914
    },
    {
      "epoch": 2.6621951219512194,
      "grad_norm": 0.1072663888335228,
      "learning_rate": 1.5482294996506164e-06,
      "loss": 0.0119,
      "step": 10915
    },
    {
      "epoch": 2.662439024390244,
      "grad_norm": 0.12290538847446442,
      "learning_rate": 1.546017921215645e-06,
      "loss": 0.0178,
      "step": 10916
    },
    {
      "epoch": 2.662682926829268,
      "grad_norm": 0.16988597810268402,
      "learning_rate": 1.5438078730812072e-06,
      "loss": 0.0134,
      "step": 10917
    },
    {
      "epoch": 2.662926829268293,
      "grad_norm": 0.21137695014476776,
      "learning_rate": 1.5415993553914932e-06,
      "loss": 0.0182,
      "step": 10918
    },
    {
      "epoch": 2.663170731707317,
      "grad_norm": 0.14471623301506042,
      "learning_rate": 1.5393923682906152e-06,
      "loss": 0.0262,
      "step": 10919
    },
    {
      "epoch": 2.6634146341463416,
      "grad_norm": 0.13806049525737762,
      "learning_rate": 1.537186911922564e-06,
      "loss": 0.0115,
      "step": 10920
    },
    {
      "epoch": 2.6636585365853658,
      "grad_norm": 0.08731032907962799,
      "learning_rate": 1.5349829864312383e-06,
      "loss": 0.011,
      "step": 10921
    },
    {
      "epoch": 2.6639024390243904,
      "grad_norm": 0.11488208174705505,
      "learning_rate": 1.532780591960445e-06,
      "loss": 0.0261,
      "step": 10922
    },
    {
      "epoch": 2.6641463414634146,
      "grad_norm": 0.13879382610321045,
      "learning_rate": 1.5305797286538692e-06,
      "loss": 0.013,
      "step": 10923
    },
    {
      "epoch": 2.664390243902439,
      "grad_norm": 0.09644719213247299,
      "learning_rate": 1.528380396655127e-06,
      "loss": 0.0061,
      "step": 10924
    },
    {
      "epoch": 2.6646341463414633,
      "grad_norm": 0.1098787933588028,
      "learning_rate": 1.5261825961077058e-06,
      "loss": 0.0115,
      "step": 10925
    },
    {
      "epoch": 2.664878048780488,
      "grad_norm": 0.13735949993133545,
      "learning_rate": 1.5239863271550108e-06,
      "loss": 0.015,
      "step": 10926
    },
    {
      "epoch": 2.665121951219512,
      "grad_norm": 0.11988382786512375,
      "learning_rate": 1.5217915899403411e-06,
      "loss": 0.0169,
      "step": 10927
    },
    {
      "epoch": 2.6653658536585367,
      "grad_norm": 0.1583365797996521,
      "learning_rate": 1.5195983846068935e-06,
      "loss": 0.0127,
      "step": 10928
    },
    {
      "epoch": 2.665609756097561,
      "grad_norm": 0.11796455085277557,
      "learning_rate": 1.5174067112977752e-06,
      "loss": 0.0125,
      "step": 10929
    },
    {
      "epoch": 2.6658536585365855,
      "grad_norm": 0.12357158213853836,
      "learning_rate": 1.5152165701559833e-06,
      "loss": 0.0106,
      "step": 10930
    },
    {
      "epoch": 2.6660975609756097,
      "grad_norm": 0.09821861237287521,
      "learning_rate": 1.513027961324412e-06,
      "loss": 0.0178,
      "step": 10931
    },
    {
      "epoch": 2.6663414634146343,
      "grad_norm": 0.1488649994134903,
      "learning_rate": 1.5108408849458689e-06,
      "loss": 0.0213,
      "step": 10932
    },
    {
      "epoch": 2.6665853658536585,
      "grad_norm": 0.10100618749856949,
      "learning_rate": 1.5086553411630483e-06,
      "loss": 0.0094,
      "step": 10933
    },
    {
      "epoch": 2.666829268292683,
      "grad_norm": 0.1215449869632721,
      "learning_rate": 1.506471330118553e-06,
      "loss": 0.0116,
      "step": 10934
    },
    {
      "epoch": 2.6670731707317072,
      "grad_norm": 0.10001598298549652,
      "learning_rate": 1.5042888519548853e-06,
      "loss": 0.0274,
      "step": 10935
    },
    {
      "epoch": 2.667317073170732,
      "grad_norm": 0.15953616797924042,
      "learning_rate": 1.5021079068144428e-06,
      "loss": 0.011,
      "step": 10936
    },
    {
      "epoch": 2.667560975609756,
      "grad_norm": 0.06445268541574478,
      "learning_rate": 1.4999284948395198e-06,
      "loss": 0.0049,
      "step": 10937
    },
    {
      "epoch": 2.6678048780487806,
      "grad_norm": 0.21184073388576508,
      "learning_rate": 1.4977506161723277e-06,
      "loss": 0.0196,
      "step": 10938
    },
    {
      "epoch": 2.668048780487805,
      "grad_norm": 0.24438953399658203,
      "learning_rate": 1.4955742709549608e-06,
      "loss": 0.0206,
      "step": 10939
    },
    {
      "epoch": 2.6682926829268294,
      "grad_norm": 0.08664246648550034,
      "learning_rate": 1.493399459329417e-06,
      "loss": 0.0183,
      "step": 10940
    },
    {
      "epoch": 2.6685365853658536,
      "grad_norm": 0.10971032828092575,
      "learning_rate": 1.4912261814375937e-06,
      "loss": 0.0229,
      "step": 10941
    },
    {
      "epoch": 2.668780487804878,
      "grad_norm": 0.13558271527290344,
      "learning_rate": 1.4890544374213e-06,
      "loss": 0.0123,
      "step": 10942
    },
    {
      "epoch": 2.6690243902439024,
      "grad_norm": 0.14120909571647644,
      "learning_rate": 1.4868842274222306e-06,
      "loss": 0.0116,
      "step": 10943
    },
    {
      "epoch": 2.669268292682927,
      "grad_norm": 0.10301737487316132,
      "learning_rate": 1.4847155515819806e-06,
      "loss": 0.0181,
      "step": 10944
    },
    {
      "epoch": 2.669512195121951,
      "grad_norm": 0.2144748866558075,
      "learning_rate": 1.4825484100420588e-06,
      "loss": 0.0174,
      "step": 10945
    },
    {
      "epoch": 2.6697560975609758,
      "grad_norm": 0.19380857050418854,
      "learning_rate": 1.4803828029438583e-06,
      "loss": 0.0225,
      "step": 10946
    },
    {
      "epoch": 2.67,
      "grad_norm": 0.09230585396289825,
      "learning_rate": 1.4782187304286764e-06,
      "loss": 0.0147,
      "step": 10947
    },
    {
      "epoch": 2.6702439024390245,
      "grad_norm": 0.09934388846158981,
      "learning_rate": 1.4760561926377176e-06,
      "loss": 0.0109,
      "step": 10948
    },
    {
      "epoch": 2.6704878048780487,
      "grad_norm": 0.12628807127475739,
      "learning_rate": 1.4738951897120794e-06,
      "loss": 0.0219,
      "step": 10949
    },
    {
      "epoch": 2.6707317073170733,
      "grad_norm": 0.16707222163677216,
      "learning_rate": 1.4717357217927552e-06,
      "loss": 0.0273,
      "step": 10950
    },
    {
      "epoch": 2.6709756097560975,
      "grad_norm": 0.10751429945230484,
      "learning_rate": 1.4695777890206514e-06,
      "loss": 0.0159,
      "step": 10951
    },
    {
      "epoch": 2.671219512195122,
      "grad_norm": 0.11696041375398636,
      "learning_rate": 1.467421391536561e-06,
      "loss": 0.0172,
      "step": 10952
    },
    {
      "epoch": 2.6714634146341463,
      "grad_norm": 0.07355649024248123,
      "learning_rate": 1.465266529481188e-06,
      "loss": 0.0108,
      "step": 10953
    },
    {
      "epoch": 2.671707317073171,
      "grad_norm": 0.0810866430401802,
      "learning_rate": 1.4631132029951282e-06,
      "loss": 0.0116,
      "step": 10954
    },
    {
      "epoch": 2.671951219512195,
      "grad_norm": 0.15994545817375183,
      "learning_rate": 1.460961412218878e-06,
      "loss": 0.0209,
      "step": 10955
    },
    {
      "epoch": 2.6721951219512197,
      "grad_norm": 0.24260397255420685,
      "learning_rate": 1.4588111572928358e-06,
      "loss": 0.0126,
      "step": 10956
    },
    {
      "epoch": 2.672439024390244,
      "grad_norm": 0.10913436114788055,
      "learning_rate": 1.4566624383572953e-06,
      "loss": 0.0155,
      "step": 10957
    },
    {
      "epoch": 2.6726829268292684,
      "grad_norm": 0.07721097767353058,
      "learning_rate": 1.4545152555524605e-06,
      "loss": 0.0096,
      "step": 10958
    },
    {
      "epoch": 2.6729268292682926,
      "grad_norm": 0.16809117794036865,
      "learning_rate": 1.4523696090184251e-06,
      "loss": 0.0185,
      "step": 10959
    },
    {
      "epoch": 2.6731707317073172,
      "grad_norm": 0.10878527909517288,
      "learning_rate": 1.4502254988951858e-06,
      "loss": 0.0153,
      "step": 10960
    },
    {
      "epoch": 2.6734146341463414,
      "grad_norm": 0.09085768461227417,
      "learning_rate": 1.4480829253226414e-06,
      "loss": 0.0164,
      "step": 10961
    },
    {
      "epoch": 2.673658536585366,
      "grad_norm": 0.08471881598234177,
      "learning_rate": 1.4459418884405856e-06,
      "loss": 0.0166,
      "step": 10962
    },
    {
      "epoch": 2.67390243902439,
      "grad_norm": 0.10944265872240067,
      "learning_rate": 1.4438023883887153e-06,
      "loss": 0.0181,
      "step": 10963
    },
    {
      "epoch": 2.674146341463415,
      "grad_norm": 0.1958717703819275,
      "learning_rate": 1.441664425306627e-06,
      "loss": 0.0143,
      "step": 10964
    },
    {
      "epoch": 2.674390243902439,
      "grad_norm": 0.14718131721019745,
      "learning_rate": 1.4395279993338146e-06,
      "loss": 0.0168,
      "step": 10965
    },
    {
      "epoch": 2.6746341463414636,
      "grad_norm": 0.280246764421463,
      "learning_rate": 1.4373931106096778e-06,
      "loss": 0.0251,
      "step": 10966
    },
    {
      "epoch": 2.6748780487804877,
      "grad_norm": 0.10574908554553986,
      "learning_rate": 1.4352597592735079e-06,
      "loss": 0.0131,
      "step": 10967
    },
    {
      "epoch": 2.6751219512195124,
      "grad_norm": 0.0737352967262268,
      "learning_rate": 1.4331279454644963e-06,
      "loss": 0.0089,
      "step": 10968
    },
    {
      "epoch": 2.6753658536585365,
      "grad_norm": 0.13997061550617218,
      "learning_rate": 1.4309976693217457e-06,
      "loss": 0.0158,
      "step": 10969
    },
    {
      "epoch": 2.675609756097561,
      "grad_norm": 0.11478610336780548,
      "learning_rate": 1.428868930984245e-06,
      "loss": 0.015,
      "step": 10970
    },
    {
      "epoch": 2.6758536585365853,
      "grad_norm": 0.13770896196365356,
      "learning_rate": 1.426741730590886e-06,
      "loss": 0.0182,
      "step": 10971
    },
    {
      "epoch": 2.67609756097561,
      "grad_norm": 0.13824240863323212,
      "learning_rate": 1.424616068280471e-06,
      "loss": 0.0203,
      "step": 10972
    },
    {
      "epoch": 2.676341463414634,
      "grad_norm": 0.15788082778453827,
      "learning_rate": 1.4224919441916784e-06,
      "loss": 0.0199,
      "step": 10973
    },
    {
      "epoch": 2.6765853658536587,
      "grad_norm": 0.18325930833816528,
      "learning_rate": 1.4203693584631112e-06,
      "loss": 0.0295,
      "step": 10974
    },
    {
      "epoch": 2.676829268292683,
      "grad_norm": 0.1339101642370224,
      "learning_rate": 1.4182483112332613e-06,
      "loss": 0.0146,
      "step": 10975
    },
    {
      "epoch": 2.6770731707317075,
      "grad_norm": 0.10236217826604843,
      "learning_rate": 1.416128802640515e-06,
      "loss": 0.0108,
      "step": 10976
    },
    {
      "epoch": 2.6773170731707316,
      "grad_norm": 0.14416305720806122,
      "learning_rate": 1.4140108328231704e-06,
      "loss": 0.01,
      "step": 10977
    },
    {
      "epoch": 2.6775609756097563,
      "grad_norm": 0.0815337523818016,
      "learning_rate": 1.4118944019194114e-06,
      "loss": 0.0168,
      "step": 10978
    },
    {
      "epoch": 2.6778048780487804,
      "grad_norm": 0.08860072493553162,
      "learning_rate": 1.4097795100673356e-06,
      "loss": 0.0174,
      "step": 10979
    },
    {
      "epoch": 2.678048780487805,
      "grad_norm": 0.23077772557735443,
      "learning_rate": 1.4076661574049299e-06,
      "loss": 0.0256,
      "step": 10980
    },
    {
      "epoch": 2.678292682926829,
      "grad_norm": 0.15220624208450317,
      "learning_rate": 1.4055543440700813e-06,
      "loss": 0.0215,
      "step": 10981
    },
    {
      "epoch": 2.678536585365854,
      "grad_norm": 0.19079917669296265,
      "learning_rate": 1.403444070200588e-06,
      "loss": 0.0264,
      "step": 10982
    },
    {
      "epoch": 2.678780487804878,
      "grad_norm": 0.07972359657287598,
      "learning_rate": 1.4013353359341314e-06,
      "loss": 0.0132,
      "step": 10983
    },
    {
      "epoch": 2.6790243902439026,
      "grad_norm": 0.06900090724229813,
      "learning_rate": 1.399228141408296e-06,
      "loss": 0.0076,
      "step": 10984
    },
    {
      "epoch": 2.6792682926829268,
      "grad_norm": 0.09700727462768555,
      "learning_rate": 1.3971224867605826e-06,
      "loss": 0.0148,
      "step": 10985
    },
    {
      "epoch": 2.6795121951219514,
      "grad_norm": 0.20120546221733093,
      "learning_rate": 1.395018372128365e-06,
      "loss": 0.0213,
      "step": 10986
    },
    {
      "epoch": 2.6797560975609755,
      "grad_norm": 0.09183236211538315,
      "learning_rate": 1.3929157976489414e-06,
      "loss": 0.0094,
      "step": 10987
    },
    {
      "epoch": 2.68,
      "grad_norm": 0.13210271298885345,
      "learning_rate": 1.3908147634594999e-06,
      "loss": 0.0214,
      "step": 10988
    },
    {
      "epoch": 2.6802439024390243,
      "grad_norm": 0.1834026575088501,
      "learning_rate": 1.3887152696971106e-06,
      "loss": 0.0327,
      "step": 10989
    },
    {
      "epoch": 2.680487804878049,
      "grad_norm": 0.1363794058561325,
      "learning_rate": 1.3866173164987727e-06,
      "loss": 0.0148,
      "step": 10990
    },
    {
      "epoch": 2.680731707317073,
      "grad_norm": 0.13005322217941284,
      "learning_rate": 1.384520904001363e-06,
      "loss": 0.012,
      "step": 10991
    },
    {
      "epoch": 2.6809756097560977,
      "grad_norm": 0.08212193846702576,
      "learning_rate": 1.382426032341677e-06,
      "loss": 0.0265,
      "step": 10992
    },
    {
      "epoch": 2.681219512195122,
      "grad_norm": 0.19106163084506989,
      "learning_rate": 1.3803327016563894e-06,
      "loss": 0.0145,
      "step": 10993
    },
    {
      "epoch": 2.6814634146341465,
      "grad_norm": 0.22075898945331573,
      "learning_rate": 1.3782409120820821e-06,
      "loss": 0.0109,
      "step": 10994
    },
    {
      "epoch": 2.6817073170731707,
      "grad_norm": 0.15356487035751343,
      "learning_rate": 1.376150663755249e-06,
      "loss": 0.0183,
      "step": 10995
    },
    {
      "epoch": 2.6819512195121953,
      "grad_norm": 0.17488215863704681,
      "learning_rate": 1.3740619568122616e-06,
      "loss": 0.0178,
      "step": 10996
    },
    {
      "epoch": 2.6821951219512195,
      "grad_norm": 0.13516607880592346,
      "learning_rate": 1.3719747913894049e-06,
      "loss": 0.0127,
      "step": 10997
    },
    {
      "epoch": 2.682439024390244,
      "grad_norm": 0.07940243184566498,
      "learning_rate": 1.3698891676228648e-06,
      "loss": 0.0132,
      "step": 10998
    },
    {
      "epoch": 2.6826829268292682,
      "grad_norm": 0.1500696986913681,
      "learning_rate": 1.3678050856487129e-06,
      "loss": 0.0163,
      "step": 10999
    },
    {
      "epoch": 2.682926829268293,
      "grad_norm": 0.06476812809705734,
      "learning_rate": 1.3657225456029376e-06,
      "loss": 0.0114,
      "step": 11000
    },
    {
      "epoch": 2.683170731707317,
      "grad_norm": 0.21389009058475494,
      "learning_rate": 1.3636415476214137e-06,
      "loss": 0.0217,
      "step": 11001
    },
    {
      "epoch": 2.6834146341463416,
      "grad_norm": 0.159785196185112,
      "learning_rate": 1.3615620918399185e-06,
      "loss": 0.0164,
      "step": 11002
    },
    {
      "epoch": 2.683658536585366,
      "grad_norm": 0.1935209184885025,
      "learning_rate": 1.3594841783941353e-06,
      "loss": 0.0306,
      "step": 11003
    },
    {
      "epoch": 2.6839024390243904,
      "grad_norm": 0.15151691436767578,
      "learning_rate": 1.357407807419639e-06,
      "loss": 0.0177,
      "step": 11004
    },
    {
      "epoch": 2.6841463414634146,
      "grad_norm": 0.1249428316950798,
      "learning_rate": 1.355332979051907e-06,
      "loss": 0.0147,
      "step": 11005
    },
    {
      "epoch": 2.684390243902439,
      "grad_norm": 0.10054038465023041,
      "learning_rate": 1.3532596934263174e-06,
      "loss": 0.0105,
      "step": 11006
    },
    {
      "epoch": 2.6846341463414634,
      "grad_norm": 0.14151008427143097,
      "learning_rate": 1.3511879506781371e-06,
      "loss": 0.0148,
      "step": 11007
    },
    {
      "epoch": 2.684878048780488,
      "grad_norm": 0.16661185026168823,
      "learning_rate": 1.3491177509425496e-06,
      "loss": 0.0202,
      "step": 11008
    },
    {
      "epoch": 2.685121951219512,
      "grad_norm": 0.1531098335981369,
      "learning_rate": 1.347049094354627e-06,
      "loss": 0.0159,
      "step": 11009
    },
    {
      "epoch": 2.6853658536585368,
      "grad_norm": 0.15356279909610748,
      "learning_rate": 1.3449819810493397e-06,
      "loss": 0.0206,
      "step": 11010
    },
    {
      "epoch": 2.685609756097561,
      "grad_norm": 0.14584989845752716,
      "learning_rate": 1.3429164111615656e-06,
      "loss": 0.0132,
      "step": 11011
    },
    {
      "epoch": 2.6858536585365855,
      "grad_norm": 0.13167621195316315,
      "learning_rate": 1.3408523848260719e-06,
      "loss": 0.0132,
      "step": 11012
    },
    {
      "epoch": 2.6860975609756097,
      "grad_norm": 0.1566343456506729,
      "learning_rate": 1.3387899021775368e-06,
      "loss": 0.0168,
      "step": 11013
    },
    {
      "epoch": 2.6863414634146343,
      "grad_norm": 0.16323703527450562,
      "learning_rate": 1.3367289633505253e-06,
      "loss": 0.0193,
      "step": 11014
    },
    {
      "epoch": 2.6865853658536585,
      "grad_norm": 0.15425005555152893,
      "learning_rate": 1.3346695684795046e-06,
      "loss": 0.0137,
      "step": 11015
    },
    {
      "epoch": 2.686829268292683,
      "grad_norm": 0.11704616248607635,
      "learning_rate": 1.3326117176988534e-06,
      "loss": 0.0212,
      "step": 11016
    },
    {
      "epoch": 2.6870731707317073,
      "grad_norm": 0.15223070979118347,
      "learning_rate": 1.3305554111428336e-06,
      "loss": 0.017,
      "step": 11017
    },
    {
      "epoch": 2.687317073170732,
      "grad_norm": 0.04843626543879509,
      "learning_rate": 1.3285006489456103e-06,
      "loss": 0.0094,
      "step": 11018
    },
    {
      "epoch": 2.687560975609756,
      "grad_norm": 0.16143371164798737,
      "learning_rate": 1.3264474312412595e-06,
      "loss": 0.022,
      "step": 11019
    },
    {
      "epoch": 2.68780487804878,
      "grad_norm": 0.11267705261707306,
      "learning_rate": 1.3243957581637406e-06,
      "loss": 0.0173,
      "step": 11020
    },
    {
      "epoch": 2.688048780487805,
      "grad_norm": 0.1392238885164261,
      "learning_rate": 1.3223456298469216e-06,
      "loss": 0.021,
      "step": 11021
    },
    {
      "epoch": 2.6882926829268294,
      "grad_norm": 0.281382292509079,
      "learning_rate": 1.320297046424565e-06,
      "loss": 0.0276,
      "step": 11022
    },
    {
      "epoch": 2.6885365853658536,
      "grad_norm": 0.07591290026903152,
      "learning_rate": 1.3182500080303334e-06,
      "loss": 0.0169,
      "step": 11023
    },
    {
      "epoch": 2.6887804878048778,
      "grad_norm": 0.1520780771970749,
      "learning_rate": 1.3162045147977948e-06,
      "loss": 0.0161,
      "step": 11024
    },
    {
      "epoch": 2.6890243902439024,
      "grad_norm": 0.14155876636505127,
      "learning_rate": 1.3141605668604035e-06,
      "loss": 0.0192,
      "step": 11025
    },
    {
      "epoch": 2.689268292682927,
      "grad_norm": 0.0550183430314064,
      "learning_rate": 1.3121181643515307e-06,
      "loss": 0.0068,
      "step": 11026
    },
    {
      "epoch": 2.689512195121951,
      "grad_norm": 0.08266853541135788,
      "learning_rate": 1.3100773074044308e-06,
      "loss": 0.0134,
      "step": 11027
    },
    {
      "epoch": 2.6897560975609753,
      "grad_norm": 0.134452685713768,
      "learning_rate": 1.3080379961522643e-06,
      "loss": 0.023,
      "step": 11028
    },
    {
      "epoch": 2.69,
      "grad_norm": 0.14330165088176727,
      "learning_rate": 1.3060002307280912e-06,
      "loss": 0.0208,
      "step": 11029
    },
    {
      "epoch": 2.6902439024390246,
      "grad_norm": 0.1631453037261963,
      "learning_rate": 1.3039640112648688e-06,
      "loss": 0.0165,
      "step": 11030
    },
    {
      "epoch": 2.6904878048780487,
      "grad_norm": 0.11916952580213547,
      "learning_rate": 1.3019293378954523e-06,
      "loss": 0.0182,
      "step": 11031
    },
    {
      "epoch": 2.690731707317073,
      "grad_norm": 0.13127802312374115,
      "learning_rate": 1.299896210752602e-06,
      "loss": 0.0254,
      "step": 11032
    },
    {
      "epoch": 2.6909756097560975,
      "grad_norm": 0.1861453354358673,
      "learning_rate": 1.2978646299689729e-06,
      "loss": 0.0118,
      "step": 11033
    },
    {
      "epoch": 2.691219512195122,
      "grad_norm": 0.17135000228881836,
      "learning_rate": 1.2958345956771118e-06,
      "loss": 0.0271,
      "step": 11034
    },
    {
      "epoch": 2.6914634146341463,
      "grad_norm": 0.18883375823497772,
      "learning_rate": 1.293806108009482e-06,
      "loss": 0.0107,
      "step": 11035
    },
    {
      "epoch": 2.6917073170731705,
      "grad_norm": 0.07508140057325363,
      "learning_rate": 1.291779167098428e-06,
      "loss": 0.0053,
      "step": 11036
    },
    {
      "epoch": 2.691951219512195,
      "grad_norm": 0.1083732321858406,
      "learning_rate": 1.2897537730762106e-06,
      "loss": 0.0132,
      "step": 11037
    },
    {
      "epoch": 2.6921951219512197,
      "grad_norm": 0.08221931010484695,
      "learning_rate": 1.2877299260749765e-06,
      "loss": 0.0093,
      "step": 11038
    },
    {
      "epoch": 2.692439024390244,
      "grad_norm": 0.1468929797410965,
      "learning_rate": 1.2857076262267704e-06,
      "loss": 0.0132,
      "step": 11039
    },
    {
      "epoch": 2.692682926829268,
      "grad_norm": 0.15550045669078827,
      "learning_rate": 1.2836868736635476e-06,
      "loss": 0.0227,
      "step": 11040
    },
    {
      "epoch": 2.6929268292682926,
      "grad_norm": 0.10720556229352951,
      "learning_rate": 1.2816676685171502e-06,
      "loss": 0.0128,
      "step": 11041
    },
    {
      "epoch": 2.6931707317073172,
      "grad_norm": 0.10371950268745422,
      "learning_rate": 1.2796500109193337e-06,
      "loss": 0.0134,
      "step": 11042
    },
    {
      "epoch": 2.6934146341463414,
      "grad_norm": 0.09662837535142899,
      "learning_rate": 1.2776339010017374e-06,
      "loss": 0.0057,
      "step": 11043
    },
    {
      "epoch": 2.6936585365853656,
      "grad_norm": 0.15176501870155334,
      "learning_rate": 1.2756193388959031e-06,
      "loss": 0.0196,
      "step": 11044
    },
    {
      "epoch": 2.69390243902439,
      "grad_norm": 0.13507038354873657,
      "learning_rate": 1.273606324733284e-06,
      "loss": 0.0165,
      "step": 11045
    },
    {
      "epoch": 2.694146341463415,
      "grad_norm": 0.21820184588432312,
      "learning_rate": 1.2715948586452197e-06,
      "loss": 0.0179,
      "step": 11046
    },
    {
      "epoch": 2.694390243902439,
      "grad_norm": 0.10981292277574539,
      "learning_rate": 1.2695849407629468e-06,
      "loss": 0.0202,
      "step": 11047
    },
    {
      "epoch": 2.694634146341463,
      "grad_norm": 0.08280587196350098,
      "learning_rate": 1.2675765712176157e-06,
      "loss": 0.0131,
      "step": 11048
    },
    {
      "epoch": 2.6948780487804878,
      "grad_norm": 0.13760940730571747,
      "learning_rate": 1.2655697501402553e-06,
      "loss": 0.0234,
      "step": 11049
    },
    {
      "epoch": 2.6951219512195124,
      "grad_norm": 0.17069564759731293,
      "learning_rate": 1.2635644776618161e-06,
      "loss": 0.0179,
      "step": 11050
    },
    {
      "epoch": 2.6953658536585365,
      "grad_norm": 0.14729008078575134,
      "learning_rate": 1.2615607539131324e-06,
      "loss": 0.0203,
      "step": 11051
    },
    {
      "epoch": 2.6956097560975607,
      "grad_norm": 0.13187982141971588,
      "learning_rate": 1.2595585790249332e-06,
      "loss": 0.0184,
      "step": 11052
    },
    {
      "epoch": 2.6958536585365853,
      "grad_norm": 0.14176255464553833,
      "learning_rate": 1.2575579531278635e-06,
      "loss": 0.0211,
      "step": 11053
    },
    {
      "epoch": 2.69609756097561,
      "grad_norm": 0.09183415025472641,
      "learning_rate": 1.2555588763524557e-06,
      "loss": 0.0156,
      "step": 11054
    },
    {
      "epoch": 2.696341463414634,
      "grad_norm": 0.1663963794708252,
      "learning_rate": 1.253561348829141e-06,
      "loss": 0.0257,
      "step": 11055
    },
    {
      "epoch": 2.6965853658536583,
      "grad_norm": 0.12539346516132355,
      "learning_rate": 1.251565370688254e-06,
      "loss": 0.0174,
      "step": 11056
    },
    {
      "epoch": 2.696829268292683,
      "grad_norm": 0.22758640348911285,
      "learning_rate": 1.2495709420600243e-06,
      "loss": 0.0164,
      "step": 11057
    },
    {
      "epoch": 2.6970731707317075,
      "grad_norm": 0.07615290582180023,
      "learning_rate": 1.2475780630745837e-06,
      "loss": 0.0121,
      "step": 11058
    },
    {
      "epoch": 2.6973170731707317,
      "grad_norm": 0.2597818374633789,
      "learning_rate": 1.2455867338619615e-06,
      "loss": 0.0312,
      "step": 11059
    },
    {
      "epoch": 2.697560975609756,
      "grad_norm": 0.09655483067035675,
      "learning_rate": 1.243596954552087e-06,
      "loss": 0.0141,
      "step": 11060
    },
    {
      "epoch": 2.6978048780487804,
      "grad_norm": 0.11943608522415161,
      "learning_rate": 1.2416087252747844e-06,
      "loss": 0.0192,
      "step": 11061
    },
    {
      "epoch": 2.698048780487805,
      "grad_norm": 0.16182641685009003,
      "learning_rate": 1.2396220461597802e-06,
      "loss": 0.0229,
      "step": 11062
    },
    {
      "epoch": 2.6982926829268292,
      "grad_norm": 0.16279226541519165,
      "learning_rate": 1.2376369173367043e-06,
      "loss": 0.0206,
      "step": 11063
    },
    {
      "epoch": 2.6985365853658534,
      "grad_norm": 0.27842217683792114,
      "learning_rate": 1.235653338935072e-06,
      "loss": 0.0342,
      "step": 11064
    },
    {
      "epoch": 2.698780487804878,
      "grad_norm": 0.10642264783382416,
      "learning_rate": 1.2336713110843085e-06,
      "loss": 0.0122,
      "step": 11065
    },
    {
      "epoch": 2.6990243902439026,
      "grad_norm": 0.15368308126926422,
      "learning_rate": 1.23169083391374e-06,
      "loss": 0.0173,
      "step": 11066
    },
    {
      "epoch": 2.699268292682927,
      "grad_norm": 0.27629515528678894,
      "learning_rate": 1.229711907552583e-06,
      "loss": 0.0171,
      "step": 11067
    },
    {
      "epoch": 2.699512195121951,
      "grad_norm": 0.06738743931055069,
      "learning_rate": 1.227734532129951e-06,
      "loss": 0.0126,
      "step": 11068
    },
    {
      "epoch": 2.6997560975609756,
      "grad_norm": 0.270916610956192,
      "learning_rate": 1.2257587077748738e-06,
      "loss": 0.0273,
      "step": 11069
    },
    {
      "epoch": 2.7,
      "grad_norm": 0.11862517148256302,
      "learning_rate": 1.223784434616254e-06,
      "loss": 0.0132,
      "step": 11070
    },
    {
      "epoch": 2.7002439024390243,
      "grad_norm": 0.291790634393692,
      "learning_rate": 1.221811712782922e-06,
      "loss": 0.0209,
      "step": 11071
    },
    {
      "epoch": 2.7004878048780485,
      "grad_norm": 0.11031472682952881,
      "learning_rate": 1.2198405424035774e-06,
      "loss": 0.0121,
      "step": 11072
    },
    {
      "epoch": 2.700731707317073,
      "grad_norm": 0.07925241440534592,
      "learning_rate": 1.2178709236068425e-06,
      "loss": 0.0103,
      "step": 11073
    },
    {
      "epoch": 2.7009756097560977,
      "grad_norm": 0.11514420807361603,
      "learning_rate": 1.2159028565212254e-06,
      "loss": 0.0175,
      "step": 11074
    },
    {
      "epoch": 2.701219512195122,
      "grad_norm": 0.13729903101921082,
      "learning_rate": 1.2139363412751349e-06,
      "loss": 0.0126,
      "step": 11075
    },
    {
      "epoch": 2.701463414634146,
      "grad_norm": 0.1144176796078682,
      "learning_rate": 1.2119713779968816e-06,
      "loss": 0.019,
      "step": 11076
    },
    {
      "epoch": 2.7017073170731707,
      "grad_norm": 0.20884639024734497,
      "learning_rate": 1.2100079668146775e-06,
      "loss": 0.0262,
      "step": 11077
    },
    {
      "epoch": 2.7019512195121953,
      "grad_norm": 0.13248766958713531,
      "learning_rate": 1.2080461078566196e-06,
      "loss": 0.011,
      "step": 11078
    },
    {
      "epoch": 2.7021951219512195,
      "grad_norm": 0.29788410663604736,
      "learning_rate": 1.2060858012507254e-06,
      "loss": 0.0282,
      "step": 11079
    },
    {
      "epoch": 2.7024390243902436,
      "grad_norm": 0.08699698746204376,
      "learning_rate": 1.2041270471248895e-06,
      "loss": 0.0215,
      "step": 11080
    },
    {
      "epoch": 2.7026829268292683,
      "grad_norm": 0.14313624799251556,
      "learning_rate": 1.2021698456069153e-06,
      "loss": 0.0161,
      "step": 11081
    },
    {
      "epoch": 2.702926829268293,
      "grad_norm": 0.07529833912849426,
      "learning_rate": 1.2002141968245118e-06,
      "loss": 0.0116,
      "step": 11082
    },
    {
      "epoch": 2.703170731707317,
      "grad_norm": 0.16876676678657532,
      "learning_rate": 1.1982601009052685e-06,
      "loss": 0.0145,
      "step": 11083
    },
    {
      "epoch": 2.703414634146341,
      "grad_norm": 0.2069402039051056,
      "learning_rate": 1.196307557976692e-06,
      "loss": 0.0243,
      "step": 11084
    },
    {
      "epoch": 2.703658536585366,
      "grad_norm": 0.09999687224626541,
      "learning_rate": 1.1943565681661772e-06,
      "loss": 0.0129,
      "step": 11085
    },
    {
      "epoch": 2.7039024390243904,
      "grad_norm": 0.09466566145420074,
      "learning_rate": 1.19240713160102e-06,
      "loss": 0.0169,
      "step": 11086
    },
    {
      "epoch": 2.7041463414634146,
      "grad_norm": 0.09825591742992401,
      "learning_rate": 1.1904592484084182e-06,
      "loss": 0.012,
      "step": 11087
    },
    {
      "epoch": 2.7043902439024388,
      "grad_norm": 0.1088506281375885,
      "learning_rate": 1.1885129187154593e-06,
      "loss": 0.0121,
      "step": 11088
    },
    {
      "epoch": 2.7046341463414634,
      "grad_norm": 0.18634182214736938,
      "learning_rate": 1.1865681426491388e-06,
      "loss": 0.0176,
      "step": 11089
    },
    {
      "epoch": 2.704878048780488,
      "grad_norm": 0.14392128586769104,
      "learning_rate": 1.1846249203363496e-06,
      "loss": 0.0205,
      "step": 11090
    },
    {
      "epoch": 2.705121951219512,
      "grad_norm": 0.2092336118221283,
      "learning_rate": 1.1826832519038738e-06,
      "loss": 0.0146,
      "step": 11091
    },
    {
      "epoch": 2.7053658536585363,
      "grad_norm": 0.06933804601430893,
      "learning_rate": 1.1807431374784072e-06,
      "loss": 0.0086,
      "step": 11092
    },
    {
      "epoch": 2.705609756097561,
      "grad_norm": 0.12132035940885544,
      "learning_rate": 1.1788045771865347e-06,
      "loss": 0.0263,
      "step": 11093
    },
    {
      "epoch": 2.7058536585365855,
      "grad_norm": 0.13367480039596558,
      "learning_rate": 1.1768675711547327e-06,
      "loss": 0.0183,
      "step": 11094
    },
    {
      "epoch": 2.7060975609756097,
      "grad_norm": 0.10897759348154068,
      "learning_rate": 1.1749321195094004e-06,
      "loss": 0.0116,
      "step": 11095
    },
    {
      "epoch": 2.706341463414634,
      "grad_norm": 0.12549175322055817,
      "learning_rate": 1.172998222376806e-06,
      "loss": 0.0163,
      "step": 11096
    },
    {
      "epoch": 2.7065853658536585,
      "grad_norm": 0.13144636154174805,
      "learning_rate": 1.1710658798831376e-06,
      "loss": 0.0258,
      "step": 11097
    },
    {
      "epoch": 2.706829268292683,
      "grad_norm": 0.09752197563648224,
      "learning_rate": 1.1691350921544747e-06,
      "loss": 0.025,
      "step": 11098
    },
    {
      "epoch": 2.7070731707317073,
      "grad_norm": 0.1606871783733368,
      "learning_rate": 1.1672058593167918e-06,
      "loss": 0.0189,
      "step": 11099
    },
    {
      "epoch": 2.7073170731707314,
      "grad_norm": 0.1980937421321869,
      "learning_rate": 1.1652781814959713e-06,
      "loss": 0.0185,
      "step": 11100
    },
    {
      "epoch": 2.707560975609756,
      "grad_norm": 0.16836431622505188,
      "learning_rate": 1.163352058817782e-06,
      "loss": 0.0189,
      "step": 11101
    },
    {
      "epoch": 2.7078048780487807,
      "grad_norm": 0.17581608891487122,
      "learning_rate": 1.161427491407896e-06,
      "loss": 0.0165,
      "step": 11102
    },
    {
      "epoch": 2.708048780487805,
      "grad_norm": 0.14983481168746948,
      "learning_rate": 1.1595044793918958e-06,
      "loss": 0.0116,
      "step": 11103
    },
    {
      "epoch": 2.708292682926829,
      "grad_norm": 0.1365719884634018,
      "learning_rate": 1.1575830228952422e-06,
      "loss": 0.0172,
      "step": 11104
    },
    {
      "epoch": 2.7085365853658536,
      "grad_norm": 0.1584348827600479,
      "learning_rate": 1.1556631220433101e-06,
      "loss": 0.0168,
      "step": 11105
    },
    {
      "epoch": 2.7087804878048782,
      "grad_norm": 0.1916152834892273,
      "learning_rate": 1.153744776961363e-06,
      "loss": 0.0257,
      "step": 11106
    },
    {
      "epoch": 2.7090243902439024,
      "grad_norm": 0.126766636967659,
      "learning_rate": 1.1518279877745675e-06,
      "loss": 0.0194,
      "step": 11107
    },
    {
      "epoch": 2.7092682926829266,
      "grad_norm": 0.13368773460388184,
      "learning_rate": 1.149912754607993e-06,
      "loss": 0.0161,
      "step": 11108
    },
    {
      "epoch": 2.709512195121951,
      "grad_norm": 0.16798578202724457,
      "learning_rate": 1.1479990775865923e-06,
      "loss": 0.0172,
      "step": 11109
    },
    {
      "epoch": 2.709756097560976,
      "grad_norm": 0.06468602269887924,
      "learning_rate": 1.1460869568352406e-06,
      "loss": 0.0097,
      "step": 11110
    },
    {
      "epoch": 2.71,
      "grad_norm": 0.16604655981063843,
      "learning_rate": 1.1441763924786908e-06,
      "loss": 0.0245,
      "step": 11111
    },
    {
      "epoch": 2.710243902439024,
      "grad_norm": 0.14273415505886078,
      "learning_rate": 1.142267384641596e-06,
      "loss": 0.0212,
      "step": 11112
    },
    {
      "epoch": 2.7104878048780487,
      "grad_norm": 0.13288672268390656,
      "learning_rate": 1.1403599334485232e-06,
      "loss": 0.0183,
      "step": 11113
    },
    {
      "epoch": 2.7107317073170734,
      "grad_norm": 0.19691413640975952,
      "learning_rate": 1.1384540390239257e-06,
      "loss": 0.0198,
      "step": 11114
    },
    {
      "epoch": 2.7109756097560975,
      "grad_norm": 0.1590796262025833,
      "learning_rate": 1.1365497014921484e-06,
      "loss": 0.016,
      "step": 11115
    },
    {
      "epoch": 2.7112195121951217,
      "grad_norm": 0.14187410473823547,
      "learning_rate": 1.1346469209774558e-06,
      "loss": 0.0162,
      "step": 11116
    },
    {
      "epoch": 2.7114634146341463,
      "grad_norm": 0.22396515309810638,
      "learning_rate": 1.1327456976039875e-06,
      "loss": 0.0197,
      "step": 11117
    },
    {
      "epoch": 2.711707317073171,
      "grad_norm": 0.12169706076383591,
      "learning_rate": 1.1308460314958025e-06,
      "loss": 0.0273,
      "step": 11118
    },
    {
      "epoch": 2.711951219512195,
      "grad_norm": 0.16438518464565277,
      "learning_rate": 1.1289479227768434e-06,
      "loss": 0.0204,
      "step": 11119
    },
    {
      "epoch": 2.7121951219512193,
      "grad_norm": 0.10344677418470383,
      "learning_rate": 1.1270513715709558e-06,
      "loss": 0.0111,
      "step": 11120
    },
    {
      "epoch": 2.712439024390244,
      "grad_norm": 0.22689557075500488,
      "learning_rate": 1.1251563780018848e-06,
      "loss": 0.0188,
      "step": 11121
    },
    {
      "epoch": 2.7126829268292685,
      "grad_norm": 0.10263828933238983,
      "learning_rate": 1.1232629421932679e-06,
      "loss": 0.0119,
      "step": 11122
    },
    {
      "epoch": 2.7129268292682926,
      "grad_norm": 0.08354169130325317,
      "learning_rate": 1.121371064268653e-06,
      "loss": 0.0168,
      "step": 11123
    },
    {
      "epoch": 2.713170731707317,
      "grad_norm": 0.08168142288923264,
      "learning_rate": 1.1194807443514783e-06,
      "loss": 0.0103,
      "step": 11124
    },
    {
      "epoch": 2.7134146341463414,
      "grad_norm": 0.1276344656944275,
      "learning_rate": 1.117591982565075e-06,
      "loss": 0.0178,
      "step": 11125
    },
    {
      "epoch": 2.713658536585366,
      "grad_norm": 0.18446466326713562,
      "learning_rate": 1.1157047790326864e-06,
      "loss": 0.0193,
      "step": 11126
    },
    {
      "epoch": 2.71390243902439,
      "grad_norm": 0.06739477068185806,
      "learning_rate": 1.1138191338774472e-06,
      "loss": 0.0073,
      "step": 11127
    },
    {
      "epoch": 2.7141463414634144,
      "grad_norm": 0.0642152726650238,
      "learning_rate": 1.1119350472223816e-06,
      "loss": 0.0084,
      "step": 11128
    },
    {
      "epoch": 2.714390243902439,
      "grad_norm": 0.0968051329255104,
      "learning_rate": 1.110052519190427e-06,
      "loss": 0.019,
      "step": 11129
    },
    {
      "epoch": 2.7146341463414636,
      "grad_norm": 0.08583157509565353,
      "learning_rate": 1.1081715499044104e-06,
      "loss": 0.0128,
      "step": 11130
    },
    {
      "epoch": 2.7148780487804878,
      "grad_norm": 0.11174972355365753,
      "learning_rate": 1.106292139487064e-06,
      "loss": 0.0184,
      "step": 11131
    },
    {
      "epoch": 2.715121951219512,
      "grad_norm": 0.10273294150829315,
      "learning_rate": 1.1044142880610092e-06,
      "loss": 0.0178,
      "step": 11132
    },
    {
      "epoch": 2.7153658536585366,
      "grad_norm": 0.13500237464904785,
      "learning_rate": 1.1025379957487675e-06,
      "loss": 0.0056,
      "step": 11133
    },
    {
      "epoch": 2.715609756097561,
      "grad_norm": 0.11458928883075714,
      "learning_rate": 1.1006632626727687e-06,
      "loss": 0.0162,
      "step": 11134
    },
    {
      "epoch": 2.7158536585365853,
      "grad_norm": 0.08626509457826614,
      "learning_rate": 1.0987900889553287e-06,
      "loss": 0.0154,
      "step": 11135
    },
    {
      "epoch": 2.7160975609756095,
      "grad_norm": 0.14610826969146729,
      "learning_rate": 1.0969184747186612e-06,
      "loss": 0.0199,
      "step": 11136
    },
    {
      "epoch": 2.716341463414634,
      "grad_norm": 0.08644228428602219,
      "learning_rate": 1.0950484200848987e-06,
      "loss": 0.0119,
      "step": 11137
    },
    {
      "epoch": 2.7165853658536587,
      "grad_norm": 0.21479493379592896,
      "learning_rate": 1.0931799251760383e-06,
      "loss": 0.0309,
      "step": 11138
    },
    {
      "epoch": 2.716829268292683,
      "grad_norm": 0.1617605835199356,
      "learning_rate": 1.0913129901140046e-06,
      "loss": 0.0143,
      "step": 11139
    },
    {
      "epoch": 2.717073170731707,
      "grad_norm": 0.21594934165477753,
      "learning_rate": 1.0894476150206085e-06,
      "loss": 0.0196,
      "step": 11140
    },
    {
      "epoch": 2.7173170731707317,
      "grad_norm": 0.1625548005104065,
      "learning_rate": 1.0875838000175553e-06,
      "loss": 0.0312,
      "step": 11141
    },
    {
      "epoch": 2.7175609756097563,
      "grad_norm": 0.10240399837493896,
      "learning_rate": 1.0857215452264563e-06,
      "loss": 0.0184,
      "step": 11142
    },
    {
      "epoch": 2.7178048780487805,
      "grad_norm": 0.11354194581508636,
      "learning_rate": 1.0838608507688168e-06,
      "loss": 0.0228,
      "step": 11143
    },
    {
      "epoch": 2.7180487804878046,
      "grad_norm": 0.19583085179328918,
      "learning_rate": 1.0820017167660456e-06,
      "loss": 0.0171,
      "step": 11144
    },
    {
      "epoch": 2.7182926829268292,
      "grad_norm": 0.16131359338760376,
      "learning_rate": 1.0801441433394427e-06,
      "loss": 0.0172,
      "step": 11145
    },
    {
      "epoch": 2.718536585365854,
      "grad_norm": 0.13649113476276398,
      "learning_rate": 1.0782881306102055e-06,
      "loss": 0.0139,
      "step": 11146
    },
    {
      "epoch": 2.718780487804878,
      "grad_norm": 0.20896349847316742,
      "learning_rate": 1.07643367869944e-06,
      "loss": 0.0313,
      "step": 11147
    },
    {
      "epoch": 2.719024390243902,
      "grad_norm": 0.10390108823776245,
      "learning_rate": 1.0745807877281382e-06,
      "loss": 0.0117,
      "step": 11148
    },
    {
      "epoch": 2.719268292682927,
      "grad_norm": 0.1447518765926361,
      "learning_rate": 1.0727294578171953e-06,
      "loss": 0.0226,
      "step": 11149
    },
    {
      "epoch": 2.7195121951219514,
      "grad_norm": 0.12465981394052505,
      "learning_rate": 1.0708796890874118e-06,
      "loss": 0.0247,
      "step": 11150
    },
    {
      "epoch": 2.7197560975609756,
      "grad_norm": 0.20761923491954803,
      "learning_rate": 1.0690314816594715e-06,
      "loss": 0.0133,
      "step": 11151
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 0.07926052808761597,
      "learning_rate": 1.0671848356539644e-06,
      "loss": 0.0169,
      "step": 11152
    },
    {
      "epoch": 2.7202439024390244,
      "grad_norm": 0.10954752564430237,
      "learning_rate": 1.0653397511913882e-06,
      "loss": 0.0183,
      "step": 11153
    },
    {
      "epoch": 2.720487804878049,
      "grad_norm": 0.1940857470035553,
      "learning_rate": 1.0634962283921163e-06,
      "loss": 0.0275,
      "step": 11154
    },
    {
      "epoch": 2.720731707317073,
      "grad_norm": 0.08637439459562302,
      "learning_rate": 1.061654267376444e-06,
      "loss": 0.0146,
      "step": 11155
    },
    {
      "epoch": 2.7209756097560973,
      "grad_norm": 0.1158909872174263,
      "learning_rate": 1.059813868264542e-06,
      "loss": 0.0048,
      "step": 11156
    },
    {
      "epoch": 2.721219512195122,
      "grad_norm": 0.12238113582134247,
      "learning_rate": 1.0579750311765003e-06,
      "loss": 0.0197,
      "step": 11157
    },
    {
      "epoch": 2.7214634146341465,
      "grad_norm": 0.11067371070384979,
      "learning_rate": 1.0561377562322976e-06,
      "loss": 0.0206,
      "step": 11158
    },
    {
      "epoch": 2.7217073170731707,
      "grad_norm": 0.10582610219717026,
      "learning_rate": 1.0543020435517997e-06,
      "loss": 0.018,
      "step": 11159
    },
    {
      "epoch": 2.721951219512195,
      "grad_norm": 0.10051877051591873,
      "learning_rate": 1.0524678932547966e-06,
      "loss": 0.0259,
      "step": 11160
    },
    {
      "epoch": 2.7221951219512195,
      "grad_norm": 0.15160438418388367,
      "learning_rate": 1.050635305460948e-06,
      "loss": 0.0148,
      "step": 11161
    },
    {
      "epoch": 2.722439024390244,
      "grad_norm": 0.2602474093437195,
      "learning_rate": 1.0488042802898306e-06,
      "loss": 0.0289,
      "step": 11162
    },
    {
      "epoch": 2.7226829268292683,
      "grad_norm": 0.10017992556095123,
      "learning_rate": 1.046974817860913e-06,
      "loss": 0.0109,
      "step": 11163
    },
    {
      "epoch": 2.7229268292682924,
      "grad_norm": 0.20113764703273773,
      "learning_rate": 1.045146918293563e-06,
      "loss": 0.0147,
      "step": 11164
    },
    {
      "epoch": 2.723170731707317,
      "grad_norm": 0.11596944183111191,
      "learning_rate": 1.0433205817070386e-06,
      "loss": 0.0178,
      "step": 11165
    },
    {
      "epoch": 2.7234146341463417,
      "grad_norm": 0.09517741948366165,
      "learning_rate": 1.0414958082205106e-06,
      "loss": 0.0269,
      "step": 11166
    },
    {
      "epoch": 2.723658536585366,
      "grad_norm": 0.09546037018299103,
      "learning_rate": 1.039672597953037e-06,
      "loss": 0.0234,
      "step": 11167
    },
    {
      "epoch": 2.72390243902439,
      "grad_norm": 0.12243417650461197,
      "learning_rate": 1.037850951023578e-06,
      "loss": 0.0147,
      "step": 11168
    },
    {
      "epoch": 2.7241463414634146,
      "grad_norm": 0.14331962168216705,
      "learning_rate": 1.0360308675509883e-06,
      "loss": 0.0243,
      "step": 11169
    },
    {
      "epoch": 2.7243902439024392,
      "grad_norm": 0.13373702764511108,
      "learning_rate": 1.034212347654026e-06,
      "loss": 0.0206,
      "step": 11170
    },
    {
      "epoch": 2.7246341463414634,
      "grad_norm": 0.14106880128383636,
      "learning_rate": 1.0323953914513406e-06,
      "loss": 0.028,
      "step": 11171
    },
    {
      "epoch": 2.7248780487804876,
      "grad_norm": 0.10774211585521698,
      "learning_rate": 1.0305799990614818e-06,
      "loss": 0.0201,
      "step": 11172
    },
    {
      "epoch": 2.725121951219512,
      "grad_norm": 0.059765227138996124,
      "learning_rate": 1.0287661706029016e-06,
      "loss": 0.011,
      "step": 11173
    },
    {
      "epoch": 2.725365853658537,
      "grad_norm": 0.1766977310180664,
      "learning_rate": 1.0269539061939476e-06,
      "loss": 0.0215,
      "step": 11174
    },
    {
      "epoch": 2.725609756097561,
      "grad_norm": 0.11590626090765,
      "learning_rate": 1.0251432059528581e-06,
      "loss": 0.0103,
      "step": 11175
    },
    {
      "epoch": 2.725853658536585,
      "grad_norm": 0.17787329852581024,
      "learning_rate": 1.0233340699977862e-06,
      "loss": 0.0127,
      "step": 11176
    },
    {
      "epoch": 2.7260975609756097,
      "grad_norm": 0.09694253653287888,
      "learning_rate": 1.0215264984467648e-06,
      "loss": 0.0147,
      "step": 11177
    },
    {
      "epoch": 2.7263414634146343,
      "grad_norm": 0.13306301832199097,
      "learning_rate": 1.0197204914177305e-06,
      "loss": 0.0138,
      "step": 11178
    },
    {
      "epoch": 2.7265853658536585,
      "grad_norm": 0.19711141288280487,
      "learning_rate": 1.0179160490285278e-06,
      "loss": 0.0197,
      "step": 11179
    },
    {
      "epoch": 2.7268292682926827,
      "grad_norm": 0.06890065968036652,
      "learning_rate": 1.0161131713968819e-06,
      "loss": 0.0131,
      "step": 11180
    },
    {
      "epoch": 2.7270731707317073,
      "grad_norm": 0.10815257579088211,
      "learning_rate": 1.0143118586404348e-06,
      "loss": 0.0202,
      "step": 11181
    },
    {
      "epoch": 2.727317073170732,
      "grad_norm": 0.18899117410182953,
      "learning_rate": 1.0125121108767093e-06,
      "loss": 0.0364,
      "step": 11182
    },
    {
      "epoch": 2.727560975609756,
      "grad_norm": 0.13328059017658234,
      "learning_rate": 1.0107139282231337e-06,
      "loss": 0.0108,
      "step": 11183
    },
    {
      "epoch": 2.7278048780487802,
      "grad_norm": 0.15692120790481567,
      "learning_rate": 1.0089173107970418e-06,
      "loss": 0.0207,
      "step": 11184
    },
    {
      "epoch": 2.728048780487805,
      "grad_norm": 0.07814135402441025,
      "learning_rate": 1.007122258715648e-06,
      "loss": 0.0225,
      "step": 11185
    },
    {
      "epoch": 2.7282926829268295,
      "grad_norm": 0.12430430203676224,
      "learning_rate": 1.0053287720960814e-06,
      "loss": 0.0182,
      "step": 11186
    },
    {
      "epoch": 2.7285365853658536,
      "grad_norm": 0.20723335444927216,
      "learning_rate": 1.0035368510553533e-06,
      "loss": 0.0225,
      "step": 11187
    },
    {
      "epoch": 2.728780487804878,
      "grad_norm": 0.1517854928970337,
      "learning_rate": 1.0017464957103844e-06,
      "loss": 0.0183,
      "step": 11188
    },
    {
      "epoch": 2.7290243902439024,
      "grad_norm": 0.27885788679122925,
      "learning_rate": 9.999577061779951e-07,
      "loss": 0.0078,
      "step": 11189
    },
    {
      "epoch": 2.729268292682927,
      "grad_norm": 0.08375494927167892,
      "learning_rate": 9.981704825748894e-07,
      "loss": 0.0216,
      "step": 11190
    },
    {
      "epoch": 2.729512195121951,
      "grad_norm": 0.116664357483387,
      "learning_rate": 9.963848250176878e-07,
      "loss": 0.0134,
      "step": 11191
    },
    {
      "epoch": 2.7297560975609754,
      "grad_norm": 0.19081977009773254,
      "learning_rate": 9.94600733622894e-07,
      "loss": 0.031,
      "step": 11192
    },
    {
      "epoch": 2.73,
      "grad_norm": 0.22428137063980103,
      "learning_rate": 9.928182085069099e-07,
      "loss": 0.0318,
      "step": 11193
    },
    {
      "epoch": 2.7302439024390246,
      "grad_norm": 0.11175251007080078,
      "learning_rate": 9.910372497860504e-07,
      "loss": 0.0139,
      "step": 11194
    },
    {
      "epoch": 2.7304878048780488,
      "grad_norm": 0.11458449810743332,
      "learning_rate": 9.892578575765087e-07,
      "loss": 0.0172,
      "step": 11195
    },
    {
      "epoch": 2.730731707317073,
      "grad_norm": 0.16663512587547302,
      "learning_rate": 9.874800319943866e-07,
      "loss": 0.0165,
      "step": 11196
    },
    {
      "epoch": 2.7309756097560975,
      "grad_norm": 0.08049078285694122,
      "learning_rate": 9.857037731556857e-07,
      "loss": 0.018,
      "step": 11197
    },
    {
      "epoch": 2.731219512195122,
      "grad_norm": 0.20187419652938843,
      "learning_rate": 9.83929081176299e-07,
      "loss": 0.0305,
      "step": 11198
    },
    {
      "epoch": 2.7314634146341463,
      "grad_norm": 0.11812123656272888,
      "learning_rate": 9.821559561720178e-07,
      "loss": 0.0149,
      "step": 11199
    },
    {
      "epoch": 2.7317073170731705,
      "grad_norm": 0.14553509652614594,
      "learning_rate": 9.803843982585353e-07,
      "loss": 0.0217,
      "step": 11200
    },
    {
      "epoch": 2.731951219512195,
      "grad_norm": 0.21599514782428741,
      "learning_rate": 9.786144075514397e-07,
      "loss": 0.0285,
      "step": 11201
    },
    {
      "epoch": 2.7321951219512197,
      "grad_norm": 0.06950702518224716,
      "learning_rate": 9.768459841662193e-07,
      "loss": 0.0102,
      "step": 11202
    },
    {
      "epoch": 2.732439024390244,
      "grad_norm": 0.06563423573970795,
      "learning_rate": 9.750791282182536e-07,
      "loss": 0.0171,
      "step": 11203
    },
    {
      "epoch": 2.732682926829268,
      "grad_norm": 0.11947820335626602,
      "learning_rate": 9.733138398228315e-07,
      "loss": 0.017,
      "step": 11204
    },
    {
      "epoch": 2.7329268292682927,
      "grad_norm": 0.0842818096280098,
      "learning_rate": 9.71550119095127e-07,
      "loss": 0.0209,
      "step": 11205
    },
    {
      "epoch": 2.7331707317073173,
      "grad_norm": 0.16418880224227905,
      "learning_rate": 9.697879661502151e-07,
      "loss": 0.0199,
      "step": 11206
    },
    {
      "epoch": 2.7334146341463414,
      "grad_norm": 0.05945023149251938,
      "learning_rate": 9.680273811030788e-07,
      "loss": 0.0078,
      "step": 11207
    },
    {
      "epoch": 2.7336585365853656,
      "grad_norm": 0.07257337123155594,
      "learning_rate": 9.662683640685872e-07,
      "loss": 0.0116,
      "step": 11208
    },
    {
      "epoch": 2.7339024390243902,
      "grad_norm": 0.07443764060735703,
      "learning_rate": 9.645109151615067e-07,
      "loss": 0.005,
      "step": 11209
    },
    {
      "epoch": 2.734146341463415,
      "grad_norm": 0.14737479388713837,
      "learning_rate": 9.627550344965126e-07,
      "loss": 0.0174,
      "step": 11210
    },
    {
      "epoch": 2.734390243902439,
      "grad_norm": 0.11778874695301056,
      "learning_rate": 9.610007221881684e-07,
      "loss": 0.0234,
      "step": 11211
    },
    {
      "epoch": 2.734634146341463,
      "grad_norm": 0.1123642548918724,
      "learning_rate": 9.592479783509328e-07,
      "loss": 0.0237,
      "step": 11212
    },
    {
      "epoch": 2.734878048780488,
      "grad_norm": 0.13159871101379395,
      "learning_rate": 9.574968030991726e-07,
      "loss": 0.0226,
      "step": 11213
    },
    {
      "epoch": 2.7351219512195124,
      "grad_norm": 0.08924096822738647,
      "learning_rate": 9.557471965471438e-07,
      "loss": 0.0196,
      "step": 11214
    },
    {
      "epoch": 2.7353658536585366,
      "grad_norm": 0.12934304773807526,
      "learning_rate": 9.539991588090075e-07,
      "loss": 0.0131,
      "step": 11215
    },
    {
      "epoch": 2.7356097560975607,
      "grad_norm": 0.15376928448677063,
      "learning_rate": 9.522526899988116e-07,
      "loss": 0.0303,
      "step": 11216
    },
    {
      "epoch": 2.7358536585365854,
      "grad_norm": 0.13248637318611145,
      "learning_rate": 9.505077902305093e-07,
      "loss": 0.0182,
      "step": 11217
    },
    {
      "epoch": 2.73609756097561,
      "grad_norm": 0.1084371954202652,
      "learning_rate": 9.48764459617954e-07,
      "loss": 0.0174,
      "step": 11218
    },
    {
      "epoch": 2.736341463414634,
      "grad_norm": 0.11334861814975739,
      "learning_rate": 9.470226982748881e-07,
      "loss": 0.0082,
      "step": 11219
    },
    {
      "epoch": 2.7365853658536583,
      "grad_norm": 0.1528991311788559,
      "learning_rate": 9.452825063149595e-07,
      "loss": 0.0145,
      "step": 11220
    },
    {
      "epoch": 2.736829268292683,
      "grad_norm": 0.10208278894424438,
      "learning_rate": 9.435438838517108e-07,
      "loss": 0.0186,
      "step": 11221
    },
    {
      "epoch": 2.7370731707317075,
      "grad_norm": 0.17446203529834747,
      "learning_rate": 9.418068309985733e-07,
      "loss": 0.0214,
      "step": 11222
    },
    {
      "epoch": 2.7373170731707317,
      "grad_norm": 0.28141263127326965,
      "learning_rate": 9.400713478688955e-07,
      "loss": 0.0144,
      "step": 11223
    },
    {
      "epoch": 2.737560975609756,
      "grad_norm": 0.09002987295389175,
      "learning_rate": 9.383374345759061e-07,
      "loss": 0.0064,
      "step": 11224
    },
    {
      "epoch": 2.7378048780487805,
      "grad_norm": 0.11227826029062271,
      "learning_rate": 9.366050912327396e-07,
      "loss": 0.0201,
      "step": 11225
    },
    {
      "epoch": 2.738048780487805,
      "grad_norm": 0.11347369104623795,
      "learning_rate": 9.34874317952425e-07,
      "loss": 0.0169,
      "step": 11226
    },
    {
      "epoch": 2.7382926829268293,
      "grad_norm": 0.1443977653980255,
      "learning_rate": 9.331451148478915e-07,
      "loss": 0.0176,
      "step": 11227
    },
    {
      "epoch": 2.7385365853658534,
      "grad_norm": 0.1066063940525055,
      "learning_rate": 9.314174820319627e-07,
      "loss": 0.0203,
      "step": 11228
    },
    {
      "epoch": 2.738780487804878,
      "grad_norm": 0.15746647119522095,
      "learning_rate": 9.29691419617365e-07,
      "loss": 0.0178,
      "step": 11229
    },
    {
      "epoch": 2.7390243902439027,
      "grad_norm": 0.08869528025388718,
      "learning_rate": 9.279669277167114e-07,
      "loss": 0.0128,
      "step": 11230
    },
    {
      "epoch": 2.739268292682927,
      "grad_norm": 0.10423639416694641,
      "learning_rate": 9.262440064425282e-07,
      "loss": 0.0262,
      "step": 11231
    },
    {
      "epoch": 2.739512195121951,
      "grad_norm": 0.1340768188238144,
      "learning_rate": 9.245226559072257e-07,
      "loss": 0.0244,
      "step": 11232
    },
    {
      "epoch": 2.7397560975609756,
      "grad_norm": 0.09087994694709778,
      "learning_rate": 9.228028762231167e-07,
      "loss": 0.007,
      "step": 11233
    },
    {
      "epoch": 2.74,
      "grad_norm": 0.15016038715839386,
      "learning_rate": 9.210846675024143e-07,
      "loss": 0.0162,
      "step": 11234
    },
    {
      "epoch": 2.7402439024390244,
      "grad_norm": 0.1210440993309021,
      "learning_rate": 9.193680298572205e-07,
      "loss": 0.0209,
      "step": 11235
    },
    {
      "epoch": 2.7404878048780485,
      "grad_norm": 0.16440747678279877,
      "learning_rate": 9.176529633995512e-07,
      "loss": 0.0262,
      "step": 11236
    },
    {
      "epoch": 2.740731707317073,
      "grad_norm": 0.05503634363412857,
      "learning_rate": 9.159394682413031e-07,
      "loss": 0.0099,
      "step": 11237
    },
    {
      "epoch": 2.7409756097560978,
      "grad_norm": 0.0775410532951355,
      "learning_rate": 9.142275444942699e-07,
      "loss": 0.0066,
      "step": 11238
    },
    {
      "epoch": 2.741219512195122,
      "grad_norm": 0.14706586301326752,
      "learning_rate": 9.125171922701597e-07,
      "loss": 0.0152,
      "step": 11239
    },
    {
      "epoch": 2.741463414634146,
      "grad_norm": 0.13796180486679077,
      "learning_rate": 9.10808411680561e-07,
      "loss": 0.0168,
      "step": 11240
    },
    {
      "epoch": 2.7417073170731707,
      "grad_norm": 0.17096854746341705,
      "learning_rate": 9.091012028369733e-07,
      "loss": 0.0088,
      "step": 11241
    },
    {
      "epoch": 2.7419512195121953,
      "grad_norm": 0.13299483060836792,
      "learning_rate": 9.073955658507826e-07,
      "loss": 0.0181,
      "step": 11242
    },
    {
      "epoch": 2.7421951219512195,
      "grad_norm": 0.14637254178524017,
      "learning_rate": 9.056915008332722e-07,
      "loss": 0.0163,
      "step": 11243
    },
    {
      "epoch": 2.7424390243902437,
      "grad_norm": 0.09407299011945724,
      "learning_rate": 9.039890078956337e-07,
      "loss": 0.0046,
      "step": 11244
    },
    {
      "epoch": 2.7426829268292683,
      "grad_norm": 0.11590403318405151,
      "learning_rate": 9.022880871489503e-07,
      "loss": 0.0152,
      "step": 11245
    },
    {
      "epoch": 2.742926829268293,
      "grad_norm": 0.08711716532707214,
      "learning_rate": 9.005887387041945e-07,
      "loss": 0.0105,
      "step": 11246
    },
    {
      "epoch": 2.743170731707317,
      "grad_norm": 0.12101363390684128,
      "learning_rate": 8.988909626722525e-07,
      "loss": 0.0139,
      "step": 11247
    },
    {
      "epoch": 2.7434146341463412,
      "grad_norm": 0.14412152767181396,
      "learning_rate": 8.971947591638912e-07,
      "loss": 0.0236,
      "step": 11248
    },
    {
      "epoch": 2.743658536585366,
      "grad_norm": 0.15478916466236115,
      "learning_rate": 8.955001282897863e-07,
      "loss": 0.0202,
      "step": 11249
    },
    {
      "epoch": 2.7439024390243905,
      "grad_norm": 0.11538586765527725,
      "learning_rate": 8.9380707016051e-07,
      "loss": 0.018,
      "step": 11250
    },
    {
      "epoch": 2.7441463414634146,
      "grad_norm": 0.21678563952445984,
      "learning_rate": 8.921155848865243e-07,
      "loss": 0.0195,
      "step": 11251
    },
    {
      "epoch": 2.744390243902439,
      "grad_norm": 0.15006542205810547,
      "learning_rate": 8.90425672578199e-07,
      "loss": 0.0193,
      "step": 11252
    },
    {
      "epoch": 2.7446341463414634,
      "grad_norm": 0.07313743233680725,
      "learning_rate": 8.887373333457849e-07,
      "loss": 0.0164,
      "step": 11253
    },
    {
      "epoch": 2.744878048780488,
      "grad_norm": 0.1905560940504074,
      "learning_rate": 8.87050567299455e-07,
      "loss": 0.0159,
      "step": 11254
    },
    {
      "epoch": 2.745121951219512,
      "grad_norm": 0.1077268198132515,
      "learning_rate": 8.853653745492546e-07,
      "loss": 0.0194,
      "step": 11255
    },
    {
      "epoch": 2.7453658536585364,
      "grad_norm": 0.11540984362363815,
      "learning_rate": 8.836817552051429e-07,
      "loss": 0.0067,
      "step": 11256
    },
    {
      "epoch": 2.745609756097561,
      "grad_norm": 0.10602562874555588,
      "learning_rate": 8.819997093769683e-07,
      "loss": 0.0218,
      "step": 11257
    },
    {
      "epoch": 2.7458536585365856,
      "grad_norm": 0.10124999284744263,
      "learning_rate": 8.803192371744845e-07,
      "loss": 0.0088,
      "step": 11258
    },
    {
      "epoch": 2.7460975609756098,
      "grad_norm": 0.1723497360944748,
      "learning_rate": 8.786403387073261e-07,
      "loss": 0.0194,
      "step": 11259
    },
    {
      "epoch": 2.746341463414634,
      "grad_norm": 0.08673235028982162,
      "learning_rate": 8.769630140850471e-07,
      "loss": 0.0165,
      "step": 11260
    },
    {
      "epoch": 2.7465853658536585,
      "grad_norm": 0.11893856525421143,
      "learning_rate": 8.752872634170823e-07,
      "loss": 0.0088,
      "step": 11261
    },
    {
      "epoch": 2.746829268292683,
      "grad_norm": 0.12294939905405045,
      "learning_rate": 8.736130868127745e-07,
      "loss": 0.0218,
      "step": 11262
    },
    {
      "epoch": 2.7470731707317073,
      "grad_norm": 0.11348024755716324,
      "learning_rate": 8.719404843813534e-07,
      "loss": 0.0166,
      "step": 11263
    },
    {
      "epoch": 2.7473170731707315,
      "grad_norm": 0.15321122109889984,
      "learning_rate": 8.70269456231948e-07,
      "loss": 0.01,
      "step": 11264
    },
    {
      "epoch": 2.747560975609756,
      "grad_norm": 0.12054590880870819,
      "learning_rate": 8.686000024735963e-07,
      "loss": 0.0116,
      "step": 11265
    },
    {
      "epoch": 2.7478048780487807,
      "grad_norm": 0.09450296312570572,
      "learning_rate": 8.669321232152223e-07,
      "loss": 0.0143,
      "step": 11266
    },
    {
      "epoch": 2.748048780487805,
      "grad_norm": 0.15237966179847717,
      "learning_rate": 8.652658185656442e-07,
      "loss": 0.0307,
      "step": 11267
    },
    {
      "epoch": 2.748292682926829,
      "grad_norm": 0.08654094487428665,
      "learning_rate": 8.636010886335921e-07,
      "loss": 0.0108,
      "step": 11268
    },
    {
      "epoch": 2.7485365853658537,
      "grad_norm": 0.10871735215187073,
      "learning_rate": 8.619379335276817e-07,
      "loss": 0.0199,
      "step": 11269
    },
    {
      "epoch": 2.7487804878048783,
      "grad_norm": 0.2278318554162979,
      "learning_rate": 8.602763533564262e-07,
      "loss": 0.0212,
      "step": 11270
    },
    {
      "epoch": 2.7490243902439024,
      "grad_norm": 0.0799688771367073,
      "learning_rate": 8.586163482282389e-07,
      "loss": 0.021,
      "step": 11271
    },
    {
      "epoch": 2.7492682926829266,
      "grad_norm": 0.13785794377326965,
      "learning_rate": 8.569579182514276e-07,
      "loss": 0.0151,
      "step": 11272
    },
    {
      "epoch": 2.749512195121951,
      "grad_norm": 0.11482403427362442,
      "learning_rate": 8.553010635342085e-07,
      "loss": 0.0205,
      "step": 11273
    },
    {
      "epoch": 2.749756097560976,
      "grad_norm": 0.1350090652704239,
      "learning_rate": 8.536457841846757e-07,
      "loss": 0.0082,
      "step": 11274
    },
    {
      "epoch": 2.75,
      "grad_norm": 0.1295301467180252,
      "learning_rate": 8.519920803108399e-07,
      "loss": 0.015,
      "step": 11275
    },
    {
      "epoch": 2.750243902439024,
      "grad_norm": 0.11351968348026276,
      "learning_rate": 8.503399520205985e-07,
      "loss": 0.0144,
      "step": 11276
    },
    {
      "epoch": 2.750487804878049,
      "grad_norm": 0.17962267994880676,
      "learning_rate": 8.486893994217427e-07,
      "loss": 0.0252,
      "step": 11277
    },
    {
      "epoch": 2.7507317073170734,
      "grad_norm": 0.08491044491529465,
      "learning_rate": 8.470404226219697e-07,
      "loss": 0.0166,
      "step": 11278
    },
    {
      "epoch": 2.7509756097560976,
      "grad_norm": 0.20124323666095734,
      "learning_rate": 8.453930217288714e-07,
      "loss": 0.0182,
      "step": 11279
    },
    {
      "epoch": 2.7512195121951217,
      "grad_norm": 0.12132277339696884,
      "learning_rate": 8.437471968499339e-07,
      "loss": 0.0196,
      "step": 11280
    },
    {
      "epoch": 2.7514634146341463,
      "grad_norm": 0.12189344316720963,
      "learning_rate": 8.421029480925407e-07,
      "loss": 0.0118,
      "step": 11281
    },
    {
      "epoch": 2.751707317073171,
      "grad_norm": 0.11493437737226486,
      "learning_rate": 8.404602755639784e-07,
      "loss": 0.0287,
      "step": 11282
    },
    {
      "epoch": 2.751951219512195,
      "grad_norm": 0.09650306403636932,
      "learning_rate": 8.388191793714195e-07,
      "loss": 0.0167,
      "step": 11283
    },
    {
      "epoch": 2.7521951219512193,
      "grad_norm": 0.23584334552288055,
      "learning_rate": 8.371796596219478e-07,
      "loss": 0.0294,
      "step": 11284
    },
    {
      "epoch": 2.752439024390244,
      "grad_norm": 0.08102799952030182,
      "learning_rate": 8.355417164225359e-07,
      "loss": 0.0145,
      "step": 11285
    },
    {
      "epoch": 2.7526829268292685,
      "grad_norm": 0.11423365771770477,
      "learning_rate": 8.339053498800515e-07,
      "loss": 0.0168,
      "step": 11286
    },
    {
      "epoch": 2.7529268292682927,
      "grad_norm": 0.25101059675216675,
      "learning_rate": 8.322705601012587e-07,
      "loss": 0.0248,
      "step": 11287
    },
    {
      "epoch": 2.753170731707317,
      "grad_norm": 0.1783190220594406,
      "learning_rate": 8.306373471928309e-07,
      "loss": 0.017,
      "step": 11288
    },
    {
      "epoch": 2.7534146341463415,
      "grad_norm": 0.0912635326385498,
      "learning_rate": 8.29005711261327e-07,
      "loss": 0.0114,
      "step": 11289
    },
    {
      "epoch": 2.753658536585366,
      "grad_norm": 0.09815633296966553,
      "learning_rate": 8.273756524132037e-07,
      "loss": 0.0136,
      "step": 11290
    },
    {
      "epoch": 2.7539024390243902,
      "grad_norm": 0.11261725425720215,
      "learning_rate": 8.257471707548231e-07,
      "loss": 0.0092,
      "step": 11291
    },
    {
      "epoch": 2.7541463414634144,
      "grad_norm": 0.13178136944770813,
      "learning_rate": 8.241202663924335e-07,
      "loss": 0.0247,
      "step": 11292
    },
    {
      "epoch": 2.754390243902439,
      "grad_norm": 0.19452877342700958,
      "learning_rate": 8.224949394321834e-07,
      "loss": 0.0116,
      "step": 11293
    },
    {
      "epoch": 2.7546341463414636,
      "grad_norm": 0.1455862671136856,
      "learning_rate": 8.208711899801297e-07,
      "loss": 0.0303,
      "step": 11294
    },
    {
      "epoch": 2.754878048780488,
      "grad_norm": 0.07518882304430008,
      "learning_rate": 8.192490181422097e-07,
      "loss": 0.0062,
      "step": 11295
    },
    {
      "epoch": 2.755121951219512,
      "grad_norm": 0.06538044661283493,
      "learning_rate": 8.176284240242638e-07,
      "loss": 0.0123,
      "step": 11296
    },
    {
      "epoch": 2.7553658536585366,
      "grad_norm": 0.1830526888370514,
      "learning_rate": 8.16009407732038e-07,
      "loss": 0.0338,
      "step": 11297
    },
    {
      "epoch": 2.755609756097561,
      "grad_norm": 0.10286908596754074,
      "learning_rate": 8.143919693711616e-07,
      "loss": 0.0196,
      "step": 11298
    },
    {
      "epoch": 2.7558536585365854,
      "grad_norm": 0.23404701054096222,
      "learning_rate": 8.127761090471697e-07,
      "loss": 0.0205,
      "step": 11299
    },
    {
      "epoch": 2.7560975609756095,
      "grad_norm": 0.17978058755397797,
      "learning_rate": 8.111618268654946e-07,
      "loss": 0.0239,
      "step": 11300
    },
    {
      "epoch": 2.756341463414634,
      "grad_norm": 0.10129091888666153,
      "learning_rate": 8.095491229314578e-07,
      "loss": 0.0152,
      "step": 11301
    },
    {
      "epoch": 2.7565853658536588,
      "grad_norm": 0.07692032307386398,
      "learning_rate": 8.079379973502943e-07,
      "loss": 0.01,
      "step": 11302
    },
    {
      "epoch": 2.756829268292683,
      "grad_norm": 0.1483909785747528,
      "learning_rate": 8.06328450227109e-07,
      "loss": 0.0227,
      "step": 11303
    },
    {
      "epoch": 2.757073170731707,
      "grad_norm": 0.12024325877428055,
      "learning_rate": 8.047204816669318e-07,
      "loss": 0.0053,
      "step": 11304
    },
    {
      "epoch": 2.7573170731707317,
      "grad_norm": 0.13248230516910553,
      "learning_rate": 8.03114091774676e-07,
      "loss": 0.0315,
      "step": 11305
    },
    {
      "epoch": 2.7575609756097563,
      "grad_norm": 0.14325624704360962,
      "learning_rate": 8.015092806551495e-07,
      "loss": 0.0154,
      "step": 11306
    },
    {
      "epoch": 2.7578048780487805,
      "grad_norm": 0.11103306710720062,
      "learning_rate": 7.999060484130627e-07,
      "loss": 0.0244,
      "step": 11307
    },
    {
      "epoch": 2.7580487804878047,
      "grad_norm": 0.12094388157129288,
      "learning_rate": 7.983043951530267e-07,
      "loss": 0.0224,
      "step": 11308
    },
    {
      "epoch": 2.7582926829268293,
      "grad_norm": 0.11678025871515274,
      "learning_rate": 7.967043209795355e-07,
      "loss": 0.014,
      "step": 11309
    },
    {
      "epoch": 2.758536585365854,
      "grad_norm": 0.13655388355255127,
      "learning_rate": 7.951058259969974e-07,
      "loss": 0.0131,
      "step": 11310
    },
    {
      "epoch": 2.758780487804878,
      "grad_norm": 0.2731236219406128,
      "learning_rate": 7.935089103097038e-07,
      "loss": 0.0237,
      "step": 11311
    },
    {
      "epoch": 2.7590243902439022,
      "grad_norm": 0.08837617933750153,
      "learning_rate": 7.91913574021852e-07,
      "loss": 0.0137,
      "step": 11312
    },
    {
      "epoch": 2.759268292682927,
      "grad_norm": 0.1162896677851677,
      "learning_rate": 7.903198172375309e-07,
      "loss": 0.0115,
      "step": 11313
    },
    {
      "epoch": 2.7595121951219515,
      "grad_norm": 0.13269612193107605,
      "learning_rate": 7.887276400607268e-07,
      "loss": 0.0089,
      "step": 11314
    },
    {
      "epoch": 2.7597560975609756,
      "grad_norm": 0.15841098129749298,
      "learning_rate": 7.871370425953289e-07,
      "loss": 0.0263,
      "step": 11315
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.13406893610954285,
      "learning_rate": 7.855480249451181e-07,
      "loss": 0.0167,
      "step": 11316
    },
    {
      "epoch": 2.7602439024390244,
      "grad_norm": 0.08863992989063263,
      "learning_rate": 7.839605872137668e-07,
      "loss": 0.0137,
      "step": 11317
    },
    {
      "epoch": 2.760487804878049,
      "grad_norm": 0.13233208656311035,
      "learning_rate": 7.823747295048616e-07,
      "loss": 0.0263,
      "step": 11318
    },
    {
      "epoch": 2.760731707317073,
      "grad_norm": 0.11376120150089264,
      "learning_rate": 7.807904519218617e-07,
      "loss": 0.0163,
      "step": 11319
    },
    {
      "epoch": 2.7609756097560973,
      "grad_norm": 0.20190514624118805,
      "learning_rate": 7.792077545681453e-07,
      "loss": 0.0174,
      "step": 11320
    },
    {
      "epoch": 2.761219512195122,
      "grad_norm": 0.1028531864285469,
      "learning_rate": 7.776266375469716e-07,
      "loss": 0.0121,
      "step": 11321
    },
    {
      "epoch": 2.7614634146341466,
      "grad_norm": 0.1227731853723526,
      "learning_rate": 7.760471009615133e-07,
      "loss": 0.0302,
      "step": 11322
    },
    {
      "epoch": 2.7617073170731707,
      "grad_norm": 0.07061675935983658,
      "learning_rate": 7.744691449148244e-07,
      "loss": 0.0138,
      "step": 11323
    },
    {
      "epoch": 2.761951219512195,
      "grad_norm": 0.19013874232769012,
      "learning_rate": 7.728927695098586e-07,
      "loss": 0.0243,
      "step": 11324
    },
    {
      "epoch": 2.7621951219512195,
      "grad_norm": 0.17538917064666748,
      "learning_rate": 7.713179748494753e-07,
      "loss": 0.0321,
      "step": 11325
    },
    {
      "epoch": 2.762439024390244,
      "grad_norm": 0.07626506686210632,
      "learning_rate": 7.697447610364256e-07,
      "loss": 0.0092,
      "step": 11326
    },
    {
      "epoch": 2.7626829268292683,
      "grad_norm": 0.14866523444652557,
      "learning_rate": 7.681731281733495e-07,
      "loss": 0.0319,
      "step": 11327
    },
    {
      "epoch": 2.7629268292682925,
      "grad_norm": 0.14273320138454437,
      "learning_rate": 7.666030763627985e-07,
      "loss": 0.0147,
      "step": 11328
    },
    {
      "epoch": 2.763170731707317,
      "grad_norm": 0.07698732614517212,
      "learning_rate": 7.650346057072127e-07,
      "loss": 0.0126,
      "step": 11329
    },
    {
      "epoch": 2.7634146341463417,
      "grad_norm": 0.10190785676240921,
      "learning_rate": 7.634677163089271e-07,
      "loss": 0.0068,
      "step": 11330
    },
    {
      "epoch": 2.763658536585366,
      "grad_norm": 0.11617960780858994,
      "learning_rate": 7.619024082701793e-07,
      "loss": 0.0129,
      "step": 11331
    },
    {
      "epoch": 2.76390243902439,
      "grad_norm": 0.1487157791852951,
      "learning_rate": 7.603386816930958e-07,
      "loss": 0.0276,
      "step": 11332
    },
    {
      "epoch": 2.7641463414634146,
      "grad_norm": 0.08621585369110107,
      "learning_rate": 7.587765366797117e-07,
      "loss": 0.0076,
      "step": 11333
    },
    {
      "epoch": 2.7643902439024393,
      "grad_norm": 0.10202256590127945,
      "learning_rate": 7.572159733319484e-07,
      "loss": 0.0152,
      "step": 11334
    },
    {
      "epoch": 2.7646341463414634,
      "grad_norm": 0.05778136104345322,
      "learning_rate": 7.556569917516299e-07,
      "loss": 0.0082,
      "step": 11335
    },
    {
      "epoch": 2.7648780487804876,
      "grad_norm": 0.13468891382217407,
      "learning_rate": 7.54099592040472e-07,
      "loss": 0.0148,
      "step": 11336
    },
    {
      "epoch": 2.765121951219512,
      "grad_norm": 0.19687585532665253,
      "learning_rate": 7.525437743000907e-07,
      "loss": 0.0254,
      "step": 11337
    },
    {
      "epoch": 2.765365853658537,
      "grad_norm": 0.12901659309864044,
      "learning_rate": 7.50989538632002e-07,
      "loss": 0.0149,
      "step": 11338
    },
    {
      "epoch": 2.765609756097561,
      "grad_norm": 0.0920402929186821,
      "learning_rate": 7.494368851376138e-07,
      "loss": 0.0117,
      "step": 11339
    },
    {
      "epoch": 2.765853658536585,
      "grad_norm": 0.16289986670017242,
      "learning_rate": 7.478858139182254e-07,
      "loss": 0.0226,
      "step": 11340
    },
    {
      "epoch": 2.7660975609756098,
      "grad_norm": 0.14259666204452515,
      "learning_rate": 7.463363250750477e-07,
      "loss": 0.0202,
      "step": 11341
    },
    {
      "epoch": 2.7663414634146344,
      "grad_norm": 0.18193206191062927,
      "learning_rate": 7.447884187091802e-07,
      "loss": 0.0422,
      "step": 11342
    },
    {
      "epoch": 2.7665853658536586,
      "grad_norm": 0.13139763474464417,
      "learning_rate": 7.432420949216118e-07,
      "loss": 0.0142,
      "step": 11343
    },
    {
      "epoch": 2.7668292682926827,
      "grad_norm": 0.0865606740117073,
      "learning_rate": 7.416973538132394e-07,
      "loss": 0.0144,
      "step": 11344
    },
    {
      "epoch": 2.7670731707317073,
      "grad_norm": 0.09332602471113205,
      "learning_rate": 7.401541954848546e-07,
      "loss": 0.0222,
      "step": 11345
    },
    {
      "epoch": 2.767317073170732,
      "grad_norm": 0.09842856228351593,
      "learning_rate": 7.386126200371435e-07,
      "loss": 0.0202,
      "step": 11346
    },
    {
      "epoch": 2.767560975609756,
      "grad_norm": 0.08441338688135147,
      "learning_rate": 7.370726275706869e-07,
      "loss": 0.0156,
      "step": 11347
    },
    {
      "epoch": 2.7678048780487803,
      "grad_norm": 0.09802393615245819,
      "learning_rate": 7.355342181859626e-07,
      "loss": 0.0106,
      "step": 11348
    },
    {
      "epoch": 2.768048780487805,
      "grad_norm": 0.15468080341815948,
      "learning_rate": 7.339973919833515e-07,
      "loss": 0.0131,
      "step": 11349
    },
    {
      "epoch": 2.7682926829268295,
      "grad_norm": 0.12249904125928879,
      "learning_rate": 7.324621490631262e-07,
      "loss": 0.0174,
      "step": 11350
    },
    {
      "epoch": 2.7685365853658537,
      "grad_norm": 0.15348972380161285,
      "learning_rate": 7.309284895254564e-07,
      "loss": 0.0165,
      "step": 11351
    },
    {
      "epoch": 2.768780487804878,
      "grad_norm": 0.09642825275659561,
      "learning_rate": 7.293964134704068e-07,
      "loss": 0.0146,
      "step": 11352
    },
    {
      "epoch": 2.7690243902439025,
      "grad_norm": 0.1306886523962021,
      "learning_rate": 7.278659209979416e-07,
      "loss": 0.0243,
      "step": 11353
    },
    {
      "epoch": 2.769268292682927,
      "grad_norm": 0.1044374480843544,
      "learning_rate": 7.2633701220792e-07,
      "loss": 0.0131,
      "step": 11354
    },
    {
      "epoch": 2.7695121951219512,
      "grad_norm": 0.20597326755523682,
      "learning_rate": 7.248096872001009e-07,
      "loss": 0.0152,
      "step": 11355
    },
    {
      "epoch": 2.7697560975609754,
      "grad_norm": 0.10003440827131271,
      "learning_rate": 7.232839460741353e-07,
      "loss": 0.0107,
      "step": 11356
    },
    {
      "epoch": 2.77,
      "grad_norm": 0.10713143646717072,
      "learning_rate": 7.21759788929574e-07,
      "loss": 0.0139,
      "step": 11357
    },
    {
      "epoch": 2.7702439024390246,
      "grad_norm": 0.12628839910030365,
      "learning_rate": 7.202372158658627e-07,
      "loss": 0.0179,
      "step": 11358
    },
    {
      "epoch": 2.770487804878049,
      "grad_norm": 0.15906092524528503,
      "learning_rate": 7.187162269823494e-07,
      "loss": 0.0163,
      "step": 11359
    },
    {
      "epoch": 2.770731707317073,
      "grad_norm": 0.14248798787593842,
      "learning_rate": 7.171968223782688e-07,
      "loss": 0.0218,
      "step": 11360
    },
    {
      "epoch": 2.7709756097560976,
      "grad_norm": 0.14596782624721527,
      "learning_rate": 7.156790021527555e-07,
      "loss": 0.0217,
      "step": 11361
    },
    {
      "epoch": 2.771219512195122,
      "grad_norm": 0.13950040936470032,
      "learning_rate": 7.141627664048495e-07,
      "loss": 0.0091,
      "step": 11362
    },
    {
      "epoch": 2.7714634146341464,
      "grad_norm": 0.09351734817028046,
      "learning_rate": 7.126481152334746e-07,
      "loss": 0.0123,
      "step": 11363
    },
    {
      "epoch": 2.7717073170731705,
      "grad_norm": 0.0959228053689003,
      "learning_rate": 7.111350487374602e-07,
      "loss": 0.0147,
      "step": 11364
    },
    {
      "epoch": 2.771951219512195,
      "grad_norm": 0.12032601982355118,
      "learning_rate": 7.096235670155299e-07,
      "loss": 0.0151,
      "step": 11365
    },
    {
      "epoch": 2.7721951219512198,
      "grad_norm": 0.07183866202831268,
      "learning_rate": 7.081136701662993e-07,
      "loss": 0.0135,
      "step": 11366
    },
    {
      "epoch": 2.772439024390244,
      "grad_norm": 0.1264556497335434,
      "learning_rate": 7.06605358288287e-07,
      "loss": 0.0169,
      "step": 11367
    },
    {
      "epoch": 2.772682926829268,
      "grad_norm": 0.13789229094982147,
      "learning_rate": 7.050986314799113e-07,
      "loss": 0.0115,
      "step": 11368
    },
    {
      "epoch": 2.7729268292682927,
      "grad_norm": 0.11949227005243301,
      "learning_rate": 7.035934898394714e-07,
      "loss": 0.0173,
      "step": 11369
    },
    {
      "epoch": 2.7731707317073173,
      "grad_norm": 0.1396062821149826,
      "learning_rate": 7.020899334651804e-07,
      "loss": 0.0132,
      "step": 11370
    },
    {
      "epoch": 2.7734146341463415,
      "grad_norm": 0.059364743530750275,
      "learning_rate": 7.00587962455132e-07,
      "loss": 0.0127,
      "step": 11371
    },
    {
      "epoch": 2.7736585365853657,
      "grad_norm": 0.0898280218243599,
      "learning_rate": 6.990875769073369e-07,
      "loss": 0.0069,
      "step": 11372
    },
    {
      "epoch": 2.7739024390243903,
      "grad_norm": 0.11886443197727203,
      "learning_rate": 6.97588776919686e-07,
      "loss": 0.0094,
      "step": 11373
    },
    {
      "epoch": 2.774146341463415,
      "grad_norm": 0.28920698165893555,
      "learning_rate": 6.960915625899677e-07,
      "loss": 0.0156,
      "step": 11374
    },
    {
      "epoch": 2.774390243902439,
      "grad_norm": 0.12687428295612335,
      "learning_rate": 6.945959340158737e-07,
      "loss": 0.0094,
      "step": 11375
    },
    {
      "epoch": 2.774634146341463,
      "grad_norm": 0.11143730580806732,
      "learning_rate": 6.931018912949921e-07,
      "loss": 0.0222,
      "step": 11376
    },
    {
      "epoch": 2.774878048780488,
      "grad_norm": 0.09135692566633224,
      "learning_rate": 6.916094345247981e-07,
      "loss": 0.0198,
      "step": 11377
    },
    {
      "epoch": 2.7751219512195124,
      "grad_norm": 0.1168225035071373,
      "learning_rate": 6.901185638026747e-07,
      "loss": 0.023,
      "step": 11378
    },
    {
      "epoch": 2.7753658536585366,
      "grad_norm": 0.1599571704864502,
      "learning_rate": 6.886292792258941e-07,
      "loss": 0.0192,
      "step": 11379
    },
    {
      "epoch": 2.7756097560975608,
      "grad_norm": 0.16767561435699463,
      "learning_rate": 6.871415808916287e-07,
      "loss": 0.0289,
      "step": 11380
    },
    {
      "epoch": 2.7758536585365854,
      "grad_norm": 0.16075371205806732,
      "learning_rate": 6.85655468896948e-07,
      "loss": 0.0112,
      "step": 11381
    },
    {
      "epoch": 2.77609756097561,
      "grad_norm": 0.22243906557559967,
      "learning_rate": 6.841709433388133e-07,
      "loss": 0.0161,
      "step": 11382
    },
    {
      "epoch": 2.776341463414634,
      "grad_norm": 0.09857310354709625,
      "learning_rate": 6.826880043140888e-07,
      "loss": 0.0157,
      "step": 11383
    },
    {
      "epoch": 2.7765853658536583,
      "grad_norm": 0.1395740807056427,
      "learning_rate": 6.812066519195276e-07,
      "loss": 0.0256,
      "step": 11384
    },
    {
      "epoch": 2.776829268292683,
      "grad_norm": 0.15498434007167816,
      "learning_rate": 6.797268862517886e-07,
      "loss": 0.0242,
      "step": 11385
    },
    {
      "epoch": 2.7770731707317076,
      "grad_norm": 0.13234198093414307,
      "learning_rate": 6.782487074074167e-07,
      "loss": 0.0216,
      "step": 11386
    },
    {
      "epoch": 2.7773170731707317,
      "grad_norm": 0.14673857390880585,
      "learning_rate": 6.767721154828572e-07,
      "loss": 0.0162,
      "step": 11387
    },
    {
      "epoch": 2.777560975609756,
      "grad_norm": 0.10603611916303635,
      "learning_rate": 6.752971105744604e-07,
      "loss": 0.0123,
      "step": 11388
    },
    {
      "epoch": 2.7778048780487805,
      "grad_norm": 0.06677804887294769,
      "learning_rate": 6.738236927784636e-07,
      "loss": 0.0111,
      "step": 11389
    },
    {
      "epoch": 2.778048780487805,
      "grad_norm": 0.2825460433959961,
      "learning_rate": 6.723518621909952e-07,
      "loss": 0.0234,
      "step": 11390
    },
    {
      "epoch": 2.7782926829268293,
      "grad_norm": 0.097871333360672,
      "learning_rate": 6.70881618908098e-07,
      "loss": 0.0149,
      "step": 11391
    },
    {
      "epoch": 2.7785365853658535,
      "grad_norm": 0.11944764852523804,
      "learning_rate": 6.694129630256951e-07,
      "loss": 0.0154,
      "step": 11392
    },
    {
      "epoch": 2.778780487804878,
      "grad_norm": 0.1759302169084549,
      "learning_rate": 6.679458946396156e-07,
      "loss": 0.0116,
      "step": 11393
    },
    {
      "epoch": 2.7790243902439027,
      "grad_norm": 0.13662563264369965,
      "learning_rate": 6.664804138455771e-07,
      "loss": 0.0088,
      "step": 11394
    },
    {
      "epoch": 2.779268292682927,
      "grad_norm": 0.2613115906715393,
      "learning_rate": 6.650165207391978e-07,
      "loss": 0.0296,
      "step": 11395
    },
    {
      "epoch": 2.779512195121951,
      "grad_norm": 0.11610884219408035,
      "learning_rate": 6.635542154159984e-07,
      "loss": 0.0194,
      "step": 11396
    },
    {
      "epoch": 2.7797560975609756,
      "grad_norm": 0.17598162591457367,
      "learning_rate": 6.620934979713833e-07,
      "loss": 0.0191,
      "step": 11397
    },
    {
      "epoch": 2.7800000000000002,
      "grad_norm": 0.10500141233205795,
      "learning_rate": 6.606343685006594e-07,
      "loss": 0.0183,
      "step": 11398
    },
    {
      "epoch": 2.7802439024390244,
      "grad_norm": 0.13304032385349274,
      "learning_rate": 6.591768270990368e-07,
      "loss": 0.0188,
      "step": 11399
    },
    {
      "epoch": 2.7804878048780486,
      "grad_norm": 0.08845651149749756,
      "learning_rate": 6.577208738616114e-07,
      "loss": 0.0083,
      "step": 11400
    },
    {
      "epoch": 2.780731707317073,
      "grad_norm": 0.1300186812877655,
      "learning_rate": 6.562665088833797e-07,
      "loss": 0.0207,
      "step": 11401
    },
    {
      "epoch": 2.780975609756098,
      "grad_norm": 0.14599010348320007,
      "learning_rate": 6.548137322592352e-07,
      "loss": 0.0215,
      "step": 11402
    },
    {
      "epoch": 2.781219512195122,
      "grad_norm": 0.09491806477308273,
      "learning_rate": 6.533625440839658e-07,
      "loss": 0.0106,
      "step": 11403
    },
    {
      "epoch": 2.781463414634146,
      "grad_norm": 0.1136436015367508,
      "learning_rate": 6.519129444522598e-07,
      "loss": 0.0076,
      "step": 11404
    },
    {
      "epoch": 2.7817073170731708,
      "grad_norm": 0.14345552027225494,
      "learning_rate": 6.504649334586943e-07,
      "loss": 0.0223,
      "step": 11405
    },
    {
      "epoch": 2.7819512195121954,
      "grad_norm": 0.22813138365745544,
      "learning_rate": 6.49018511197752e-07,
      "loss": 0.0166,
      "step": 11406
    },
    {
      "epoch": 2.7821951219512195,
      "grad_norm": 0.08571531623601913,
      "learning_rate": 6.475736777638102e-07,
      "loss": 0.0081,
      "step": 11407
    },
    {
      "epoch": 2.7824390243902437,
      "grad_norm": 0.034219905734062195,
      "learning_rate": 6.461304332511297e-07,
      "loss": 0.0021,
      "step": 11408
    },
    {
      "epoch": 2.7826829268292683,
      "grad_norm": 0.2526950538158417,
      "learning_rate": 6.446887777538879e-07,
      "loss": 0.019,
      "step": 11409
    },
    {
      "epoch": 2.782926829268293,
      "grad_norm": 0.08615034818649292,
      "learning_rate": 6.432487113661456e-07,
      "loss": 0.0143,
      "step": 11410
    },
    {
      "epoch": 2.783170731707317,
      "grad_norm": 0.07476077973842621,
      "learning_rate": 6.418102341818583e-07,
      "loss": 0.0131,
      "step": 11411
    },
    {
      "epoch": 2.7834146341463413,
      "grad_norm": 0.13466954231262207,
      "learning_rate": 6.403733462948869e-07,
      "loss": 0.013,
      "step": 11412
    },
    {
      "epoch": 2.783658536585366,
      "grad_norm": 0.1413688361644745,
      "learning_rate": 6.389380477989842e-07,
      "loss": 0.0133,
      "step": 11413
    },
    {
      "epoch": 2.7839024390243905,
      "grad_norm": 0.12536336481571198,
      "learning_rate": 6.37504338787795e-07,
      "loss": 0.022,
      "step": 11414
    },
    {
      "epoch": 2.7841463414634147,
      "grad_norm": 0.1340416967868805,
      "learning_rate": 6.360722193548691e-07,
      "loss": 0.0138,
      "step": 11415
    },
    {
      "epoch": 2.784390243902439,
      "grad_norm": 0.08945706486701965,
      "learning_rate": 6.34641689593643e-07,
      "loss": 0.011,
      "step": 11416
    },
    {
      "epoch": 2.7846341463414634,
      "grad_norm": 0.18048977851867676,
      "learning_rate": 6.332127495974643e-07,
      "loss": 0.0084,
      "step": 11417
    },
    {
      "epoch": 2.784878048780488,
      "grad_norm": 0.149583637714386,
      "learning_rate": 6.31785399459553e-07,
      "loss": 0.0183,
      "step": 11418
    },
    {
      "epoch": 2.7851219512195122,
      "grad_norm": 0.07620259374380112,
      "learning_rate": 6.303596392730482e-07,
      "loss": 0.0132,
      "step": 11419
    },
    {
      "epoch": 2.7853658536585364,
      "grad_norm": 0.1713683009147644,
      "learning_rate": 6.289354691309757e-07,
      "loss": 0.0221,
      "step": 11420
    },
    {
      "epoch": 2.785609756097561,
      "grad_norm": 0.0963665321469307,
      "learning_rate": 6.275128891262527e-07,
      "loss": 0.0161,
      "step": 11421
    },
    {
      "epoch": 2.7858536585365856,
      "grad_norm": 0.10698787122964859,
      "learning_rate": 6.260918993517051e-07,
      "loss": 0.0141,
      "step": 11422
    },
    {
      "epoch": 2.78609756097561,
      "grad_norm": 0.09662683308124542,
      "learning_rate": 6.246724999000475e-07,
      "loss": 0.0166,
      "step": 11423
    },
    {
      "epoch": 2.786341463414634,
      "grad_norm": 0.1320096105337143,
      "learning_rate": 6.232546908638837e-07,
      "loss": 0.0166,
      "step": 11424
    },
    {
      "epoch": 2.7865853658536586,
      "grad_norm": 0.08829456567764282,
      "learning_rate": 6.218384723357312e-07,
      "loss": 0.0107,
      "step": 11425
    },
    {
      "epoch": 2.786829268292683,
      "grad_norm": 0.09988385438919067,
      "learning_rate": 6.204238444079885e-07,
      "loss": 0.0118,
      "step": 11426
    },
    {
      "epoch": 2.7870731707317074,
      "grad_norm": 0.1246192529797554,
      "learning_rate": 6.190108071729567e-07,
      "loss": 0.0229,
      "step": 11427
    },
    {
      "epoch": 2.7873170731707315,
      "grad_norm": 0.06906572729349136,
      "learning_rate": 6.175993607228314e-07,
      "loss": 0.0066,
      "step": 11428
    },
    {
      "epoch": 2.787560975609756,
      "grad_norm": 0.17446501553058624,
      "learning_rate": 6.161895051497057e-07,
      "loss": 0.0156,
      "step": 11429
    },
    {
      "epoch": 2.7878048780487807,
      "grad_norm": 0.18524540960788727,
      "learning_rate": 6.147812405455727e-07,
      "loss": 0.0186,
      "step": 11430
    },
    {
      "epoch": 2.788048780487805,
      "grad_norm": 0.10225626081228256,
      "learning_rate": 6.133745670023116e-07,
      "loss": 0.0106,
      "step": 11431
    },
    {
      "epoch": 2.788292682926829,
      "grad_norm": 0.1190110594034195,
      "learning_rate": 6.119694846117046e-07,
      "loss": 0.0221,
      "step": 11432
    },
    {
      "epoch": 2.7885365853658537,
      "grad_norm": 0.11499451100826263,
      "learning_rate": 6.105659934654284e-07,
      "loss": 0.016,
      "step": 11433
    },
    {
      "epoch": 2.7887804878048783,
      "grad_norm": 0.11409692466259003,
      "learning_rate": 6.091640936550625e-07,
      "loss": 0.0151,
      "step": 11434
    },
    {
      "epoch": 2.7890243902439025,
      "grad_norm": 0.09939514845609665,
      "learning_rate": 6.077637852720697e-07,
      "loss": 0.0154,
      "step": 11435
    },
    {
      "epoch": 2.7892682926829266,
      "grad_norm": 0.07725751399993896,
      "learning_rate": 6.063650684078187e-07,
      "loss": 0.0137,
      "step": 11436
    },
    {
      "epoch": 2.7895121951219513,
      "grad_norm": 0.20119774341583252,
      "learning_rate": 6.049679431535671e-07,
      "loss": 0.0137,
      "step": 11437
    },
    {
      "epoch": 2.789756097560976,
      "grad_norm": 0.17916150391101837,
      "learning_rate": 6.035724096004808e-07,
      "loss": 0.0252,
      "step": 11438
    },
    {
      "epoch": 2.79,
      "grad_norm": 0.09311922639608383,
      "learning_rate": 6.021784678396092e-07,
      "loss": 0.013,
      "step": 11439
    },
    {
      "epoch": 2.790243902439024,
      "grad_norm": 0.14077141880989075,
      "learning_rate": 6.007861179619018e-07,
      "loss": 0.0237,
      "step": 11440
    },
    {
      "epoch": 2.790487804878049,
      "grad_norm": 0.08043529093265533,
      "learning_rate": 5.99395360058208e-07,
      "loss": 0.0108,
      "step": 11441
    },
    {
      "epoch": 2.7907317073170734,
      "grad_norm": 0.13089345395565033,
      "learning_rate": 5.980061942192694e-07,
      "loss": 0.0146,
      "step": 11442
    },
    {
      "epoch": 2.7909756097560976,
      "grad_norm": 0.11010124534368515,
      "learning_rate": 5.966186205357272e-07,
      "loss": 0.0145,
      "step": 11443
    },
    {
      "epoch": 2.7912195121951218,
      "grad_norm": 0.1630217730998993,
      "learning_rate": 5.952326390981117e-07,
      "loss": 0.0196,
      "step": 11444
    },
    {
      "epoch": 2.7914634146341464,
      "grad_norm": 0.12011858075857162,
      "learning_rate": 5.938482499968562e-07,
      "loss": 0.0168,
      "step": 11445
    },
    {
      "epoch": 2.791707317073171,
      "grad_norm": 0.13377223908901215,
      "learning_rate": 5.924654533222884e-07,
      "loss": 0.021,
      "step": 11446
    },
    {
      "epoch": 2.791951219512195,
      "grad_norm": 0.0882079005241394,
      "learning_rate": 5.910842491646334e-07,
      "loss": 0.012,
      "step": 11447
    },
    {
      "epoch": 2.7921951219512193,
      "grad_norm": 0.11559467762708664,
      "learning_rate": 5.897046376140053e-07,
      "loss": 0.0142,
      "step": 11448
    },
    {
      "epoch": 2.792439024390244,
      "grad_norm": 0.11699667572975159,
      "learning_rate": 5.883266187604237e-07,
      "loss": 0.0098,
      "step": 11449
    },
    {
      "epoch": 2.7926829268292686,
      "grad_norm": 0.06926624476909637,
      "learning_rate": 5.869501926937998e-07,
      "loss": 0.0072,
      "step": 11450
    },
    {
      "epoch": 2.7929268292682927,
      "grad_norm": 0.0904773473739624,
      "learning_rate": 5.855753595039398e-07,
      "loss": 0.0071,
      "step": 11451
    },
    {
      "epoch": 2.793170731707317,
      "grad_norm": 0.19615577161312103,
      "learning_rate": 5.84202119280547e-07,
      "loss": 0.0119,
      "step": 11452
    },
    {
      "epoch": 2.7934146341463415,
      "grad_norm": 0.08605273813009262,
      "learning_rate": 5.828304721132244e-07,
      "loss": 0.0109,
      "step": 11453
    },
    {
      "epoch": 2.793658536585366,
      "grad_norm": 0.1330181360244751,
      "learning_rate": 5.814604180914646e-07,
      "loss": 0.0232,
      "step": 11454
    },
    {
      "epoch": 2.7939024390243903,
      "grad_norm": 0.09909454733133316,
      "learning_rate": 5.80091957304657e-07,
      "loss": 0.0179,
      "step": 11455
    },
    {
      "epoch": 2.7941463414634145,
      "grad_norm": 0.13212884962558746,
      "learning_rate": 5.787250898420942e-07,
      "loss": 0.0233,
      "step": 11456
    },
    {
      "epoch": 2.794390243902439,
      "grad_norm": 0.06261271983385086,
      "learning_rate": 5.773598157929605e-07,
      "loss": 0.0077,
      "step": 11457
    },
    {
      "epoch": 2.7946341463414637,
      "grad_norm": 0.11999164521694183,
      "learning_rate": 5.759961352463317e-07,
      "loss": 0.0135,
      "step": 11458
    },
    {
      "epoch": 2.794878048780488,
      "grad_norm": 0.14750756323337555,
      "learning_rate": 5.746340482911867e-07,
      "loss": 0.0243,
      "step": 11459
    },
    {
      "epoch": 2.795121951219512,
      "grad_norm": 0.12862101197242737,
      "learning_rate": 5.732735550163986e-07,
      "loss": 0.0114,
      "step": 11460
    },
    {
      "epoch": 2.7953658536585366,
      "grad_norm": 0.08020251989364624,
      "learning_rate": 5.719146555107302e-07,
      "loss": 0.0045,
      "step": 11461
    },
    {
      "epoch": 2.7956097560975612,
      "grad_norm": 0.09745676815509796,
      "learning_rate": 5.705573498628519e-07,
      "loss": 0.0126,
      "step": 11462
    },
    {
      "epoch": 2.7958536585365854,
      "grad_norm": 0.14981822669506073,
      "learning_rate": 5.69201638161318e-07,
      "loss": 0.0207,
      "step": 11463
    },
    {
      "epoch": 2.7960975609756096,
      "grad_norm": 0.2711992859840393,
      "learning_rate": 5.67847520494591e-07,
      "loss": 0.0491,
      "step": 11464
    },
    {
      "epoch": 2.796341463414634,
      "grad_norm": 0.16707922518253326,
      "learning_rate": 5.664949969510169e-07,
      "loss": 0.024,
      "step": 11465
    },
    {
      "epoch": 2.796585365853659,
      "grad_norm": 0.12213724106550217,
      "learning_rate": 5.651440676188447e-07,
      "loss": 0.0252,
      "step": 11466
    },
    {
      "epoch": 2.796829268292683,
      "grad_norm": 0.08660343289375305,
      "learning_rate": 5.637947325862258e-07,
      "loss": 0.0157,
      "step": 11467
    },
    {
      "epoch": 2.797073170731707,
      "grad_norm": 0.1015811488032341,
      "learning_rate": 5.624469919411873e-07,
      "loss": 0.0089,
      "step": 11468
    },
    {
      "epoch": 2.7973170731707317,
      "grad_norm": 0.5224908590316772,
      "learning_rate": 5.611008457716783e-07,
      "loss": 0.0323,
      "step": 11469
    },
    {
      "epoch": 2.7975609756097564,
      "grad_norm": 0.07551659643650055,
      "learning_rate": 5.597562941655199e-07,
      "loss": 0.0056,
      "step": 11470
    },
    {
      "epoch": 2.7978048780487805,
      "grad_norm": 0.10158896446228027,
      "learning_rate": 5.584133372104449e-07,
      "loss": 0.0262,
      "step": 11471
    },
    {
      "epoch": 2.7980487804878047,
      "grad_norm": 0.10283052921295166,
      "learning_rate": 5.570719749940778e-07,
      "loss": 0.0051,
      "step": 11472
    },
    {
      "epoch": 2.7982926829268293,
      "grad_norm": 0.11753919720649719,
      "learning_rate": 5.557322076039373e-07,
      "loss": 0.0122,
      "step": 11473
    },
    {
      "epoch": 2.798536585365854,
      "grad_norm": 0.15600943565368652,
      "learning_rate": 5.543940351274396e-07,
      "loss": 0.027,
      "step": 11474
    },
    {
      "epoch": 2.798780487804878,
      "grad_norm": 0.10260883718729019,
      "learning_rate": 5.530574576518954e-07,
      "loss": 0.0203,
      "step": 11475
    },
    {
      "epoch": 2.7990243902439023,
      "grad_norm": 0.10805967450141907,
      "learning_rate": 5.5172247526451e-07,
      "loss": 0.0181,
      "step": 11476
    },
    {
      "epoch": 2.799268292682927,
      "grad_norm": 0.09312140196561813,
      "learning_rate": 5.503890880523944e-07,
      "loss": 0.0153,
      "step": 11477
    },
    {
      "epoch": 2.7995121951219515,
      "grad_norm": 0.21926249563694,
      "learning_rate": 5.490572961025425e-07,
      "loss": 0.0303,
      "step": 11478
    },
    {
      "epoch": 2.7997560975609757,
      "grad_norm": 0.16892258822917938,
      "learning_rate": 5.477270995018463e-07,
      "loss": 0.0207,
      "step": 11479
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.12086272239685059,
      "learning_rate": 5.463984983371057e-07,
      "loss": 0.0132,
      "step": 11480
    },
    {
      "epoch": 2.8002439024390244,
      "grad_norm": 0.09727369248867035,
      "learning_rate": 5.45071492695004e-07,
      "loss": 0.01,
      "step": 11481
    },
    {
      "epoch": 2.800487804878049,
      "grad_norm": 0.1190553605556488,
      "learning_rate": 5.437460826621194e-07,
      "loss": 0.023,
      "step": 11482
    },
    {
      "epoch": 2.800731707317073,
      "grad_norm": 0.14774775505065918,
      "learning_rate": 5.424222683249408e-07,
      "loss": 0.0213,
      "step": 11483
    },
    {
      "epoch": 2.8009756097560974,
      "grad_norm": 0.15950457751750946,
      "learning_rate": 5.411000497698326e-07,
      "loss": 0.0156,
      "step": 11484
    },
    {
      "epoch": 2.801219512195122,
      "grad_norm": 0.1202915608882904,
      "learning_rate": 5.397794270830731e-07,
      "loss": 0.024,
      "step": 11485
    },
    {
      "epoch": 2.8014634146341466,
      "grad_norm": 0.16579285264015198,
      "learning_rate": 5.384604003508265e-07,
      "loss": 0.0109,
      "step": 11486
    },
    {
      "epoch": 2.8017073170731708,
      "grad_norm": 0.16472750902175903,
      "learning_rate": 5.371429696591517e-07,
      "loss": 0.015,
      "step": 11487
    },
    {
      "epoch": 2.801951219512195,
      "grad_norm": 0.19866079092025757,
      "learning_rate": 5.358271350940108e-07,
      "loss": 0.0184,
      "step": 11488
    },
    {
      "epoch": 2.8021951219512196,
      "grad_norm": 0.09151232242584229,
      "learning_rate": 5.345128967412572e-07,
      "loss": 0.0129,
      "step": 11489
    },
    {
      "epoch": 2.802439024390244,
      "grad_norm": 0.09433133155107498,
      "learning_rate": 5.332002546866415e-07,
      "loss": 0.0152,
      "step": 11490
    },
    {
      "epoch": 2.8026829268292683,
      "grad_norm": 0.08187264949083328,
      "learning_rate": 5.318892090158123e-07,
      "loss": 0.0166,
      "step": 11491
    },
    {
      "epoch": 2.8029268292682925,
      "grad_norm": 0.11293118447065353,
      "learning_rate": 5.305797598143036e-07,
      "loss": 0.0046,
      "step": 11492
    },
    {
      "epoch": 2.803170731707317,
      "grad_norm": 0.0787639245390892,
      "learning_rate": 5.292719071675584e-07,
      "loss": 0.0081,
      "step": 11493
    },
    {
      "epoch": 2.8034146341463417,
      "grad_norm": 0.13454249501228333,
      "learning_rate": 5.27965651160911e-07,
      "loss": 0.011,
      "step": 11494
    },
    {
      "epoch": 2.803658536585366,
      "grad_norm": 0.18418879806995392,
      "learning_rate": 5.266609918795878e-07,
      "loss": 0.0126,
      "step": 11495
    },
    {
      "epoch": 2.80390243902439,
      "grad_norm": 0.06840166449546814,
      "learning_rate": 5.253579294087151e-07,
      "loss": 0.0135,
      "step": 11496
    },
    {
      "epoch": 2.8041463414634147,
      "grad_norm": 0.13456539809703827,
      "learning_rate": 5.240564638333111e-07,
      "loss": 0.0231,
      "step": 11497
    },
    {
      "epoch": 2.8043902439024393,
      "grad_norm": 0.11282695084810257,
      "learning_rate": 5.227565952382968e-07,
      "loss": 0.0126,
      "step": 11498
    },
    {
      "epoch": 2.8046341463414635,
      "grad_norm": 0.14206206798553467,
      "learning_rate": 5.214583237084847e-07,
      "loss": 0.0152,
      "step": 11499
    },
    {
      "epoch": 2.8048780487804876,
      "grad_norm": 0.06596602499485016,
      "learning_rate": 5.201616493285793e-07,
      "loss": 0.004,
      "step": 11500
    },
    {
      "epoch": 2.8051219512195122,
      "grad_norm": 0.11143577843904495,
      "learning_rate": 5.188665721831854e-07,
      "loss": 0.0156,
      "step": 11501
    },
    {
      "epoch": 2.805365853658537,
      "grad_norm": 0.14626885950565338,
      "learning_rate": 5.175730923568045e-07,
      "loss": 0.019,
      "step": 11502
    },
    {
      "epoch": 2.805609756097561,
      "grad_norm": 0.11118268966674805,
      "learning_rate": 5.162812099338305e-07,
      "loss": 0.0157,
      "step": 11503
    },
    {
      "epoch": 2.805853658536585,
      "grad_norm": 0.09966043382883072,
      "learning_rate": 5.149909249985568e-07,
      "loss": 0.0113,
      "step": 11504
    },
    {
      "epoch": 2.80609756097561,
      "grad_norm": 0.10771805793046951,
      "learning_rate": 5.137022376351692e-07,
      "loss": 0.0183,
      "step": 11505
    },
    {
      "epoch": 2.8063414634146344,
      "grad_norm": 0.1121436357498169,
      "learning_rate": 5.124151479277501e-07,
      "loss": 0.0146,
      "step": 11506
    },
    {
      "epoch": 2.8065853658536586,
      "grad_norm": 0.1251152753829956,
      "learning_rate": 5.111296559602824e-07,
      "loss": 0.0205,
      "step": 11507
    },
    {
      "epoch": 2.8068292682926828,
      "grad_norm": 0.18091970682144165,
      "learning_rate": 5.098457618166325e-07,
      "loss": 0.0094,
      "step": 11508
    },
    {
      "epoch": 2.8070731707317074,
      "grad_norm": 0.1881626397371292,
      "learning_rate": 5.085634655805749e-07,
      "loss": 0.0223,
      "step": 11509
    },
    {
      "epoch": 2.807317073170732,
      "grad_norm": 0.13146436214447021,
      "learning_rate": 5.072827673357761e-07,
      "loss": 0.0274,
      "step": 11510
    },
    {
      "epoch": 2.807560975609756,
      "grad_norm": 0.18549185991287231,
      "learning_rate": 5.060036671657997e-07,
      "loss": 0.0227,
      "step": 11511
    },
    {
      "epoch": 2.8078048780487803,
      "grad_norm": 0.30093804001808167,
      "learning_rate": 5.047261651541013e-07,
      "loss": 0.0393,
      "step": 11512
    },
    {
      "epoch": 2.808048780487805,
      "grad_norm": 0.15403638780117035,
      "learning_rate": 5.034502613840309e-07,
      "loss": 0.0155,
      "step": 11513
    },
    {
      "epoch": 2.8082926829268295,
      "grad_norm": 0.13133402168750763,
      "learning_rate": 5.021759559388411e-07,
      "loss": 0.0137,
      "step": 11514
    },
    {
      "epoch": 2.8085365853658537,
      "grad_norm": 0.09036138653755188,
      "learning_rate": 5.009032489016741e-07,
      "loss": 0.0144,
      "step": 11515
    },
    {
      "epoch": 2.808780487804878,
      "grad_norm": 0.1333112269639969,
      "learning_rate": 4.996321403555742e-07,
      "loss": 0.0256,
      "step": 11516
    },
    {
      "epoch": 2.8090243902439025,
      "grad_norm": 0.14724642038345337,
      "learning_rate": 4.983626303834726e-07,
      "loss": 0.0163,
      "step": 11517
    },
    {
      "epoch": 2.809268292682927,
      "grad_norm": 0.07458020001649857,
      "learning_rate": 4.970947190682001e-07,
      "loss": 0.0076,
      "step": 11518
    },
    {
      "epoch": 2.8095121951219513,
      "grad_norm": 0.12844330072402954,
      "learning_rate": 4.958284064924906e-07,
      "loss": 0.0154,
      "step": 11519
    },
    {
      "epoch": 2.8097560975609754,
      "grad_norm": 0.08013942092657089,
      "learning_rate": 4.945636927389613e-07,
      "loss": 0.0107,
      "step": 11520
    },
    {
      "epoch": 2.81,
      "grad_norm": 0.16434337198734283,
      "learning_rate": 4.933005778901295e-07,
      "loss": 0.0175,
      "step": 11521
    },
    {
      "epoch": 2.8102439024390247,
      "grad_norm": 0.10187879204750061,
      "learning_rate": 4.920390620284182e-07,
      "loss": 0.013,
      "step": 11522
    },
    {
      "epoch": 2.810487804878049,
      "grad_norm": 0.1251244693994522,
      "learning_rate": 4.907791452361282e-07,
      "loss": 0.03,
      "step": 11523
    },
    {
      "epoch": 2.810731707317073,
      "grad_norm": 0.09964939951896667,
      "learning_rate": 4.895208275954688e-07,
      "loss": 0.013,
      "step": 11524
    },
    {
      "epoch": 2.8109756097560976,
      "grad_norm": 0.15164010226726532,
      "learning_rate": 4.882641091885437e-07,
      "loss": 0.0161,
      "step": 11525
    },
    {
      "epoch": 2.8112195121951222,
      "grad_norm": 0.14641351997852325,
      "learning_rate": 4.870089900973485e-07,
      "loss": 0.0088,
      "step": 11526
    },
    {
      "epoch": 2.8114634146341464,
      "grad_norm": 0.13550624251365662,
      "learning_rate": 4.857554704037731e-07,
      "loss": 0.0193,
      "step": 11527
    },
    {
      "epoch": 2.8117073170731706,
      "grad_norm": 0.11654082685709,
      "learning_rate": 4.845035501896106e-07,
      "loss": 0.0152,
      "step": 11528
    },
    {
      "epoch": 2.811951219512195,
      "grad_norm": 0.11238501965999603,
      "learning_rate": 4.8325322953654e-07,
      "loss": 0.0147,
      "step": 11529
    },
    {
      "epoch": 2.81219512195122,
      "grad_norm": 0.10232504457235336,
      "learning_rate": 4.820045085261432e-07,
      "loss": 0.0194,
      "step": 11530
    },
    {
      "epoch": 2.812439024390244,
      "grad_norm": 0.22467543184757233,
      "learning_rate": 4.807573872398968e-07,
      "loss": 0.0282,
      "step": 11531
    },
    {
      "epoch": 2.812682926829268,
      "grad_norm": 0.13908708095550537,
      "learning_rate": 4.79511865759169e-07,
      "loss": 0.022,
      "step": 11532
    },
    {
      "epoch": 2.8129268292682927,
      "grad_norm": 0.11190256476402283,
      "learning_rate": 4.78267944165231e-07,
      "loss": 0.0198,
      "step": 11533
    },
    {
      "epoch": 2.813170731707317,
      "grad_norm": 0.05857640504837036,
      "learning_rate": 4.770256225392372e-07,
      "loss": 0.0098,
      "step": 11534
    },
    {
      "epoch": 2.8134146341463415,
      "grad_norm": 0.1430595964193344,
      "learning_rate": 4.757849009622506e-07,
      "loss": 0.0132,
      "step": 11535
    },
    {
      "epoch": 2.8136585365853657,
      "grad_norm": 0.1764497458934784,
      "learning_rate": 4.7454577951522305e-07,
      "loss": 0.0195,
      "step": 11536
    },
    {
      "epoch": 2.8139024390243903,
      "grad_norm": 0.09633439779281616,
      "learning_rate": 4.7330825827900384e-07,
      "loss": 0.0227,
      "step": 11537
    },
    {
      "epoch": 2.8141463414634145,
      "grad_norm": 0.10762901604175568,
      "learning_rate": 4.7207233733433944e-07,
      "loss": 0.0183,
      "step": 11538
    },
    {
      "epoch": 2.814390243902439,
      "grad_norm": 0.12935638427734375,
      "learning_rate": 4.708380167618653e-07,
      "loss": 0.0227,
      "step": 11539
    },
    {
      "epoch": 2.8146341463414632,
      "grad_norm": 0.13549642264842987,
      "learning_rate": 4.696052966421199e-07,
      "loss": 0.0186,
      "step": 11540
    },
    {
      "epoch": 2.814878048780488,
      "grad_norm": 0.11548138409852982,
      "learning_rate": 4.683741770555361e-07,
      "loss": 0.0199,
      "step": 11541
    },
    {
      "epoch": 2.815121951219512,
      "grad_norm": 0.17612090706825256,
      "learning_rate": 4.671446580824357e-07,
      "loss": 0.0167,
      "step": 11542
    },
    {
      "epoch": 2.8153658536585366,
      "grad_norm": 0.10513471066951752,
      "learning_rate": 4.6591673980304616e-07,
      "loss": 0.0199,
      "step": 11543
    },
    {
      "epoch": 2.815609756097561,
      "grad_norm": 0.07917897403240204,
      "learning_rate": 4.6469042229748407e-07,
      "loss": 0.0135,
      "step": 11544
    },
    {
      "epoch": 2.8158536585365854,
      "grad_norm": 0.12790246307849884,
      "learning_rate": 4.634657056457603e-07,
      "loss": 0.023,
      "step": 11545
    },
    {
      "epoch": 2.8160975609756096,
      "grad_norm": 0.1877870410680771,
      "learning_rate": 4.62242589927786e-07,
      "loss": 0.0248,
      "step": 11546
    },
    {
      "epoch": 2.816341463414634,
      "grad_norm": 0.157087042927742,
      "learning_rate": 4.6102107522336403e-07,
      "loss": 0.0297,
      "step": 11547
    },
    {
      "epoch": 2.8165853658536584,
      "grad_norm": 0.1131555438041687,
      "learning_rate": 4.598011616122e-07,
      "loss": 0.0189,
      "step": 11548
    },
    {
      "epoch": 2.816829268292683,
      "grad_norm": 0.139815092086792,
      "learning_rate": 4.5858284917388303e-07,
      "loss": 0.0151,
      "step": 11549
    },
    {
      "epoch": 2.817073170731707,
      "grad_norm": 0.1527852863073349,
      "learning_rate": 4.5736613798790796e-07,
      "loss": 0.0161,
      "step": 11550
    },
    {
      "epoch": 2.8173170731707318,
      "grad_norm": 0.2219427078962326,
      "learning_rate": 4.561510281336584e-07,
      "loss": 0.017,
      "step": 11551
    },
    {
      "epoch": 2.817560975609756,
      "grad_norm": 0.0868292897939682,
      "learning_rate": 4.549375196904182e-07,
      "loss": 0.0088,
      "step": 11552
    },
    {
      "epoch": 2.8178048780487805,
      "grad_norm": 0.16066332161426544,
      "learning_rate": 4.537256127373685e-07,
      "loss": 0.0167,
      "step": 11553
    },
    {
      "epoch": 2.8180487804878047,
      "grad_norm": 0.10445234179496765,
      "learning_rate": 4.525153073535765e-07,
      "loss": 0.0107,
      "step": 11554
    },
    {
      "epoch": 2.8182926829268293,
      "grad_norm": 0.09282373636960983,
      "learning_rate": 4.513066036180125e-07,
      "loss": 0.0123,
      "step": 11555
    },
    {
      "epoch": 2.8185365853658535,
      "grad_norm": 0.20424695312976837,
      "learning_rate": 4.5009950160954384e-07,
      "loss": 0.0329,
      "step": 11556
    },
    {
      "epoch": 2.818780487804878,
      "grad_norm": 0.2821677625179291,
      "learning_rate": 4.4889400140692707e-07,
      "loss": 0.0258,
      "step": 11557
    },
    {
      "epoch": 2.8190243902439023,
      "grad_norm": 0.10607343167066574,
      "learning_rate": 4.4769010308881867e-07,
      "loss": 0.0224,
      "step": 11558
    },
    {
      "epoch": 2.819268292682927,
      "grad_norm": 0.0931672751903534,
      "learning_rate": 4.4648780673376967e-07,
      "loss": 0.0171,
      "step": 11559
    },
    {
      "epoch": 2.819512195121951,
      "grad_norm": 0.13957659900188446,
      "learning_rate": 4.4528711242022294e-07,
      "loss": 0.0174,
      "step": 11560
    },
    {
      "epoch": 2.8197560975609757,
      "grad_norm": 0.06435076892375946,
      "learning_rate": 4.4408802022652686e-07,
      "loss": 0.0064,
      "step": 11561
    },
    {
      "epoch": 2.82,
      "grad_norm": 0.18130594491958618,
      "learning_rate": 4.428905302309161e-07,
      "loss": 0.0265,
      "step": 11562
    },
    {
      "epoch": 2.8202439024390245,
      "grad_norm": 0.24327677488327026,
      "learning_rate": 4.4169464251151704e-07,
      "loss": 0.0329,
      "step": 11563
    },
    {
      "epoch": 2.8204878048780486,
      "grad_norm": 0.14332804083824158,
      "learning_rate": 4.405003571463645e-07,
      "loss": 0.0217,
      "step": 11564
    },
    {
      "epoch": 2.8207317073170732,
      "grad_norm": 0.1951020210981369,
      "learning_rate": 4.3930767421337947e-07,
      "loss": 0.0121,
      "step": 11565
    },
    {
      "epoch": 2.8209756097560974,
      "grad_norm": 0.08349129557609558,
      "learning_rate": 4.3811659379038305e-07,
      "loss": 0.0136,
      "step": 11566
    },
    {
      "epoch": 2.821219512195122,
      "grad_norm": 0.23214717209339142,
      "learning_rate": 4.369271159550853e-07,
      "loss": 0.0264,
      "step": 11567
    },
    {
      "epoch": 2.821463414634146,
      "grad_norm": 0.09828490763902664,
      "learning_rate": 4.3573924078509907e-07,
      "loss": 0.0161,
      "step": 11568
    },
    {
      "epoch": 2.821707317073171,
      "grad_norm": 0.09966626018285751,
      "learning_rate": 4.3455296835792913e-07,
      "loss": 0.0192,
      "step": 11569
    },
    {
      "epoch": 2.821951219512195,
      "grad_norm": 0.0984138771891594,
      "learning_rate": 4.333682987509774e-07,
      "loss": 0.0113,
      "step": 11570
    },
    {
      "epoch": 2.8221951219512196,
      "grad_norm": 0.07056957483291626,
      "learning_rate": 4.321852320415348e-07,
      "loss": 0.014,
      "step": 11571
    },
    {
      "epoch": 2.8224390243902437,
      "grad_norm": 0.22686339914798737,
      "learning_rate": 4.310037683068008e-07,
      "loss": 0.0118,
      "step": 11572
    },
    {
      "epoch": 2.8226829268292684,
      "grad_norm": 0.17429937422275543,
      "learning_rate": 4.298239076238553e-07,
      "loss": 0.0245,
      "step": 11573
    },
    {
      "epoch": 2.8229268292682925,
      "grad_norm": 0.20704329013824463,
      "learning_rate": 4.2864565006968393e-07,
      "loss": 0.0237,
      "step": 11574
    },
    {
      "epoch": 2.823170731707317,
      "grad_norm": 0.10555054247379303,
      "learning_rate": 4.274689957211642e-07,
      "loss": 0.0145,
      "step": 11575
    },
    {
      "epoch": 2.8234146341463413,
      "grad_norm": 0.09870723634958267,
      "learning_rate": 4.262939446550679e-07,
      "loss": 0.0184,
      "step": 11576
    },
    {
      "epoch": 2.823658536585366,
      "grad_norm": 0.1323590874671936,
      "learning_rate": 4.251204969480643e-07,
      "loss": 0.0114,
      "step": 11577
    },
    {
      "epoch": 2.82390243902439,
      "grad_norm": 0.268856018781662,
      "learning_rate": 4.2394865267671724e-07,
      "loss": 0.0213,
      "step": 11578
    },
    {
      "epoch": 2.8241463414634147,
      "grad_norm": 0.14152012765407562,
      "learning_rate": 4.227784119174877e-07,
      "loss": 0.0217,
      "step": 11579
    },
    {
      "epoch": 2.824390243902439,
      "grad_norm": 0.12353930622339249,
      "learning_rate": 4.2160977474672847e-07,
      "loss": 0.0068,
      "step": 11580
    },
    {
      "epoch": 2.8246341463414635,
      "grad_norm": 0.18141640722751617,
      "learning_rate": 4.2044274124068697e-07,
      "loss": 0.0148,
      "step": 11581
    },
    {
      "epoch": 2.8248780487804876,
      "grad_norm": 0.11653254926204681,
      "learning_rate": 4.1927731147551606e-07,
      "loss": 0.0177,
      "step": 11582
    },
    {
      "epoch": 2.8251219512195123,
      "grad_norm": 0.07520020753145218,
      "learning_rate": 4.1811348552724674e-07,
      "loss": 0.0108,
      "step": 11583
    },
    {
      "epoch": 2.8253658536585364,
      "grad_norm": 0.10100773721933365,
      "learning_rate": 4.169512634718237e-07,
      "loss": 0.0092,
      "step": 11584
    },
    {
      "epoch": 2.825609756097561,
      "grad_norm": 0.08602342754602432,
      "learning_rate": 4.1579064538507516e-07,
      "loss": 0.0153,
      "step": 11585
    },
    {
      "epoch": 2.825853658536585,
      "grad_norm": 0.05206061154603958,
      "learning_rate": 4.1463163134272397e-07,
      "loss": 0.0093,
      "step": 11586
    },
    {
      "epoch": 2.82609756097561,
      "grad_norm": 0.13778473436832428,
      "learning_rate": 4.134742214203985e-07,
      "loss": 0.0257,
      "step": 11587
    },
    {
      "epoch": 2.826341463414634,
      "grad_norm": 0.14390593767166138,
      "learning_rate": 4.123184156936133e-07,
      "loss": 0.0122,
      "step": 11588
    },
    {
      "epoch": 2.8265853658536586,
      "grad_norm": 0.17884942889213562,
      "learning_rate": 4.111642142377803e-07,
      "loss": 0.0127,
      "step": 11589
    },
    {
      "epoch": 2.8268292682926828,
      "grad_norm": 0.09522104263305664,
      "learning_rate": 4.100116171282087e-07,
      "loss": 0.0144,
      "step": 11590
    },
    {
      "epoch": 2.8270731707317074,
      "grad_norm": 0.30031245946884155,
      "learning_rate": 4.088606244401022e-07,
      "loss": 0.036,
      "step": 11591
    },
    {
      "epoch": 2.8273170731707316,
      "grad_norm": 0.08779194205999374,
      "learning_rate": 4.077112362485591e-07,
      "loss": 0.0148,
      "step": 11592
    },
    {
      "epoch": 2.827560975609756,
      "grad_norm": 0.15170297026634216,
      "learning_rate": 4.06563452628575e-07,
      "loss": 0.0127,
      "step": 11593
    },
    {
      "epoch": 2.8278048780487803,
      "grad_norm": 0.12073557078838348,
      "learning_rate": 4.0541727365503434e-07,
      "loss": 0.0166,
      "step": 11594
    },
    {
      "epoch": 2.828048780487805,
      "grad_norm": 0.2883228361606598,
      "learning_rate": 4.042726994027302e-07,
      "loss": 0.0263,
      "step": 11595
    },
    {
      "epoch": 2.828292682926829,
      "grad_norm": 0.14319293200969696,
      "learning_rate": 4.03129729946336e-07,
      "loss": 0.0146,
      "step": 11596
    },
    {
      "epoch": 2.8285365853658537,
      "grad_norm": 0.07309209555387497,
      "learning_rate": 4.019883653604256e-07,
      "loss": 0.0158,
      "step": 11597
    },
    {
      "epoch": 2.828780487804878,
      "grad_norm": 0.16299933195114136,
      "learning_rate": 4.0084860571947546e-07,
      "loss": 0.0305,
      "step": 11598
    },
    {
      "epoch": 2.8290243902439025,
      "grad_norm": 0.10626816749572754,
      "learning_rate": 3.997104510978483e-07,
      "loss": 0.0092,
      "step": 11599
    },
    {
      "epoch": 2.8292682926829267,
      "grad_norm": 0.1932753026485443,
      "learning_rate": 3.98573901569807e-07,
      "loss": 0.0167,
      "step": 11600
    },
    {
      "epoch": 2.8295121951219513,
      "grad_norm": 0.22364720702171326,
      "learning_rate": 3.974389572095061e-07,
      "loss": 0.0239,
      "step": 11601
    },
    {
      "epoch": 2.8297560975609755,
      "grad_norm": 0.16739733517169952,
      "learning_rate": 3.9630561809099465e-07,
      "loss": 0.0299,
      "step": 11602
    },
    {
      "epoch": 2.83,
      "grad_norm": 0.12741675972938538,
      "learning_rate": 3.951738842882247e-07,
      "loss": 0.0177,
      "step": 11603
    },
    {
      "epoch": 2.8302439024390242,
      "grad_norm": 0.09887845814228058,
      "learning_rate": 3.940437558750343e-07,
      "loss": 0.0158,
      "step": 11604
    },
    {
      "epoch": 2.830487804878049,
      "grad_norm": 0.07080091536045074,
      "learning_rate": 3.929152329251645e-07,
      "loss": 0.0103,
      "step": 11605
    },
    {
      "epoch": 2.830731707317073,
      "grad_norm": 0.08900385349988937,
      "learning_rate": 3.9178831551224536e-07,
      "loss": 0.0224,
      "step": 11606
    },
    {
      "epoch": 2.8309756097560976,
      "grad_norm": 0.08905252814292908,
      "learning_rate": 3.9066300370980415e-07,
      "loss": 0.01,
      "step": 11607
    },
    {
      "epoch": 2.831219512195122,
      "grad_norm": 0.08405128866434097,
      "learning_rate": 3.895392975912682e-07,
      "loss": 0.0233,
      "step": 11608
    },
    {
      "epoch": 2.8314634146341464,
      "grad_norm": 0.16781839728355408,
      "learning_rate": 3.8841719722995386e-07,
      "loss": 0.0326,
      "step": 11609
    },
    {
      "epoch": 2.8317073170731706,
      "grad_norm": 0.1936081200838089,
      "learning_rate": 3.8729670269907205e-07,
      "loss": 0.04,
      "step": 11610
    },
    {
      "epoch": 2.831951219512195,
      "grad_norm": 0.176870658993721,
      "learning_rate": 3.861778140717337e-07,
      "loss": 0.0222,
      "step": 11611
    },
    {
      "epoch": 2.8321951219512194,
      "grad_norm": 0.10269361734390259,
      "learning_rate": 3.8506053142094435e-07,
      "loss": 0.0248,
      "step": 11612
    },
    {
      "epoch": 2.832439024390244,
      "grad_norm": 0.12254967540502548,
      "learning_rate": 3.8394485481960116e-07,
      "loss": 0.012,
      "step": 11613
    },
    {
      "epoch": 2.832682926829268,
      "grad_norm": 0.2438080608844757,
      "learning_rate": 3.828307843404988e-07,
      "loss": 0.0272,
      "step": 11614
    },
    {
      "epoch": 2.8329268292682928,
      "grad_norm": 0.05649254098534584,
      "learning_rate": 3.81718320056329e-07,
      "loss": 0.0068,
      "step": 11615
    },
    {
      "epoch": 2.833170731707317,
      "grad_norm": 0.15580882132053375,
      "learning_rate": 3.8060746203967266e-07,
      "loss": 0.0146,
      "step": 11616
    },
    {
      "epoch": 2.8334146341463415,
      "grad_norm": 0.1590210497379303,
      "learning_rate": 3.7949821036301624e-07,
      "loss": 0.0206,
      "step": 11617
    },
    {
      "epoch": 2.8336585365853657,
      "grad_norm": 0.151724174618721,
      "learning_rate": 3.7839056509872405e-07,
      "loss": 0.0246,
      "step": 11618
    },
    {
      "epoch": 2.8339024390243903,
      "grad_norm": 0.06992164999246597,
      "learning_rate": 3.7728452631907997e-07,
      "loss": 0.0078,
      "step": 11619
    },
    {
      "epoch": 2.8341463414634145,
      "grad_norm": 0.15403883159160614,
      "learning_rate": 3.7618009409623734e-07,
      "loss": 0.0149,
      "step": 11620
    },
    {
      "epoch": 2.834390243902439,
      "grad_norm": 0.08779506385326385,
      "learning_rate": 3.750772685022691e-07,
      "loss": 0.0108,
      "step": 11621
    },
    {
      "epoch": 2.8346341463414633,
      "grad_norm": 0.09938928484916687,
      "learning_rate": 3.739760496091205e-07,
      "loss": 0.0239,
      "step": 11622
    },
    {
      "epoch": 2.834878048780488,
      "grad_norm": 0.22293153405189514,
      "learning_rate": 3.728764374886479e-07,
      "loss": 0.0258,
      "step": 11623
    },
    {
      "epoch": 2.835121951219512,
      "grad_norm": 0.15269936621189117,
      "learning_rate": 3.7177843221259943e-07,
      "loss": 0.0223,
      "step": 11624
    },
    {
      "epoch": 2.8353658536585367,
      "grad_norm": 0.1276111602783203,
      "learning_rate": 3.7068203385261235e-07,
      "loss": 0.025,
      "step": 11625
    },
    {
      "epoch": 2.835609756097561,
      "grad_norm": 0.11813360452651978,
      "learning_rate": 3.695872424802238e-07,
      "loss": 0.0107,
      "step": 11626
    },
    {
      "epoch": 2.8358536585365854,
      "grad_norm": 0.1401381492614746,
      "learning_rate": 3.6849405816686834e-07,
      "loss": 0.018,
      "step": 11627
    },
    {
      "epoch": 2.8360975609756096,
      "grad_norm": 0.14525307714939117,
      "learning_rate": 3.6740248098386944e-07,
      "loss": 0.0155,
      "step": 11628
    },
    {
      "epoch": 2.8363414634146342,
      "grad_norm": 0.1306852400302887,
      "learning_rate": 3.6631251100245344e-07,
      "loss": 0.0221,
      "step": 11629
    },
    {
      "epoch": 2.8365853658536584,
      "grad_norm": 0.13093718886375427,
      "learning_rate": 3.6522414829373297e-07,
      "loss": 0.0122,
      "step": 11630
    },
    {
      "epoch": 2.836829268292683,
      "grad_norm": 0.10765953361988068,
      "learning_rate": 3.641373929287234e-07,
      "loss": 0.0125,
      "step": 11631
    },
    {
      "epoch": 2.837073170731707,
      "grad_norm": 0.06956183165311813,
      "learning_rate": 3.630522449783347e-07,
      "loss": 0.0154,
      "step": 11632
    },
    {
      "epoch": 2.837317073170732,
      "grad_norm": 0.10167475044727325,
      "learning_rate": 3.6196870451336016e-07,
      "loss": 0.0206,
      "step": 11633
    },
    {
      "epoch": 2.837560975609756,
      "grad_norm": 0.18045860528945923,
      "learning_rate": 3.6088677160450723e-07,
      "loss": 0.0197,
      "step": 11634
    },
    {
      "epoch": 2.8378048780487806,
      "grad_norm": 0.10792037099599838,
      "learning_rate": 3.598064463223638e-07,
      "loss": 0.0188,
      "step": 11635
    },
    {
      "epoch": 2.8380487804878047,
      "grad_norm": 0.11110924184322357,
      "learning_rate": 3.58727728737418e-07,
      "loss": 0.0158,
      "step": 11636
    },
    {
      "epoch": 2.8382926829268293,
      "grad_norm": 0.11737949401140213,
      "learning_rate": 3.576506189200551e-07,
      "loss": 0.0141,
      "step": 11637
    },
    {
      "epoch": 2.8385365853658535,
      "grad_norm": 0.08083879947662354,
      "learning_rate": 3.565751169405523e-07,
      "loss": 0.0132,
      "step": 11638
    },
    {
      "epoch": 2.838780487804878,
      "grad_norm": 0.12731772661209106,
      "learning_rate": 3.555012228690813e-07,
      "loss": 0.0151,
      "step": 11639
    },
    {
      "epoch": 2.8390243902439023,
      "grad_norm": 0.09403172135353088,
      "learning_rate": 3.5442893677571085e-07,
      "loss": 0.0065,
      "step": 11640
    },
    {
      "epoch": 2.839268292682927,
      "grad_norm": 0.09506136178970337,
      "learning_rate": 3.533582587304046e-07,
      "loss": 0.0094,
      "step": 11641
    },
    {
      "epoch": 2.839512195121951,
      "grad_norm": 0.2768464684486389,
      "learning_rate": 3.522891888030233e-07,
      "loss": 0.0105,
      "step": 11642
    },
    {
      "epoch": 2.8397560975609757,
      "grad_norm": 0.07928428053855896,
      "learning_rate": 3.512217270633195e-07,
      "loss": 0.0066,
      "step": 11643
    },
    {
      "epoch": 2.84,
      "grad_norm": 0.14573687314987183,
      "learning_rate": 3.5015587358093735e-07,
      "loss": 0.0205,
      "step": 11644
    },
    {
      "epoch": 2.8402439024390245,
      "grad_norm": 0.12074997276067734,
      "learning_rate": 3.49091628425427e-07,
      "loss": 0.0176,
      "step": 11645
    },
    {
      "epoch": 2.8404878048780486,
      "grad_norm": 0.21358028054237366,
      "learning_rate": 3.4802899166622436e-07,
      "loss": 0.023,
      "step": 11646
    },
    {
      "epoch": 2.8407317073170733,
      "grad_norm": 0.11121534556150436,
      "learning_rate": 3.4696796337266026e-07,
      "loss": 0.0136,
      "step": 11647
    },
    {
      "epoch": 2.8409756097560974,
      "grad_norm": 0.24448265135288239,
      "learning_rate": 3.4590854361397084e-07,
      "loss": 0.0277,
      "step": 11648
    },
    {
      "epoch": 2.841219512195122,
      "grad_norm": 0.1252882033586502,
      "learning_rate": 3.4485073245927035e-07,
      "loss": 0.0202,
      "step": 11649
    },
    {
      "epoch": 2.841463414634146,
      "grad_norm": 0.12487640231847763,
      "learning_rate": 3.437945299775841e-07,
      "loss": 0.012,
      "step": 11650
    },
    {
      "epoch": 2.841707317073171,
      "grad_norm": 0.1442241221666336,
      "learning_rate": 3.4273993623782916e-07,
      "loss": 0.0244,
      "step": 11651
    },
    {
      "epoch": 2.841951219512195,
      "grad_norm": 0.10274135321378708,
      "learning_rate": 3.4168695130880337e-07,
      "loss": 0.0194,
      "step": 11652
    },
    {
      "epoch": 2.8421951219512196,
      "grad_norm": 0.2582552134990692,
      "learning_rate": 3.4063557525922385e-07,
      "loss": 0.0108,
      "step": 11653
    },
    {
      "epoch": 2.8424390243902438,
      "grad_norm": 0.09871543943881989,
      "learning_rate": 3.395858081576775e-07,
      "loss": 0.0088,
      "step": 11654
    },
    {
      "epoch": 2.8426829268292684,
      "grad_norm": 0.1293981373310089,
      "learning_rate": 3.3853765007267057e-07,
      "loss": 0.0139,
      "step": 11655
    },
    {
      "epoch": 2.8429268292682925,
      "grad_norm": 0.14494729042053223,
      "learning_rate": 3.3749110107258173e-07,
      "loss": 0.0212,
      "step": 11656
    },
    {
      "epoch": 2.843170731707317,
      "grad_norm": 0.18313392996788025,
      "learning_rate": 3.3644616122570084e-07,
      "loss": 0.0307,
      "step": 11657
    },
    {
      "epoch": 2.8434146341463413,
      "grad_norm": 0.10322993248701096,
      "learning_rate": 3.354028306002066e-07,
      "loss": 0.0267,
      "step": 11658
    },
    {
      "epoch": 2.843658536585366,
      "grad_norm": 0.14451970160007477,
      "learning_rate": 3.343611092641724e-07,
      "loss": 0.0197,
      "step": 11659
    },
    {
      "epoch": 2.84390243902439,
      "grad_norm": 0.14497140049934387,
      "learning_rate": 3.3332099728556333e-07,
      "loss": 0.0218,
      "step": 11660
    },
    {
      "epoch": 2.8441463414634147,
      "grad_norm": 0.11433672904968262,
      "learning_rate": 3.3228249473225003e-07,
      "loss": 0.0145,
      "step": 11661
    },
    {
      "epoch": 2.844390243902439,
      "grad_norm": 0.08336620777845383,
      "learning_rate": 3.312456016719895e-07,
      "loss": 0.0188,
      "step": 11662
    },
    {
      "epoch": 2.8446341463414635,
      "grad_norm": 0.052158910781145096,
      "learning_rate": 3.3021031817243307e-07,
      "loss": 0.0064,
      "step": 11663
    },
    {
      "epoch": 2.8448780487804877,
      "grad_norm": 0.1138065904378891,
      "learning_rate": 3.291766443011324e-07,
      "loss": 0.0156,
      "step": 11664
    },
    {
      "epoch": 2.8451219512195123,
      "grad_norm": 0.11046532541513443,
      "learning_rate": 3.281445801255334e-07,
      "loss": 0.0174,
      "step": 11665
    },
    {
      "epoch": 2.8453658536585364,
      "grad_norm": 0.20974382758140564,
      "learning_rate": 3.271141257129712e-07,
      "loss": 0.0258,
      "step": 11666
    },
    {
      "epoch": 2.845609756097561,
      "grad_norm": 0.11484380066394806,
      "learning_rate": 3.2608528113068084e-07,
      "loss": 0.0121,
      "step": 11667
    },
    {
      "epoch": 2.8458536585365852,
      "grad_norm": 0.14308477938175201,
      "learning_rate": 3.2505804644579194e-07,
      "loss": 0.0142,
      "step": 11668
    },
    {
      "epoch": 2.84609756097561,
      "grad_norm": 0.09590748697519302,
      "learning_rate": 3.240324217253288e-07,
      "loss": 0.0178,
      "step": 11669
    },
    {
      "epoch": 2.846341463414634,
      "grad_norm": 0.11590136587619781,
      "learning_rate": 3.2300840703620714e-07,
      "loss": 0.0122,
      "step": 11670
    },
    {
      "epoch": 2.8465853658536586,
      "grad_norm": 0.14424188435077667,
      "learning_rate": 3.2198600244524593e-07,
      "loss": 0.0135,
      "step": 11671
    },
    {
      "epoch": 2.846829268292683,
      "grad_norm": 0.14417392015457153,
      "learning_rate": 3.2096520801915277e-07,
      "loss": 0.0146,
      "step": 11672
    },
    {
      "epoch": 2.8470731707317074,
      "grad_norm": 0.1554402858018875,
      "learning_rate": 3.199460238245272e-07,
      "loss": 0.0149,
      "step": 11673
    },
    {
      "epoch": 2.8473170731707316,
      "grad_norm": 0.15161389112472534,
      "learning_rate": 3.1892844992787165e-07,
      "loss": 0.0189,
      "step": 11674
    },
    {
      "epoch": 2.847560975609756,
      "grad_norm": 0.1422004997730255,
      "learning_rate": 3.1791248639558015e-07,
      "loss": 0.0239,
      "step": 11675
    },
    {
      "epoch": 2.8478048780487804,
      "grad_norm": 0.17112258076667786,
      "learning_rate": 3.1689813329393583e-07,
      "loss": 0.0161,
      "step": 11676
    },
    {
      "epoch": 2.848048780487805,
      "grad_norm": 0.11391045898199081,
      "learning_rate": 3.1588539068913014e-07,
      "loss": 0.0247,
      "step": 11677
    },
    {
      "epoch": 2.848292682926829,
      "grad_norm": 0.10384703427553177,
      "learning_rate": 3.148742586472353e-07,
      "loss": 0.0068,
      "step": 11678
    },
    {
      "epoch": 2.8485365853658537,
      "grad_norm": 0.1261400729417801,
      "learning_rate": 3.13864737234229e-07,
      "loss": 0.0085,
      "step": 11679
    },
    {
      "epoch": 2.848780487804878,
      "grad_norm": 0.11357858777046204,
      "learning_rate": 3.1285682651597535e-07,
      "loss": 0.0167,
      "step": 11680
    },
    {
      "epoch": 2.8490243902439025,
      "grad_norm": 0.08630840480327606,
      "learning_rate": 3.118505265582383e-07,
      "loss": 0.0167,
      "step": 11681
    },
    {
      "epoch": 2.8492682926829267,
      "grad_norm": 0.11259377002716064,
      "learning_rate": 3.108458374266793e-07,
      "loss": 0.0134,
      "step": 11682
    },
    {
      "epoch": 2.8495121951219513,
      "grad_norm": 0.10914414376020432,
      "learning_rate": 3.098427591868458e-07,
      "loss": 0.0128,
      "step": 11683
    },
    {
      "epoch": 2.8497560975609755,
      "grad_norm": 0.07427402585744858,
      "learning_rate": 3.088412919041911e-07,
      "loss": 0.006,
      "step": 11684
    },
    {
      "epoch": 2.85,
      "grad_norm": 0.1900080293416977,
      "learning_rate": 3.078414356440545e-07,
      "loss": 0.0202,
      "step": 11685
    },
    {
      "epoch": 2.8502439024390243,
      "grad_norm": 0.0808214321732521,
      "learning_rate": 3.068431904716701e-07,
      "loss": 0.0141,
      "step": 11686
    },
    {
      "epoch": 2.850487804878049,
      "grad_norm": 0.08655283600091934,
      "learning_rate": 3.0584655645218007e-07,
      "loss": 0.0079,
      "step": 11687
    },
    {
      "epoch": 2.850731707317073,
      "grad_norm": 0.17618328332901,
      "learning_rate": 3.048515336506047e-07,
      "loss": 0.0143,
      "step": 11688
    },
    {
      "epoch": 2.8509756097560977,
      "grad_norm": 0.17874588072299957,
      "learning_rate": 3.038581221318643e-07,
      "loss": 0.0179,
      "step": 11689
    },
    {
      "epoch": 2.851219512195122,
      "grad_norm": 0.1936235874891281,
      "learning_rate": 3.0286632196078466e-07,
      "loss": 0.0177,
      "step": 11690
    },
    {
      "epoch": 2.8514634146341464,
      "grad_norm": 0.12737876176834106,
      "learning_rate": 3.0187613320206696e-07,
      "loss": 0.0088,
      "step": 11691
    },
    {
      "epoch": 2.8517073170731706,
      "grad_norm": 0.084729865193367,
      "learning_rate": 3.0088755592032614e-07,
      "loss": 0.0087,
      "step": 11692
    },
    {
      "epoch": 2.851951219512195,
      "grad_norm": 0.20015043020248413,
      "learning_rate": 2.999005901800633e-07,
      "loss": 0.0183,
      "step": 11693
    },
    {
      "epoch": 2.8521951219512194,
      "grad_norm": 0.14734646677970886,
      "learning_rate": 2.9891523604566874e-07,
      "loss": 0.0176,
      "step": 11694
    },
    {
      "epoch": 2.852439024390244,
      "grad_norm": 0.09126312285661697,
      "learning_rate": 2.979314935814409e-07,
      "loss": 0.02,
      "step": 11695
    },
    {
      "epoch": 2.852682926829268,
      "grad_norm": 0.11219993978738785,
      "learning_rate": 2.969493628515618e-07,
      "loss": 0.0153,
      "step": 11696
    },
    {
      "epoch": 2.8529268292682928,
      "grad_norm": 0.06937044113874435,
      "learning_rate": 2.9596884392011083e-07,
      "loss": 0.0144,
      "step": 11697
    },
    {
      "epoch": 2.853170731707317,
      "grad_norm": 0.19631099700927734,
      "learning_rate": 2.9498993685107e-07,
      "loss": 0.0265,
      "step": 11698
    },
    {
      "epoch": 2.8534146341463416,
      "grad_norm": 0.13260848820209503,
      "learning_rate": 2.94012641708305e-07,
      "loss": 0.0156,
      "step": 11699
    },
    {
      "epoch": 2.8536585365853657,
      "grad_norm": 0.1297294944524765,
      "learning_rate": 2.930369585555842e-07,
      "loss": 0.0136,
      "step": 11700
    },
    {
      "epoch": 2.8539024390243903,
      "grad_norm": 0.23034250736236572,
      "learning_rate": 2.920628874565651e-07,
      "loss": 0.0284,
      "step": 11701
    },
    {
      "epoch": 2.8541463414634145,
      "grad_norm": 0.10882164537906647,
      "learning_rate": 2.9109042847480507e-07,
      "loss": 0.0153,
      "step": 11702
    },
    {
      "epoch": 2.854390243902439,
      "grad_norm": 0.10881728678941727,
      "learning_rate": 2.9011958167375343e-07,
      "loss": 0.0114,
      "step": 11703
    },
    {
      "epoch": 2.8546341463414633,
      "grad_norm": 0.1148451492190361,
      "learning_rate": 2.8915034711675116e-07,
      "loss": 0.0108,
      "step": 11704
    },
    {
      "epoch": 2.854878048780488,
      "grad_norm": 0.10751858353614807,
      "learning_rate": 2.8818272486704766e-07,
      "loss": 0.0126,
      "step": 11705
    },
    {
      "epoch": 2.855121951219512,
      "grad_norm": 0.10751263797283173,
      "learning_rate": 2.8721671498776746e-07,
      "loss": 0.0172,
      "step": 11706
    },
    {
      "epoch": 2.8553658536585367,
      "grad_norm": 0.12670189142227173,
      "learning_rate": 2.862523175419435e-07,
      "loss": 0.0146,
      "step": 11707
    },
    {
      "epoch": 2.855609756097561,
      "grad_norm": 0.11894957721233368,
      "learning_rate": 2.8528953259250037e-07,
      "loss": 0.0192,
      "step": 11708
    },
    {
      "epoch": 2.8558536585365855,
      "grad_norm": 0.10943121463060379,
      "learning_rate": 2.8432836020225727e-07,
      "loss": 0.0275,
      "step": 11709
    },
    {
      "epoch": 2.8560975609756096,
      "grad_norm": 0.16924600303173065,
      "learning_rate": 2.833688004339252e-07,
      "loss": 0.0347,
      "step": 11710
    },
    {
      "epoch": 2.8563414634146342,
      "grad_norm": 0.10057032108306885,
      "learning_rate": 2.824108533501152e-07,
      "loss": 0.0148,
      "step": 11711
    },
    {
      "epoch": 2.8565853658536584,
      "grad_norm": 0.20610174536705017,
      "learning_rate": 2.814545190133272e-07,
      "loss": 0.0321,
      "step": 11712
    },
    {
      "epoch": 2.856829268292683,
      "grad_norm": 0.0856904610991478,
      "learning_rate": 2.804997974859641e-07,
      "loss": 0.0091,
      "step": 11713
    },
    {
      "epoch": 2.857073170731707,
      "grad_norm": 0.1465199738740921,
      "learning_rate": 2.79546688830315e-07,
      "loss": 0.0214,
      "step": 11714
    },
    {
      "epoch": 2.857317073170732,
      "grad_norm": 0.17241059243679047,
      "learning_rate": 2.78595193108569e-07,
      "loss": 0.0263,
      "step": 11715
    },
    {
      "epoch": 2.857560975609756,
      "grad_norm": 0.16734465956687927,
      "learning_rate": 2.776453103828042e-07,
      "loss": 0.0202,
      "step": 11716
    },
    {
      "epoch": 2.8578048780487806,
      "grad_norm": 0.1092146560549736,
      "learning_rate": 2.7669704071500157e-07,
      "loss": 0.0117,
      "step": 11717
    },
    {
      "epoch": 2.8580487804878048,
      "grad_norm": 0.1287715584039688,
      "learning_rate": 2.757503841670311e-07,
      "loss": 0.0118,
      "step": 11718
    },
    {
      "epoch": 2.8582926829268294,
      "grad_norm": 0.16189157962799072,
      "learning_rate": 2.748053408006629e-07,
      "loss": 0.0234,
      "step": 11719
    },
    {
      "epoch": 2.8585365853658535,
      "grad_norm": 0.13584014773368835,
      "learning_rate": 2.7386191067755027e-07,
      "loss": 0.0133,
      "step": 11720
    },
    {
      "epoch": 2.858780487804878,
      "grad_norm": 0.14727579057216644,
      "learning_rate": 2.7292009385925797e-07,
      "loss": 0.0162,
      "step": 11721
    },
    {
      "epoch": 2.8590243902439023,
      "grad_norm": 0.1871383637189865,
      "learning_rate": 2.7197989040723126e-07,
      "loss": 0.0173,
      "step": 11722
    },
    {
      "epoch": 2.859268292682927,
      "grad_norm": 0.14779140055179596,
      "learning_rate": 2.710413003828155e-07,
      "loss": 0.0121,
      "step": 11723
    },
    {
      "epoch": 2.859512195121951,
      "grad_norm": 0.10808758437633514,
      "learning_rate": 2.7010432384725336e-07,
      "loss": 0.0239,
      "step": 11724
    },
    {
      "epoch": 2.8597560975609757,
      "grad_norm": 0.09028132259845734,
      "learning_rate": 2.691689608616793e-07,
      "loss": 0.0178,
      "step": 11725
    },
    {
      "epoch": 2.86,
      "grad_norm": 0.11831317842006683,
      "learning_rate": 2.6823521148712226e-07,
      "loss": 0.0163,
      "step": 11726
    },
    {
      "epoch": 2.8602439024390245,
      "grad_norm": 0.19605079293251038,
      "learning_rate": 2.6730307578450577e-07,
      "loss": 0.0191,
      "step": 11727
    },
    {
      "epoch": 2.8604878048780487,
      "grad_norm": 0.10868358612060547,
      "learning_rate": 2.663725538146505e-07,
      "loss": 0.0172,
      "step": 11728
    },
    {
      "epoch": 2.8607317073170733,
      "grad_norm": 0.09254112839698792,
      "learning_rate": 2.654436456382692e-07,
      "loss": 0.0169,
      "step": 11729
    },
    {
      "epoch": 2.8609756097560974,
      "grad_norm": 0.24820543825626373,
      "learning_rate": 2.6451635131596874e-07,
      "loss": 0.0231,
      "step": 11730
    },
    {
      "epoch": 2.861219512195122,
      "grad_norm": 0.13954399526119232,
      "learning_rate": 2.635906709082564e-07,
      "loss": 0.0245,
      "step": 11731
    },
    {
      "epoch": 2.861463414634146,
      "grad_norm": 0.17755018174648285,
      "learning_rate": 2.626666044755255e-07,
      "loss": 0.0221,
      "step": 11732
    },
    {
      "epoch": 2.861707317073171,
      "grad_norm": 0.13529156148433685,
      "learning_rate": 2.617441520780695e-07,
      "loss": 0.0165,
      "step": 11733
    },
    {
      "epoch": 2.861951219512195,
      "grad_norm": 0.2019951492547989,
      "learning_rate": 2.6082331377607904e-07,
      "loss": 0.0205,
      "step": 11734
    },
    {
      "epoch": 2.8621951219512196,
      "grad_norm": 0.1491285264492035,
      "learning_rate": 2.599040896296312e-07,
      "loss": 0.0111,
      "step": 11735
    },
    {
      "epoch": 2.862439024390244,
      "grad_norm": 0.10356751829385757,
      "learning_rate": 2.5898647969870573e-07,
      "loss": 0.018,
      "step": 11736
    },
    {
      "epoch": 2.8626829268292684,
      "grad_norm": 0.15790972113609314,
      "learning_rate": 2.5807048404317135e-07,
      "loss": 0.0177,
      "step": 11737
    },
    {
      "epoch": 2.8629268292682926,
      "grad_norm": 0.19214700162410736,
      "learning_rate": 2.57156102722797e-07,
      "loss": 0.0144,
      "step": 11738
    },
    {
      "epoch": 2.863170731707317,
      "grad_norm": 0.15079793334007263,
      "learning_rate": 2.5624333579724324e-07,
      "loss": 0.014,
      "step": 11739
    },
    {
      "epoch": 2.8634146341463413,
      "grad_norm": 0.09350215643644333,
      "learning_rate": 2.5533218332606245e-07,
      "loss": 0.0163,
      "step": 11740
    },
    {
      "epoch": 2.863658536585366,
      "grad_norm": 0.1415366381406784,
      "learning_rate": 2.544226453687043e-07,
      "loss": 0.0217,
      "step": 11741
    },
    {
      "epoch": 2.86390243902439,
      "grad_norm": 0.12849301099777222,
      "learning_rate": 2.5351472198451854e-07,
      "loss": 0.0191,
      "step": 11742
    },
    {
      "epoch": 2.8641463414634147,
      "grad_norm": 0.14182600378990173,
      "learning_rate": 2.5260841323273834e-07,
      "loss": 0.0143,
      "step": 11743
    },
    {
      "epoch": 2.864390243902439,
      "grad_norm": 0.15334917604923248,
      "learning_rate": 2.5170371917250244e-07,
      "loss": 0.0136,
      "step": 11744
    },
    {
      "epoch": 2.8646341463414635,
      "grad_norm": 0.0704476535320282,
      "learning_rate": 2.5080063986283597e-07,
      "loss": 0.0088,
      "step": 11745
    },
    {
      "epoch": 2.8648780487804877,
      "grad_norm": 0.11901354789733887,
      "learning_rate": 2.498991753626612e-07,
      "loss": 0.0145,
      "step": 11746
    },
    {
      "epoch": 2.8651219512195123,
      "grad_norm": 0.11757885664701462,
      "learning_rate": 2.4899932573080045e-07,
      "loss": 0.0105,
      "step": 11747
    },
    {
      "epoch": 2.8653658536585365,
      "grad_norm": 0.10025656968355179,
      "learning_rate": 2.4810109102596515e-07,
      "loss": 0.0137,
      "step": 11748
    },
    {
      "epoch": 2.865609756097561,
      "grad_norm": 0.11800815910100937,
      "learning_rate": 2.4720447130675564e-07,
      "loss": 0.0188,
      "step": 11749
    },
    {
      "epoch": 2.8658536585365852,
      "grad_norm": 0.09364738315343857,
      "learning_rate": 2.463094666316834e-07,
      "loss": 0.0135,
      "step": 11750
    },
    {
      "epoch": 2.86609756097561,
      "grad_norm": 0.21129481494426727,
      "learning_rate": 2.4541607705913793e-07,
      "loss": 0.0192,
      "step": 11751
    },
    {
      "epoch": 2.866341463414634,
      "grad_norm": 0.11607211828231812,
      "learning_rate": 2.4452430264741143e-07,
      "loss": 0.021,
      "step": 11752
    },
    {
      "epoch": 2.8665853658536586,
      "grad_norm": 0.07522282749414444,
      "learning_rate": 2.436341434546935e-07,
      "loss": 0.014,
      "step": 11753
    },
    {
      "epoch": 2.866829268292683,
      "grad_norm": 0.2051926553249359,
      "learning_rate": 2.4274559953905706e-07,
      "loss": 0.0188,
      "step": 11754
    },
    {
      "epoch": 2.8670731707317074,
      "grad_norm": 0.12295422703027725,
      "learning_rate": 2.418586709584836e-07,
      "loss": 0.0127,
      "step": 11755
    },
    {
      "epoch": 2.8673170731707316,
      "grad_norm": 0.12941807508468628,
      "learning_rate": 2.409733577708406e-07,
      "loss": 0.0167,
      "step": 11756
    },
    {
      "epoch": 2.867560975609756,
      "grad_norm": 0.05834461376070976,
      "learning_rate": 2.4008966003389033e-07,
      "loss": 0.01,
      "step": 11757
    },
    {
      "epoch": 2.8678048780487804,
      "grad_norm": 0.11843335628509521,
      "learning_rate": 2.3920757780529215e-07,
      "loss": 0.0122,
      "step": 11758
    },
    {
      "epoch": 2.868048780487805,
      "grad_norm": 0.07840963453054428,
      "learning_rate": 2.3832711114259997e-07,
      "loss": 0.0153,
      "step": 11759
    },
    {
      "epoch": 2.868292682926829,
      "grad_norm": 0.10961928963661194,
      "learning_rate": 2.374482601032596e-07,
      "loss": 0.0155,
      "step": 11760
    },
    {
      "epoch": 2.8685365853658538,
      "grad_norm": 0.146396666765213,
      "learning_rate": 2.3657102474461678e-07,
      "loss": 0.0275,
      "step": 11761
    },
    {
      "epoch": 2.868780487804878,
      "grad_norm": 0.12286026030778885,
      "learning_rate": 2.3569540512390632e-07,
      "loss": 0.0287,
      "step": 11762
    },
    {
      "epoch": 2.8690243902439025,
      "grad_norm": 0.11383669078350067,
      "learning_rate": 2.3482140129826024e-07,
      "loss": 0.0119,
      "step": 11763
    },
    {
      "epoch": 2.8692682926829267,
      "grad_norm": 0.12264416366815567,
      "learning_rate": 2.3394901332470242e-07,
      "loss": 0.0119,
      "step": 11764
    },
    {
      "epoch": 2.8695121951219513,
      "grad_norm": 0.16478319466114044,
      "learning_rate": 2.3307824126015676e-07,
      "loss": 0.0191,
      "step": 11765
    },
    {
      "epoch": 2.8697560975609755,
      "grad_norm": 0.22352401912212372,
      "learning_rate": 2.3220908516143892e-07,
      "loss": 0.0205,
      "step": 11766
    },
    {
      "epoch": 2.87,
      "grad_norm": 0.22102032601833344,
      "learning_rate": 2.3134154508525352e-07,
      "loss": 0.0163,
      "step": 11767
    },
    {
      "epoch": 2.8702439024390243,
      "grad_norm": 0.14293842017650604,
      "learning_rate": 2.3047562108820809e-07,
      "loss": 0.0136,
      "step": 11768
    },
    {
      "epoch": 2.870487804878049,
      "grad_norm": 0.09279633313417435,
      "learning_rate": 2.2961131322680186e-07,
      "loss": 0.0146,
      "step": 11769
    },
    {
      "epoch": 2.870731707317073,
      "grad_norm": 0.07174745202064514,
      "learning_rate": 2.287486215574286e-07,
      "loss": 0.013,
      "step": 11770
    },
    {
      "epoch": 2.8709756097560977,
      "grad_norm": 0.06802190095186234,
      "learning_rate": 2.2788754613637665e-07,
      "loss": 0.0122,
      "step": 11771
    },
    {
      "epoch": 2.871219512195122,
      "grad_norm": 0.11550094932317734,
      "learning_rate": 2.2702808701982603e-07,
      "loss": 0.021,
      "step": 11772
    },
    {
      "epoch": 2.8714634146341464,
      "grad_norm": 0.1760236918926239,
      "learning_rate": 2.261702442638569e-07,
      "loss": 0.021,
      "step": 11773
    },
    {
      "epoch": 2.8717073170731706,
      "grad_norm": 0.1281031221151352,
      "learning_rate": 2.253140179244384e-07,
      "loss": 0.0203,
      "step": 11774
    },
    {
      "epoch": 2.8719512195121952,
      "grad_norm": 0.18754631280899048,
      "learning_rate": 2.2445940805743692e-07,
      "loss": 0.0315,
      "step": 11775
    },
    {
      "epoch": 2.8721951219512194,
      "grad_norm": 0.08435875922441483,
      "learning_rate": 2.236064147186162e-07,
      "loss": 0.0186,
      "step": 11776
    },
    {
      "epoch": 2.872439024390244,
      "grad_norm": 0.16242550313472748,
      "learning_rate": 2.2275503796362617e-07,
      "loss": 0.01,
      "step": 11777
    },
    {
      "epoch": 2.872682926829268,
      "grad_norm": 0.07275155931711197,
      "learning_rate": 2.219052778480224e-07,
      "loss": 0.015,
      "step": 11778
    },
    {
      "epoch": 2.872926829268293,
      "grad_norm": 0.13888217508792877,
      "learning_rate": 2.2105713442724384e-07,
      "loss": 0.0214,
      "step": 11779
    },
    {
      "epoch": 2.873170731707317,
      "grad_norm": 0.14871376752853394,
      "learning_rate": 2.202106077566324e-07,
      "loss": 0.0188,
      "step": 11780
    },
    {
      "epoch": 2.8734146341463416,
      "grad_norm": 0.14392323791980743,
      "learning_rate": 2.193656978914216e-07,
      "loss": 0.0275,
      "step": 11781
    },
    {
      "epoch": 2.8736585365853657,
      "grad_norm": 0.11189621686935425,
      "learning_rate": 2.185224048867396e-07,
      "loss": 0.016,
      "step": 11782
    },
    {
      "epoch": 2.8739024390243904,
      "grad_norm": 0.08942429721355438,
      "learning_rate": 2.176807287976035e-07,
      "loss": 0.0106,
      "step": 11783
    },
    {
      "epoch": 2.8741463414634145,
      "grad_norm": 0.39003539085388184,
      "learning_rate": 2.1684066967893602e-07,
      "loss": 0.0148,
      "step": 11784
    },
    {
      "epoch": 2.874390243902439,
      "grad_norm": 0.19500389695167542,
      "learning_rate": 2.1600222758554335e-07,
      "loss": 0.0222,
      "step": 11785
    },
    {
      "epoch": 2.8746341463414633,
      "grad_norm": 0.2116474062204361,
      "learning_rate": 2.1516540257213726e-07,
      "loss": 0.0163,
      "step": 11786
    },
    {
      "epoch": 2.874878048780488,
      "grad_norm": 0.08681415021419525,
      "learning_rate": 2.1433019469331571e-07,
      "loss": 0.0123,
      "step": 11787
    },
    {
      "epoch": 2.875121951219512,
      "grad_norm": 0.1612817496061325,
      "learning_rate": 2.134966040035713e-07,
      "loss": 0.0141,
      "step": 11788
    },
    {
      "epoch": 2.8753658536585367,
      "grad_norm": 0.16672447323799133,
      "learning_rate": 2.126646305572938e-07,
      "loss": 0.0157,
      "step": 11789
    },
    {
      "epoch": 2.875609756097561,
      "grad_norm": 0.22189028561115265,
      "learning_rate": 2.1183427440877035e-07,
      "loss": 0.0246,
      "step": 11790
    },
    {
      "epoch": 2.8758536585365855,
      "grad_norm": 0.10757580399513245,
      "learning_rate": 2.1100553561217428e-07,
      "loss": 0.0179,
      "step": 11791
    },
    {
      "epoch": 2.8760975609756096,
      "grad_norm": 0.12261233478784561,
      "learning_rate": 2.101784142215818e-07,
      "loss": 0.0199,
      "step": 11792
    },
    {
      "epoch": 2.8763414634146343,
      "grad_norm": 0.10833048820495605,
      "learning_rate": 2.0935291029095804e-07,
      "loss": 0.0183,
      "step": 11793
    },
    {
      "epoch": 2.8765853658536584,
      "grad_norm": 0.07873238623142242,
      "learning_rate": 2.085290238741655e-07,
      "loss": 0.0146,
      "step": 11794
    },
    {
      "epoch": 2.876829268292683,
      "grad_norm": 0.08440878242254257,
      "learning_rate": 2.0770675502496117e-07,
      "loss": 0.0199,
      "step": 11795
    },
    {
      "epoch": 2.877073170731707,
      "grad_norm": 0.15991170704364777,
      "learning_rate": 2.068861037969938e-07,
      "loss": 0.0203,
      "step": 11796
    },
    {
      "epoch": 2.877317073170732,
      "grad_norm": 0.181096613407135,
      "learning_rate": 2.0606707024380944e-07,
      "loss": 0.02,
      "step": 11797
    },
    {
      "epoch": 2.877560975609756,
      "grad_norm": 0.12673139572143555,
      "learning_rate": 2.052496544188487e-07,
      "loss": 0.0154,
      "step": 11798
    },
    {
      "epoch": 2.8778048780487806,
      "grad_norm": 0.15609781444072723,
      "learning_rate": 2.044338563754411e-07,
      "loss": 0.0165,
      "step": 11799
    },
    {
      "epoch": 2.8780487804878048,
      "grad_norm": 0.18747757375240326,
      "learning_rate": 2.036196761668191e-07,
      "loss": 0.015,
      "step": 11800
    },
    {
      "epoch": 2.8782926829268294,
      "grad_norm": 0.101994588971138,
      "learning_rate": 2.0280711384610408e-07,
      "loss": 0.0111,
      "step": 11801
    },
    {
      "epoch": 2.8785365853658536,
      "grad_norm": 0.1060759425163269,
      "learning_rate": 2.0199616946631473e-07,
      "loss": 0.0134,
      "step": 11802
    },
    {
      "epoch": 2.878780487804878,
      "grad_norm": 0.10392194986343384,
      "learning_rate": 2.011868430803615e-07,
      "loss": 0.0272,
      "step": 11803
    },
    {
      "epoch": 2.8790243902439023,
      "grad_norm": 0.09813513606786728,
      "learning_rate": 2.0037913474104663e-07,
      "loss": 0.0164,
      "step": 11804
    },
    {
      "epoch": 2.879268292682927,
      "grad_norm": 0.1495741903781891,
      "learning_rate": 1.9957304450107794e-07,
      "loss": 0.0137,
      "step": 11805
    },
    {
      "epoch": 2.879512195121951,
      "grad_norm": 0.14687032997608185,
      "learning_rate": 1.987685724130467e-07,
      "loss": 0.0279,
      "step": 11806
    },
    {
      "epoch": 2.8797560975609757,
      "grad_norm": 0.10423873364925385,
      "learning_rate": 1.9796571852943868e-07,
      "loss": 0.0275,
      "step": 11807
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.12586398422718048,
      "learning_rate": 1.9716448290264532e-07,
      "loss": 0.0117,
      "step": 11808
    },
    {
      "epoch": 2.8802439024390245,
      "grad_norm": 0.1755269169807434,
      "learning_rate": 1.9636486558493873e-07,
      "loss": 0.0312,
      "step": 11809
    },
    {
      "epoch": 2.8804878048780487,
      "grad_norm": 0.11711817979812622,
      "learning_rate": 1.9556686662849377e-07,
      "loss": 0.0148,
      "step": 11810
    },
    {
      "epoch": 2.8807317073170733,
      "grad_norm": 0.17258112132549286,
      "learning_rate": 1.9477048608537717e-07,
      "loss": 0.0056,
      "step": 11811
    },
    {
      "epoch": 2.8809756097560975,
      "grad_norm": 0.10959146171808243,
      "learning_rate": 1.9397572400755015e-07,
      "loss": 0.0217,
      "step": 11812
    },
    {
      "epoch": 2.881219512195122,
      "grad_norm": 0.19598031044006348,
      "learning_rate": 1.9318258044686843e-07,
      "loss": 0.0127,
      "step": 11813
    },
    {
      "epoch": 2.8814634146341462,
      "grad_norm": 0.14079807698726654,
      "learning_rate": 1.923910554550823e-07,
      "loss": 0.0128,
      "step": 11814
    },
    {
      "epoch": 2.881707317073171,
      "grad_norm": 0.1908968687057495,
      "learning_rate": 1.9160114908383652e-07,
      "loss": 0.0261,
      "step": 11815
    },
    {
      "epoch": 2.881951219512195,
      "grad_norm": 0.16646666824817657,
      "learning_rate": 1.9081286138467046e-07,
      "loss": 0.0144,
      "step": 11816
    },
    {
      "epoch": 2.8821951219512196,
      "grad_norm": 0.17427417635917664,
      "learning_rate": 1.90026192409018e-07,
      "loss": 0.0182,
      "step": 11817
    },
    {
      "epoch": 2.882439024390244,
      "grad_norm": 0.13585762679576874,
      "learning_rate": 1.8924114220820465e-07,
      "loss": 0.0106,
      "step": 11818
    },
    {
      "epoch": 2.8826829268292684,
      "grad_norm": 0.12074170261621475,
      "learning_rate": 1.8845771083345343e-07,
      "loss": 0.0158,
      "step": 11819
    },
    {
      "epoch": 2.8829268292682926,
      "grad_norm": 0.16798698902130127,
      "learning_rate": 1.8767589833587894e-07,
      "loss": 0.0125,
      "step": 11820
    },
    {
      "epoch": 2.883170731707317,
      "grad_norm": 0.15981385111808777,
      "learning_rate": 1.8689570476649877e-07,
      "loss": 0.0328,
      "step": 11821
    },
    {
      "epoch": 2.8834146341463414,
      "grad_norm": 0.08403175324201584,
      "learning_rate": 1.86117130176211e-07,
      "loss": 0.0133,
      "step": 11822
    },
    {
      "epoch": 2.883658536585366,
      "grad_norm": 0.11669114977121353,
      "learning_rate": 1.8534017461581954e-07,
      "loss": 0.0136,
      "step": 11823
    },
    {
      "epoch": 2.88390243902439,
      "grad_norm": 0.24096542596817017,
      "learning_rate": 1.8456483813601433e-07,
      "loss": 0.0211,
      "step": 11824
    },
    {
      "epoch": 2.8841463414634148,
      "grad_norm": 0.20057788491249084,
      "learning_rate": 1.8379112078738824e-07,
      "loss": 0.0277,
      "step": 11825
    },
    {
      "epoch": 2.884390243902439,
      "grad_norm": 0.053954485803842545,
      "learning_rate": 1.8301902262042038e-07,
      "loss": 0.0139,
      "step": 11826
    },
    {
      "epoch": 2.8846341463414635,
      "grad_norm": 0.06740690022706985,
      "learning_rate": 1.8224854368548983e-07,
      "loss": 0.0074,
      "step": 11827
    },
    {
      "epoch": 2.8848780487804877,
      "grad_norm": 0.11167261004447937,
      "learning_rate": 1.8147968403286752e-07,
      "loss": 0.0164,
      "step": 11828
    },
    {
      "epoch": 2.8851219512195123,
      "grad_norm": 0.08301317691802979,
      "learning_rate": 1.8071244371271888e-07,
      "loss": 0.0065,
      "step": 11829
    },
    {
      "epoch": 2.8853658536585365,
      "grad_norm": 0.12135070562362671,
      "learning_rate": 1.799468227751039e-07,
      "loss": 0.0172,
      "step": 11830
    },
    {
      "epoch": 2.885609756097561,
      "grad_norm": 0.0914117619395256,
      "learning_rate": 1.7918282126997977e-07,
      "loss": 0.0137,
      "step": 11831
    },
    {
      "epoch": 2.8858536585365853,
      "grad_norm": 0.0952097699046135,
      "learning_rate": 1.7842043924718998e-07,
      "loss": 0.0111,
      "step": 11832
    },
    {
      "epoch": 2.88609756097561,
      "grad_norm": 0.08960025012493134,
      "learning_rate": 1.7765967675648087e-07,
      "loss": 0.0203,
      "step": 11833
    },
    {
      "epoch": 2.886341463414634,
      "grad_norm": 0.10322980582714081,
      "learning_rate": 1.7690053384748773e-07,
      "loss": 0.0171,
      "step": 11834
    },
    {
      "epoch": 2.8865853658536587,
      "grad_norm": 0.05661483481526375,
      "learning_rate": 1.7614301056974591e-07,
      "loss": 0.0045,
      "step": 11835
    },
    {
      "epoch": 2.886829268292683,
      "grad_norm": 0.21071474254131317,
      "learning_rate": 1.753871069726798e-07,
      "loss": 0.0209,
      "step": 11836
    },
    {
      "epoch": 2.8870731707317074,
      "grad_norm": 0.14528916776180267,
      "learning_rate": 1.7463282310560825e-07,
      "loss": 0.0154,
      "step": 11837
    },
    {
      "epoch": 2.8873170731707316,
      "grad_norm": 0.18843336403369904,
      "learning_rate": 1.7388015901774744e-07,
      "loss": 0.0241,
      "step": 11838
    },
    {
      "epoch": 2.887560975609756,
      "grad_norm": 0.11425352841615677,
      "learning_rate": 1.7312911475820815e-07,
      "loss": 0.0159,
      "step": 11839
    },
    {
      "epoch": 2.8878048780487804,
      "grad_norm": 0.1350012719631195,
      "learning_rate": 1.7237969037599e-07,
      "loss": 0.0154,
      "step": 11840
    },
    {
      "epoch": 2.888048780487805,
      "grad_norm": 0.09650397300720215,
      "learning_rate": 1.716318859199928e-07,
      "loss": 0.0098,
      "step": 11841
    },
    {
      "epoch": 2.888292682926829,
      "grad_norm": 0.06359037756919861,
      "learning_rate": 1.7088570143900806e-07,
      "loss": 0.0132,
      "step": 11842
    },
    {
      "epoch": 2.888536585365854,
      "grad_norm": 0.07895959913730621,
      "learning_rate": 1.7014113698172186e-07,
      "loss": 0.0108,
      "step": 11843
    },
    {
      "epoch": 2.888780487804878,
      "grad_norm": 0.1233009323477745,
      "learning_rate": 1.6939819259671475e-07,
      "loss": 0.0209,
      "step": 11844
    },
    {
      "epoch": 2.8890243902439026,
      "grad_norm": 0.2038964182138443,
      "learning_rate": 1.6865686833246464e-07,
      "loss": 0.0185,
      "step": 11845
    },
    {
      "epoch": 2.8892682926829267,
      "grad_norm": 0.10825059562921524,
      "learning_rate": 1.6791716423733283e-07,
      "loss": 0.0134,
      "step": 11846
    },
    {
      "epoch": 2.8895121951219513,
      "grad_norm": 0.0728246420621872,
      "learning_rate": 1.6717908035959184e-07,
      "loss": 0.0104,
      "step": 11847
    },
    {
      "epoch": 2.8897560975609755,
      "grad_norm": 0.07669612765312195,
      "learning_rate": 1.664426167473948e-07,
      "loss": 0.0066,
      "step": 11848
    },
    {
      "epoch": 2.89,
      "grad_norm": 0.1204405203461647,
      "learning_rate": 1.657077734487922e-07,
      "loss": 0.0155,
      "step": 11849
    },
    {
      "epoch": 2.8902439024390243,
      "grad_norm": 0.1170836091041565,
      "learning_rate": 1.649745505117345e-07,
      "loss": 0.0181,
      "step": 11850
    },
    {
      "epoch": 2.890487804878049,
      "grad_norm": 0.18702509999275208,
      "learning_rate": 1.642429479840585e-07,
      "loss": 0.0286,
      "step": 11851
    },
    {
      "epoch": 2.890731707317073,
      "grad_norm": 0.15557457506656647,
      "learning_rate": 1.6351296591350097e-07,
      "loss": 0.0242,
      "step": 11852
    },
    {
      "epoch": 2.8909756097560977,
      "grad_norm": 0.0822329968214035,
      "learning_rate": 1.6278460434769049e-07,
      "loss": 0.0151,
      "step": 11853
    },
    {
      "epoch": 2.891219512195122,
      "grad_norm": 0.19216911494731903,
      "learning_rate": 1.620578633341502e-07,
      "loss": 0.0209,
      "step": 11854
    },
    {
      "epoch": 2.8914634146341465,
      "grad_norm": 0.10235141217708588,
      "learning_rate": 1.6133274292030042e-07,
      "loss": 0.0238,
      "step": 11855
    },
    {
      "epoch": 2.8917073170731706,
      "grad_norm": 0.0711449384689331,
      "learning_rate": 1.6060924315344784e-07,
      "loss": 0.0107,
      "step": 11856
    },
    {
      "epoch": 2.8919512195121952,
      "grad_norm": 0.08979594707489014,
      "learning_rate": 1.5988736408080462e-07,
      "loss": 0.0182,
      "step": 11857
    },
    {
      "epoch": 2.8921951219512194,
      "grad_norm": 0.11230508238077164,
      "learning_rate": 1.5916710574946647e-07,
      "loss": 0.0087,
      "step": 11858
    },
    {
      "epoch": 2.892439024390244,
      "grad_norm": 0.15858037769794464,
      "learning_rate": 1.5844846820642634e-07,
      "loss": 0.0176,
      "step": 11859
    },
    {
      "epoch": 2.892682926829268,
      "grad_norm": 0.22077052295207977,
      "learning_rate": 1.5773145149858005e-07,
      "loss": 0.0229,
      "step": 11860
    },
    {
      "epoch": 2.892926829268293,
      "grad_norm": 0.11284525692462921,
      "learning_rate": 1.5701605567270684e-07,
      "loss": 0.0151,
      "step": 11861
    },
    {
      "epoch": 2.893170731707317,
      "grad_norm": 0.11900648474693298,
      "learning_rate": 1.5630228077548326e-07,
      "loss": 0.0132,
      "step": 11862
    },
    {
      "epoch": 2.8934146341463416,
      "grad_norm": 0.20992763340473175,
      "learning_rate": 1.5559012685348317e-07,
      "loss": 0.0408,
      "step": 11863
    },
    {
      "epoch": 2.8936585365853658,
      "grad_norm": 0.10261086374521255,
      "learning_rate": 1.5487959395316943e-07,
      "loss": 0.0239,
      "step": 11864
    },
    {
      "epoch": 2.8939024390243904,
      "grad_norm": 0.07048527896404266,
      "learning_rate": 1.5417068212090768e-07,
      "loss": 0.01,
      "step": 11865
    },
    {
      "epoch": 2.8941463414634145,
      "grad_norm": 0.12443497776985168,
      "learning_rate": 1.534633914029443e-07,
      "loss": 0.0123,
      "step": 11866
    },
    {
      "epoch": 2.894390243902439,
      "grad_norm": 0.14237868785858154,
      "learning_rate": 1.5275772184543403e-07,
      "loss": 0.02,
      "step": 11867
    },
    {
      "epoch": 2.8946341463414633,
      "grad_norm": 0.12127762287855148,
      "learning_rate": 1.520536734944178e-07,
      "loss": 0.0219,
      "step": 11868
    },
    {
      "epoch": 2.894878048780488,
      "grad_norm": 0.08578749746084213,
      "learning_rate": 1.513512463958311e-07,
      "loss": 0.018,
      "step": 11869
    },
    {
      "epoch": 2.895121951219512,
      "grad_norm": 0.15276718139648438,
      "learning_rate": 1.506504405955067e-07,
      "loss": 0.019,
      "step": 11870
    },
    {
      "epoch": 2.8953658536585367,
      "grad_norm": 0.0908704400062561,
      "learning_rate": 1.4995125613917193e-07,
      "loss": 0.0135,
      "step": 11871
    },
    {
      "epoch": 2.895609756097561,
      "grad_norm": 0.07355985790491104,
      "learning_rate": 1.4925369307244307e-07,
      "loss": 0.0087,
      "step": 11872
    },
    {
      "epoch": 2.8958536585365855,
      "grad_norm": 0.09193333983421326,
      "learning_rate": 1.485577514408365e-07,
      "loss": 0.0199,
      "step": 11873
    },
    {
      "epoch": 2.8960975609756097,
      "grad_norm": 0.07743965089321136,
      "learning_rate": 1.4786343128975754e-07,
      "loss": 0.0107,
      "step": 11874
    },
    {
      "epoch": 2.8963414634146343,
      "grad_norm": 0.12671856582164764,
      "learning_rate": 1.4717073266450888e-07,
      "loss": 0.0187,
      "step": 11875
    },
    {
      "epoch": 2.8965853658536584,
      "grad_norm": 0.1266806423664093,
      "learning_rate": 1.4647965561029043e-07,
      "loss": 0.0173,
      "step": 11876
    },
    {
      "epoch": 2.896829268292683,
      "grad_norm": 0.08935528993606567,
      "learning_rate": 1.4579020017219113e-07,
      "loss": 0.019,
      "step": 11877
    },
    {
      "epoch": 2.8970731707317072,
      "grad_norm": 0.1840052306652069,
      "learning_rate": 1.451023663951917e-07,
      "loss": 0.0208,
      "step": 11878
    },
    {
      "epoch": 2.897317073170732,
      "grad_norm": 0.10860300809144974,
      "learning_rate": 1.4441615432417843e-07,
      "loss": 0.0102,
      "step": 11879
    },
    {
      "epoch": 2.897560975609756,
      "grad_norm": 0.24092599749565125,
      "learning_rate": 1.4373156400392106e-07,
      "loss": 0.0183,
      "step": 11880
    },
    {
      "epoch": 2.8978048780487806,
      "grad_norm": 0.1227291151881218,
      "learning_rate": 1.4304859547908389e-07,
      "loss": 0.0172,
      "step": 11881
    },
    {
      "epoch": 2.898048780487805,
      "grad_norm": 0.1730928122997284,
      "learning_rate": 1.4236724879423124e-07,
      "loss": 0.0218,
      "step": 11882
    },
    {
      "epoch": 2.8982926829268294,
      "grad_norm": 0.1474890410900116,
      "learning_rate": 1.4168752399382202e-07,
      "loss": 0.0244,
      "step": 11883
    },
    {
      "epoch": 2.8985365853658536,
      "grad_norm": 0.08257985860109329,
      "learning_rate": 1.410094211222013e-07,
      "loss": 0.0108,
      "step": 11884
    },
    {
      "epoch": 2.898780487804878,
      "grad_norm": 0.07331645488739014,
      "learning_rate": 1.4033294022361697e-07,
      "loss": 0.0156,
      "step": 11885
    },
    {
      "epoch": 2.8990243902439023,
      "grad_norm": 0.1051940992474556,
      "learning_rate": 1.3965808134220326e-07,
      "loss": 0.0159,
      "step": 11886
    },
    {
      "epoch": 2.899268292682927,
      "grad_norm": 0.14944066107273102,
      "learning_rate": 1.3898484452199712e-07,
      "loss": 0.0178,
      "step": 11887
    },
    {
      "epoch": 2.899512195121951,
      "grad_norm": 0.08344987034797668,
      "learning_rate": 1.3831322980691896e-07,
      "loss": 0.015,
      "step": 11888
    },
    {
      "epoch": 2.8997560975609757,
      "grad_norm": 0.14741162955760956,
      "learning_rate": 1.3764323724079765e-07,
      "loss": 0.0314,
      "step": 11889
    },
    {
      "epoch": 2.9,
      "grad_norm": 0.14488330483436584,
      "learning_rate": 1.369748668673426e-07,
      "loss": 0.0277,
      "step": 11890
    },
    {
      "epoch": 2.9002439024390245,
      "grad_norm": 0.14113372564315796,
      "learning_rate": 1.3630811873016623e-07,
      "loss": 0.0126,
      "step": 11891
    },
    {
      "epoch": 2.9004878048780487,
      "grad_norm": 0.09762240946292877,
      "learning_rate": 1.3564299287276704e-07,
      "loss": 0.0174,
      "step": 11892
    },
    {
      "epoch": 2.9007317073170733,
      "grad_norm": 0.10126645117998123,
      "learning_rate": 1.3497948933854642e-07,
      "loss": 0.0134,
      "step": 11893
    },
    {
      "epoch": 2.9009756097560975,
      "grad_norm": 0.16567863523960114,
      "learning_rate": 1.3431760817079753e-07,
      "loss": 0.0152,
      "step": 11894
    },
    {
      "epoch": 2.901219512195122,
      "grad_norm": 0.10913869738578796,
      "learning_rate": 1.3365734941270247e-07,
      "loss": 0.0202,
      "step": 11895
    },
    {
      "epoch": 2.9014634146341463,
      "grad_norm": 0.15510816872119904,
      "learning_rate": 1.3299871310734068e-07,
      "loss": 0.017,
      "step": 11896
    },
    {
      "epoch": 2.901707317073171,
      "grad_norm": 0.13236258924007416,
      "learning_rate": 1.323416992976889e-07,
      "loss": 0.0144,
      "step": 11897
    },
    {
      "epoch": 2.901951219512195,
      "grad_norm": 0.06897401064634323,
      "learning_rate": 1.3168630802661286e-07,
      "loss": 0.0152,
      "step": 11898
    },
    {
      "epoch": 2.9021951219512196,
      "grad_norm": 0.09351836144924164,
      "learning_rate": 1.3103253933687554e-07,
      "loss": 0.0157,
      "step": 11899
    },
    {
      "epoch": 2.902439024390244,
      "grad_norm": 0.19634994864463806,
      "learning_rate": 1.3038039327113726e-07,
      "loss": 0.0149,
      "step": 11900
    },
    {
      "epoch": 2.9026829268292684,
      "grad_norm": 0.19629599153995514,
      "learning_rate": 1.297298698719418e-07,
      "loss": 0.02,
      "step": 11901
    },
    {
      "epoch": 2.9029268292682926,
      "grad_norm": 0.13193926215171814,
      "learning_rate": 1.290809691817385e-07,
      "loss": 0.031,
      "step": 11902
    },
    {
      "epoch": 2.903170731707317,
      "grad_norm": 0.17414310574531555,
      "learning_rate": 1.2843369124286297e-07,
      "loss": 0.0194,
      "step": 11903
    },
    {
      "epoch": 2.9034146341463414,
      "grad_norm": 0.17227846384048462,
      "learning_rate": 1.2778803609755086e-07,
      "loss": 0.0139,
      "step": 11904
    },
    {
      "epoch": 2.903658536585366,
      "grad_norm": 0.16805730760097504,
      "learning_rate": 1.2714400378792956e-07,
      "loss": 0.0137,
      "step": 11905
    },
    {
      "epoch": 2.90390243902439,
      "grad_norm": 0.10936826467514038,
      "learning_rate": 1.2650159435601828e-07,
      "loss": 0.0157,
      "step": 11906
    },
    {
      "epoch": 2.9041463414634148,
      "grad_norm": 0.08525405824184418,
      "learning_rate": 1.2586080784373066e-07,
      "loss": 0.0115,
      "step": 11907
    },
    {
      "epoch": 2.904390243902439,
      "grad_norm": 0.13267002999782562,
      "learning_rate": 1.2522164429288053e-07,
      "loss": 0.0137,
      "step": 11908
    },
    {
      "epoch": 2.9046341463414636,
      "grad_norm": 0.16735264658927917,
      "learning_rate": 1.245841037451706e-07,
      "loss": 0.0162,
      "step": 11909
    },
    {
      "epoch": 2.9048780487804877,
      "grad_norm": 0.15558673441410065,
      "learning_rate": 1.239481862421954e-07,
      "loss": 0.0302,
      "step": 11910
    },
    {
      "epoch": 2.9051219512195123,
      "grad_norm": 0.11059900373220444,
      "learning_rate": 1.2331389182544673e-07,
      "loss": 0.0157,
      "step": 11911
    },
    {
      "epoch": 2.9053658536585365,
      "grad_norm": 0.18275359272956848,
      "learning_rate": 1.226812205363137e-07,
      "loss": 0.0224,
      "step": 11912
    },
    {
      "epoch": 2.905609756097561,
      "grad_norm": 0.16715914011001587,
      "learning_rate": 1.2205017241607442e-07,
      "loss": 0.0261,
      "step": 11913
    },
    {
      "epoch": 2.9058536585365853,
      "grad_norm": 0.09251800179481506,
      "learning_rate": 1.2142074750590149e-07,
      "loss": 0.0172,
      "step": 11914
    },
    {
      "epoch": 2.90609756097561,
      "grad_norm": 0.09711147844791412,
      "learning_rate": 1.2079294584686484e-07,
      "loss": 0.0205,
      "step": 11915
    },
    {
      "epoch": 2.906341463414634,
      "grad_norm": 0.1401490420103073,
      "learning_rate": 1.2016676747992338e-07,
      "loss": 0.0125,
      "step": 11916
    },
    {
      "epoch": 2.9065853658536587,
      "grad_norm": 0.12082473933696747,
      "learning_rate": 1.195422124459389e-07,
      "loss": 0.0075,
      "step": 11917
    },
    {
      "epoch": 2.906829268292683,
      "grad_norm": 0.2119503915309906,
      "learning_rate": 1.1891928078565651e-07,
      "loss": 0.0368,
      "step": 11918
    },
    {
      "epoch": 2.9070731707317075,
      "grad_norm": 0.11686377227306366,
      "learning_rate": 1.182979725397243e-07,
      "loss": 0.0152,
      "step": 11919
    },
    {
      "epoch": 2.9073170731707316,
      "grad_norm": 0.15242357552051544,
      "learning_rate": 1.1767828774867928e-07,
      "loss": 0.0236,
      "step": 11920
    },
    {
      "epoch": 2.9075609756097562,
      "grad_norm": 0.1067819595336914,
      "learning_rate": 1.17060226452953e-07,
      "loss": 0.0131,
      "step": 11921
    },
    {
      "epoch": 2.9078048780487804,
      "grad_norm": 0.2205100804567337,
      "learning_rate": 1.164437886928743e-07,
      "loss": 0.0285,
      "step": 11922
    },
    {
      "epoch": 2.908048780487805,
      "grad_norm": 0.1328686773777008,
      "learning_rate": 1.1582897450866104e-07,
      "loss": 0.0229,
      "step": 11923
    },
    {
      "epoch": 2.908292682926829,
      "grad_norm": 0.08011242747306824,
      "learning_rate": 1.1521578394043109e-07,
      "loss": 0.0166,
      "step": 11924
    },
    {
      "epoch": 2.908536585365854,
      "grad_norm": 0.1266147792339325,
      "learning_rate": 1.1460421702818857e-07,
      "loss": 0.0188,
      "step": 11925
    },
    {
      "epoch": 2.908780487804878,
      "grad_norm": 0.09551789611577988,
      "learning_rate": 1.1399427381184325e-07,
      "loss": 0.0116,
      "step": 11926
    },
    {
      "epoch": 2.9090243902439026,
      "grad_norm": 0.09955243766307831,
      "learning_rate": 1.1338595433118549e-07,
      "loss": 0.0101,
      "step": 11927
    },
    {
      "epoch": 2.9092682926829267,
      "grad_norm": 0.10348468273878098,
      "learning_rate": 1.1277925862590854e-07,
      "loss": 0.0097,
      "step": 11928
    },
    {
      "epoch": 2.9095121951219514,
      "grad_norm": 0.13432082533836365,
      "learning_rate": 1.121741867356002e-07,
      "loss": 0.0088,
      "step": 11929
    },
    {
      "epoch": 2.9097560975609755,
      "grad_norm": 0.13480235636234283,
      "learning_rate": 1.1157073869973722e-07,
      "loss": 0.0141,
      "step": 11930
    },
    {
      "epoch": 2.91,
      "grad_norm": 0.1291143000125885,
      "learning_rate": 1.1096891455769087e-07,
      "loss": 0.0225,
      "step": 11931
    },
    {
      "epoch": 2.9102439024390243,
      "grad_norm": 0.1344655156135559,
      "learning_rate": 1.1036871434872975e-07,
      "loss": 0.0192,
      "step": 11932
    },
    {
      "epoch": 2.910487804878049,
      "grad_norm": 0.09150263667106628,
      "learning_rate": 1.0977013811201974e-07,
      "loss": 0.0141,
      "step": 11933
    },
    {
      "epoch": 2.910731707317073,
      "grad_norm": 0.11928292363882065,
      "learning_rate": 1.0917318588660742e-07,
      "loss": 0.0104,
      "step": 11934
    },
    {
      "epoch": 2.9109756097560977,
      "grad_norm": 0.13026733696460724,
      "learning_rate": 1.0857785771144769e-07,
      "loss": 0.0119,
      "step": 11935
    },
    {
      "epoch": 2.911219512195122,
      "grad_norm": 0.19100823998451233,
      "learning_rate": 1.079841536253845e-07,
      "loss": 0.0222,
      "step": 11936
    },
    {
      "epoch": 2.9114634146341465,
      "grad_norm": 0.18867814540863037,
      "learning_rate": 1.0739207366715076e-07,
      "loss": 0.0199,
      "step": 11937
    },
    {
      "epoch": 2.9117073170731707,
      "grad_norm": 0.10782577097415924,
      "learning_rate": 1.0680161787538223e-07,
      "loss": 0.0167,
      "step": 11938
    },
    {
      "epoch": 2.9119512195121953,
      "grad_norm": 0.23609210550785065,
      "learning_rate": 1.0621278628860365e-07,
      "loss": 0.019,
      "step": 11939
    },
    {
      "epoch": 2.9121951219512194,
      "grad_norm": 0.30108290910720825,
      "learning_rate": 1.056255789452315e-07,
      "loss": 0.024,
      "step": 11940
    },
    {
      "epoch": 2.912439024390244,
      "grad_norm": 0.1550692766904831,
      "learning_rate": 1.0503999588358238e-07,
      "loss": 0.0209,
      "step": 11941
    },
    {
      "epoch": 2.912682926829268,
      "grad_norm": 0.1530732363462448,
      "learning_rate": 1.0445603714186458e-07,
      "loss": 0.0133,
      "step": 11942
    },
    {
      "epoch": 2.912926829268293,
      "grad_norm": 0.13974297046661377,
      "learning_rate": 1.0387370275817543e-07,
      "loss": 0.0064,
      "step": 11943
    },
    {
      "epoch": 2.913170731707317,
      "grad_norm": 0.10075964033603668,
      "learning_rate": 1.032929927705123e-07,
      "loss": 0.012,
      "step": 11944
    },
    {
      "epoch": 2.9134146341463416,
      "grad_norm": 0.09429749101400375,
      "learning_rate": 1.027139072167671e-07,
      "loss": 0.0183,
      "step": 11945
    },
    {
      "epoch": 2.9136585365853658,
      "grad_norm": 0.08495061844587326,
      "learning_rate": 1.0213644613472073e-07,
      "loss": 0.0091,
      "step": 11946
    },
    {
      "epoch": 2.9139024390243904,
      "grad_norm": 0.10685836523771286,
      "learning_rate": 1.0156060956205138e-07,
      "loss": 0.0178,
      "step": 11947
    },
    {
      "epoch": 2.9141463414634146,
      "grad_norm": 0.16626493632793427,
      "learning_rate": 1.0098639753633177e-07,
      "loss": 0.0243,
      "step": 11948
    },
    {
      "epoch": 2.914390243902439,
      "grad_norm": 0.08045407384634018,
      "learning_rate": 1.0041381009502638e-07,
      "loss": 0.0127,
      "step": 11949
    },
    {
      "epoch": 2.9146341463414633,
      "grad_norm": 0.12225478142499924,
      "learning_rate": 9.98428472754942e-08,
      "loss": 0.0176,
      "step": 11950
    },
    {
      "epoch": 2.914878048780488,
      "grad_norm": 0.2149074226617813,
      "learning_rate": 9.927350911498878e-08,
      "loss": 0.0151,
      "step": 11951
    },
    {
      "epoch": 2.915121951219512,
      "grad_norm": 0.14589446783065796,
      "learning_rate": 9.870579565066095e-08,
      "loss": 0.0093,
      "step": 11952
    },
    {
      "epoch": 2.9153658536585367,
      "grad_norm": 0.10808935016393661,
      "learning_rate": 9.8139706919545e-08,
      "loss": 0.0105,
      "step": 11953
    },
    {
      "epoch": 2.915609756097561,
      "grad_norm": 0.17394022643566132,
      "learning_rate": 9.757524295858633e-08,
      "loss": 0.0131,
      "step": 11954
    },
    {
      "epoch": 2.9158536585365855,
      "grad_norm": 0.12598423659801483,
      "learning_rate": 9.70124038046083e-08,
      "loss": 0.0085,
      "step": 11955
    },
    {
      "epoch": 2.9160975609756097,
      "grad_norm": 0.14187602698802948,
      "learning_rate": 9.64511894943343e-08,
      "loss": 0.0241,
      "step": 11956
    },
    {
      "epoch": 2.9163414634146343,
      "grad_norm": 0.083457812666893,
      "learning_rate": 9.589160006438225e-08,
      "loss": 0.0082,
      "step": 11957
    },
    {
      "epoch": 2.9165853658536585,
      "grad_norm": 0.07617417722940445,
      "learning_rate": 9.53336355512674e-08,
      "loss": 0.0076,
      "step": 11958
    },
    {
      "epoch": 2.916829268292683,
      "grad_norm": 0.09977883845567703,
      "learning_rate": 9.47772959913884e-08,
      "loss": 0.0126,
      "step": 11959
    },
    {
      "epoch": 2.9170731707317072,
      "grad_norm": 0.1432901918888092,
      "learning_rate": 9.422258142105233e-08,
      "loss": 0.0242,
      "step": 11960
    },
    {
      "epoch": 2.917317073170732,
      "grad_norm": 0.15323162078857422,
      "learning_rate": 9.366949187644692e-08,
      "loss": 0.0197,
      "step": 11961
    },
    {
      "epoch": 2.917560975609756,
      "grad_norm": 0.24814894795417786,
      "learning_rate": 9.311802739366271e-08,
      "loss": 0.034,
      "step": 11962
    },
    {
      "epoch": 2.9178048780487806,
      "grad_norm": 0.1338878870010376,
      "learning_rate": 9.256818800868205e-08,
      "loss": 0.0204,
      "step": 11963
    },
    {
      "epoch": 2.918048780487805,
      "grad_norm": 0.13405144214630127,
      "learning_rate": 9.201997375737348e-08,
      "loss": 0.0195,
      "step": 11964
    },
    {
      "epoch": 2.9182926829268294,
      "grad_norm": 0.1579757034778595,
      "learning_rate": 9.147338467551392e-08,
      "loss": 0.0162,
      "step": 11965
    },
    {
      "epoch": 2.9185365853658536,
      "grad_norm": 0.20664024353027344,
      "learning_rate": 9.092842079876374e-08,
      "loss": 0.0169,
      "step": 11966
    },
    {
      "epoch": 2.918780487804878,
      "grad_norm": 0.07476264983415604,
      "learning_rate": 9.038508216267783e-08,
      "loss": 0.0076,
      "step": 11967
    },
    {
      "epoch": 2.9190243902439024,
      "grad_norm": 0.11213189363479614,
      "learning_rate": 8.984336880271116e-08,
      "loss": 0.0094,
      "step": 11968
    },
    {
      "epoch": 2.919268292682927,
      "grad_norm": 0.09854638576507568,
      "learning_rate": 8.930328075420769e-08,
      "loss": 0.0127,
      "step": 11969
    },
    {
      "epoch": 2.919512195121951,
      "grad_norm": 0.1051688864827156,
      "learning_rate": 8.876481805240589e-08,
      "loss": 0.0153,
      "step": 11970
    },
    {
      "epoch": 2.9197560975609758,
      "grad_norm": 0.15065518021583557,
      "learning_rate": 8.822798073243877e-08,
      "loss": 0.0214,
      "step": 11971
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.11388611793518066,
      "learning_rate": 8.769276882933386e-08,
      "loss": 0.0098,
      "step": 11972
    },
    {
      "epoch": 2.9202439024390245,
      "grad_norm": 0.0825912281870842,
      "learning_rate": 8.715918237801323e-08,
      "loss": 0.0121,
      "step": 11973
    },
    {
      "epoch": 2.9204878048780487,
      "grad_norm": 0.14687615633010864,
      "learning_rate": 8.662722141328794e-08,
      "loss": 0.0193,
      "step": 11974
    },
    {
      "epoch": 2.9207317073170733,
      "grad_norm": 0.15218447148799896,
      "learning_rate": 8.60968859698691e-08,
      "loss": 0.0234,
      "step": 11975
    },
    {
      "epoch": 2.9209756097560975,
      "grad_norm": 0.12958434224128723,
      "learning_rate": 8.556817608236234e-08,
      "loss": 0.0151,
      "step": 11976
    },
    {
      "epoch": 2.921219512195122,
      "grad_norm": 0.20779241621494293,
      "learning_rate": 8.504109178525954e-08,
      "loss": 0.0225,
      "step": 11977
    },
    {
      "epoch": 2.9214634146341463,
      "grad_norm": 0.17550097405910492,
      "learning_rate": 8.451563311295541e-08,
      "loss": 0.0176,
      "step": 11978
    },
    {
      "epoch": 2.921707317073171,
      "grad_norm": 0.09974058717489243,
      "learning_rate": 8.399180009973084e-08,
      "loss": 0.0143,
      "step": 11979
    },
    {
      "epoch": 2.921951219512195,
      "grad_norm": 0.20579621195793152,
      "learning_rate": 8.346959277976685e-08,
      "loss": 0.0217,
      "step": 11980
    },
    {
      "epoch": 2.9221951219512197,
      "grad_norm": 0.1389528512954712,
      "learning_rate": 8.294901118713616e-08,
      "loss": 0.0116,
      "step": 11981
    },
    {
      "epoch": 2.922439024390244,
      "grad_norm": 0.1040944829583168,
      "learning_rate": 8.243005535580328e-08,
      "loss": 0.0142,
      "step": 11982
    },
    {
      "epoch": 2.9226829268292684,
      "grad_norm": 0.20140783488750458,
      "learning_rate": 8.191272531963001e-08,
      "loss": 0.0205,
      "step": 11983
    },
    {
      "epoch": 2.9229268292682926,
      "grad_norm": 0.12040584534406662,
      "learning_rate": 8.139702111236713e-08,
      "loss": 0.0157,
      "step": 11984
    },
    {
      "epoch": 2.9231707317073172,
      "grad_norm": 0.0978143960237503,
      "learning_rate": 8.088294276766827e-08,
      "loss": 0.0223,
      "step": 11985
    },
    {
      "epoch": 2.9234146341463414,
      "grad_norm": 0.13684017956256866,
      "learning_rate": 8.037049031907051e-08,
      "loss": 0.0173,
      "step": 11986
    },
    {
      "epoch": 2.923658536585366,
      "grad_norm": 0.1624458283185959,
      "learning_rate": 7.985966380001375e-08,
      "loss": 0.0144,
      "step": 11987
    },
    {
      "epoch": 2.92390243902439,
      "grad_norm": 0.08639200031757355,
      "learning_rate": 7.935046324382412e-08,
      "loss": 0.0122,
      "step": 11988
    },
    {
      "epoch": 2.924146341463415,
      "grad_norm": 0.1710619032382965,
      "learning_rate": 7.884288868373057e-08,
      "loss": 0.0142,
      "step": 11989
    },
    {
      "epoch": 2.924390243902439,
      "grad_norm": 0.13933488726615906,
      "learning_rate": 7.833694015284277e-08,
      "loss": 0.0154,
      "step": 11990
    },
    {
      "epoch": 2.9246341463414636,
      "grad_norm": 0.1414436399936676,
      "learning_rate": 7.783261768418149e-08,
      "loss": 0.0126,
      "step": 11991
    },
    {
      "epoch": 2.9248780487804877,
      "grad_norm": 0.15228380262851715,
      "learning_rate": 7.732992131064543e-08,
      "loss": 0.0213,
      "step": 11992
    },
    {
      "epoch": 2.9251219512195124,
      "grad_norm": 0.15397502481937408,
      "learning_rate": 7.682885106503612e-08,
      "loss": 0.0228,
      "step": 11993
    },
    {
      "epoch": 2.9253658536585365,
      "grad_norm": 0.3216291666030884,
      "learning_rate": 7.632940698004965e-08,
      "loss": 0.02,
      "step": 11994
    },
    {
      "epoch": 2.925609756097561,
      "grad_norm": 0.10185901820659637,
      "learning_rate": 7.583158908826826e-08,
      "loss": 0.0158,
      "step": 11995
    },
    {
      "epoch": 2.9258536585365853,
      "grad_norm": 0.08579309284687042,
      "learning_rate": 7.53353974221771e-08,
      "loss": 0.014,
      "step": 11996
    },
    {
      "epoch": 2.92609756097561,
      "grad_norm": 0.18978756666183472,
      "learning_rate": 7.484083201415027e-08,
      "loss": 0.0217,
      "step": 11997
    },
    {
      "epoch": 2.926341463414634,
      "grad_norm": 0.12586192786693573,
      "learning_rate": 7.43478928964536e-08,
      "loss": 0.0093,
      "step": 11998
    },
    {
      "epoch": 2.9265853658536587,
      "grad_norm": 0.10800733417272568,
      "learning_rate": 7.385658010125307e-08,
      "loss": 0.0127,
      "step": 11999
    },
    {
      "epoch": 2.926829268292683,
      "grad_norm": 0.17334206402301788,
      "learning_rate": 7.336689366060357e-08,
      "loss": 0.0143,
      "step": 12000
    },
    {
      "epoch": 2.9270731707317075,
      "grad_norm": 0.13755083084106445,
      "learning_rate": 7.28788336064573e-08,
      "loss": 0.0207,
      "step": 12001
    },
    {
      "epoch": 2.9273170731707316,
      "grad_norm": 0.10546160489320755,
      "learning_rate": 7.239239997065827e-08,
      "loss": 0.0136,
      "step": 12002
    },
    {
      "epoch": 2.9275609756097563,
      "grad_norm": 0.13510702550411224,
      "learning_rate": 7.190759278494497e-08,
      "loss": 0.017,
      "step": 12003
    },
    {
      "epoch": 2.9278048780487804,
      "grad_norm": 0.11459934711456299,
      "learning_rate": 7.142441208094763e-08,
      "loss": 0.0283,
      "step": 12004
    },
    {
      "epoch": 2.928048780487805,
      "grad_norm": 0.10580258816480637,
      "learning_rate": 7.094285789019384e-08,
      "loss": 0.0139,
      "step": 12005
    },
    {
      "epoch": 2.928292682926829,
      "grad_norm": 0.1256955862045288,
      "learning_rate": 7.046293024410289e-08,
      "loss": 0.0311,
      "step": 12006
    },
    {
      "epoch": 2.928536585365854,
      "grad_norm": 0.10778599232435226,
      "learning_rate": 6.99846291739914e-08,
      "loss": 0.0101,
      "step": 12007
    },
    {
      "epoch": 2.928780487804878,
      "grad_norm": 0.18502052128314972,
      "learning_rate": 6.95079547110622e-08,
      "loss": 0.0284,
      "step": 12008
    },
    {
      "epoch": 2.9290243902439026,
      "grad_norm": 0.09638281911611557,
      "learning_rate": 6.903290688642094e-08,
      "loss": 0.0066,
      "step": 12009
    },
    {
      "epoch": 2.9292682926829268,
      "grad_norm": 0.1535772979259491,
      "learning_rate": 6.85594857310623e-08,
      "loss": 0.022,
      "step": 12010
    },
    {
      "epoch": 2.9295121951219514,
      "grad_norm": 0.18076103925704956,
      "learning_rate": 6.808769127587267e-08,
      "loss": 0.0122,
      "step": 12011
    },
    {
      "epoch": 2.9297560975609755,
      "grad_norm": 0.1780724972486496,
      "learning_rate": 6.761752355163853e-08,
      "loss": 0.0174,
      "step": 12012
    },
    {
      "epoch": 2.93,
      "grad_norm": 0.07796292752027512,
      "learning_rate": 6.714898258903534e-08,
      "loss": 0.0085,
      "step": 12013
    },
    {
      "epoch": 2.9302439024390243,
      "grad_norm": 0.08144361525774002,
      "learning_rate": 6.668206841863589e-08,
      "loss": 0.0078,
      "step": 12014
    },
    {
      "epoch": 2.930487804878049,
      "grad_norm": 0.11331894993782043,
      "learning_rate": 6.62167810709019e-08,
      "loss": 0.0132,
      "step": 12015
    },
    {
      "epoch": 2.930731707317073,
      "grad_norm": 0.08000081032514572,
      "learning_rate": 6.575312057619243e-08,
      "loss": 0.0103,
      "step": 12016
    },
    {
      "epoch": 2.9309756097560977,
      "grad_norm": 0.1186235174536705,
      "learning_rate": 6.529108696476104e-08,
      "loss": 0.0129,
      "step": 12017
    },
    {
      "epoch": 2.931219512195122,
      "grad_norm": 0.17249596118927002,
      "learning_rate": 6.483068026675309e-08,
      "loss": 0.0215,
      "step": 12018
    },
    {
      "epoch": 2.9314634146341465,
      "grad_norm": 0.09694153815507889,
      "learning_rate": 6.437190051221121e-08,
      "loss": 0.0087,
      "step": 12019
    },
    {
      "epoch": 2.9317073170731707,
      "grad_norm": 0.07546366751194,
      "learning_rate": 6.391474773106699e-08,
      "loss": 0.0087,
      "step": 12020
    },
    {
      "epoch": 2.9319512195121953,
      "grad_norm": 0.09338684380054474,
      "learning_rate": 6.345922195314657e-08,
      "loss": 0.0093,
      "step": 12021
    },
    {
      "epoch": 2.9321951219512195,
      "grad_norm": 0.09319532662630081,
      "learning_rate": 6.300532320817621e-08,
      "loss": 0.0102,
      "step": 12022
    },
    {
      "epoch": 2.932439024390244,
      "grad_norm": 0.09007786959409714,
      "learning_rate": 6.25530515257683e-08,
      "loss": 0.0102,
      "step": 12023
    },
    {
      "epoch": 2.9326829268292682,
      "grad_norm": 0.2839449942111969,
      "learning_rate": 6.210240693543256e-08,
      "loss": 0.0226,
      "step": 12024
    },
    {
      "epoch": 2.932926829268293,
      "grad_norm": 0.14119377732276917,
      "learning_rate": 6.165338946657607e-08,
      "loss": 0.0233,
      "step": 12025
    },
    {
      "epoch": 2.933170731707317,
      "grad_norm": 0.13120508193969727,
      "learning_rate": 6.120599914848924e-08,
      "loss": 0.0229,
      "step": 12026
    },
    {
      "epoch": 2.9334146341463416,
      "grad_norm": 0.17736349999904633,
      "learning_rate": 6.07602360103654e-08,
      "loss": 0.0239,
      "step": 12027
    },
    {
      "epoch": 2.933658536585366,
      "grad_norm": 0.132155641913414,
      "learning_rate": 6.03161000812924e-08,
      "loss": 0.0139,
      "step": 12028
    },
    {
      "epoch": 2.9339024390243904,
      "grad_norm": 0.16994906961917877,
      "learning_rate": 5.987359139024151e-08,
      "loss": 0.0135,
      "step": 12029
    },
    {
      "epoch": 2.9341463414634146,
      "grad_norm": 0.12313224375247955,
      "learning_rate": 5.943270996609518e-08,
      "loss": 0.0186,
      "step": 12030
    },
    {
      "epoch": 2.934390243902439,
      "grad_norm": 0.17466804385185242,
      "learning_rate": 5.899345583761096e-08,
      "loss": 0.0144,
      "step": 12031
    },
    {
      "epoch": 2.9346341463414634,
      "grad_norm": 0.14218945801258087,
      "learning_rate": 5.8555829033452025e-08,
      "loss": 0.0135,
      "step": 12032
    },
    {
      "epoch": 2.934878048780488,
      "grad_norm": 0.07416193187236786,
      "learning_rate": 5.8119829582173324e-08,
      "loss": 0.0076,
      "step": 12033
    },
    {
      "epoch": 2.935121951219512,
      "grad_norm": 0.05475476011633873,
      "learning_rate": 5.768545751221876e-08,
      "loss": 0.0093,
      "step": 12034
    },
    {
      "epoch": 2.9353658536585368,
      "grad_norm": 0.10775706171989441,
      "learning_rate": 5.725271285193512e-08,
      "loss": 0.018,
      "step": 12035
    },
    {
      "epoch": 2.935609756097561,
      "grad_norm": 0.1414707899093628,
      "learning_rate": 5.6821595629555356e-08,
      "loss": 0.0148,
      "step": 12036
    },
    {
      "epoch": 2.9358536585365855,
      "grad_norm": 0.09560796618461609,
      "learning_rate": 5.639210587320698e-08,
      "loss": 0.0207,
      "step": 12037
    },
    {
      "epoch": 2.9360975609756097,
      "grad_norm": 0.1139652356505394,
      "learning_rate": 5.596424361091202e-08,
      "loss": 0.018,
      "step": 12038
    },
    {
      "epoch": 2.9363414634146343,
      "grad_norm": 0.21706722676753998,
      "learning_rate": 5.553800887059257e-08,
      "loss": 0.0234,
      "step": 12039
    },
    {
      "epoch": 2.9365853658536585,
      "grad_norm": 0.14060930907726288,
      "learning_rate": 5.511340168005419e-08,
      "loss": 0.0134,
      "step": 12040
    },
    {
      "epoch": 2.936829268292683,
      "grad_norm": 0.11526215076446533,
      "learning_rate": 5.469042206700248e-08,
      "loss": 0.0208,
      "step": 12041
    },
    {
      "epoch": 2.9370731707317073,
      "grad_norm": 0.16201959550380707,
      "learning_rate": 5.426907005903481e-08,
      "loss": 0.0308,
      "step": 12042
    },
    {
      "epoch": 2.937317073170732,
      "grad_norm": 0.1404191255569458,
      "learning_rate": 5.384934568364586e-08,
      "loss": 0.0152,
      "step": 12043
    },
    {
      "epoch": 2.937560975609756,
      "grad_norm": 0.11701415479183197,
      "learning_rate": 5.343124896821927e-08,
      "loss": 0.0196,
      "step": 12044
    },
    {
      "epoch": 2.93780487804878,
      "grad_norm": 0.1346813291311264,
      "learning_rate": 5.3014779940033235e-08,
      "loss": 0.0187,
      "step": 12045
    },
    {
      "epoch": 2.938048780487805,
      "grad_norm": 0.21911123394966125,
      "learning_rate": 5.2599938626266e-08,
      "loss": 0.0111,
      "step": 12046
    },
    {
      "epoch": 2.9382926829268294,
      "grad_norm": 0.15428495407104492,
      "learning_rate": 5.218672505397926e-08,
      "loss": 0.0258,
      "step": 12047
    },
    {
      "epoch": 2.9385365853658536,
      "grad_norm": 0.11559946835041046,
      "learning_rate": 5.177513925013755e-08,
      "loss": 0.0266,
      "step": 12048
    },
    {
      "epoch": 2.9387804878048778,
      "grad_norm": 0.18545383214950562,
      "learning_rate": 5.136518124159162e-08,
      "loss": 0.0233,
      "step": 12049
    },
    {
      "epoch": 2.9390243902439024,
      "grad_norm": 0.09248188138008118,
      "learning_rate": 5.0956851055095066e-08,
      "loss": 0.0159,
      "step": 12050
    },
    {
      "epoch": 2.939268292682927,
      "grad_norm": 0.2200731486082077,
      "learning_rate": 5.055014871728492e-08,
      "loss": 0.0328,
      "step": 12051
    },
    {
      "epoch": 2.939512195121951,
      "grad_norm": 0.13619758188724518,
      "learning_rate": 5.014507425470383e-08,
      "loss": 0.0255,
      "step": 12052
    },
    {
      "epoch": 2.9397560975609753,
      "grad_norm": 0.10672421008348465,
      "learning_rate": 4.974162769377511e-08,
      "loss": 0.0217,
      "step": 12053
    },
    {
      "epoch": 2.94,
      "grad_norm": 0.13322971761226654,
      "learning_rate": 4.933980906082491e-08,
      "loss": 0.0199,
      "step": 12054
    },
    {
      "epoch": 2.9402439024390246,
      "grad_norm": 0.08221044391393661,
      "learning_rate": 4.8939618382071175e-08,
      "loss": 0.0121,
      "step": 12055
    },
    {
      "epoch": 2.9404878048780487,
      "grad_norm": 0.09091200679540634,
      "learning_rate": 4.854105568362632e-08,
      "loss": 0.0111,
      "step": 12056
    },
    {
      "epoch": 2.940731707317073,
      "grad_norm": 0.13570263981819153,
      "learning_rate": 4.814412099149179e-08,
      "loss": 0.0158,
      "step": 12057
    },
    {
      "epoch": 2.9409756097560975,
      "grad_norm": 0.2115521878004074,
      "learning_rate": 4.774881433156908e-08,
      "loss": 0.0134,
      "step": 12058
    },
    {
      "epoch": 2.941219512195122,
      "grad_norm": 0.19413655996322632,
      "learning_rate": 4.735513572965145e-08,
      "loss": 0.0357,
      "step": 12059
    },
    {
      "epoch": 2.9414634146341463,
      "grad_norm": 0.0986747071146965,
      "learning_rate": 4.6963085211421123e-08,
      "loss": 0.0077,
      "step": 12060
    },
    {
      "epoch": 2.9417073170731705,
      "grad_norm": 0.11557206511497498,
      "learning_rate": 4.657266280246319e-08,
      "loss": 0.0072,
      "step": 12061
    },
    {
      "epoch": 2.941951219512195,
      "grad_norm": 0.19616644084453583,
      "learning_rate": 4.6183868528248964e-08,
      "loss": 0.0247,
      "step": 12062
    },
    {
      "epoch": 2.9421951219512197,
      "grad_norm": 0.3409780263900757,
      "learning_rate": 4.5796702414144247e-08,
      "loss": 0.0131,
      "step": 12063
    },
    {
      "epoch": 2.942439024390244,
      "grad_norm": 0.12931634485721588,
      "learning_rate": 4.5411164485414936e-08,
      "loss": 0.0206,
      "step": 12064
    },
    {
      "epoch": 2.942682926829268,
      "grad_norm": 0.10230139642953873,
      "learning_rate": 4.5027254767213147e-08,
      "loss": 0.013,
      "step": 12065
    },
    {
      "epoch": 2.9429268292682926,
      "grad_norm": 0.1081642135977745,
      "learning_rate": 4.4644973284588275e-08,
      "loss": 0.0159,
      "step": 12066
    },
    {
      "epoch": 2.9431707317073172,
      "grad_norm": 0.14142541587352753,
      "learning_rate": 4.4264320062484265e-08,
      "loss": 0.0197,
      "step": 12067
    },
    {
      "epoch": 2.9434146341463414,
      "grad_norm": 0.0625821128487587,
      "learning_rate": 4.3885295125736804e-08,
      "loss": 0.01,
      "step": 12068
    },
    {
      "epoch": 2.9436585365853656,
      "grad_norm": 0.1689811497926712,
      "learning_rate": 4.350789849907333e-08,
      "loss": 0.0223,
      "step": 12069
    },
    {
      "epoch": 2.94390243902439,
      "grad_norm": 0.1477499157190323,
      "learning_rate": 4.313213020712137e-08,
      "loss": 0.0174,
      "step": 12070
    },
    {
      "epoch": 2.944146341463415,
      "grad_norm": 0.16895897686481476,
      "learning_rate": 4.275799027439742e-08,
      "loss": 0.0199,
      "step": 12071
    },
    {
      "epoch": 2.944390243902439,
      "grad_norm": 0.13021521270275116,
      "learning_rate": 4.2385478725315284e-08,
      "loss": 0.0208,
      "step": 12072
    },
    {
      "epoch": 2.944634146341463,
      "grad_norm": 0.1237812265753746,
      "learning_rate": 4.2014595584174976e-08,
      "loss": 0.017,
      "step": 12073
    },
    {
      "epoch": 2.9448780487804878,
      "grad_norm": 0.28566038608551025,
      "learning_rate": 4.164534087517935e-08,
      "loss": 0.0217,
      "step": 12074
    },
    {
      "epoch": 2.9451219512195124,
      "grad_norm": 0.16505227982997894,
      "learning_rate": 4.1277714622423026e-08,
      "loss": 0.022,
      "step": 12075
    },
    {
      "epoch": 2.9453658536585365,
      "grad_norm": 0.12514297664165497,
      "learning_rate": 4.091171684988682e-08,
      "loss": 0.0156,
      "step": 12076
    },
    {
      "epoch": 2.9456097560975607,
      "grad_norm": 0.14170850813388824,
      "learning_rate": 4.0547347581454396e-08,
      "loss": 0.0129,
      "step": 12077
    },
    {
      "epoch": 2.9458536585365853,
      "grad_norm": 0.06319819390773773,
      "learning_rate": 4.0184606840901195e-08,
      "loss": 0.0071,
      "step": 12078
    },
    {
      "epoch": 2.94609756097561,
      "grad_norm": 0.07751186937093735,
      "learning_rate": 3.9823494651891615e-08,
      "loss": 0.0099,
      "step": 12079
    },
    {
      "epoch": 2.946341463414634,
      "grad_norm": 0.12560828030109406,
      "learning_rate": 3.946401103799013e-08,
      "loss": 0.015,
      "step": 12080
    },
    {
      "epoch": 2.9465853658536583,
      "grad_norm": 0.11550100147724152,
      "learning_rate": 3.910615602264745e-08,
      "loss": 0.0106,
      "step": 12081
    },
    {
      "epoch": 2.946829268292683,
      "grad_norm": 0.18938322365283966,
      "learning_rate": 3.874992962921709e-08,
      "loss": 0.0227,
      "step": 12082
    },
    {
      "epoch": 2.9470731707317075,
      "grad_norm": 0.14767424762248993,
      "learning_rate": 3.839533188094158e-08,
      "loss": 0.0161,
      "step": 12083
    },
    {
      "epoch": 2.9473170731707317,
      "grad_norm": 0.09247751533985138,
      "learning_rate": 3.8042362800952416e-08,
      "loss": 0.0105,
      "step": 12084
    },
    {
      "epoch": 2.947560975609756,
      "grad_norm": 0.12772728502750397,
      "learning_rate": 3.7691022412286726e-08,
      "loss": 0.0148,
      "step": 12085
    },
    {
      "epoch": 2.9478048780487804,
      "grad_norm": 0.11268308758735657,
      "learning_rate": 3.734131073786229e-08,
      "loss": 0.0133,
      "step": 12086
    },
    {
      "epoch": 2.948048780487805,
      "grad_norm": 0.24390341341495514,
      "learning_rate": 3.699322780050252e-08,
      "loss": 0.02,
      "step": 12087
    },
    {
      "epoch": 2.9482926829268292,
      "grad_norm": 0.09976265579462051,
      "learning_rate": 3.6646773622911465e-08,
      "loss": 0.0226,
      "step": 12088
    },
    {
      "epoch": 2.9485365853658534,
      "grad_norm": 0.10328637808561325,
      "learning_rate": 3.630194822770161e-08,
      "loss": 0.0135,
      "step": 12089
    },
    {
      "epoch": 2.948780487804878,
      "grad_norm": 0.0819450244307518,
      "learning_rate": 3.595875163736606e-08,
      "loss": 0.0132,
      "step": 12090
    },
    {
      "epoch": 2.9490243902439026,
      "grad_norm": 0.0941239669919014,
      "learning_rate": 3.56171838743008e-08,
      "loss": 0.0167,
      "step": 12091
    },
    {
      "epoch": 2.949268292682927,
      "grad_norm": 0.069972924888134,
      "learning_rate": 3.5277244960790766e-08,
      "loss": 0.0087,
      "step": 12092
    },
    {
      "epoch": 2.949512195121951,
      "grad_norm": 0.17559665441513062,
      "learning_rate": 3.493893491901545e-08,
      "loss": 0.013,
      "step": 12093
    },
    {
      "epoch": 2.9497560975609756,
      "grad_norm": 0.12289922684431076,
      "learning_rate": 3.460225377105164e-08,
      "loss": 0.0222,
      "step": 12094
    },
    {
      "epoch": 2.95,
      "grad_norm": 0.08980078995227814,
      "learning_rate": 3.4267201538862316e-08,
      "loss": 0.0144,
      "step": 12095
    },
    {
      "epoch": 2.9502439024390243,
      "grad_norm": 0.11564626544713974,
      "learning_rate": 3.393377824431054e-08,
      "loss": 0.0153,
      "step": 12096
    },
    {
      "epoch": 2.9504878048780485,
      "grad_norm": 0.08622660487890244,
      "learning_rate": 3.3601983909151144e-08,
      "loss": 0.007,
      "step": 12097
    },
    {
      "epoch": 2.950731707317073,
      "grad_norm": 0.18668264150619507,
      "learning_rate": 3.3271818555033476e-08,
      "loss": 0.0222,
      "step": 12098
    },
    {
      "epoch": 2.9509756097560977,
      "grad_norm": 0.12784872949123383,
      "learning_rate": 3.29432822035014e-08,
      "loss": 0.0138,
      "step": 12099
    },
    {
      "epoch": 2.951219512195122,
      "grad_norm": 0.0987984836101532,
      "learning_rate": 3.261637487598501e-08,
      "loss": 0.0183,
      "step": 12100
    },
    {
      "epoch": 2.951463414634146,
      "grad_norm": 0.056268662214279175,
      "learning_rate": 3.2291096593820017e-08,
      "loss": 0.0057,
      "step": 12101
    },
    {
      "epoch": 2.9517073170731707,
      "grad_norm": 0.11380524188280106,
      "learning_rate": 3.196744737822555e-08,
      "loss": 0.0141,
      "step": 12102
    },
    {
      "epoch": 2.9519512195121953,
      "grad_norm": 0.08680699020624161,
      "learning_rate": 3.1645427250320825e-08,
      "loss": 0.0125,
      "step": 12103
    },
    {
      "epoch": 2.9521951219512195,
      "grad_norm": 0.07993973046541214,
      "learning_rate": 3.132503623111682e-08,
      "loss": 0.0082,
      "step": 12104
    },
    {
      "epoch": 2.9524390243902436,
      "grad_norm": 0.1542886346578598,
      "learning_rate": 3.100627434151904e-08,
      "loss": 0.0232,
      "step": 12105
    },
    {
      "epoch": 2.9526829268292683,
      "grad_norm": 0.19150717556476593,
      "learning_rate": 3.0689141602324725e-08,
      "loss": 0.0172,
      "step": 12106
    },
    {
      "epoch": 2.952926829268293,
      "grad_norm": 0.19570770859718323,
      "learning_rate": 3.0373638034222886e-08,
      "loss": 0.0098,
      "step": 12107
    },
    {
      "epoch": 2.953170731707317,
      "grad_norm": 0.08054430782794952,
      "learning_rate": 3.005976365780261e-08,
      "loss": 0.0142,
      "step": 12108
    },
    {
      "epoch": 2.953414634146341,
      "grad_norm": 0.18191583454608917,
      "learning_rate": 2.9747518493544735e-08,
      "loss": 0.0229,
      "step": 12109
    },
    {
      "epoch": 2.953658536585366,
      "grad_norm": 0.12688368558883667,
      "learning_rate": 2.943690256181908e-08,
      "loss": 0.0155,
      "step": 12110
    },
    {
      "epoch": 2.9539024390243904,
      "grad_norm": 0.11558312922716141,
      "learning_rate": 2.9127915882892766e-08,
      "loss": 0.0221,
      "step": 12111
    },
    {
      "epoch": 2.9541463414634146,
      "grad_norm": 0.1659969985485077,
      "learning_rate": 2.8820558476927438e-08,
      "loss": 0.0282,
      "step": 12112
    },
    {
      "epoch": 2.9543902439024388,
      "grad_norm": 0.1066875159740448,
      "learning_rate": 2.8514830363973733e-08,
      "loss": 0.0179,
      "step": 12113
    },
    {
      "epoch": 2.9546341463414634,
      "grad_norm": 0.14861007034778595,
      "learning_rate": 2.8210731563985126e-08,
      "loss": 0.0146,
      "step": 12114
    },
    {
      "epoch": 2.954878048780488,
      "grad_norm": 0.10954692959785461,
      "learning_rate": 2.7908262096798533e-08,
      "loss": 0.012,
      "step": 12115
    },
    {
      "epoch": 2.955121951219512,
      "grad_norm": 0.08895940333604813,
      "learning_rate": 2.7607421982153713e-08,
      "loss": 0.016,
      "step": 12116
    },
    {
      "epoch": 2.9553658536585363,
      "grad_norm": 0.17438922822475433,
      "learning_rate": 2.7308211239673865e-08,
      "loss": 0.0174,
      "step": 12117
    },
    {
      "epoch": 2.955609756097561,
      "grad_norm": 0.21591730415821075,
      "learning_rate": 2.701062988888503e-08,
      "loss": 0.0164,
      "step": 12118
    },
    {
      "epoch": 2.9558536585365855,
      "grad_norm": 0.22827714681625366,
      "learning_rate": 2.6714677949205014e-08,
      "loss": 0.0295,
      "step": 12119
    },
    {
      "epoch": 2.9560975609756097,
      "grad_norm": 0.15963275730609894,
      "learning_rate": 2.6420355439940593e-08,
      "loss": 0.012,
      "step": 12120
    },
    {
      "epoch": 2.956341463414634,
      "grad_norm": 0.07458095252513885,
      "learning_rate": 2.612766238029585e-08,
      "loss": 0.0086,
      "step": 12121
    },
    {
      "epoch": 2.9565853658536585,
      "grad_norm": 0.14899568259716034,
      "learning_rate": 2.5836598789369392e-08,
      "loss": 0.011,
      "step": 12122
    },
    {
      "epoch": 2.956829268292683,
      "grad_norm": 0.2503502070903778,
      "learning_rate": 2.5547164686151592e-08,
      "loss": 0.0298,
      "step": 12123
    },
    {
      "epoch": 2.9570731707317073,
      "grad_norm": 0.17094169557094574,
      "learning_rate": 2.5259360089524564e-08,
      "loss": 0.0144,
      "step": 12124
    },
    {
      "epoch": 2.9573170731707314,
      "grad_norm": 0.1429089605808258,
      "learning_rate": 2.497318501827328e-08,
      "loss": 0.0215,
      "step": 12125
    },
    {
      "epoch": 2.957560975609756,
      "grad_norm": 0.18565182387828827,
      "learning_rate": 2.468863949106337e-08,
      "loss": 0.0235,
      "step": 12126
    },
    {
      "epoch": 2.9578048780487807,
      "grad_norm": 0.1885863095521927,
      "learning_rate": 2.4405723526463308e-08,
      "loss": 0.0261,
      "step": 12127
    },
    {
      "epoch": 2.958048780487805,
      "grad_norm": 0.19808253645896912,
      "learning_rate": 2.412443714293333e-08,
      "loss": 0.015,
      "step": 12128
    },
    {
      "epoch": 2.958292682926829,
      "grad_norm": 0.12029127776622772,
      "learning_rate": 2.384478035882265e-08,
      "loss": 0.0168,
      "step": 12129
    },
    {
      "epoch": 2.9585365853658536,
      "grad_norm": 0.07732336968183517,
      "learning_rate": 2.3566753192383327e-08,
      "loss": 0.0134,
      "step": 12130
    },
    {
      "epoch": 2.9587804878048782,
      "grad_norm": 0.11354129761457443,
      "learning_rate": 2.3290355661753638e-08,
      "loss": 0.0265,
      "step": 12131
    },
    {
      "epoch": 2.9590243902439024,
      "grad_norm": 0.2085598111152649,
      "learning_rate": 2.3015587784966376e-08,
      "loss": 0.0178,
      "step": 12132
    },
    {
      "epoch": 2.9592682926829266,
      "grad_norm": 0.11834979802370071,
      "learning_rate": 2.2742449579948864e-08,
      "loss": 0.0131,
      "step": 12133
    },
    {
      "epoch": 2.959512195121951,
      "grad_norm": 0.10400927811861038,
      "learning_rate": 2.2470941064522967e-08,
      "loss": 0.0135,
      "step": 12134
    },
    {
      "epoch": 2.959756097560976,
      "grad_norm": 0.09589029848575592,
      "learning_rate": 2.2201062256407833e-08,
      "loss": 0.0095,
      "step": 12135
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.1793544590473175,
      "learning_rate": 2.1932813173208833e-08,
      "loss": 0.0158,
      "step": 12136
    },
    {
      "epoch": 2.960243902439024,
      "grad_norm": 0.15436199307441711,
      "learning_rate": 2.1666193832425853e-08,
      "loss": 0.0278,
      "step": 12137
    },
    {
      "epoch": 2.9604878048780487,
      "grad_norm": 0.0956411138176918,
      "learning_rate": 2.1401204251458863e-08,
      "loss": 0.0177,
      "step": 12138
    },
    {
      "epoch": 2.9607317073170734,
      "grad_norm": 0.1368732452392578,
      "learning_rate": 2.113784444759681e-08,
      "loss": 0.0216,
      "step": 12139
    },
    {
      "epoch": 2.9609756097560975,
      "grad_norm": 0.11009759455919266,
      "learning_rate": 2.0876114438023175e-08,
      "loss": 0.0209,
      "step": 12140
    },
    {
      "epoch": 2.9612195121951217,
      "grad_norm": 0.15090706944465637,
      "learning_rate": 2.0616014239813188e-08,
      "loss": 0.0383,
      "step": 12141
    },
    {
      "epoch": 2.9614634146341463,
      "grad_norm": 0.08886661380529404,
      "learning_rate": 2.0357543869939378e-08,
      "loss": 0.0103,
      "step": 12142
    },
    {
      "epoch": 2.961707317073171,
      "grad_norm": 0.140238955616951,
      "learning_rate": 2.0100703345266035e-08,
      "loss": 0.0215,
      "step": 12143
    },
    {
      "epoch": 2.961951219512195,
      "grad_norm": 0.1105962023139,
      "learning_rate": 1.984549268255198e-08,
      "loss": 0.0082,
      "step": 12144
    },
    {
      "epoch": 2.9621951219512193,
      "grad_norm": 0.0843103900551796,
      "learning_rate": 1.959191189844778e-08,
      "loss": 0.0163,
      "step": 12145
    },
    {
      "epoch": 2.962439024390244,
      "grad_norm": 0.07056225091218948,
      "learning_rate": 1.9339961009498532e-08,
      "loss": 0.0145,
      "step": 12146
    },
    {
      "epoch": 2.9626829268292685,
      "grad_norm": 0.12723100185394287,
      "learning_rate": 1.9089640032143863e-08,
      "loss": 0.0126,
      "step": 12147
    },
    {
      "epoch": 2.9629268292682926,
      "grad_norm": 0.07955808192491531,
      "learning_rate": 1.8840948982715158e-08,
      "loss": 0.0083,
      "step": 12148
    },
    {
      "epoch": 2.963170731707317,
      "grad_norm": 0.1788756549358368,
      "learning_rate": 1.8593887877441097e-08,
      "loss": 0.0266,
      "step": 12149
    },
    {
      "epoch": 2.9634146341463414,
      "grad_norm": 0.13851484656333923,
      "learning_rate": 1.8348456732439345e-08,
      "loss": 0.0251,
      "step": 12150
    },
    {
      "epoch": 2.963658536585366,
      "grad_norm": 0.1690119504928589,
      "learning_rate": 1.8104655563727645e-08,
      "loss": 0.016,
      "step": 12151
    },
    {
      "epoch": 2.96390243902439,
      "grad_norm": 0.10017134249210358,
      "learning_rate": 1.786248438720717e-08,
      "loss": 0.0094,
      "step": 12152
    },
    {
      "epoch": 2.9641463414634144,
      "grad_norm": 0.10255946218967438,
      "learning_rate": 1.7621943218681936e-08,
      "loss": 0.0158,
      "step": 12153
    },
    {
      "epoch": 2.964390243902439,
      "grad_norm": 0.1271989941596985,
      "learning_rate": 1.7383032073847728e-08,
      "loss": 0.0252,
      "step": 12154
    },
    {
      "epoch": 2.9646341463414636,
      "grad_norm": 0.1471603810787201,
      "learning_rate": 1.7145750968289297e-08,
      "loss": 0.0078,
      "step": 12155
    },
    {
      "epoch": 2.9648780487804878,
      "grad_norm": 0.07724452018737793,
      "learning_rate": 1.691009991749426e-08,
      "loss": 0.0079,
      "step": 12156
    },
    {
      "epoch": 2.965121951219512,
      "grad_norm": 0.10163620859384537,
      "learning_rate": 1.667607893683365e-08,
      "loss": 0.0125,
      "step": 12157
    },
    {
      "epoch": 2.9653658536585366,
      "grad_norm": 0.22418133914470673,
      "learning_rate": 1.644368804157581e-08,
      "loss": 0.0157,
      "step": 12158
    },
    {
      "epoch": 2.965609756097561,
      "grad_norm": 0.11130522191524506,
      "learning_rate": 1.6212927246886388e-08,
      "loss": 0.0209,
      "step": 12159
    },
    {
      "epoch": 2.9658536585365853,
      "grad_norm": 0.11552812904119492,
      "learning_rate": 1.5983796567820008e-08,
      "loss": 0.0244,
      "step": 12160
    },
    {
      "epoch": 2.9660975609756095,
      "grad_norm": 0.11973554641008377,
      "learning_rate": 1.5756296019328598e-08,
      "loss": 0.009,
      "step": 12161
    },
    {
      "epoch": 2.966341463414634,
      "grad_norm": 0.10193721950054169,
      "learning_rate": 1.553042561625584e-08,
      "loss": 0.0151,
      "step": 12162
    },
    {
      "epoch": 2.9665853658536587,
      "grad_norm": 0.11821768432855606,
      "learning_rate": 1.5306185373337167e-08,
      "loss": 0.0179,
      "step": 12163
    },
    {
      "epoch": 2.966829268292683,
      "grad_norm": 0.13896258175373077,
      "learning_rate": 1.5083575305202546e-08,
      "loss": 0.0205,
      "step": 12164
    },
    {
      "epoch": 2.967073170731707,
      "grad_norm": 0.08907781541347504,
      "learning_rate": 1.4862595426379245e-08,
      "loss": 0.0102,
      "step": 12165
    },
    {
      "epoch": 2.9673170731707317,
      "grad_norm": 0.10251495987176895,
      "learning_rate": 1.4643245751286283e-08,
      "loss": 0.0161,
      "step": 12166
    },
    {
      "epoch": 2.9675609756097563,
      "grad_norm": 0.20348626375198364,
      "learning_rate": 1.4425526294231662e-08,
      "loss": 0.0198,
      "step": 12167
    },
    {
      "epoch": 2.9678048780487805,
      "grad_norm": 0.19920335710048676,
      "learning_rate": 1.4209437069426235e-08,
      "loss": 0.0174,
      "step": 12168
    },
    {
      "epoch": 2.9680487804878046,
      "grad_norm": 0.1075395792722702,
      "learning_rate": 1.3994978090964283e-08,
      "loss": 0.0252,
      "step": 12169
    },
    {
      "epoch": 2.9682926829268292,
      "grad_norm": 0.10998053103685379,
      "learning_rate": 1.378214937284017e-08,
      "loss": 0.0135,
      "step": 12170
    },
    {
      "epoch": 2.968536585365854,
      "grad_norm": 0.10932116210460663,
      "learning_rate": 1.3570950928940008e-08,
      "loss": 0.0197,
      "step": 12171
    },
    {
      "epoch": 2.968780487804878,
      "grad_norm": 0.11253757029771805,
      "learning_rate": 1.3361382773047216e-08,
      "loss": 0.0178,
      "step": 12172
    },
    {
      "epoch": 2.969024390243902,
      "grad_norm": 0.10261745750904083,
      "learning_rate": 1.3153444918831415e-08,
      "loss": 0.0092,
      "step": 12173
    },
    {
      "epoch": 2.969268292682927,
      "grad_norm": 0.20758096873760223,
      "learning_rate": 1.2947137379859531e-08,
      "loss": 0.0234,
      "step": 12174
    },
    {
      "epoch": 2.9695121951219514,
      "grad_norm": 0.20626826584339142,
      "learning_rate": 1.2742460169595794e-08,
      "loss": 0.0158,
      "step": 12175
    },
    {
      "epoch": 2.9697560975609756,
      "grad_norm": 0.15527567267417908,
      "learning_rate": 1.253941330139341e-08,
      "loss": 0.02,
      "step": 12176
    },
    {
      "epoch": 2.9699999999999998,
      "grad_norm": 0.1522776484489441,
      "learning_rate": 1.2337996788500117e-08,
      "loss": 0.0209,
      "step": 12177
    },
    {
      "epoch": 2.9702439024390244,
      "grad_norm": 0.10422519594430923,
      "learning_rate": 1.2138210644058179e-08,
      "loss": 0.015,
      "step": 12178
    },
    {
      "epoch": 2.970487804878049,
      "grad_norm": 0.0796724334359169,
      "learning_rate": 1.194005488110439e-08,
      "loss": 0.0109,
      "step": 12179
    },
    {
      "epoch": 2.970731707317073,
      "grad_norm": 0.0834072157740593,
      "learning_rate": 1.1743529512564522e-08,
      "loss": 0.0087,
      "step": 12180
    },
    {
      "epoch": 2.9709756097560973,
      "grad_norm": 0.11572841554880142,
      "learning_rate": 1.154863455126165e-08,
      "loss": 0.0132,
      "step": 12181
    },
    {
      "epoch": 2.971219512195122,
      "grad_norm": 0.0934818759560585,
      "learning_rate": 1.1355370009916155e-08,
      "loss": 0.0179,
      "step": 12182
    },
    {
      "epoch": 2.9714634146341465,
      "grad_norm": 0.1354818344116211,
      "learning_rate": 1.1163735901131845e-08,
      "loss": 0.0144,
      "step": 12183
    },
    {
      "epoch": 2.9717073170731707,
      "grad_norm": 0.13612350821495056,
      "learning_rate": 1.0973732237418155e-08,
      "loss": 0.0179,
      "step": 12184
    },
    {
      "epoch": 2.971951219512195,
      "grad_norm": 0.18124476075172424,
      "learning_rate": 1.0785359031167953e-08,
      "loss": 0.0293,
      "step": 12185
    },
    {
      "epoch": 2.9721951219512195,
      "grad_norm": 0.11968228965997696,
      "learning_rate": 1.0598616294676955e-08,
      "loss": 0.0174,
      "step": 12186
    },
    {
      "epoch": 2.972439024390244,
      "grad_norm": 0.06141022965312004,
      "learning_rate": 1.0413504040124312e-08,
      "loss": 0.0107,
      "step": 12187
    },
    {
      "epoch": 2.9726829268292683,
      "grad_norm": 0.10180578380823135,
      "learning_rate": 1.0230022279589246e-08,
      "loss": 0.0111,
      "step": 12188
    },
    {
      "epoch": 2.9729268292682924,
      "grad_norm": 0.20050151646137238,
      "learning_rate": 1.0048171025045517e-08,
      "loss": 0.0185,
      "step": 12189
    },
    {
      "epoch": 2.973170731707317,
      "grad_norm": 0.08324643224477768,
      "learning_rate": 9.867950288355853e-09,
      "loss": 0.0103,
      "step": 12190
    },
    {
      "epoch": 2.9734146341463417,
      "grad_norm": 0.15930487215518951,
      "learning_rate": 9.689360081280296e-09,
      "loss": 0.0133,
      "step": 12191
    },
    {
      "epoch": 2.973658536585366,
      "grad_norm": 0.08783511072397232,
      "learning_rate": 9.512400415470635e-09,
      "loss": 0.0169,
      "step": 12192
    },
    {
      "epoch": 2.97390243902439,
      "grad_norm": 0.1361243575811386,
      "learning_rate": 9.337071302475964e-09,
      "loss": 0.0166,
      "step": 12193
    },
    {
      "epoch": 2.9741463414634146,
      "grad_norm": 0.15069569647312164,
      "learning_rate": 9.16337275373158e-09,
      "loss": 0.0178,
      "step": 12194
    },
    {
      "epoch": 2.9743902439024392,
      "grad_norm": 0.07746589928865433,
      "learning_rate": 8.991304780575637e-09,
      "loss": 0.0072,
      "step": 12195
    },
    {
      "epoch": 2.9746341463414634,
      "grad_norm": 0.09429249167442322,
      "learning_rate": 8.820867394229715e-09,
      "loss": 0.0208,
      "step": 12196
    },
    {
      "epoch": 2.9748780487804876,
      "grad_norm": 0.06921833753585815,
      "learning_rate": 8.652060605818246e-09,
      "loss": 0.0095,
      "step": 12197
    },
    {
      "epoch": 2.975121951219512,
      "grad_norm": 0.08412987738847733,
      "learning_rate": 8.484884426351869e-09,
      "loss": 0.0215,
      "step": 12198
    },
    {
      "epoch": 2.975365853658537,
      "grad_norm": 0.15988586843013763,
      "learning_rate": 8.3193388667413e-09,
      "loss": 0.0211,
      "step": 12199
    },
    {
      "epoch": 2.975609756097561,
      "grad_norm": 0.09337078034877777,
      "learning_rate": 8.155423937789009e-09,
      "loss": 0.0145,
      "step": 12200
    },
    {
      "epoch": 2.975853658536585,
      "grad_norm": 0.12974078953266144,
      "learning_rate": 7.993139650186443e-09,
      "loss": 0.0125,
      "step": 12201
    },
    {
      "epoch": 2.9760975609756097,
      "grad_norm": 0.12691161036491394,
      "learning_rate": 7.832486014522355e-09,
      "loss": 0.0261,
      "step": 12202
    },
    {
      "epoch": 2.9763414634146343,
      "grad_norm": 0.10425850749015808,
      "learning_rate": 7.673463041282802e-09,
      "loss": 0.0145,
      "step": 12203
    },
    {
      "epoch": 2.9765853658536585,
      "grad_norm": 0.19587863981723785,
      "learning_rate": 7.516070740840043e-09,
      "loss": 0.0218,
      "step": 12204
    },
    {
      "epoch": 2.9768292682926827,
      "grad_norm": 0.11930235475301743,
      "learning_rate": 7.360309123463638e-09,
      "loss": 0.0154,
      "step": 12205
    },
    {
      "epoch": 2.9770731707317073,
      "grad_norm": 0.1315324753522873,
      "learning_rate": 7.206178199317682e-09,
      "loss": 0.0177,
      "step": 12206
    },
    {
      "epoch": 2.977317073170732,
      "grad_norm": 0.12931057810783386,
      "learning_rate": 7.053677978458018e-09,
      "loss": 0.0196,
      "step": 12207
    },
    {
      "epoch": 2.977560975609756,
      "grad_norm": 0.19916823506355286,
      "learning_rate": 6.90280847083502e-09,
      "loss": 0.0198,
      "step": 12208
    },
    {
      "epoch": 2.9778048780487802,
      "grad_norm": 0.10954253375530243,
      "learning_rate": 6.753569686290817e-09,
      "loss": 0.0174,
      "step": 12209
    },
    {
      "epoch": 2.978048780487805,
      "grad_norm": 0.09574128687381744,
      "learning_rate": 6.605961634567615e-09,
      "loss": 0.0092,
      "step": 12210
    },
    {
      "epoch": 2.9782926829268295,
      "grad_norm": 0.12341822683811188,
      "learning_rate": 6.459984325291046e-09,
      "loss": 0.0182,
      "step": 12211
    },
    {
      "epoch": 2.9785365853658536,
      "grad_norm": 0.10047461837530136,
      "learning_rate": 6.315637767989601e-09,
      "loss": 0.0153,
      "step": 12212
    },
    {
      "epoch": 2.978780487804878,
      "grad_norm": 0.16830860078334808,
      "learning_rate": 6.172921972077972e-09,
      "loss": 0.0135,
      "step": 12213
    },
    {
      "epoch": 2.9790243902439024,
      "grad_norm": 0.13804790377616882,
      "learning_rate": 6.031836946873703e-09,
      "loss": 0.0168,
      "step": 12214
    },
    {
      "epoch": 2.979268292682927,
      "grad_norm": 0.08868633955717087,
      "learning_rate": 5.892382701574994e-09,
      "loss": 0.0123,
      "step": 12215
    },
    {
      "epoch": 2.979512195121951,
      "grad_norm": 0.1201871708035469,
      "learning_rate": 5.754559245282898e-09,
      "loss": 0.0149,
      "step": 12216
    },
    {
      "epoch": 2.9797560975609754,
      "grad_norm": 0.06306108087301254,
      "learning_rate": 5.618366586992996e-09,
      "loss": 0.0102,
      "step": 12217
    },
    {
      "epoch": 2.98,
      "grad_norm": 0.1477898508310318,
      "learning_rate": 5.483804735589848e-09,
      "loss": 0.0121,
      "step": 12218
    },
    {
      "epoch": 2.9802439024390246,
      "grad_norm": 0.09544731676578522,
      "learning_rate": 5.350873699852543e-09,
      "loss": 0.0119,
      "step": 12219
    },
    {
      "epoch": 2.9804878048780488,
      "grad_norm": 0.127427339553833,
      "learning_rate": 5.219573488454699e-09,
      "loss": 0.013,
      "step": 12220
    },
    {
      "epoch": 2.980731707317073,
      "grad_norm": 0.15994061529636383,
      "learning_rate": 5.089904109961685e-09,
      "loss": 0.0091,
      "step": 12221
    },
    {
      "epoch": 2.9809756097560975,
      "grad_norm": 0.08786624670028687,
      "learning_rate": 4.961865572836177e-09,
      "loss": 0.0089,
      "step": 12222
    },
    {
      "epoch": 2.981219512195122,
      "grad_norm": 0.10536102950572968,
      "learning_rate": 4.835457885432604e-09,
      "loss": 0.0238,
      "step": 12223
    },
    {
      "epoch": 2.9814634146341463,
      "grad_norm": 0.1667744368314743,
      "learning_rate": 4.7106810559999215e-09,
      "loss": 0.0216,
      "step": 12224
    },
    {
      "epoch": 2.9817073170731705,
      "grad_norm": 0.12215020507574081,
      "learning_rate": 4.587535092673289e-09,
      "loss": 0.0152,
      "step": 12225
    },
    {
      "epoch": 2.981951219512195,
      "grad_norm": 0.12125933915376663,
      "learning_rate": 4.466020003496274e-09,
      "loss": 0.0104,
      "step": 12226
    },
    {
      "epoch": 2.9821951219512197,
      "grad_norm": 0.24500654637813568,
      "learning_rate": 4.346135796390316e-09,
      "loss": 0.0182,
      "step": 12227
    },
    {
      "epoch": 2.982439024390244,
      "grad_norm": 0.29178935289382935,
      "learning_rate": 4.227882479179712e-09,
      "loss": 0.0184,
      "step": 12228
    },
    {
      "epoch": 2.982682926829268,
      "grad_norm": 0.11575894057750702,
      "learning_rate": 4.111260059580513e-09,
      "loss": 0.0182,
      "step": 12229
    },
    {
      "epoch": 2.9829268292682927,
      "grad_norm": 0.2874469459056854,
      "learning_rate": 3.996268545200521e-09,
      "loss": 0.0254,
      "step": 12230
    },
    {
      "epoch": 2.9831707317073173,
      "grad_norm": 0.13435982167720795,
      "learning_rate": 3.882907943544844e-09,
      "loss": 0.008,
      "step": 12231
    },
    {
      "epoch": 2.9834146341463414,
      "grad_norm": 0.11479473114013672,
      "learning_rate": 3.771178262010344e-09,
      "loss": 0.0187,
      "step": 12232
    },
    {
      "epoch": 2.9836585365853656,
      "grad_norm": 0.1017850860953331,
      "learning_rate": 3.661079507885634e-09,
      "loss": 0.0181,
      "step": 12233
    },
    {
      "epoch": 2.9839024390243902,
      "grad_norm": 0.2502436339855194,
      "learning_rate": 3.5526116883538573e-09,
      "loss": 0.0231,
      "step": 12234
    },
    {
      "epoch": 2.984146341463415,
      "grad_norm": 0.1542510986328125,
      "learning_rate": 3.44577481048991e-09,
      "loss": 0.022,
      "step": 12235
    },
    {
      "epoch": 2.984390243902439,
      "grad_norm": 0.15302938222885132,
      "learning_rate": 3.3405688812715443e-09,
      "loss": 0.0214,
      "step": 12236
    },
    {
      "epoch": 2.984634146341463,
      "grad_norm": 0.13188175857067108,
      "learning_rate": 3.236993907557162e-09,
      "loss": 0.0176,
      "step": 12237
    },
    {
      "epoch": 2.984878048780488,
      "grad_norm": 0.15787272155284882,
      "learning_rate": 3.1350498961052468e-09,
      "loss": 0.0151,
      "step": 12238
    },
    {
      "epoch": 2.9851219512195124,
      "grad_norm": 0.13557808101177216,
      "learning_rate": 3.034736853568809e-09,
      "loss": 0.0192,
      "step": 12239
    },
    {
      "epoch": 2.9853658536585366,
      "grad_norm": 0.14229601621627808,
      "learning_rate": 2.93605478649539e-09,
      "loss": 0.018,
      "step": 12240
    },
    {
      "epoch": 2.9856097560975607,
      "grad_norm": 0.1577652096748352,
      "learning_rate": 2.839003701318732e-09,
      "loss": 0.0281,
      "step": 12241
    },
    {
      "epoch": 2.9858536585365854,
      "grad_norm": 0.1073174849152565,
      "learning_rate": 2.7435836043754327e-09,
      "loss": 0.0187,
      "step": 12242
    },
    {
      "epoch": 2.98609756097561,
      "grad_norm": 0.17827723920345306,
      "learning_rate": 2.649794501888292e-09,
      "loss": 0.0249,
      "step": 12243
    },
    {
      "epoch": 2.986341463414634,
      "grad_norm": 0.13126830756664276,
      "learning_rate": 2.5576363999774146e-09,
      "loss": 0.0153,
      "step": 12244
    },
    {
      "epoch": 2.9865853658536583,
      "grad_norm": 0.11335168033838272,
      "learning_rate": 2.4671093046546578e-09,
      "loss": 0.024,
      "step": 12245
    },
    {
      "epoch": 2.986829268292683,
      "grad_norm": 0.16614656150341034,
      "learning_rate": 2.37821322183196e-09,
      "loss": 0.0325,
      "step": 12246
    },
    {
      "epoch": 2.9870731707317075,
      "grad_norm": 0.10593347996473312,
      "learning_rate": 2.290948157301909e-09,
      "loss": 0.0196,
      "step": 12247
    },
    {
      "epoch": 2.9873170731707317,
      "grad_norm": 0.08215508610010147,
      "learning_rate": 2.2053141167655e-09,
      "loss": 0.0142,
      "step": 12248
    },
    {
      "epoch": 2.987560975609756,
      "grad_norm": 0.16147367656230927,
      "learning_rate": 2.12131110580438e-09,
      "loss": 0.0139,
      "step": 12249
    },
    {
      "epoch": 2.9878048780487805,
      "grad_norm": 0.1420791745185852,
      "learning_rate": 2.0389391299030503e-09,
      "loss": 0.0218,
      "step": 12250
    },
    {
      "epoch": 2.988048780487805,
      "grad_norm": 0.15881110727787018,
      "learning_rate": 1.95819819443499e-09,
      "loss": 0.019,
      "step": 12251
    },
    {
      "epoch": 2.9882926829268293,
      "grad_norm": 0.1176038607954979,
      "learning_rate": 1.879088304665433e-09,
      "loss": 0.016,
      "step": 12252
    },
    {
      "epoch": 2.9885365853658534,
      "grad_norm": 0.12047935277223587,
      "learning_rate": 1.801609465762466e-09,
      "loss": 0.0147,
      "step": 12253
    },
    {
      "epoch": 2.988780487804878,
      "grad_norm": 0.17931897938251495,
      "learning_rate": 1.7257616827776045e-09,
      "loss": 0.0215,
      "step": 12254
    },
    {
      "epoch": 2.9890243902439027,
      "grad_norm": 0.11169692128896713,
      "learning_rate": 1.6515449606568923e-09,
      "loss": 0.008,
      "step": 12255
    },
    {
      "epoch": 2.989268292682927,
      "grad_norm": 0.2909274697303772,
      "learning_rate": 1.5789593042464523e-09,
      "loss": 0.0199,
      "step": 12256
    },
    {
      "epoch": 2.989512195121951,
      "grad_norm": 0.08640946447849274,
      "learning_rate": 1.5080047182813862e-09,
      "loss": 0.0182,
      "step": 12257
    },
    {
      "epoch": 2.9897560975609756,
      "grad_norm": 0.10909619182348251,
      "learning_rate": 1.4386812073940993e-09,
      "loss": 0.0072,
      "step": 12258
    },
    {
      "epoch": 2.99,
      "grad_norm": 0.07227876782417297,
      "learning_rate": 1.3709887761031993e-09,
      "loss": 0.0055,
      "step": 12259
    },
    {
      "epoch": 2.9902439024390244,
      "grad_norm": 0.12208624184131622,
      "learning_rate": 1.3049274288245984e-09,
      "loss": 0.0185,
      "step": 12260
    },
    {
      "epoch": 2.9904878048780485,
      "grad_norm": 0.12287736684083939,
      "learning_rate": 1.2404971698742884e-09,
      "loss": 0.0119,
      "step": 12261
    },
    {
      "epoch": 2.990731707317073,
      "grad_norm": 0.11210624128580093,
      "learning_rate": 1.1776980034516882e-09,
      "loss": 0.026,
      "step": 12262
    },
    {
      "epoch": 2.9909756097560978,
      "grad_norm": 0.1107126772403717,
      "learning_rate": 1.1165299336562963e-09,
      "loss": 0.0125,
      "step": 12263
    },
    {
      "epoch": 2.991219512195122,
      "grad_norm": 0.17958995699882507,
      "learning_rate": 1.0569929644765886e-09,
      "loss": 0.0098,
      "step": 12264
    },
    {
      "epoch": 2.991463414634146,
      "grad_norm": 0.09769025444984436,
      "learning_rate": 9.990870998011214e-10,
      "loss": 0.0155,
      "step": 12265
    },
    {
      "epoch": 2.9917073170731707,
      "grad_norm": 0.13913622498512268,
      "learning_rate": 9.428123434046531e-10,
      "loss": 0.0214,
      "step": 12266
    },
    {
      "epoch": 2.9919512195121953,
      "grad_norm": 0.18669970333576202,
      "learning_rate": 8.88168698962022e-10,
      "loss": 0.0153,
      "step": 12267
    },
    {
      "epoch": 2.9921951219512195,
      "grad_norm": 0.06292992830276489,
      "learning_rate": 8.351561700342681e-10,
      "loss": 0.0098,
      "step": 12268
    },
    {
      "epoch": 2.9924390243902437,
      "grad_norm": 0.18342608213424683,
      "learning_rate": 7.837747600852874e-10,
      "loss": 0.0163,
      "step": 12269
    },
    {
      "epoch": 2.9926829268292683,
      "grad_norm": 0.14316019415855408,
      "learning_rate": 7.340244724624024e-10,
      "loss": 0.0198,
      "step": 12270
    },
    {
      "epoch": 2.992926829268293,
      "grad_norm": 0.2695276737213135,
      "learning_rate": 6.859053104157909e-10,
      "loss": 0.0388,
      "step": 12271
    },
    {
      "epoch": 2.993170731707317,
      "grad_norm": 0.19687892496585846,
      "learning_rate": 6.39417277081833e-10,
      "loss": 0.0111,
      "step": 12272
    },
    {
      "epoch": 2.9934146341463412,
      "grad_norm": 0.08835991472005844,
      "learning_rate": 5.945603754997642e-10,
      "loss": 0.0256,
      "step": 12273
    },
    {
      "epoch": 2.993658536585366,
      "grad_norm": 0.06371189653873444,
      "learning_rate": 5.513346085894711e-10,
      "loss": 0.0125,
      "step": 12274
    },
    {
      "epoch": 2.9939024390243905,
      "grad_norm": 0.15144477784633636,
      "learning_rate": 5.097399791736956e-10,
      "loss": 0.0169,
      "step": 12275
    },
    {
      "epoch": 2.9941463414634146,
      "grad_norm": 0.10236357897520065,
      "learning_rate": 4.697764899669332e-10,
      "loss": 0.0145,
      "step": 12276
    },
    {
      "epoch": 2.994390243902439,
      "grad_norm": 0.3074941039085388,
      "learning_rate": 4.314441435754324e-10,
      "loss": 0.0354,
      "step": 12277
    },
    {
      "epoch": 2.9946341463414634,
      "grad_norm": 0.1228146180510521,
      "learning_rate": 3.9474294250274603e-10,
      "loss": 0.0172,
      "step": 12278
    },
    {
      "epoch": 2.994878048780488,
      "grad_norm": 0.12666606903076172,
      "learning_rate": 3.5967288914140473e-10,
      "loss": 0.017,
      "step": 12279
    },
    {
      "epoch": 2.995121951219512,
      "grad_norm": 0.15804974734783173,
      "learning_rate": 3.26233985778468e-10,
      "loss": 0.0275,
      "step": 12280
    },
    {
      "epoch": 2.9953658536585364,
      "grad_norm": 0.21944504976272583,
      "learning_rate": 2.9442623459829955e-10,
      "loss": 0.0274,
      "step": 12281
    },
    {
      "epoch": 2.995609756097561,
      "grad_norm": 0.11234164237976074,
      "learning_rate": 2.642496376770165e-10,
      "loss": 0.0158,
      "step": 12282
    },
    {
      "epoch": 2.9958536585365856,
      "grad_norm": 0.1683160811662674,
      "learning_rate": 2.357041969797136e-10,
      "loss": 0.0166,
      "step": 12283
    },
    {
      "epoch": 2.9960975609756098,
      "grad_norm": 0.08053617924451828,
      "learning_rate": 2.0878991437434102e-10,
      "loss": 0.0106,
      "step": 12284
    },
    {
      "epoch": 2.996341463414634,
      "grad_norm": 0.40161260962486267,
      "learning_rate": 1.8350679160950013e-10,
      "loss": 0.0232,
      "step": 12285
    },
    {
      "epoch": 2.9965853658536585,
      "grad_norm": 0.09565877169370651,
      "learning_rate": 1.5985483034219873e-10,
      "loss": 0.0149,
      "step": 12286
    },
    {
      "epoch": 2.996829268292683,
      "grad_norm": 0.08595836162567139,
      "learning_rate": 1.378340321100957e-10,
      "loss": 0.0161,
      "step": 12287
    },
    {
      "epoch": 2.9970731707317073,
      "grad_norm": 0.1292954385280609,
      "learning_rate": 1.17444398356481e-10,
      "loss": 0.0156,
      "step": 12288
    },
    {
      "epoch": 2.9973170731707315,
      "grad_norm": 0.1480533927679062,
      "learning_rate": 9.868593040252006e-11,
      "loss": 0.0087,
      "step": 12289
    },
    {
      "epoch": 2.997560975609756,
      "grad_norm": 0.07419956475496292,
      "learning_rate": 8.155862948056036e-11,
      "loss": 0.0119,
      "step": 12290
    },
    {
      "epoch": 2.9978048780487807,
      "grad_norm": 0.08331301063299179,
      "learning_rate": 6.606249670360054e-11,
      "loss": 0.0112,
      "step": 12291
    },
    {
      "epoch": 2.998048780487805,
      "grad_norm": 0.07276083528995514,
      "learning_rate": 5.2197533084719084e-11,
      "loss": 0.0141,
      "step": 12292
    },
    {
      "epoch": 2.998292682926829,
      "grad_norm": 0.1678003966808319,
      "learning_rate": 3.996373952874777e-11,
      "loss": 0.0315,
      "step": 12293
    },
    {
      "epoch": 2.9985365853658537,
      "grad_norm": 0.11557450890541077,
      "learning_rate": 2.9361116829496047e-11,
      "loss": 0.0222,
      "step": 12294
    },
    {
      "epoch": 2.9987804878048783,
      "grad_norm": 0.20938214659690857,
      "learning_rate": 2.038966568362888e-11,
      "loss": 0.0155,
      "step": 12295
    },
    {
      "epoch": 2.9990243902439024,
      "grad_norm": 0.10971727967262268,
      "learning_rate": 1.30493866767889e-11,
      "loss": 0.023,
      "step": 12296
    },
    {
      "epoch": 2.9992682926829266,
      "grad_norm": 0.1002853587269783,
      "learning_rate": 7.340280286372014e-12,
      "loss": 0.0143,
      "step": 12297
    },
    {
      "epoch": 2.999512195121951,
      "grad_norm": 0.09095455706119537,
      "learning_rate": 3.2623468815273783e-12,
      "loss": 0.0177,
      "step": 12298
    },
    {
      "epoch": 2.999756097560976,
      "grad_norm": 0.08840300142765045,
      "learning_rate": 8.155867342596324e-13,
      "loss": 0.008,
      "step": 12299
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.09281301498413086,
      "learning_rate": 0.0,
      "loss": 0.0124,
      "step": 12300
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.43829268292682927,
      "eval_f1": 0.6472577015372045,
      "eval_loss": 0.019352922216057777,
      "eval_precision": 0.6996905985090108,
      "eval_recall": 0.6187687101392685,
      "eval_roc_auc": NaN,
      "eval_runtime": 21.4018,
      "eval_samples_per_second": 191.573,
      "eval_steps_per_second": 23.97,
      "step": 12300
    }
  ],
  "logging_steps": 1,
  "max_steps": 12300,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.59270885122048e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
